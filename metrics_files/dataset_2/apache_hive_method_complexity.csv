Method,ev(G),iv(G),v(G)
"null.accept(Path)",1,2,2
"null.array()",1,1,1
"null.arrayList()",1,1,1
"null.assignDescriptors(FileDescriptor)",1,1,1
"null.checkType(String)",1,1,1
"null.combine(ExprType)",1,1,1
"null.compare(Integer,Integer)",1,1,1
"null.compare(String,String)",1,1,1
"null.compare(byte[],byte[])",1,1,1
"null.create(Token)",1,1,1
"null.defaultValueString(ConfVars)",1,1,1
"null.dupNode(Object)",1,1,1
"null.errorNode(TokenStream,Token,Token,RecognitionException)",1,1,1
"null.findValueByNumber(int)",1,1,1
"null.get(DoubleWritable)",1,1,1
"null.get(HiveDecimalWritable)",1,1,1
"null.get(LongWritable)",1,1,1
"null.getAggregator(Configuration)",1,1,1
"null.getConverter(PrimitiveType,int,HiveGroupConverter)",1,1,1
"null.getPublisher(Configuration)",1,1,1
"null.hashMap(int)",1,1,1
"null.hashMapEntry()",1,1,1
"null.hashSet(int)",1,1,1
"null.inRange(String,Object,Object)",1,1,2
"null.initialValue()",1,1,1
"null.linkedHashMap(int)",1,1,1
"null.linkedList(int)",1,1,1
"null.makeInstance(Schema)",1,1,1
"null.memoryAlign()",1,1,1
"null.newInstance()",1,1,1
"null.object()",1,1,1
"null.parsePartialFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"null.postHook(Set<String>,List<String>)",1,1,1
"null.preHook(Set<String>,List<String>)",1,1,1
"null.ref()",1,1,1
"null.remove()",1,3,3
"null.run()",1,3,3
"null.set(Double,DoubleWritable)",1,1,1
"null.set(HiveDecimal,HiveDecimalWritable)",1,1,1
"null.set(Long,LongWritable)",1,1,1
"org.apache.hadoop.fs.DefaultFileAccess.checkFileAccess(FileSystem,FileStatus,FsAction)",1,1,1
"org.apache.hadoop.fs.DefaultFileAccess.checkFileAccess(FileSystem,FileStatus,FsAction,String,List<String>)",7,4,8
"org.apache.hadoop.fs.DefaultFileAccess.getSuperGroupName(Configuration)",1,1,1
"org.apache.hadoop.fs.DefaultFileAccess.userBelongsToSuperGroup(String,List<String>)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.ProxyFileSystem()",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.ProxyFileSystem(FileSystem)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.ProxyFileSystem(FileSystem,URI)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.append(Path,int,Progressable)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.checkPath(Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.completeLocalOutput(Path,Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.copyFromLocalFile(boolean,Path,Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.copyFromLocalFile(boolean,boolean,Path,Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.copyFromLocalFile(boolean,boolean,Path[],Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.copyToLocalFile(boolean,Path,Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.create(Path,FsPermission,boolean,int,short,long,Progressable)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.delete(Path,boolean)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.deleteOnExit(Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.getContentSummary(Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.getFileBlockLocations(FileStatus,long,long)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.getFileChecksum(Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.getFileStatus(Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.getHomeDirectory()",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.getName()",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.getUri()",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.getWorkingDirectory()",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.initialize(URI,Configuration)",1,1,2
"org.apache.hadoop.fs.ProxyFileSystem.listStatus(Path)",1,2,2
"org.apache.hadoop.fs.ProxyFileSystem.makeQualified(Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.mkdirs(Path,FsPermission)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.open(Path,int)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.rename(Path,Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.resolvePath(Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.setOwner(Path,String,String)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.setPermission(Path,FsPermission)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.setReplication(Path,short)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.setTimes(Path,long,long)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.setWorkingDirectory(Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.startLocalOutput(Path,Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.swizzleFileStatus(FileStatus,boolean)",1,2,2
"org.apache.hadoop.fs.ProxyFileSystem.swizzleParamPath(Path)",1,1,1
"org.apache.hadoop.fs.ProxyFileSystem.swizzleReturnPath(Path)",1,1,1
"org.apache.hadoop.fs.ProxyLocalFileSystem.ProxyLocalFileSystem()",1,1,1
"org.apache.hadoop.fs.ProxyLocalFileSystem.ProxyLocalFileSystem(FileSystem)",1,1,1
"org.apache.hadoop.fs.ProxyLocalFileSystem.initialize(URI,Configuration)",1,3,3
"org.apache.hadoop.hive.accumulo.AccumuloConnectionParameters.AccumuloConnectionParameters(Configuration)",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloConnectionParameters.getAccumuloInstanceName()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloConnectionParameters.getAccumuloPassword()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloConnectionParameters.getAccumuloTableName()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloConnectionParameters.getAccumuloUserName()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloConnectionParameters.getConf()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloConnectionParameters.getConnector()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloConnectionParameters.getConnector(Instance)",3,1,3
"org.apache.hadoop.hive.accumulo.AccumuloConnectionParameters.getInstance()",4,1,4
"org.apache.hadoop.hive.accumulo.AccumuloConnectionParameters.getZooKeepers()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloConnectionParameters.useMockInstance()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.AccumuloHiveRow()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.AccumuloHiveRow(String)",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.ColumnTuple.ColumnTuple(Text,Text,byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.ColumnTuple.equals(Object)",10,4,10
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.ColumnTuple.getCf()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.ColumnTuple.getCq()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.ColumnTuple.getValue()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.ColumnTuple.hashCode()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.ColumnTuple.toString()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.add(String,String,byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.add(Text,Text,byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.clear()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.equals(Object)",5,3,5
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.getRowId()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.getTuples()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.getValue(Text,Text)",3,4,4
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.hasFamAndQual(Text,Text)",3,3,4
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.readFields(DataInput)",1,4,4
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.setRowId(String)",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.toString()",1,2,2
"org.apache.hadoop.hive.accumulo.AccumuloHiveRow.write(DataOutput)",1,3,3
"org.apache.hadoop.hive.accumulo.AccumuloQTestUtil.AccumuloQTestUtil(String,String,MiniClusterType,AccumuloTestSetup,String,String)",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloQTestUtil.init()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.commitCreateTable(Table)",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.commitDropTable(Table,boolean)",2,7,7
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.configureInputJobProperties(TableDesc,Map<String, String>)",3,6,7
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.configureJobConf(TableDesc,JobConf)",1,4,4
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.configureOutputJobProperties(TableDesc,Map<String, String>)",1,4,4
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.configureTableJobProperties(TableDesc,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.decomposePredicate(JobConf,Deserializer,ExprNodeDesc)",3,3,3
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.getAuthorizationProvider()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.getConf()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.getInputFormatClass()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.getMetaHook()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.getOutputFormatClass()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.getSerDeClass()",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.getTableName(Table)",4,2,4
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.getTableName(TableDesc)",3,2,3
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.isExternalTable(Table)",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.preCreateTable(Table)",6,6,9
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.preDropTable(Table)",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.rollbackCreateTable(Table)",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.rollbackDropTable(Table)",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloTestSetup.AccumuloTestSetup(Test)",1,1,1
"org.apache.hadoop.hive.accumulo.AccumuloTestSetup.createAccumuloTable(Connector)",1,3,3
"org.apache.hadoop.hive.accumulo.AccumuloTestSetup.setupWithHiveConf(HiveConf)",1,2,2
"org.apache.hadoop.hive.accumulo.AccumuloTestSetup.tearDown()",1,2,2
"org.apache.hadoop.hive.accumulo.LazyAccumuloMap.LazyAccumuloMap(LazyMapObjectInspector)",1,1,1
"org.apache.hadoop.hive.accumulo.LazyAccumuloMap.getMap()",1,2,2
"org.apache.hadoop.hive.accumulo.LazyAccumuloMap.getMapSize()",1,2,2
"org.apache.hadoop.hive.accumulo.LazyAccumuloMap.getMapValueElement(Object)",4,5,6
"org.apache.hadoop.hive.accumulo.LazyAccumuloMap.init(AccumuloHiveRow,HiveAccumuloMapColumnMapping)",1,1,1
"org.apache.hadoop.hive.accumulo.LazyAccumuloMap.parse()",3,4,5
"org.apache.hadoop.hive.accumulo.LazyAccumuloRow.LazyAccumuloRow(LazySimpleStructObjectInspector)",1,1,1
"org.apache.hadoop.hive.accumulo.LazyAccumuloRow.createLazyField(int,StructField)",3,3,3
"org.apache.hadoop.hive.accumulo.LazyAccumuloRow.getField(int)",1,2,2
"org.apache.hadoop.hive.accumulo.LazyAccumuloRow.getFieldsAsList()",1,3,3
"org.apache.hadoop.hive.accumulo.LazyAccumuloRow.init(AccumuloHiveRow,List<ColumnMapping>,AccumuloRowIdFactory)",1,1,1
"org.apache.hadoop.hive.accumulo.LazyAccumuloRow.parse()",1,3,3
"org.apache.hadoop.hive.accumulo.LazyAccumuloRow.uncheckedGetField(int)",6,6,6
"org.apache.hadoop.hive.accumulo.TestAccumuloConnectionParameters.testInstantiatesWithNullConfiguration()",1,1,2
"org.apache.hadoop.hive.accumulo.TestAccumuloConnectionParameters.testMissingInstanceName()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloConnectionParameters.testMissingPassword()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloConnectionParameters.testMissingUserName()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloConnectionParameters.testMissingZooKeepers()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloHiveRow.testGetValueFromColumn()",1,3,3
"org.apache.hadoop.hive.accumulo.TestAccumuloHiveRow.testHasFamilyAndQualifier()",1,3,3
"org.apache.hadoop.hive.accumulo.TestAccumuloHiveRow.testWritableEmptyRow()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloHiveRow.testWritableWithColumns()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.setup()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.testDropTableWithoutDeleteLeavesTableIntact()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.testEmptyIteratorPushdownValue()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.testExternalNonExistentTableFails()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.testMissingColumnMappingFails()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.testNonBooleanIteratorPushdownValue()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.testNonExternalExistentTable()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.testNonNullLocation()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.testPreCreateTable()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.testRollbackCreateTableDeletesExistentTable()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.testRollbackCreateTableDoesntDeleteExternalExistentTable()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.testRollbackCreateTableOnNonExistentTable()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.testTableJobPropertiesCallsInputAndOutputMethods()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.testTablePropertiesPassedToInputJobProperties()",1,1,1
"org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.testTablePropertiesPassedToOutputJobProperties()",1,1,1
"org.apache.hadoop.hive.accumulo.TestLazyAccumuloMap.testBinaryIntMap()",1,1,1
"org.apache.hadoop.hive.accumulo.TestLazyAccumuloMap.testIntMap()",1,1,1
"org.apache.hadoop.hive.accumulo.TestLazyAccumuloMap.testMixedSerializationMap()",1,1,1
"org.apache.hadoop.hive.accumulo.TestLazyAccumuloMap.testStringMapWithProjection()",1,1,1
"org.apache.hadoop.hive.accumulo.TestLazyAccumuloMap.toBytes(int)",1,1,1
"org.apache.hadoop.hive.accumulo.TestLazyAccumuloRow.testDeserializationOfBinaryEncoding()",1,1,1
"org.apache.hadoop.hive.accumulo.TestLazyAccumuloRow.testExpectedDeserializationOfColumns()",1,1,1
"org.apache.hadoop.hive.accumulo.TestLazyAccumuloRow.testNullInit()",1,1,1
"org.apache.hadoop.hive.accumulo.Utils.addDependencyJars(Configuration,Class<?>...)",6,4,6
"org.apache.hadoop.hive.accumulo.Utils.copyToZipStream(InputStream,ZipEntry,ZipOutputStream)",1,2,2
"org.apache.hadoop.hive.accumulo.Utils.createJar(File,File)",3,3,3
"org.apache.hadoop.hive.accumulo.Utils.findContainingJar(Class<?>,Map<String, String>)",3,4,4
"org.apache.hadoop.hive.accumulo.Utils.findOrCreateJar(Class<?>,FileSystem,Map<String, String>)",2,4,5
"org.apache.hadoop.hive.accumulo.Utils.getJar(Class<?>)",1,3,4
"org.apache.hadoop.hive.accumulo.Utils.jarDir(File,String,ZipOutputStream)",1,2,2
"org.apache.hadoop.hive.accumulo.Utils.jarFinderGetJar(Class)",5,7,8
"org.apache.hadoop.hive.accumulo.Utils.updateMap(String,Map<String, String>)",2,5,6
"org.apache.hadoop.hive.accumulo.Utils.zipDir(File,String,ZipOutputStream,boolean)",1,6,6
"org.apache.hadoop.hive.accumulo.columns.ColumnEncoding.ColumnEncoding(String,String)",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnEncoding.fromCode(String)",2,1,2
"org.apache.hadoop.hive.accumulo.columns.ColumnEncoding.fromName(String)",2,1,2
"org.apache.hadoop.hive.accumulo.columns.ColumnEncoding.get(String)",3,1,3
"org.apache.hadoop.hive.accumulo.columns.ColumnEncoding.getCode()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnEncoding.getColumnEncoding(String)",2,2,3
"org.apache.hadoop.hive.accumulo.columns.ColumnEncoding.getDefault()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnEncoding.getFromMapping(String)",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnEncoding.getMapEncoding(String)",2,1,2
"org.apache.hadoop.hive.accumulo.columns.ColumnEncoding.getName()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnEncoding.hasColumnEncoding(String)",2,2,3
"org.apache.hadoop.hive.accumulo.columns.ColumnEncoding.isMapEncoding(String)",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnEncoding.stripCode(String)",2,3,4
"org.apache.hadoop.hive.accumulo.columns.ColumnMapper.ColumnMapper(String,String,List<String>,List<TypeInfo>)",6,6,8
"org.apache.hadoop.hive.accumulo.columns.ColumnMapper.get(int)",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnMapper.getColumnMappingForHiveColumn(List<String>,String)",3,4,4
"org.apache.hadoop.hive.accumulo.columns.ColumnMapper.getColumnMappings()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnMapper.getRowIdMapping()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnMapper.getRowIdOffset()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnMapper.getTypesString()",5,6,6
"org.apache.hadoop.hive.accumulo.columns.ColumnMapper.hasRowIdMapping()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnMapper.size()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnMapper.toString()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnMapping.ColumnMapping(String,ColumnEncoding,String,String)",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnMapping.ColumnMapping(String,ColumnEncoding,String,TypeInfo)",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnMapping.getColumnName()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnMapping.getColumnType()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnMapping.getEncoding()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnMapping.getMappingSpec()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnMappingFactory.get(String,ColumnEncoding,String,TypeInfo)",7,7,8
"org.apache.hadoop.hive.accumulo.columns.ColumnMappingFactory.getMap(String,ColumnEncoding,ColumnEncoding,String,TypeInfo)",1,1,1
"org.apache.hadoop.hive.accumulo.columns.ColumnMappingFactory.isPrefix(String)",3,3,3
"org.apache.hadoop.hive.accumulo.columns.ColumnMappingFactory.parseMapping(String)",6,7,8
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloColumnMapping.HiveAccumuloColumnMapping(String,String,ColumnEncoding,String,String)",1,1,1
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloColumnMapping.getColumnFamily()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloColumnMapping.getColumnFamilyBytes()",1,2,2
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloColumnMapping.getColumnQualifier()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloColumnMapping.getColumnQualifierBytes()",1,2,2
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloColumnMapping.serialize()",1,2,2
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloColumnMapping.toString()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloMapColumnMapping.HiveAccumuloMapColumnMapping(String,String,ColumnEncoding,ColumnEncoding,String,String)",1,1,3
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloMapColumnMapping.equals(Object)",2,5,5
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloMapColumnMapping.getColumnFamily()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloMapColumnMapping.getColumnQualifierPrefix()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloMapColumnMapping.getKeyEncoding()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloMapColumnMapping.getValueEncoding()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloMapColumnMapping.hashCode()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloMapColumnMapping.toString()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloRowIdColumnMapping.HiveAccumuloRowIdColumnMapping(String,ColumnEncoding,String,String)",1,1,1
"org.apache.hadoop.hive.accumulo.columns.HiveAccumuloRowIdColumnMapping.toString()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.HiveColumn.HiveColumn(String,TypeInfo)",1,1,1
"org.apache.hadoop.hive.accumulo.columns.HiveColumn.getColumnName()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.HiveColumn.getColumnType()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.InvalidColumnMappingException.InvalidColumnMappingException()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.InvalidColumnMappingException.InvalidColumnMappingException(String)",1,1,1
"org.apache.hadoop.hive.accumulo.columns.InvalidColumnMappingException.InvalidColumnMappingException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.accumulo.columns.InvalidColumnMappingException.InvalidColumnMappingException(Throwable)",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testBinaryEncoding()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testColumnEncodingSpecified()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testEscapedPoundIsNoEncodingSpecified()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testEscapedPoundWithRealPound()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testInvalidCodeThrowsException()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testMapEncoding()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testMapEncodingParsing()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testMissingColumnEncoding()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testMissingEncodingOnParse()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testParse()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testParseWithEscapedPound()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testStringEncoding()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testStripCode()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testStripCodeWithEscapedPound()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnEncoding.testStripNonExistentCodeFails()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMapper.testDefaultBinary()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMapper.testGetMappingFromHiveColumn()",1,2,2
"org.apache.hadoop.hive.accumulo.columns.TestColumnMapper.testGetTypesString()",1,2,2
"org.apache.hadoop.hive.accumulo.columns.TestColumnMapper.testMap()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMapper.testMultipleRowIDsFails()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMapper.testNormalMapping()",1,4,4
"org.apache.hadoop.hive.accumulo.columns.TestColumnMappingFactory.testCaseInsensitiveRowId()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMappingFactory.testColumnMappingCreatesAccumuloColumnMapping()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMappingFactory.testColumnMappingRequiresCfAndCq()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMappingFactory.testColumnMappingWithMultipleColons()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMappingFactory.testEscapedAsterisk()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMappingFactory.testEscapedColumnFamily()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMappingFactory.testEscapedColumnFamilyAndQualifier()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMappingFactory.testGetMap()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMappingFactory.testGetMapWithPrefix()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMappingFactory.testInlineEncodingOverridesDefault()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMappingFactory.testNullArgumentsFailFast()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMappingFactory.testPrefixWithEscape()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestColumnMappingFactory.testRowIdCreatesRowIdMapping()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestHiveAccumuloColumnMapping.testColumnMappingWithMultipleColons()",1,1,1
"org.apache.hadoop.hive.accumulo.columns.TestHiveRowIdColumnMapping.testNonRowIdMappingFails()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloRecordReader.HiveAccumuloRecordReader(RecordReader<Text, PeekingIterator<Entry<Key, Value>>>,int)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloRecordReader.close()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloRecordReader.getProgress()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloRecordReader.next(Text,AccumuloHiveRow)",2,5,5
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloRecordReader.pushToValue(List<Key>,List<Value>,AccumuloHiveRow)",1,2,2
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloSplit.HiveAccumuloSplit()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloSplit.HiveAccumuloSplit(RangeInputSplit,Path)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloSplit.getLength()",1,2,2
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloSplit.getLocations()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloSplit.getSplit()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloSplit.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloSplit.toString()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloSplit.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.addIterators(JobConf,List<IteratorSetting>)",1,2,2
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.configure(JobConf,Instance,Connector,AccumuloConnectionParameters,ColumnMapper,List<IteratorSetting>,Collection<Range>)",1,6,6
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.fetchColumns(JobConf,Set<Pair<Text, Text>>)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.getColumnMapper(Configuration)",3,1,3
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.getPairCollection(List<ColumnMapping>)",1,5,5
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,6,7
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.getSplits(JobConf,int)",3,6,9
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.getTableName(RangeInputSplit)",2,7,12
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.setConnectorInfo(JobConf,String,AuthenticationToken)",1,2,2
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.setInputTableName(JobConf,String)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.setMockInstance(JobConf,String)",1,2,2
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.setRanges(JobConf,Collection<Range>)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.setScanAuthorizations(JobConf,Authorizations)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.setTableName(RangeInputSplit,String)",2,7,12
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.setZooKeeperInstance(JobConf,String,String)",1,2,2
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableOutputFormat.checkOutputSpecs(FileSystem,JobConf)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableOutputFormat.configureAccumuloOutputFormat(JobConf)",1,3,3
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableOutputFormat.setAccumuloConnectorInfo(JobConf,String,AuthenticationToken)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableOutputFormat.setAccumuloMockInstance(JobConf,String)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableOutputFormat.setAccumuloZooKeeperInstance(JobConf,String,String)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableOutputFormat.setDefaultAccumuloTableName(JobConf,String)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.createMockKeyValues()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.parseDoubleBytes(String)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.parseIntBytes(String)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.parseLongBytes(String)",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testColumnMappingsToPairs()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testConfigureAccumuloInputFormat()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testConfigureAccumuloInputFormatWithAuthorizations()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testConfigureAccumuloInputFormatWithEmptyColumns()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testConfigureAccumuloInputFormatWithIterators()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testConfigureMockAccumuloInputFormat()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testDegreesAndMillis()",1,7,9
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testGetNone()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testGetOnlyName()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testGetProtectedField()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testGreaterThan1Sid()",1,7,9
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testHiveAccumuloRecord()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testIteratorNotInSplitsCompensation()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testMapColumnPairs()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.testNameEqualBrian()",1,7,7
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableOutputFormat.setup()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableOutputFormat.testBasicConfiguration()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableOutputFormat.testBinarySerializationOnStringFallsBackToUtf8()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableOutputFormat.testMockInstance()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableOutputFormat.testWriteMap()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableOutputFormat.testWriteToMockInstance()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableOutputFormat.testWriteToMockInstanceWithVisibility()",1,1,1
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTypes.testBinaryTypes()",1,2,2
"org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTypes.testUtf8Types()",1,2,2
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.AccumuloPredicateHandler()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.cOpKeyset()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.decompose(Configuration,ExprNodeDesc)",2,3,3
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.generateRanges(ColumnMapper,String,ExprNodeDesc)",1,1,2
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.getCompareOp(String,IndexSearchCondition)",1,2,4
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.getCompareOpClass(String)",2,1,2
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.getExpression(Configuration)",2,1,2
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.getInstance()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.getIterators(Configuration,ColumnMapper)",3,6,9
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.getPrimitiveComparison(String,IndexSearchCondition)",1,2,4
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.getPrimitiveComparisonClass(String)",2,1,2
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.getRanges(Configuration,ColumnMapper)",7,4,7
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.getSearchConditions(Configuration)",3,2,3
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.newAnalyzer(Configuration)",1,3,3
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.pComparisonKeyset()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.toSetting(HiveAccumuloColumnMapping,IndexSearchCondition)",1,1,3
"org.apache.hadoop.hive.accumulo.predicate.AccumuloRangeGenerator.AccumuloRangeGenerator(AccumuloPredicateHandler,HiveAccumuloRowIdColumnMapping,String)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.AccumuloRangeGenerator.getBinaryValue(ConstantObjectInspector)",2,2,2
"org.apache.hadoop.hive.accumulo.predicate.AccumuloRangeGenerator.getColumnOpConstantRange(Class<? extends CompareOp>,Text)",6,5,6
"org.apache.hadoop.hive.accumulo.predicate.AccumuloRangeGenerator.getConstantOpColumnRange(Class<? extends CompareOp>,Text)",6,5,6
"org.apache.hadoop.hive.accumulo.predicate.AccumuloRangeGenerator.getUtf8Value(ConstantObjectInspector)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.AccumuloRangeGenerator.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",5,4,5
"org.apache.hadoop.hive.accumulo.predicate.AccumuloRangeGenerator.processAndOpNode(Node,Object[])",5,11,13
"org.apache.hadoop.hive.accumulo.predicate.AccumuloRangeGenerator.processExpression(ExprNodeGenericFuncDesc,Object[])",6,5,16
"org.apache.hadoop.hive.accumulo.predicate.AccumuloRangeGenerator.processOrOpNode(Node,Object[])",6,6,6
"org.apache.hadoop.hive.accumulo.predicate.NoSuchCompareOpException.NoSuchCompareOpException()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.NoSuchCompareOpException.NoSuchCompareOpException(String)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.NoSuchCompareOpException.NoSuchCompareOpException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.NoSuchPrimitiveComparisonException.NoSuchPrimitiveComparisonException()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.NoSuchPrimitiveComparisonException.NoSuchPrimitiveComparisonException(String)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.NoSuchPrimitiveComparisonException.NoSuchPrimitiveComparisonException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.accept(Collection<Key>,Collection<Value>)",3,3,3
"org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.filter(Text,List<Key>,List<Value>)",1,2,3
"org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.getConstant(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.init(SortedKeyValueIterator<Key, Value>,Map<String, String>,IteratorEnvironment)",1,1,4
"org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.matchQualAndFam(Key)",1,2,2
"org.apache.hadoop.hive.accumulo.predicate.PushdownTuple.PushdownTuple(IndexSearchCondition,PrimitiveComparison,CompareOp)",1,2,3
"org.apache.hadoop.hive.accumulo.predicate.PushdownTuple.getConstVal()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.PushdownTuple.getConstantAsBytes(Writable)",5,5,5
"org.apache.hadoop.hive.accumulo.predicate.PushdownTuple.getcOpt()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.PushdownTuple.getpCompare()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.rangeGreaterThanOrEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.rangeLessThan()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.rangeLessThanOrEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.setup()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testBasicOptLookup()",1,7,8
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testBinaryRangeGeneration()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testCreateIteratorSettings()",1,11,11
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testDisjointRanges()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testEmptyListRangeGeneratorOutput()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testGetRowIDSearchCondition()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testIgnoreIteratorPushdown()",1,2,2
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testIteratorIgnoreRowIDFields()",1,2,2
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testManyRangesGeneratorOutput()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testMultipleRanges()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testNoOptFound()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testNullRangeGeneratorOutput()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testPrimitiveComparsionLookup()",1,5,6
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testPushdownColumnTypeNotSupported()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testPushdownComparisonOptNotSupported()",1,3,3
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testPushdownTuple()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testRangeEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testRangeGreaterThan()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testRowRangeGeneration()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testRowRangeIntersection()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.testSingleRangeGeneratorOutput()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloRangeGenerator.setup()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloRangeGenerator.testCastExpression()",1,1,2
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloRangeGenerator.testDateRangeConjunction()",1,1,2
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloRangeGenerator.testPartialRangeConjunction()",1,1,2
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloRangeGenerator.testRangeConjunction()",1,1,2
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloRangeGenerator.testRangeConjunctionWithDisjunction()",1,1,2
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloRangeGenerator.testRangeDisjunction()",1,1,2
"org.apache.hadoop.hive.accumulo.predicate.TestAccumuloRangeGenerator.testRangeOverNonRowIdField()",1,1,2
"org.apache.hadoop.hive.accumulo.predicate.TestPrimitiveComparisonFilter.testBase64ConstantEncode()",1,2,2
"org.apache.hadoop.hive.accumulo.predicate.TestPrimitiveComparisonFilter.testNumericBase64ConstantEncode()",1,2,2
"org.apache.hadoop.hive.accumulo.predicate.compare.DoubleCompare.greaterThan(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.DoubleCompare.greaterThanOrEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.DoubleCompare.init(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.DoubleCompare.isEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.DoubleCompare.isNotEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.DoubleCompare.lessThan(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.DoubleCompare.lessThanOrEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.DoubleCompare.like(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.DoubleCompare.serialize(byte[])",1,2,2
"org.apache.hadoop.hive.accumulo.predicate.compare.Equal.Equal()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.Equal.Equal(PrimitiveComparison)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.Equal.accept(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.Equal.getPrimitiveCompare()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.Equal.setPrimitiveCompare(PrimitiveComparison)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.GreaterThan.GreaterThan()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.GreaterThan.GreaterThan(PrimitiveComparison)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.GreaterThan.accept(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.GreaterThan.getPrimitiveCompare()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.GreaterThan.setPrimitiveCompare(PrimitiveComparison)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.GreaterThanOrEqual.GreaterThanOrEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.GreaterThanOrEqual.GreaterThanOrEqual(PrimitiveComparison)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.GreaterThanOrEqual.accept(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.GreaterThanOrEqual.getPrimitiveCompare()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.GreaterThanOrEqual.setPrimitiveCompare(PrimitiveComparison)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.IntCompare.greaterThan(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.IntCompare.greaterThanOrEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.IntCompare.init(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.IntCompare.isEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.IntCompare.isNotEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.IntCompare.lessThan(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.IntCompare.lessThanOrEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.IntCompare.like(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.IntCompare.serialize(byte[])",1,2,2
"org.apache.hadoop.hive.accumulo.predicate.compare.LessThan.LessThan()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LessThan.LessThan(PrimitiveComparison)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LessThan.accept(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LessThan.getPrimitiveCompare()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LessThan.setPrimitiveCompare(PrimitiveComparison)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LessThanOrEqual.LessThanOrEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LessThanOrEqual.LessThanOrEqual(PrimitiveComparison)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LessThanOrEqual.accept(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LessThanOrEqual.getPrimitiveCompare()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LessThanOrEqual.setPrimitiveCompare(PrimitiveComparison)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.Like.Like()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.Like.Like(PrimitiveComparison)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.Like.accept(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.Like.getPrimitiveCompare()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.Like.setPrimitiveCompare(PrimitiveComparison)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LongCompare.greaterThan(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LongCompare.greaterThanOrEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LongCompare.init(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LongCompare.isEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LongCompare.isNotEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LongCompare.lessThan(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LongCompare.lessThanOrEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LongCompare.like(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.LongCompare.serialize(byte[])",1,2,2
"org.apache.hadoop.hive.accumulo.predicate.compare.NotEqual.NotEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.NotEqual.NotEqual(PrimitiveComparison)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.NotEqual.accept(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.NotEqual.getPrimitiveCompare()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.NotEqual.setPrimitiveCompare(PrimitiveComparison)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.StringCompare.greaterThan(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.StringCompare.greaterThanOrEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.StringCompare.init(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.StringCompare.isEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.StringCompare.isNotEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.StringCompare.lessThan(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.StringCompare.lessThanOrEqual(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.StringCompare.like(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.StringCompare.serialize(byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestDoubleCompare.equal()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestDoubleCompare.getBytes(double)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestDoubleCompare.greaterThan()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestDoubleCompare.greaterThanOrEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestDoubleCompare.invalidSerialization()",1,2,2
"org.apache.hadoop.hive.accumulo.predicate.compare.TestDoubleCompare.lessThan()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestDoubleCompare.lessThanOrEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestDoubleCompare.like()",1,2,2
"org.apache.hadoop.hive.accumulo.predicate.compare.TestDoubleCompare.notEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestDoubleCompare.setup()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestIntCompare.equal()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestIntCompare.getBytes(int)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestIntCompare.greaterThan()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestIntCompare.greaterThanOrEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestIntCompare.lessThan()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestIntCompare.lessThanOrEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestIntCompare.like()",1,2,2
"org.apache.hadoop.hive.accumulo.predicate.compare.TestIntCompare.notEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestIntCompare.setup()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestLongComparison.equal()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestLongComparison.getBytes(long)",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestLongComparison.greaterThan()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestLongComparison.greaterThanOrEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestLongComparison.invalidSerialization()",1,2,2
"org.apache.hadoop.hive.accumulo.predicate.compare.TestLongComparison.lessThan()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestLongComparison.lessThanOrEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestLongComparison.like()",1,2,2
"org.apache.hadoop.hive.accumulo.predicate.compare.TestLongComparison.notEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestLongComparison.setup()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestStringCompare.equal()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestStringCompare.greaterThan()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestStringCompare.greaterThanOrEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestStringCompare.lessThan()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestStringCompare.lessThanOrEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestStringCompare.like()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestStringCompare.notEqual()",1,1,1
"org.apache.hadoop.hive.accumulo.predicate.compare.TestStringCompare.setup()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloCompositeRowId.AccumuloCompositeRowId(LazySimpleStructObjectInspector)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloCompositeRowId.getFieldsAsList()",1,2,2
"org.apache.hadoop.hive.accumulo.serde.AccumuloCompositeRowId.toLazyObject(int,byte[])",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloRowSerializer.AccumuloRowSerializer(int,SerDeParameters,List<ColumnMapping>,ColumnVisibility,AccumuloRowIdFactory)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloRowSerializer.getSerializedValue(ObjectInspector,Object,Output,ColumnMapping)",1,2,2
"org.apache.hadoop.hive.accumulo.serde.AccumuloRowSerializer.getVisibility()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloRowSerializer.serialize(Object,ObjectInspector)",8,6,8
"org.apache.hadoop.hive.accumulo.serde.AccumuloRowSerializer.serializeColumnMapping(HiveAccumuloColumnMapping,ObjectInspector,Object,Mutation)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloRowSerializer.serializeColumnMapping(HiveAccumuloMapColumnMapping,ObjectInspector,Object,Mutation)",2,3,4
"org.apache.hadoop.hive.accumulo.serde.AccumuloRowSerializer.serializeRowId(Object,StructField,ColumnMapping)",3,3,4
"org.apache.hadoop.hive.accumulo.serde.AccumuloRowSerializer.writeBinary(Output,Object,PrimitiveObjectInspector)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloRowSerializer.writeSerializedPrimitive(PrimitiveObjectInspector,Output,Object,ColumnEncoding)",1,3,3
"org.apache.hadoop.hive.accumulo.serde.AccumuloRowSerializer.writeString(Output,Object,PrimitiveObjectInspector)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloRowSerializer.writeWithLevel(ObjectInspector,Object,Output,ColumnMapping,int)",5,12,16
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe.deserialize(Writable)",2,2,2
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe.getCachedRow()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe.getColumnObjectInspectors(List<TypeInfo>,SerDeParameters,List<ColumnMapping>,AccumuloRowIdFactory)",1,3,3
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe.getIteratorPushdown()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe.getParams()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe.getSerializer()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe.initialize(Configuration,Properties)",1,2,2
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe.serialize(Object,ObjectInspector)",1,1,2
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.AccumuloSerDeParameters(Configuration,Properties,String)",3,3,4
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.createRowIdFactory(Configuration,Properties)",3,3,3
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getAuthorizations()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getAuthorizationsFromConf(Configuration)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getAuthorizationsFromValue(String)",2,1,2
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getColumnMapper()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getColumnMappingForHiveColumn(String)",3,4,4
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getColumnMappingValue()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getColumnMappings()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getColumnTypeValue()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getHiveColumnNames()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getHiveColumnTypes()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getIteratorPushdown()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getRowIdColumnMapping()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getRowIdFactory()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getRowIdHiveColumnName()",3,2,4
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getRowIdOffset()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getSerDeName()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getSerDeParameters()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getTableProperties()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getTableVisibilityLabel()",2,2,3
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.getTypeForHiveColumn(String)",3,4,4
"org.apache.hadoop.hive.accumulo.serde.AccumuloSerDeParameters.initRowIdFactory(Configuration,Properties)",1,2,3
"org.apache.hadoop.hive.accumulo.serde.CompositeAccumuloRowIdFactory.CompositeAccumuloRowIdFactory(Class<T>)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.CompositeAccumuloRowIdFactory.addDependencyJars(Configuration)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.CompositeAccumuloRowIdFactory.createRowId(ObjectInspector)",1,1,2
"org.apache.hadoop.hive.accumulo.serde.DefaultAccumuloRowIdFactory.addDependencyJars(Configuration)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.DefaultAccumuloRowIdFactory.createRowId(ObjectInspector)",1,2,2
"org.apache.hadoop.hive.accumulo.serde.DefaultAccumuloRowIdFactory.createRowIdObjectInspector(TypeInfo)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.DefaultAccumuloRowIdFactory.init(AccumuloSerDeParameters,Properties)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.DefaultAccumuloRowIdFactory.serializeRowId(Object,StructField,Output)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.DelimitedAccumuloRowIdFactory.createRowId(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.DelimitedAccumuloRowIdFactory.createRowIdObjectInspector(TypeInfo)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.DelimitedAccumuloRowIdFactory.init(AccumuloSerDeParameters,Properties)",2,3,4
"org.apache.hadoop.hive.accumulo.serde.DelimitedAccumuloRowIdFactory.serializeRowId(Object,StructField,Output)",2,4,4
"org.apache.hadoop.hive.accumulo.serde.FirstCharAccumuloCompositeRowId.FirstCharAccumuloCompositeRowId(LazySimpleStructObjectInspector,Properties,Configuration)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.FirstCharAccumuloCompositeRowId.getField(int)",1,1,2
"org.apache.hadoop.hive.accumulo.serde.FirstCharAccumuloCompositeRowId.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloRowSerializer.testBinarySerialization()",1,2,2
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloRowSerializer.testBufferResetBeforeUse()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloRowSerializer.testInvalidRowIdOffset()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloRowSerializer.testMapSerialization()",1,2,2
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloRowSerializer.testVisibilityLabel()",1,2,2
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.deserialization()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.deserializeWithTooFewHiveColumns()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.emptyConfiguration()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.invalidColMapping()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.moreAccumuloColumnsThanHiveColumns()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.moreHiveColumnsThanAccumuloColumns()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.setup()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.simpleColumnMapping()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.testArraySerialization()",1,2,2
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.testColumnVisibilityForSerializer()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.testCompositeKeyDeserialization()",1,2,2
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.testMapSerialization()",1,2,2
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.testNoVisibilitySetsEmptyVisibility()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.testStructOfMapSerialization()",1,3,3
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.withRowID()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDeParameters.testNullAuthsFromConf()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDeParameters.testNullAuthsFromProperties()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDeParameters.testParseAuthorizationsFromConf()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDeParameters.testParseAuthorizationsFromnProperties()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDeParameters.testParseColumnVisibility()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestDefaultAccumuloRowIdFactory.testBinaryStringRowId()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestDefaultAccumuloRowIdFactory.testCorrectComplexInspectors()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TestDefaultAccumuloRowIdFactory.testCorrectPrimitiveInspectors()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TooManyAccumuloColumnsException.TooManyAccumuloColumnsException()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TooManyAccumuloColumnsException.TooManyAccumuloColumnsException(String)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TooManyAccumuloColumnsException.TooManyAccumuloColumnsException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TooManyAccumuloColumnsException.TooManyAccumuloColumnsException(Throwable)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TooManyHiveColumnsException.TooManyHiveColumnsException()",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TooManyHiveColumnsException.TooManyHiveColumnsException(String)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TooManyHiveColumnsException.TooManyHiveColumnsException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.accumulo.serde.TooManyHiveColumnsException.TooManyHiveColumnsException(Throwable)",1,1,1
"org.apache.hadoop.hive.ant.DistinctElementsClassPath.DistinctElementsClassPath(Project)",1,1,1
"org.apache.hadoop.hive.ant.DistinctElementsClassPath.DistinctElementsClassPath(Project,String)",1,1,1
"org.apache.hadoop.hive.ant.DistinctElementsClassPath.list()",1,7,7
"org.apache.hadoop.hive.ant.DistinctElementsClassPath.toString()",1,1,1
"org.apache.hadoop.hive.ant.GenHiveTemplate.appendElement(Element,String,String)",1,2,2
"org.apache.hadoop.hive.ant.GenHiveTemplate.execute()",1,1,2
"org.apache.hadoop.hive.ant.GenHiveTemplate.generate()",4,4,5
"org.apache.hadoop.hive.ant.GenHiveTemplate.generateTemplate()",3,2,3
"org.apache.hadoop.hive.ant.GenHiveTemplate.getTemplateFile()",1,1,1
"org.apache.hadoop.hive.ant.GenHiveTemplate.main(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenHiveTemplate.normalize(String)",2,3,4
"org.apache.hadoop.hive.ant.GenHiveTemplate.setTemplateFile(String)",1,1,1
"org.apache.hadoop.hive.ant.GenHiveTemplate.writeToFile(File,Document)",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.execute()",1,1,2
"org.apache.hadoop.hive.ant.GenVectorCode.generate()",43,46,46
"org.apache.hadoop.hive.ant.GenVectorCode.generateColumnArithmeticColumn(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateColumnArithmeticColumnDecimal(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateColumnArithmeticScalar(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateColumnArithmeticScalarDecimal(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateColumnBinaryOperatorColumn(String[],String,String)",1,2,3
"org.apache.hadoop.hive.ant.GenVectorCode.generateColumnBinaryOperatorScalar(String[],String,String)",1,2,3
"org.apache.hadoop.hive.ant.GenVectorCode.generateColumnCompareColumn(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateColumnCompareScalar(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateColumnDivideColumnDecimal(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateColumnDivideScalarDecimal(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateColumnUnaryFunc(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateColumnUnaryMinus(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateDecimalColumnCompare(String[],String)",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateDecimalColumnUnaryFunc(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateFilterColumnBetween(String[])",1,1,2
"org.apache.hadoop.hive.ant.GenVectorCode.generateFilterColumnCompareColumn(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateFilterColumnCompareScalar(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateFilterDecimalColumnBetween(String[])",1,1,2
"org.apache.hadoop.hive.ant.GenVectorCode.generateFilterDecimalColumnCompareColumn(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateFilterDecimalColumnCompareScalar(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateFilterDecimalScalarCompareColumn(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateFilterScalarCompareColumn(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateFilterStringColumnBetween(String[])",1,1,2
"org.apache.hadoop.hive.ant.GenVectorCode.generateFilterStringColumnCompareColumn(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateFilterStringColumnCompareScalar(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateFilterStringScalarCompareColumn(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateIfExprColumnColumn(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateIfExprColumnScalar(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateIfExprScalarColumn(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateIfExprScalarScalar(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateScalarArithmeticColumn(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateScalarArithmeticColumnDecimal(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateScalarBinaryOperatorColumn(String[],String,String)",1,2,3
"org.apache.hadoop.hive.ant.GenVectorCode.generateScalarCompareColumn(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateScalarDivideColumnDecimal(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateStringColumnCompareColumn(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateStringColumnCompareScalar(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateStringColumnCompareScalar(String[],String)",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateStringScalarCompareColumn(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateVectorUDAFAvg(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateVectorUDAFMinMax(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateVectorUDAFMinMaxDecimal(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateVectorUDAFMinMaxString(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateVectorUDAFSum(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateVectorUDAFVar(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.generateVectorUDAFVarDecimal(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.getArithmeticReturnType(String,String)",2,2,3
"org.apache.hadoop.hive.ant.GenVectorCode.getCamelCaseType(String)",5,3,5
"org.apache.hadoop.hive.ant.GenVectorCode.getColumnVectorType(String)",5,4,5
"org.apache.hadoop.hive.ant.GenVectorCode.getInitialCapWord(String)",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.getOutputObjectInspector(String)",4,3,4
"org.apache.hadoop.hive.ant.GenVectorCode.getOutputWritableType(String)",4,3,4
"org.apache.hadoop.hive.ant.GenVectorCode.init(String,String)",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.joinPath(String...)",1,1,2
"org.apache.hadoop.hive.ant.GenVectorCode.main(String[])",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.readFile(File)",1,2,2
"org.apache.hadoop.hive.ant.GenVectorCode.readFile(String)",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.setBuildDir(String)",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.setTemplateBaseDir(String)",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.writeFile(File,String)",1,1,1
"org.apache.hadoop.hive.ant.GenVectorCode.writeFile(long,String,String,String,String)",2,3,4
"org.apache.hadoop.hive.ant.GenVectorTestCode.GenVectorTestCode(String,String)",1,2,2
"org.apache.hadoop.hive.ant.GenVectorTestCode.addColumnColumnFilterTestCases(String,String,String,String)",1,2,2
"org.apache.hadoop.hive.ant.GenVectorTestCode.addColumnColumnOperationTestCases(String,String,String,String)",1,2,2
"org.apache.hadoop.hive.ant.GenVectorTestCode.addColumnScalarFilterTestCases(boolean,String,String,String,String)",1,3,3
"org.apache.hadoop.hive.ant.GenVectorTestCode.addColumnScalarOperationTestCases(boolean,String,String,String,String)",1,3,3
"org.apache.hadoop.hive.ant.GenVectorTestCode.createNullRepeatingNameFragment(String,boolean,boolean)",2,1,5
"org.apache.hadoop.hive.ant.GenVectorTestCode.generateTestSuites()",1,2,2
"org.apache.hadoop.hive.ant.GenVectorTestCode.removeTemplateComments(String)",1,1,1
"org.apache.hadoop.hive.ant.GetVersionPref.execute()",3,3,5
"org.apache.hadoop.hive.ant.GetVersionPref.getInput()",1,1,1
"org.apache.hadoop.hive.ant.GetVersionPref.getProperty()",1,1,1
"org.apache.hadoop.hive.ant.GetVersionPref.setInput(String)",1,1,1
"org.apache.hadoop.hive.ant.GetVersionPref.setProperty(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.DisabledQFileFilter.DisabledQFileFilter(Set<String>)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.DisabledQFileFilter.accept(File)",2,2,3
"org.apache.hadoop.hive.ant.QTestGenTask.IncludeFilter.IncludeFilter(Set<String>)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.IncludeFilter.accept(File)",1,2,2
"org.apache.hadoop.hive.ant.QTestGenTask.QFileFilter.QFileFilter(Set<String>)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.QFileFilter.accept(File)",3,2,4
"org.apache.hadoop.hive.ant.QTestGenTask.QFileRegexFilter.QFileRegexFilter(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.QFileRegexFilter.accept(File)",2,2,3
"org.apache.hadoop.hive.ant.QTestGenTask.escapePath(String)",2,2,2
"org.apache.hadoop.hive.ant.QTestGenTask.execute()",14,29,42
"org.apache.hadoop.hive.ant.QTestGenTask.getClassName()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getCleanupScript()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getClusterMode()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getExcludeQueryFile()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getHadoopVersion()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getHiveConfDir()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getHiveRootDirectory()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getIncludeQueryFile()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getInitScript()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getLogDirectory()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getLogFile()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getOutputDirectory()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getQueryDirectory()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getQueryFile()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getQueryFileRegex()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getResultsDirectory()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getRunDisabled()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getTemplate()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.getTemplatePath()",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.relativePath(File,File)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setClassName(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setCleanupScript(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setClusterMode(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setExcludeQueryFile(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setHadoopVersion(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setHiveConfDir(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setHiveRootDirectory(File)",1,1,2
"org.apache.hadoop.hive.ant.QTestGenTask.setIncludeQueryFile(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setInitScript(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setLogDirectory(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setLogFile(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setOutputDirectory(File)",1,1,2
"org.apache.hadoop.hive.ant.QTestGenTask.setQueryDirectory(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setQueryFile(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setQueryFileRegex(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setResultsDirectory(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setRunDisabled(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setTemplate(String)",1,1,1
"org.apache.hadoop.hive.ant.QTestGenTask.setTemplatePath(String)",1,2,2
"org.apache.hadoop.hive.cli.CliDriver.CliDriver()",1,2,2
"org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliSessionState,HiveConf,OptionsProcessor)",5,15,16
"org.apache.hadoop.hive.cli.CliDriver.getCommandCompletor()",1,13,14
"org.apache.hadoop.hive.cli.CliDriver.getConsoleReader()",1,1,1
"org.apache.hadoop.hive.cli.CliDriver.getFirstCmd(String,int)",1,1,1
"org.apache.hadoop.hive.cli.CliDriver.getFormattedDb(HiveConf,CliSessionState)",3,1,3
"org.apache.hadoop.hive.cli.CliDriver.main(String[])",1,1,1
"org.apache.hadoop.hive.cli.CliDriver.printHeader(Driver,PrintStream)",1,5,5
"org.apache.hadoop.hive.cli.CliDriver.processCmd(String)",1,20,20
"org.apache.hadoop.hive.cli.CliDriver.processFile(String)",1,2,2
"org.apache.hadoop.hive.cli.CliDriver.processInitFiles(CliSessionState)",1,13,13
"org.apache.hadoop.hive.cli.CliDriver.processLine(String)",1,1,1
"org.apache.hadoop.hive.cli.CliDriver.processLine(String,boolean)",5,7,10
"org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(String,CommandProcessor,CliSessionState)",7,14,17
"org.apache.hadoop.hive.cli.CliDriver.processReader(BufferedReader)",1,3,3
"org.apache.hadoop.hive.cli.CliDriver.processSelectDatabase(CliSessionState)",1,3,3
"org.apache.hadoop.hive.cli.CliDriver.run(String[])",3,6,9
"org.apache.hadoop.hive.cli.CliDriver.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.cli.CliDriver.setHiveVariables(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.cli.CliDriver.spacesForString(String)",2,2,3
"org.apache.hadoop.hive.cli.CliDriver.tokenizeCmd(String)",1,1,1
"org.apache.hadoop.hive.cli.CliSessionState.CliSessionState(HiveConf)",1,1,1
"org.apache.hadoop.hive.cli.CliSessionState.close()",1,4,4
"org.apache.hadoop.hive.cli.CliSessionState.connect()",1,1,1
"org.apache.hadoop.hive.cli.CliSessionState.getClient()",1,1,1
"org.apache.hadoop.hive.cli.CliSessionState.getHost()",1,1,1
"org.apache.hadoop.hive.cli.CliSessionState.getPort()",1,1,1
"org.apache.hadoop.hive.cli.CliSessionState.isRemoteMode()",1,1,1
"org.apache.hadoop.hive.cli.CliSessionState.setHost(String)",1,1,1
"org.apache.hadoop.hive.cli.OptionsProcessor.OptionsProcessor()",1,1,1
"org.apache.hadoop.hive.cli.OptionsProcessor.getHiveVariables()",1,1,1
"org.apache.hadoop.hive.cli.OptionsProcessor.printUsage()",1,1,1
"org.apache.hadoop.hive.cli.OptionsProcessor.process_stage1(String[])",1,5,5
"org.apache.hadoop.hive.cli.OptionsProcessor.process_stage2(CliSessionState)",3,6,7
"org.apache.hadoop.hive.cli.RCFileCat.RCFileCat()",1,1,1
"org.apache.hadoop.hive.cli.RCFileCat.getConf()",1,1,1
"org.apache.hadoop.hive.cli.RCFileCat.main(String[])",1,2,2
"org.apache.hadoop.hive.cli.RCFileCat.printRecord(BytesRefArrayWritable,StringBuilder)",1,3,3
"org.apache.hadoop.hive.cli.RCFileCat.printUsage(String)",1,2,2
"org.apache.hadoop.hive.cli.RCFileCat.run(String[])",12,22,34
"org.apache.hadoop.hive.cli.RCFileCat.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.cli.RCFileCat.setupBufferedOutput()",1,1,2
"org.apache.hadoop.hive.cli.TestCliDriverMethods.ExitException.ExitException(int)",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.ExitException.getStatus()",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.FakeCliDriver.getConsoleReader()",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.FakeConsoleReader.FakeConsoleReader()",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.FakeConsoleReader.readLine(String)",6,3,9
"org.apache.hadoop.hive.cli.TestCliDriverMethods.MyCliSessionState.MyCliSessionState(HiveConf,ClientResult)",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.MyCliSessionState.getClient()",4,5,8
"org.apache.hadoop.hive.cli.TestCliDriverMethods.MyCliSessionState.isRemoteMode()",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.NoExitSecurityManager.NoExitSecurityManager(SecurityManager)",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.NoExitSecurityManager.checkExit(int)",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.NoExitSecurityManager.checkPermission(Permission)",1,2,2
"org.apache.hadoop.hive.cli.TestCliDriverMethods.NoExitSecurityManager.checkPermission(Permission,Object)",1,2,2
"org.apache.hadoop.hive.cli.TestCliDriverMethods.headerPrintingTestDriver(Schema)",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.setEnv(String,String)",1,4,4
"org.apache.hadoop.hive.cli.TestCliDriverMethods.setUp()",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.tearDown()",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.testGetCommandCompletor()",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.testProcessSelectDatabase()",1,2,2
"org.apache.hadoop.hive.cli.TestCliDriverMethods.testQuit()",1,3,4
"org.apache.hadoop.hive.cli.TestCliDriverMethods.testRemoteCall()",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.testRun()",1,2,2
"org.apache.hadoop.hive.cli.TestCliDriverMethods.testServerException()",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.testServerTException()",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.testThatCliDriverPrintsHeaderForCommandsWithSchema()",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.testThatCliDriverPrintsNoHeaderForCommandsWithNoSchema()",1,1,1
"org.apache.hadoop.hive.cli.TestCliDriverMethods.testprocessInitFiles()",1,4,4
"org.apache.hadoop.hive.cli.TestCliSessionState.TCPServer.getPort()",1,1,1
"org.apache.hadoop.hive.cli.TestCliSessionState.TCPServer.run()",1,2,3
"org.apache.hadoop.hive.cli.TestCliSessionState.TCPServer.stop()",1,1,1
"org.apache.hadoop.hive.cli.TestCliSessionState.start()",1,2,2
"org.apache.hadoop.hive.cli.TestCliSessionState.stop()",1,1,1
"org.apache.hadoop.hive.cli.TestCliSessionState.testConnect()",1,1,1
"org.apache.hadoop.hive.cli.TestCliSessionState.testgetDbName()",1,1,1
"org.apache.hadoop.hive.cli.TestOptionsProcessor.testFiles()",1,1,1
"org.apache.hadoop.hive.cli.TestOptionsProcessor.testOptionsProcessor()",1,1,1
"org.apache.hadoop.hive.cli.TestRCFileCat.testRCFileCat()",1,1,1
"org.apache.hadoop.hive.cli.TestRCFileCat.write(Writer,byte[][])",1,2,2
"org.apache.hadoop.hive.common.CompressionUtils.tar(String,String[],String)",1,2,2
"org.apache.hadoop.hive.common.FileUtils.AcceptAllPathFilter.accept(Path)",1,1,1
"org.apache.hadoop.hive.common.FileUtils.FileUtils()",1,1,1
"org.apache.hadoop.hive.common.FileUtils.checkDeletePermission(Path,Configuration,String)",5,1,5
"org.apache.hadoop.hive.common.FileUtils.checkFileAccessWithImpersonation(FileSystem,FileStatus,FsAction,String)",2,3,3
"org.apache.hadoop.hive.common.FileUtils.copy(FileSystem,Path,FileSystem,Path,boolean,boolean,HiveConf)",1,3,4
"org.apache.hadoop.hive.common.FileUtils.equalsFileSystem(FileSystem,FileSystem)",1,1,1
"org.apache.hadoop.hive.common.FileUtils.escapePathName(String)",1,1,1
"org.apache.hadoop.hive.common.FileUtils.escapePathName(String,String)",3,4,6
"org.apache.hadoop.hive.common.FileUtils.getPathOrParentThatExists(FileSystem,Path)",2,2,2
"org.apache.hadoop.hive.common.FileUtils.isActionPermittedForFileHierarchy(FileSystem,FileStatus,String,FsAction)",4,3,6
"org.apache.hadoop.hive.common.FileUtils.isLocalFile(HiveConf,String)",1,3,3
"org.apache.hadoop.hive.common.FileUtils.isOwnerOfFileHierarchy(FileSystem,FileStatus,String)",5,2,5
"org.apache.hadoop.hive.common.FileUtils.listStatusRecursively(FileSystem,FileStatus,List<FileStatus>)",1,4,4
"org.apache.hadoop.hive.common.FileUtils.makeDefaultListBucketingDirName(List<String>,String)",1,3,3
"org.apache.hadoop.hive.common.FileUtils.makeListBucketingDirName(List<String>,List<String>)",1,3,3
"org.apache.hadoop.hive.common.FileUtils.makePartName(List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.common.FileUtils.makePartName(List<String>,List<String>,String)",1,3,3
"org.apache.hadoop.hive.common.FileUtils.makeQualified(Path,Configuration)",2,6,7
"org.apache.hadoop.hive.common.FileUtils.mkdir(FileSystem,Path,boolean,Configuration)",3,5,6
"org.apache.hadoop.hive.common.FileUtils.moveToTrash(FileSystem,Path,Configuration)",2,3,3
"org.apache.hadoop.hive.common.FileUtils.needsEscaping(char)",1,3,3
"org.apache.hadoop.hive.common.FileUtils.renameWithPerms(FileSystem,Path,Path,boolean,Configuration)",3,4,4
"org.apache.hadoop.hive.common.FileUtils.trashFilesUnderDir(FileSystem,Path,Configuration)",1,2,2
"org.apache.hadoop.hive.common.FileUtils.unescapePathName(String)",4,5,6
"org.apache.hadoop.hive.common.HiveInterruptUtils.add(HiveInterruptCallback)",1,1,1
"org.apache.hadoop.hive.common.HiveInterruptUtils.checkInterrupted()",2,2,3
"org.apache.hadoop.hive.common.HiveInterruptUtils.interrupt()",1,2,2
"org.apache.hadoop.hive.common.HiveInterruptUtils.remove(HiveInterruptCallback)",1,1,1
"org.apache.hadoop.hive.common.HiveStatsUtils.getFileStatusRecurse(Path,int,FileSystem)",2,3,4
"org.apache.hadoop.hive.common.JavaUtils.JavaUtils()",1,1,1
"org.apache.hadoop.hive.common.JavaUtils.closeClassLoader(ClassLoader)",4,5,7
"org.apache.hadoop.hive.common.JavaUtils.closeClassLoadersTo(ClassLoader,ClassLoader)",2,3,5
"org.apache.hadoop.hive.common.JavaUtils.getClassLoader()",1,2,2
"org.apache.hadoop.hive.common.JavaUtils.isValidHierarchy(ClassLoader,ClassLoader)",2,1,6
"org.apache.hadoop.hive.common.LogUtils.LogInitializationException.LogInitializationException(String)",1,1,1
"org.apache.hadoop.hive.common.LogUtils.initHiveExecLog4j()",1,1,1
"org.apache.hadoop.hive.common.LogUtils.initHiveLog4j()",1,1,1
"org.apache.hadoop.hive.common.LogUtils.initHiveLog4jCommon(ConfVars)",3,6,6
"org.apache.hadoop.hive.common.LogUtils.initHiveLog4jDefault(HiveConf,String,ConfVars)",3,4,6
"org.apache.hadoop.hive.common.LogUtils.logConfigLocation(HiveConf)",1,3,3
"org.apache.hadoop.hive.common.ObjectPair.ObjectPair()",1,1,1
"org.apache.hadoop.hive.common.ObjectPair.ObjectPair(F,S)",1,1,1
"org.apache.hadoop.hive.common.ObjectPair.create(T1,T2)",1,1,1
"org.apache.hadoop.hive.common.ObjectPair.equals(Object)",3,2,3
"org.apache.hadoop.hive.common.ObjectPair.equals(ObjectPair<F, S>)",2,2,3
"org.apache.hadoop.hive.common.ObjectPair.getFirst()",1,1,1
"org.apache.hadoop.hive.common.ObjectPair.getSecond()",1,1,1
"org.apache.hadoop.hive.common.ObjectPair.setFirst(F)",1,1,1
"org.apache.hadoop.hive.common.ObjectPair.setSecond(S)",1,1,1
"org.apache.hadoop.hive.common.ObjectPair.toString()",1,1,1
"org.apache.hadoop.hive.common.ServerUtils.cleanUpScratchDir(HiveConf)",1,3,3
"org.apache.hadoop.hive.common.StatsSetupConst.areStatsUptoDate(Map<String, String>)",1,2,2
"org.apache.hadoop.hive.common.ValidTxnListImpl.ValidTxnListImpl()",1,1,1
"org.apache.hadoop.hive.common.ValidTxnListImpl.ValidTxnListImpl(String)",1,1,1
"org.apache.hadoop.hive.common.ValidTxnListImpl.ValidTxnListImpl(long[],long)",1,2,2
"org.apache.hadoop.hive.common.ValidTxnListImpl.getHighWatermark()",1,1,1
"org.apache.hadoop.hive.common.ValidTxnListImpl.getOpenTransactions()",1,1,1
"org.apache.hadoop.hive.common.ValidTxnListImpl.isTxnCommitted(long)",2,1,2
"org.apache.hadoop.hive.common.ValidTxnListImpl.isTxnRangeCommitted(long,long)",5,1,9
"org.apache.hadoop.hive.common.ValidTxnListImpl.readFromString(String)",1,3,3
"org.apache.hadoop.hive.common.ValidTxnListImpl.toString()",1,1,1
"org.apache.hadoop.hive.common.ValidTxnListImpl.writeToString()",1,3,3
"org.apache.hadoop.hive.common.classification.InterfaceAudience.InterfaceAudience()",1,1,1
"org.apache.hadoop.hive.common.cli.CommonCliOptions.CommonCliOptions(String,boolean)",1,2,2
"org.apache.hadoop.hive.common.cli.CommonCliOptions.addHiveconfToSystemProperties()",1,3,3
"org.apache.hadoop.hive.common.cli.CommonCliOptions.isVerbose()",1,1,1
"org.apache.hadoop.hive.common.cli.CommonCliOptions.parse(String[])",1,3,4
"org.apache.hadoop.hive.common.cli.CommonCliOptions.printUsage()",1,1,1
"org.apache.hadoop.hive.common.cli.HiveFileProcessor.processFile(String)",1,1,1
"org.apache.hadoop.hive.common.cli.HiveFileProcessor.processLine(String)",4,3,5
"org.apache.hadoop.hive.common.cli.HiveFileProcessor.processReader(BufferedReader)",1,3,3
"org.apache.hadoop.hive.common.cli.ShellCmdExecutor.ShellCmdExecutor(String,PrintStream,PrintStream)",1,1,1
"org.apache.hadoop.hive.common.cli.ShellCmdExecutor.execute()",1,1,2
"org.apache.hadoop.hive.common.io.CachingPrintStream.CachingPrintStream(OutputStream)",1,1,1
"org.apache.hadoop.hive.common.io.CachingPrintStream.CachingPrintStream(OutputStream,boolean,String)",1,1,1
"org.apache.hadoop.hive.common.io.CachingPrintStream.flush()",1,1,1
"org.apache.hadoop.hive.common.io.CachingPrintStream.getOutput()",1,1,1
"org.apache.hadoop.hive.common.io.CachingPrintStream.println(String)",1,1,1
"org.apache.hadoop.hive.common.io.DigestPrintStream.DigestPrintStream(OutputStream,String)",1,1,1
"org.apache.hadoop.hive.common.io.DigestPrintStream.process(String)",1,1,1
"org.apache.hadoop.hive.common.io.DigestPrintStream.processFinal()",1,1,1
"org.apache.hadoop.hive.common.io.FetchConverter.FetchConverter(OutputStream,boolean,String)",1,1,1
"org.apache.hadoop.hive.common.io.FetchConverter.byPass()",1,1,2
"org.apache.hadoop.hive.common.io.FetchConverter.fetchFinished()",1,2,2
"org.apache.hadoop.hive.common.io.FetchConverter.fetchStarted()",1,1,1
"org.apache.hadoop.hive.common.io.FetchConverter.flush()",1,2,2
"org.apache.hadoop.hive.common.io.FetchConverter.foundQuery(boolean)",1,1,1
"org.apache.hadoop.hive.common.io.FetchConverter.printDirect(String)",1,1,1
"org.apache.hadoop.hive.common.io.FetchConverter.println(String)",1,2,2
"org.apache.hadoop.hive.common.io.NonSyncByteArrayInputStream.NonSyncByteArrayInputStream()",1,1,1
"org.apache.hadoop.hive.common.io.NonSyncByteArrayInputStream.NonSyncByteArrayInputStream(byte[])",1,1,1
"org.apache.hadoop.hive.common.io.NonSyncByteArrayInputStream.NonSyncByteArrayInputStream(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.common.io.NonSyncByteArrayInputStream.available()",1,1,1
"org.apache.hadoop.hive.common.io.NonSyncByteArrayInputStream.getLength()",1,1,1
"org.apache.hadoop.hive.common.io.NonSyncByteArrayInputStream.getPosition()",1,1,1
"org.apache.hadoop.hive.common.io.NonSyncByteArrayInputStream.read()",1,1,2
"org.apache.hadoop.hive.common.io.NonSyncByteArrayInputStream.read(byte[],int,int)",5,1,8
"org.apache.hadoop.hive.common.io.NonSyncByteArrayInputStream.reset(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.common.io.NonSyncByteArrayInputStream.skip(long)",2,1,3
"org.apache.hadoop.hive.common.io.NonSyncByteArrayOutputStream.NonSyncByteArrayOutputStream()",1,1,1
"org.apache.hadoop.hive.common.io.NonSyncByteArrayOutputStream.NonSyncByteArrayOutputStream(int)",1,1,1
"org.apache.hadoop.hive.common.io.NonSyncByteArrayOutputStream.enLargeBuffer(int)",1,2,3
"org.apache.hadoop.hive.common.io.NonSyncByteArrayOutputStream.getData()",1,1,1
"org.apache.hadoop.hive.common.io.NonSyncByteArrayOutputStream.getLength()",1,1,1
"org.apache.hadoop.hive.common.io.NonSyncByteArrayOutputStream.reset()",1,1,1
"org.apache.hadoop.hive.common.io.NonSyncByteArrayOutputStream.write(DataInput,int)",1,1,1
"org.apache.hadoop.hive.common.io.NonSyncByteArrayOutputStream.write(byte[],int,int)",3,1,7
"org.apache.hadoop.hive.common.io.NonSyncByteArrayOutputStream.write(int)",1,1,1
"org.apache.hadoop.hive.common.io.NonSyncByteArrayOutputStream.writeTo(OutputStream)",1,1,1
"org.apache.hadoop.hive.common.io.SortAndDigestPrintStream.SortAndDigestPrintStream(OutputStream,String)",1,1,1
"org.apache.hadoop.hive.common.io.SortAndDigestPrintStream.processFinal()",1,2,2
"org.apache.hadoop.hive.common.io.SortPrintStream.SortPrintStream(OutputStream,String)",1,1,1
"org.apache.hadoop.hive.common.io.SortPrintStream.process(String)",1,1,1
"org.apache.hadoop.hive.common.io.SortPrintStream.processFinal()",1,2,2
"org.apache.hadoop.hive.common.metrics.Metrics.Metrics()",1,1,1
"org.apache.hadoop.hive.common.metrics.Metrics.MetricsScope.MetricsScope(String)",1,1,1
"org.apache.hadoop.hive.common.metrics.Metrics.MetricsScope.close()",2,3,4
"org.apache.hadoop.hive.common.metrics.Metrics.MetricsScope.getNumCounter()",1,1,1
"org.apache.hadoop.hive.common.metrics.Metrics.MetricsScope.getTimeCounter()",1,1,1
"org.apache.hadoop.hive.common.metrics.Metrics.MetricsScope.open()",2,2,2
"org.apache.hadoop.hive.common.metrics.Metrics.MetricsScope.reopen()",1,2,2
"org.apache.hadoop.hive.common.metrics.Metrics.endScope(String)",2,2,3
"org.apache.hadoop.hive.common.metrics.Metrics.get(String)",2,1,2
"org.apache.hadoop.hive.common.metrics.Metrics.getScope(String)",3,2,3
"org.apache.hadoop.hive.common.metrics.Metrics.incrementCounter(String)",2,1,2
"org.apache.hadoop.hive.common.metrics.Metrics.incrementCounter(String,long)",2,2,3
"org.apache.hadoop.hive.common.metrics.Metrics.init()",1,2,2
"org.apache.hadoop.hive.common.metrics.Metrics.set(String,Object)",2,1,2
"org.apache.hadoop.hive.common.metrics.Metrics.startScope(String)",2,2,3
"org.apache.hadoop.hive.common.metrics.Metrics.uninit()",1,3,3
"org.apache.hadoop.hive.common.metrics.MetricsMBeanImpl.clear()",1,1,1
"org.apache.hadoop.hive.common.metrics.MetricsMBeanImpl.get(String)",1,1,4
"org.apache.hadoop.hive.common.metrics.MetricsMBeanImpl.getAttribute(String)",2,2,2
"org.apache.hadoop.hive.common.metrics.MetricsMBeanImpl.getAttributes(String[])",1,2,2
"org.apache.hadoop.hive.common.metrics.MetricsMBeanImpl.getMBeanInfo()",1,3,3
"org.apache.hadoop.hive.common.metrics.MetricsMBeanImpl.hasKey(String)",1,1,1
"org.apache.hadoop.hive.common.metrics.MetricsMBeanImpl.invoke(String,Object[],String[])",2,2,2
"org.apache.hadoop.hive.common.metrics.MetricsMBeanImpl.put(String,Object)",1,1,2
"org.apache.hadoop.hive.common.metrics.MetricsMBeanImpl.reset()",1,2,2
"org.apache.hadoop.hive.common.metrics.MetricsMBeanImpl.setAttribute(Attribute)",1,1,2
"org.apache.hadoop.hive.common.metrics.MetricsMBeanImpl.setAttributes(AttributeList)",1,2,6
"org.apache.hadoop.hive.common.metrics.TestMetrics.after()",1,1,1
"org.apache.hadoop.hive.common.metrics.TestMetrics.before()",1,1,1
"org.apache.hadoop.hive.common.metrics.TestMetrics.expectIOE(Callable<T>)",1,1,2
"org.apache.hadoop.hive.common.metrics.TestMetrics.testMetricsMBean()",5,4,5
"org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency()",1,2,2
"org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeImpl(int)",1,1,1
"org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeSingleThread()",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.Decimal128()",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.Decimal128(Decimal128)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.Decimal128(String,short)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.Decimal128(UnsignedInt128,short,boolean)",1,1,3
"org.apache.hadoop.hive.common.type.Decimal128.Decimal128(char[],int,int,short)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.Decimal128(double,short)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.Decimal128(long)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.Decimal128(long,short)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.absDestructive()",1,1,2
"org.apache.hadoop.hive.common.type.Decimal128.add(Decimal128,Decimal128,Decimal128,short)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.addDestructive(Decimal128,short)",3,3,6
"org.apache.hadoop.hive.common.type.Decimal128.changeScaleDestructive(short)",2,3,5
"org.apache.hadoop.hive.common.type.Decimal128.checkPrecisionOverflow(int)",2,2,4
"org.apache.hadoop.hive.common.type.Decimal128.checkScaleRange(short)",3,1,3
"org.apache.hadoop.hive.common.type.Decimal128.compareTo(Decimal128)",3,2,4
"org.apache.hadoop.hive.common.type.Decimal128.divide(Decimal128,Decimal128,Decimal128,short)",2,1,3
"org.apache.hadoop.hive.common.type.Decimal128.divideDestructive(Decimal128,short)",2,1,2
"org.apache.hadoop.hive.common.type.Decimal128.divideDestructiveNativeDecimal128(Decimal128,short,Decimal128)",2,4,6
"org.apache.hadoop.hive.common.type.Decimal128.doubleValue()",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.equals(Object)",5,1,5
"org.apache.hadoop.hive.common.type.Decimal128.fastSerializeForHiveDecimal(Decimal128FastBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.fastUpdateFromInternalStorage(byte[],short)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.floatValue()",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.getHiveDecimalString()",1,12,12
"org.apache.hadoop.hive.common.type.Decimal128.getIntsPerElement(int)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.getScale()",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.getSignum()",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.getUnscaledValue()",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.hashCode()",2,1,2
"org.apache.hadoop.hive.common.type.Decimal128.intValue()",2,2,3
"org.apache.hadoop.hive.common.type.Decimal128.isZero()",1,4,4
"org.apache.hadoop.hive.common.type.Decimal128.longValue()",4,2,4
"org.apache.hadoop.hive.common.type.Decimal128.modulo(Decimal128,Decimal128,Decimal128,short)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.multiply(Decimal128,Decimal128,Decimal128,short)",2,1,3
"org.apache.hadoop.hive.common.type.Decimal128.multiplyDestructive(Decimal128,short)",2,1,2
"org.apache.hadoop.hive.common.type.Decimal128.multiplyDestructiveNativeDecimal128(Decimal128,short)",2,3,5
"org.apache.hadoop.hive.common.type.Decimal128.negateDestructive()",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.powAsDouble(double)",2,3,4
"org.apache.hadoop.hive.common.type.Decimal128.serializeTo(IntBuffer,int)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.serializeTo(int[],int,int)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.serializeTo128(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.serializeTo128(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.serializeTo32(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.serializeTo32(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.serializeTo64(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.serializeTo64(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.serializeTo96(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.serializeTo96(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.setNullDataValue()",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.setScale(short)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.setSignum(byte)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.setUnscaledValue(UnsignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.sqrtAsDouble()",3,1,3
"org.apache.hadoop.hive.common.type.Decimal128.squareDestructive()",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.subtract(Decimal128,Decimal128,Decimal128,short)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.subtractDestructive(Decimal128,short)",3,3,6
"org.apache.hadoop.hive.common.type.Decimal128.toBigDecimal()",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.toFormalString()",2,6,7
"org.apache.hadoop.hive.common.type.Decimal128.toString()",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update(BigDecimal)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update(BigInteger,short)",1,3,3
"org.apache.hadoop.hive.common.type.Decimal128.update(Decimal128)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update(Decimal128,short)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update(IntBuffer,int)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update(String,short)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update(char[],int,int,short)",7,8,27
"org.apache.hadoop.hive.common.type.Decimal128.update(double,short)",3,5,11
"org.apache.hadoop.hive.common.type.Decimal128.update(int[],int,int)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update(long)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update(long,short)",1,4,4
"org.apache.hadoop.hive.common.type.Decimal128.update128(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update128(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update32(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update32(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update64(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update64(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update96(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.update96(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.updateFixedPoint(long,short)",1,3,3
"org.apache.hadoop.hive.common.type.Decimal128.updateVarianceDestructive(Decimal128,Decimal128,Decimal128,long)",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.zeroClear()",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.zeroFractionPart()",1,1,1
"org.apache.hadoop.hive.common.type.Decimal128.zeroFractionPart(UnsignedInt128)",2,1,3
"org.apache.hadoop.hive.common.type.HiveBaseChar.HiveBaseChar()",1,1,1
"org.apache.hadoop.hive.common.type.HiveBaseChar.enforceMaxLength(String,int)",1,3,3
"org.apache.hadoop.hive.common.type.HiveBaseChar.getCharacterLength()",1,1,1
"org.apache.hadoop.hive.common.type.HiveBaseChar.getPaddedValue(String,int)",3,3,4
"org.apache.hadoop.hive.common.type.HiveBaseChar.getValue()",1,1,1
"org.apache.hadoop.hive.common.type.HiveBaseChar.hashCode()",1,1,1
"org.apache.hadoop.hive.common.type.HiveBaseChar.setValue(HiveBaseChar,int)",1,1,1
"org.apache.hadoop.hive.common.type.HiveBaseChar.setValue(String,int)",1,1,1
"org.apache.hadoop.hive.common.type.HiveBaseChar.toString()",1,1,1
"org.apache.hadoop.hive.common.type.HiveChar.HiveChar()",1,1,1
"org.apache.hadoop.hive.common.type.HiveChar.HiveChar(HiveChar,int)",1,1,1
"org.apache.hadoop.hive.common.type.HiveChar.HiveChar(String,int)",1,1,1
"org.apache.hadoop.hive.common.type.HiveChar.compareTo(HiveChar)",2,1,2
"org.apache.hadoop.hive.common.type.HiveChar.equals(Object)",3,2,4
"org.apache.hadoop.hive.common.type.HiveChar.getCharacterLength()",1,1,1
"org.apache.hadoop.hive.common.type.HiveChar.getPaddedValue()",1,1,1
"org.apache.hadoop.hive.common.type.HiveChar.getStrippedValue()",1,1,1
"org.apache.hadoop.hive.common.type.HiveChar.hashCode()",1,1,1
"org.apache.hadoop.hive.common.type.HiveChar.setValue(String)",1,1,1
"org.apache.hadoop.hive.common.type.HiveChar.setValue(String,int)",1,1,1
"org.apache.hadoop.hive.common.type.HiveChar.toString()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.HiveDecimal(BigDecimal)",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.abs()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.add(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.bigDecimalValue()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.byteValue()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.compareTo(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.create(BigDecimal)",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.create(BigDecimal,boolean)",1,1,2
"org.apache.hadoop.hive.common.type.HiveDecimal.create(BigInteger)",1,1,2
"org.apache.hadoop.hive.common.type.HiveDecimal.create(BigInteger,int)",1,1,2
"org.apache.hadoop.hive.common.type.HiveDecimal.create(String)",1,1,3
"org.apache.hadoop.hive.common.type.HiveDecimal.create(int)",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.create(long)",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.divide(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.doubleValue()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.enforcePrecisionScale(BigDecimal,int,int)",3,2,4
"org.apache.hadoop.hive.common.type.HiveDecimal.equals(Object)",2,2,3
"org.apache.hadoop.hive.common.type.HiveDecimal.floatValue()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.hashCode()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.intValue()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.longValue()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.multiply(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.negate()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.normalize(BigDecimal,boolean)",3,3,5
"org.apache.hadoop.hive.common.type.HiveDecimal.pow(int)",1,1,2
"org.apache.hadoop.hive.common.type.HiveDecimal.precision()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.remainder(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.scale()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.scaleByPowerOfTen(int)",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.setScale(int)",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.setScale(int,int)",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.shortValue()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.signum()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.subtract(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.toString()",1,1,1
"org.apache.hadoop.hive.common.type.HiveDecimal.trim(BigDecimal)",1,3,3
"org.apache.hadoop.hive.common.type.HiveDecimal.unscaledValue()",1,1,1
"org.apache.hadoop.hive.common.type.HiveVarchar.HiveVarchar()",1,1,1
"org.apache.hadoop.hive.common.type.HiveVarchar.HiveVarchar(HiveVarchar,int)",1,1,1
"org.apache.hadoop.hive.common.type.HiveVarchar.HiveVarchar(String,int)",1,1,1
"org.apache.hadoop.hive.common.type.HiveVarchar.compareTo(HiveVarchar)",2,1,2
"org.apache.hadoop.hive.common.type.HiveVarchar.equals(HiveVarchar)",2,1,2
"org.apache.hadoop.hive.common.type.HiveVarchar.setValue(HiveVarchar)",1,1,1
"org.apache.hadoop.hive.common.type.HiveVarchar.setValue(String)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.SignedInt128()",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.SignedInt128(SignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.SignedInt128(String)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.SignedInt128(UnsignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.SignedInt128(char[],int,int)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.SignedInt128(int,int,int,int)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.SignedInt128(long)",1,1,2
"org.apache.hadoop.hive.common.type.SignedInt128.abs(SignedInt128,SignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.absDestructive()",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.add(SignedInt128,SignedInt128,SignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.addDestructive(SignedInt128)",2,3,4
"org.apache.hadoop.hive.common.type.SignedInt128.compareTo(SignedInt128)",4,4,4
"org.apache.hadoop.hive.common.type.SignedInt128.decrement(SignedInt128,SignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.decrementDestructive()",1,4,4
"org.apache.hadoop.hive.common.type.SignedInt128.divide(SignedInt128,SignedInt128,SignedInt128,SignedInt128)",2,1,3
"org.apache.hadoop.hive.common.type.SignedInt128.divideDestructive(SignedInt128,SignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.divideDestructive(int)",1,2,4
"org.apache.hadoop.hive.common.type.SignedInt128.doubleValue()",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.equals(Object)",2,3,3
"org.apache.hadoop.hive.common.type.SignedInt128.equals(SignedInt128)",1,2,2
"org.apache.hadoop.hive.common.type.SignedInt128.floatValue()",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.getIntsPerElement(int)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.getV0()",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.getV1()",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.getV2()",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.getV3()",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.hashCode()",1,2,2
"org.apache.hadoop.hive.common.type.SignedInt128.increment(SignedInt128,SignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.incrementDestructive()",1,3,4
"org.apache.hadoop.hive.common.type.SignedInt128.intValue()",1,1,2
"org.apache.hadoop.hive.common.type.SignedInt128.isZero()",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.longValue()",1,1,2
"org.apache.hadoop.hive.common.type.SignedInt128.multiply(SignedInt128,SignedInt128,SignedInt128)",2,1,3
"org.apache.hadoop.hive.common.type.SignedInt128.multiplyDestructive(SignedInt128)",1,2,2
"org.apache.hadoop.hive.common.type.SignedInt128.multiplyDestructive(int)",1,3,4
"org.apache.hadoop.hive.common.type.SignedInt128.negate(SignedInt128,SignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.negateDestructive()",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.scaleDownTen(SignedInt128,SignedInt128,short)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.scaleDownTenDestructive(short)",1,2,3
"org.apache.hadoop.hive.common.type.SignedInt128.scaleUpTen(SignedInt128,SignedInt128,short)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.scaleUpTenDestructive(short)",1,2,2
"org.apache.hadoop.hive.common.type.SignedInt128.serializeTo128(IntBuffer)",1,1,2
"org.apache.hadoop.hive.common.type.SignedInt128.serializeTo128(int[],int)",1,1,2
"org.apache.hadoop.hive.common.type.SignedInt128.serializeTo32(IntBuffer)",1,4,5
"org.apache.hadoop.hive.common.type.SignedInt128.serializeTo32(int[],int)",1,4,5
"org.apache.hadoop.hive.common.type.SignedInt128.serializeTo64(IntBuffer)",1,3,4
"org.apache.hadoop.hive.common.type.SignedInt128.serializeTo64(int[],int)",1,3,4
"org.apache.hadoop.hive.common.type.SignedInt128.serializeTo96(IntBuffer)",1,2,3
"org.apache.hadoop.hive.common.type.SignedInt128.serializeTo96(int[],int)",1,2,3
"org.apache.hadoop.hive.common.type.SignedInt128.shiftLeft(SignedInt128,SignedInt128,int)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.shiftLeftDestructive(int)",1,2,2
"org.apache.hadoop.hive.common.type.SignedInt128.shiftRight(SignedInt128,SignedInt128,int,boolean)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.shiftRightDestructive(int,boolean)",1,2,3
"org.apache.hadoop.hive.common.type.SignedInt128.subtract(SignedInt128,SignedInt128,SignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.subtractDestructive(SignedInt128)",2,3,4
"org.apache.hadoop.hive.common.type.SignedInt128.toBigIntegerSlow()",1,2,2
"org.apache.hadoop.hive.common.type.SignedInt128.toFormalString()",2,2,2
"org.apache.hadoop.hive.common.type.SignedInt128.toString()",1,1,2
"org.apache.hadoop.hive.common.type.SignedInt128.update(SignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.update(String)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.update(char[],int,int)",2,2,5
"org.apache.hadoop.hive.common.type.SignedInt128.update(long)",1,1,2
"org.apache.hadoop.hive.common.type.SignedInt128.update128(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.update128(int,int,int,int)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.update128(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.update32(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.update32(int)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.update32(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.update64(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.update64(int,int)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.update64(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.update96(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.update96(int,int,int)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.update96(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.SignedInt128.zeroClear()",1,1,1
"org.apache.hadoop.hive.common.type.SqlMathUtil.SqlMathUtil()",1,1,1
"org.apache.hadoop.hive.common.type.SqlMathUtil.arrayValidLength(int[])",1,1,4
"org.apache.hadoop.hive.common.type.SqlMathUtil.bitLength(int,int,int,int)",4,4,4
"org.apache.hadoop.hive.common.type.SqlMathUtil.bitLengthInWord(int)",5,1,5
"org.apache.hadoop.hive.common.type.SqlMathUtil.combineInts(int,int)",1,1,1
"org.apache.hadoop.hive.common.type.SqlMathUtil.compareUnsignedInt(int,int)",3,1,3
"org.apache.hadoop.hive.common.type.SqlMathUtil.compareUnsignedLong(long,long)",3,1,3
"org.apache.hadoop.hive.common.type.SqlMathUtil.divideMultiPrecision(int[],int)",1,1,2
"org.apache.hadoop.hive.common.type.SqlMathUtil.divideMultiPrecision(int[],int[],int[])",6,10,13
"org.apache.hadoop.hive.common.type.SqlMathUtil.divideUnsignedLong(long,long)",4,3,5
"org.apache.hadoop.hive.common.type.SqlMathUtil.extractHiInt(long)",1,1,1
"org.apache.hadoop.hive.common.type.SqlMathUtil.extractLowInt(long)",1,1,1
"org.apache.hadoop.hive.common.type.SqlMathUtil.multiplyMultiPrecision(int[],int)",1,2,3
"org.apache.hadoop.hive.common.type.SqlMathUtil.remainderUnsignedLong(long,long)",4,3,5
"org.apache.hadoop.hive.common.type.SqlMathUtil.setSignBitInt(int,boolean)",2,1,2
"org.apache.hadoop.hive.common.type.SqlMathUtil.setSignBitLong(long,boolean)",2,1,2
"org.apache.hadoop.hive.common.type.SqlMathUtil.throwOverflowException()",1,1,1
"org.apache.hadoop.hive.common.type.SqlMathUtil.throwZeroDivisionException()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.assertNotEquals(double,double,double)",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.makeNumericString(int)",1,2,3
"org.apache.hadoop.hive.common.type.TestDecimal128.setUp()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.tearDown()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testAdd()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testCalculateTenThirtySeven()",1,2,2
"org.apache.hadoop.hive.common.type.TestDecimal128.testCompareTo()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testDivide()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testDoubleValue()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testEquals()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testFloatValue()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testHashCode()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testHighPrecisionDecimal128Add()",1,2,2
"org.apache.hadoop.hive.common.type.TestDecimal128.testHighPrecisionDecimal128Divide()",1,2,2
"org.apache.hadoop.hive.common.type.TestDecimal128.testHighPrecisionDecimal128Multiply()",1,2,2
"org.apache.hadoop.hive.common.type.TestDecimal128.testHighPrecisionDecimal128Subtract()",1,2,2
"org.apache.hadoop.hive.common.type.TestDecimal128.testKnownPriorErrors()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testMultiply()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testPiArcsine()",1,3,3
"org.apache.hadoop.hive.common.type.TestDecimal128.testPiNewton()",1,2,2
"org.apache.hadoop.hive.common.type.TestDecimal128.testPowAsDouble()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testPrecisionOverflow()",1,1,7
"org.apache.hadoop.hive.common.type.TestDecimal128.testRandomAddSubtractInverse()",1,2,2
"org.apache.hadoop.hive.common.type.TestDecimal128.testRandomMultiplyDivideInverse()",1,2,2
"org.apache.hadoop.hive.common.type.TestDecimal128.testSqrtAsDouble()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testSubtract()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testText()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testToHiveDecimalString()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testToLong()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.testUpdateWithScale()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.verifyAddSubtractInverse(long,long)",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.verifyHighPrecisionAddSingle()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.verifyHighPrecisionDivideSingle()",3,1,6
"org.apache.hadoop.hive.common.type.TestDecimal128.verifyHighPrecisionMultiplySingle()",1,1,3
"org.apache.hadoop.hive.common.type.TestDecimal128.verifyHighPrecisionMultiplySingle(String,String)",1,1,5
"org.apache.hadoop.hive.common.type.TestDecimal128.verifyHighPrecisionSubtractSingle()",1,1,1
"org.apache.hadoop.hive.common.type.TestDecimal128.verifyMultiplyDivideInverse(long,long)",2,1,2
"org.apache.hadoop.hive.common.type.TestHiveBaseChar.createRandomSupplementaryCharString(int)",1,2,2
"org.apache.hadoop.hive.common.type.TestHiveBaseChar.getRandomCodePoint()",1,3,3
"org.apache.hadoop.hive.common.type.TestHiveBaseChar.getRandomCodePoint(int)",3,2,3
"org.apache.hadoop.hive.common.type.TestHiveBaseChar.getRandomSupplementaryChar()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveBaseChar.testGetPaddedValue()",1,2,2
"org.apache.hadoop.hive.common.type.TestHiveBaseChar.testStringLength()",1,3,3
"org.apache.hadoop.hive.common.type.TestHiveChar.testBasic()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveChar.testComparison()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveChar.testStringLength()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveDecimal.testBinaryConversion()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveDecimal.testBinaryConversion(String)",1,1,3
"org.apache.hadoop.hive.common.type.TestHiveDecimal.testDivide()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveDecimal.testException()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveDecimal.testHashCode()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveDecimal.testMultiply()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveDecimal.testPlus()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveDecimal.testPosMod()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveDecimal.testPow()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveDecimal.testPrecisionScaleEnforcement()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveDecimal.testSubtract()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveVarchar.TestHiveVarchar()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveVarchar.getRandomCodePoint()",1,3,3
"org.apache.hadoop.hive.common.type.TestHiveVarchar.getRandomCodePoint(int)",3,2,3
"org.apache.hadoop.hive.common.type.TestHiveVarchar.getRandomSupplementaryChar()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveVarchar.testComparison()",1,1,1
"org.apache.hadoop.hive.common.type.TestHiveVarchar.testStringLength()",1,6,6
"org.apache.hadoop.hive.common.type.TestSignedInt128.setUp()",1,1,1
"org.apache.hadoop.hive.common.type.TestSignedInt128.tearDown()",1,1,1
"org.apache.hadoop.hive.common.type.TestSignedInt128.testAddDestructive()",1,2,3
"org.apache.hadoop.hive.common.type.TestSignedInt128.testCompareTo()",1,1,1
"org.apache.hadoop.hive.common.type.TestSignedInt128.testDivideDestructiveInt()",1,1,2
"org.apache.hadoop.hive.common.type.TestSignedInt128.testDivideDestructiveSignedInt128()",1,1,2
"org.apache.hadoop.hive.common.type.TestSignedInt128.testDivideDestructiveSignedInt128Again()",1,1,1
"org.apache.hadoop.hive.common.type.TestSignedInt128.testEquals()",1,1,1
"org.apache.hadoop.hive.common.type.TestSignedInt128.testHashCode()",1,1,1
"org.apache.hadoop.hive.common.type.TestSignedInt128.testMultiplyDestructiveInt()",1,1,2
"org.apache.hadoop.hive.common.type.TestSignedInt128.testMultiplyDestructiveSignedInt128()",1,1,3
"org.apache.hadoop.hive.common.type.TestSignedInt128.testShiftDestructive()",1,1,1
"org.apache.hadoop.hive.common.type.TestSignedInt128.testSignedInt128()",1,1,1
"org.apache.hadoop.hive.common.type.TestSignedInt128.testSignedInt128IntIntIntInt()",1,1,1
"org.apache.hadoop.hive.common.type.TestSignedInt128.testSignedInt128SignedInt128()",1,1,1
"org.apache.hadoop.hive.common.type.TestSignedInt128.testSubtractDestructive()",1,1,1
"org.apache.hadoop.hive.common.type.TestSignedInt128.testToFormalString()",1,1,1
"org.apache.hadoop.hive.common.type.TestSignedInt128.testZeroClear()",1,1,1
"org.apache.hadoop.hive.common.type.TestSqlMathUtil.testDivision()",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.assertNotEquals(UnsignedInt128,UnsignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.assertNotEquals(int,int)",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.assertNotEquals(long,long)",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.setUp()",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.tearDown()",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testAddDestructive()",1,2,3
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testBigIntConversion()",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testCompareTo()",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testCompareToScaleTen()",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testDivideDestructiveInt()",1,1,2
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testDivideDestructiveUnsignedInt128()",1,1,2
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testDivideDestructiveUnsignedInt128Again()",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testEquals()",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testHashCode()",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testMultiplyDestructiveInt()",1,1,2
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testMultiplyDestructiveUnsignedInt128()",1,1,3
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testMultiplyScaleDownTenDestructiveScaleTen()",1,4,4
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testShiftDestructive()",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testSubtractDestructive()",1,1,2
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testToFormalString()",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testUnsignedInt128()",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testUnsignedInt128IntIntIntInt()",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testUnsignedInt128UnsignedInt128()",1,1,1
"org.apache.hadoop.hive.common.type.TestUnsignedInt128.testZeroClear()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.UnsignedInt128()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.UnsignedInt128(BigInteger)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.UnsignedInt128(String)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.UnsignedInt128(UnsignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.UnsignedInt128(char[],int,int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.UnsignedInt128(int,int,int,int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.UnsignedInt128(long)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.addConstructive(UnsignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.addDestructive(UnsignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.addDestructive(int)",1,5,5
"org.apache.hadoop.hive.common.type.UnsignedInt128.addDestructive(int[])",1,2,3
"org.apache.hadoop.hive.common.type.UnsignedInt128.addDestructiveScaleTen(UnsignedInt128,short)",2,5,5
"org.apache.hadoop.hive.common.type.UnsignedInt128.asLong()",1,2,3
"org.apache.hadoop.hive.common.type.UnsignedInt128.bitLength()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.clone()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.compareTo(UnsignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.compareTo(int,int,int,int)",4,4,4
"org.apache.hadoop.hive.common.type.UnsignedInt128.compareTo(int,int,int,int,int,int,int,int)",5,5,5
"org.apache.hadoop.hive.common.type.UnsignedInt128.compareTo(int[])",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.compareToScaleTen(UnsignedInt128,short)",9,9,16
"org.apache.hadoop.hive.common.type.UnsignedInt128.decrementArray(int[])",3,3,4
"org.apache.hadoop.hive.common.type.UnsignedInt128.decrementConstructive()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.decrementDestructive()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.difference(UnsignedInt128,UnsignedInt128,UnsignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.differenceInternal(UnsignedInt128,int[],UnsignedInt128)",2,3,7
"org.apache.hadoop.hive.common.type.UnsignedInt128.differenceScaleTen(UnsignedInt128,UnsignedInt128,UnsignedInt128,short)",2,4,4
"org.apache.hadoop.hive.common.type.UnsignedInt128.divideCheckRound(int[],int)",1,1,2
"org.apache.hadoop.hive.common.type.UnsignedInt128.divideConstructive(UnsignedInt128,UnsignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.divideConstructive(int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.divideDestructive(UnsignedInt128,UnsignedInt128)",2,3,3
"org.apache.hadoop.hive.common.type.UnsignedInt128.divideDestructive(int)",1,1,2
"org.apache.hadoop.hive.common.type.UnsignedInt128.divideDestructive(long)",1,1,2
"org.apache.hadoop.hive.common.type.UnsignedInt128.divideScaleUpTenDestructive(UnsignedInt128,short,UnsignedInt128)",1,2,2
"org.apache.hadoop.hive.common.type.UnsignedInt128.equals(Object)",2,1,2
"org.apache.hadoop.hive.common.type.UnsignedInt128.equals(UnsignedInt128)",1,1,4
"org.apache.hadoop.hive.common.type.UnsignedInt128.equals(int,int,int,int)",1,1,4
"org.apache.hadoop.hive.common.type.UnsignedInt128.exceedsTenToThirtyEight()",3,1,6
"org.apache.hadoop.hive.common.type.UnsignedInt128.fastSerializeForHiveDecimal(Decimal128FastBuffer,byte)",2,2,8
"org.apache.hadoop.hive.common.type.UnsignedInt128.fastSerializeIntPartForHiveDecimal(ByteBuffer,int,int,byte,boolean)",1,1,4
"org.apache.hadoop.hive.common.type.UnsignedInt128.fastUpdateFromInternalStorage(byte[])",7,3,30
"org.apache.hadoop.hive.common.type.UnsignedInt128.fastUpdateIntFromInternalStorage(byte[],byte,int,int)",2,2,9
"org.apache.hadoop.hive.common.type.UnsignedInt128.fitsInt32()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.getCount()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.getDigitsArray(int[])",2,2,8
"org.apache.hadoop.hive.common.type.UnsignedInt128.getIntsPerElement(int)",4,1,5
"org.apache.hadoop.hive.common.type.UnsignedInt128.getV()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.getV0()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.getV1()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.getV2()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.getV3()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.hashCode()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.incrementArray(int[])",3,3,4
"org.apache.hadoop.hive.common.type.UnsignedInt128.incrementConstructive()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.incrementDestructive()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.isOne()",1,1,2
"org.apache.hadoop.hive.common.type.UnsignedInt128.isZero()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.multiplyArrays4And4To4NoOverflow(int[],int[])",1,3,11
"org.apache.hadoop.hive.common.type.UnsignedInt128.multiplyArrays4And4To8(int[],int[])",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.multiplyConstructive(UnsignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.multiplyConstructive(int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.multiplyConstructive256(UnsignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.multiplyDestructive(UnsignedInt128)",2,3,3
"org.apache.hadoop.hive.common.type.UnsignedInt128.multiplyDestructive(int)",3,3,5
"org.apache.hadoop.hive.common.type.UnsignedInt128.multiplyDestructiveFitsInt32(UnsignedInt128,short,short)",3,7,8
"org.apache.hadoop.hive.common.type.UnsignedInt128.multiplyScaleDownTenDestructive(UnsignedInt128,short)",2,3,3
"org.apache.hadoop.hive.common.type.UnsignedInt128.multiplyShiftDestructive(UnsignedInt128,short)",2,3,3
"org.apache.hadoop.hive.common.type.UnsignedInt128.scaleDownFiveArray(int[],short)",3,3,3
"org.apache.hadoop.hive.common.type.UnsignedInt128.scaleDownFiveArrayRoundUp(int[],short)",1,2,2
"org.apache.hadoop.hive.common.type.UnsignedInt128.scaleDownFiveDestructive(short)",4,1,4
"org.apache.hadoop.hive.common.type.UnsignedInt128.scaleDownTenArray4RoundUp(int[],short)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.scaleDownTenArray8RoundUp(int[],short)",4,10,16
"org.apache.hadoop.hive.common.type.UnsignedInt128.scaleDownTenDestructive(short)",4,1,4
"org.apache.hadoop.hive.common.type.UnsignedInt128.scaleUpFiveDestructive(short)",4,2,5
"org.apache.hadoop.hive.common.type.UnsignedInt128.scaleUpTenArray(int[],short)",3,2,4
"org.apache.hadoop.hive.common.type.UnsignedInt128.scaleUpTenDestructive(short)",4,1,4
"org.apache.hadoop.hive.common.type.UnsignedInt128.serializeTo(IntBuffer,int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.serializeTo(int[],int,int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.serializeTo128(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.serializeTo128(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.serializeTo32(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.serializeTo32(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.serializeTo64(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.serializeTo64(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.serializeTo96(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.serializeTo96(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.setCount(byte)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.setV(int[])",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.setV0(int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.setV1(int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.setV2(int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.setV3(int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.shiftLeftConstructive(int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.shiftLeftDestructive(int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.shiftLeftDestructive(int,int)",4,3,15
"org.apache.hadoop.hive.common.type.UnsignedInt128.shiftLeftDestructiveCheckOverflow(int)",1,2,2
"org.apache.hadoop.hive.common.type.UnsignedInt128.shiftRightArray(int,int[],int[],boolean)",1,9,17
"org.apache.hadoop.hive.common.type.UnsignedInt128.shiftRightConstructive(int,boolean)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.shiftRightDestructive(int,boolean)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.shiftRightDestructive(int,int,boolean)",4,4,21
"org.apache.hadoop.hive.common.type.UnsignedInt128.subtractConstructive(UnsignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.subtractDestructive(UnsignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.subtractDestructive(int[])",1,2,3
"org.apache.hadoop.hive.common.type.UnsignedInt128.throwIfExceedsTenToThirtyEight()",1,2,2
"org.apache.hadoop.hive.common.type.UnsignedInt128.toBigIntegerSlow()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.toFormalString()",2,2,7
"org.apache.hadoop.hive.common.type.UnsignedInt128.toString()",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.update(BigInteger)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.update(IntBuffer,int)",2,2,6
"org.apache.hadoop.hive.common.type.UnsignedInt128.update(String)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.update(UnsignedInt128)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.update(char[],int,int)",4,6,10
"org.apache.hadoop.hive.common.type.UnsignedInt128.update(int,int,int,int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.update(int[],int,int)",2,2,6
"org.apache.hadoop.hive.common.type.UnsignedInt128.update(long)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.update128(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.update128(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.update32(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.update32(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.update64(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.update64(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.update96(IntBuffer)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.update96(int[],int)",1,1,1
"org.apache.hadoop.hive.common.type.UnsignedInt128.updateCount()",1,1,5
"org.apache.hadoop.hive.common.type.UnsignedInt128.zeroClear()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.ConfVars(String,Object,String)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.ConfVars(String,Object,String,boolean)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.ConfVars(String,Object,Validator,String)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.ConfVars(String,Object,Validator,String,boolean,boolean)",6,7,8
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.ConfVars(String,String,boolean,String)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.VarType.defaultValueString(ConfVars)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.VarType.isType(String)",1,1,2
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.VarType.typeString()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.findHadoopBinary()",1,2,4
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.getDefaultExpr()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.getDefaultValue()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.getDescription()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.isCaseSensitive()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.isExcluded()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.isType(String)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.toString()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.typeString()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.ConfVars.validate(String)",1,2,2
"org.apache.hadoop.hive.conf.HiveConf.HiveConf()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.HiveConf(Class<?>)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.HiveConf(Configuration,Class<?>)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.HiveConf(HiveConf)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.addToModifiableWhiteList(String)",2,1,2
"org.apache.hadoop.hive.conf.HiveConf.addToRestrictList(String)",2,3,4
"org.apache.hadoop.hive.conf.HiveConf.applyDefaultNonNullConfVars(Configuration)",3,2,3
"org.apache.hadoop.hive.conf.HiveConf.applySystemProperties()",1,2,2
"org.apache.hadoop.hive.conf.HiveConf.getAllProperties()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getAuxJars()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getBoolVar(ConfVars)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getBoolVar(Configuration,ConfVars)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getBoolVar(Configuration,ConfVars,boolean)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getChangedProperties()",1,3,3
"org.apache.hadoop.hive.conf.HiveConf.getColumnInternalName(int)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getConfSystemProperties()",1,4,4
"org.apache.hadoop.hive.conf.HiveConf.getConfVarInputStream()",2,2,3
"org.apache.hadoop.hive.conf.HiveConf.getConfVars(String)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getFloatVar(ConfVars)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getFloatVar(Configuration,ConfVars)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getFloatVar(Configuration,ConfVars,float)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getHiveDefaultLocation()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getHiveServer2SiteLocation()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getHiveSiteLocation()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getIntVar(ConfVars)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getIntVar(Configuration,ConfVars)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getJar()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getLongVar(ConfVars)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getLongVar(Configuration,ConfVars)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getLongVar(Configuration,ConfVars,long)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getMetaConf(String)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getMetastoreSiteLocation()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getPositionFromInternalName(String)",2,2,2
"org.apache.hadoop.hive.conf.HiveConf.getProperties(Configuration)",1,2,2
"org.apache.hadoop.hive.conf.HiveConf.getUser()",1,1,2
"org.apache.hadoop.hive.conf.HiveConf.getVar(ConfVars)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getVar(Configuration,ConfVars)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.getVar(Configuration,ConfVars,String)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.initialize(Class<?>)",4,21,22
"org.apache.hadoop.hive.conf.HiveConf.isLoadHiveServer2Config()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.isLoadMetastoreConfig()",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.logVars(PrintStream)",1,3,3
"org.apache.hadoop.hive.conf.HiveConf.setAuxJars(String)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setBoolVar(ConfVars,boolean)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setBoolVar(Configuration,ConfVars,boolean)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setFloatVar(ConfVars,float)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setFloatVar(Configuration,ConfVars,float)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setHiveSiteLocation(URL)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setIntVar(ConfVars,int)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setIntVar(Configuration,ConfVars,int)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setIsModWhiteListEnabled(boolean)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setLoadHiveServer2Config(boolean)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setLoadMetastoreConfig(boolean)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setLongVar(ConfVars,long)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setLongVar(Configuration,ConfVars,long)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setVar(ConfVars,String)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setVar(Configuration,ConfVars,String)",1,1,1
"org.apache.hadoop.hive.conf.HiveConf.setupRestrictList()",1,3,3
"org.apache.hadoop.hive.conf.HiveConf.verifyAndSet(String,String)",4,2,4
"org.apache.hadoop.hive.conf.HiveConfUtil.isEmbeddedMetaStore(String)",1,2,2
"org.apache.hadoop.hive.conf.LoopingByteArrayInputStream.LoopingByteArrayInputStream(byte[])",1,1,1
"org.apache.hadoop.hive.conf.LoopingByteArrayInputStream.available()",1,1,1
"org.apache.hadoop.hive.conf.LoopingByteArrayInputStream.close()",1,1,1
"org.apache.hadoop.hive.conf.LoopingByteArrayInputStream.getByteArrayInputStream()",1,2,2
"org.apache.hadoop.hive.conf.LoopingByteArrayInputStream.mark(int)",1,1,1
"org.apache.hadoop.hive.conf.LoopingByteArrayInputStream.markSupported()",1,1,1
"org.apache.hadoop.hive.conf.LoopingByteArrayInputStream.read()",1,1,1
"org.apache.hadoop.hive.conf.LoopingByteArrayInputStream.read(byte[])",1,1,1
"org.apache.hadoop.hive.conf.LoopingByteArrayInputStream.read(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.conf.LoopingByteArrayInputStream.reset()",1,1,1
"org.apache.hadoop.hive.conf.LoopingByteArrayInputStream.skip(long)",1,1,1
"org.apache.hadoop.hive.conf.SystemVariables.containsVar(String)",1,2,2
"org.apache.hadoop.hive.conf.SystemVariables.getSubstitute(Configuration,String)",1,8,8
"org.apache.hadoop.hive.conf.SystemVariables.substitute(Configuration,String)",1,2,2
"org.apache.hadoop.hive.conf.SystemVariables.substitute(Configuration,String,int)",4,3,6
"org.apache.hadoop.hive.conf.SystemVariables.substitute(String)",1,2,2
"org.apache.hadoop.hive.conf.TestHiveConf.checkConfVar(ConfVars,String)",1,1,1
"org.apache.hadoop.hive.conf.TestHiveConf.checkHadoopConf(String,String)",1,1,1
"org.apache.hadoop.hive.conf.TestHiveConf.checkHiveConf(String,String)",1,1,1
"org.apache.hadoop.hive.conf.TestHiveConf.testColumnNameMapping()",1,2,2
"org.apache.hadoop.hive.conf.TestHiveConf.testConfProperties()",1,1,1
"org.apache.hadoop.hive.conf.TestHiveConf.testHiveSitePath()",1,2,2
"org.apache.hadoop.hive.conf.TestHiveConfRestrictList.setUp()",1,1,1
"org.apache.hadoop.hive.conf.TestHiveConfRestrictList.testAppendRestriction()",1,1,1
"org.apache.hadoop.hive.conf.TestHiveConfRestrictList.testRestrictList()",1,1,1
"org.apache.hadoop.hive.conf.TestHiveConfRestrictList.testRestriction()",1,1,1
"org.apache.hadoop.hive.conf.TestHiveConfRestrictList.verifyRestriction(String,String)",1,1,2
"org.apache.hadoop.hive.conf.TestHiveLogging.RunTest(String,String,String,String,String)",1,1,1
"org.apache.hadoop.hive.conf.TestHiveLogging.TestHiveLogging()",1,1,1
"org.apache.hadoop.hive.conf.TestHiveLogging.configLog(String,String)",1,1,1
"org.apache.hadoop.hive.conf.TestHiveLogging.getCmdOutput(String)",1,2,3
"org.apache.hadoop.hive.conf.TestHiveLogging.runCmd(String)",1,1,1
"org.apache.hadoop.hive.conf.TestHiveLogging.testHiveLogging()",1,1,1
"org.apache.hadoop.hive.conf.Validator.PatternSet.PatternSet(String...)",1,2,2
"org.apache.hadoop.hive.conf.Validator.PatternSet.validate(String)",4,2,4
"org.apache.hadoop.hive.conf.Validator.RANGE_TYPE.valueOf(Object,Object)",4,1,7
"org.apache.hadoop.hive.conf.Validator.RangeValidator.RangeValidator(Object,Object)",1,1,1
"org.apache.hadoop.hive.conf.Validator.RangeValidator.validate(String)",3,2,4
"org.apache.hadoop.hive.conf.Validator.RatioValidator.validate(String)",2,2,4
"org.apache.hadoop.hive.conf.Validator.StringSet.StringSet(String...)",1,2,2
"org.apache.hadoop.hive.conf.Validator.StringSet.validate(String)",2,2,3
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.Base64LineRecordReader.Base64LineRecordReader(LineRecordReader)",1,1,1
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.Base64LineRecordReader.close()",1,1,1
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.Base64LineRecordReader.configure(JobConf)",1,3,3
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.Base64LineRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.Base64LineRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.Base64LineRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.Base64LineRecordReader.getProgress()",1,1,1
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.Base64LineRecordReader.next(LongWritable,BytesWritable)",3,4,7
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.Base64TextInputFormat()",1,1,1
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.configure(JobConf)",1,1,1
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.createBase64()",1,2,5
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.getSplits(JobConf,int)",1,1,1
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat.Base64RecordWriter.Base64RecordWriter(RecordWriter)",1,1,1
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat.Base64RecordWriter.close(boolean)",1,1,1
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat.Base64RecordWriter.configure(JobConf)",1,3,3
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat.Base64RecordWriter.write(Writable)",1,2,4
"org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat.getHiveRecordWriter(JobConf,Path,Class<? extends Writable>,boolean,Properties,Progressable)",1,1,1
"org.apache.hadoop.hive.contrib.genericudf.example.GenericUDFDBOutput.evaluate(DeferredObject[])",1,6,6
"org.apache.hadoop.hive.contrib.genericudf.example.GenericUDFDBOutput.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.contrib.genericudf.example.GenericUDFDBOutput.initialize(ObjectInspector[])",6,6,6
"org.apache.hadoop.hive.contrib.metastore.hooks.TestURLHook.getJdoConnectionUrl(Configuration)",2,2,2
"org.apache.hadoop.hive.contrib.metastore.hooks.TestURLHook.notifyBadConnectionUrl(String)",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.KeyRecordIterator.KeyRecordIterator(String,RecordReader)",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.KeyRecordIterator.hasNext()",1,2,2
"org.apache.hadoop.hive.contrib.mr.GenericMR.KeyRecordIterator.next()",2,1,2
"org.apache.hadoop.hive.contrib.mr.GenericMR.KeyRecordIterator.remove()",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.OutputStreamOutput.OutputStreamOutput(OutputStream)",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.OutputStreamOutput.OutputStreamOutput(Writer)",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.OutputStreamOutput._join(String[],String)",2,3,5
"org.apache.hadoop.hive.contrib.mr.GenericMR.OutputStreamOutput.close()",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.OutputStreamOutput.collect(String[])",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.RecordReader.RecordReader(InputStream)",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.RecordReader.RecordReader(Reader)",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.RecordReader.close()",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.RecordReader.hasNext()",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.RecordReader.next()",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.RecordReader.peek()",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.RecordReader.readNext()",1,2,3
"org.apache.hadoop.hive.contrib.mr.GenericMR.handle(Reader,Writer,RecordProcessor)",1,2,2
"org.apache.hadoop.hive.contrib.mr.GenericMR.map(InputStream,OutputStream,Mapper)",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.map(Reader,Writer,Mapper)",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.reduce(InputStream,OutputStream,Reducer)",1,1,1
"org.apache.hadoop.hive.contrib.mr.GenericMR.reduce(Reader,Writer,Reducer)",1,1,1
"org.apache.hadoop.hive.contrib.mr.TestGenericMR.getOsSpecificOutput(String)",1,2,2
"org.apache.hadoop.hive.contrib.mr.TestGenericMR.identityMapper()",1,1,1
"org.apache.hadoop.hive.contrib.mr.TestGenericMR.identityReducer()",1,2,2
"org.apache.hadoop.hive.contrib.mr.TestGenericMR.testEmptyMap()",1,1,1
"org.apache.hadoop.hive.contrib.mr.TestGenericMR.testIdentityMap()",1,1,1
"org.apache.hadoop.hive.contrib.mr.TestGenericMR.testIdentityReduce()",1,1,1
"org.apache.hadoop.hive.contrib.mr.TestGenericMR.testKVSplitMap()",1,2,2
"org.apache.hadoop.hive.contrib.mr.TestGenericMR.testReduceTooFar()",1,2,3
"org.apache.hadoop.hive.contrib.mr.TestGenericMR.testWordCountReduce()",1,2,2
"org.apache.hadoop.hive.contrib.mr.example.IdentityMapper.IdentityMapper()",1,1,1
"org.apache.hadoop.hive.contrib.mr.example.IdentityMapper.main(String[])",1,1,1
"org.apache.hadoop.hive.contrib.mr.example.WordCountReduce.WordCountReduce()",1,1,1
"org.apache.hadoop.hive.contrib.mr.example.WordCountReduce.main(String[])",1,2,2
"org.apache.hadoop.hive.contrib.serde2.RegexSerDe.deserialize(Writable)",3,6,7
"org.apache.hadoop.hive.contrib.serde2.RegexSerDe.getNextNumberToDisplay(long)",1,1,1
"org.apache.hadoop.hive.contrib.serde2.RegexSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.contrib.serde2.RegexSerDe.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.contrib.serde2.RegexSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.contrib.serde2.RegexSerDe.initialize(Configuration,Properties)",3,6,7
"org.apache.hadoop.hive.contrib.serde2.RegexSerDe.serialize(Object,ObjectInspector)",3,3,5
"org.apache.hadoop.hive.contrib.serde2.TestRegexSerDe.createSerDe(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.contrib.serde2.TestRegexSerDe.testRegexSerDe()",1,2,2
"org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.deserialize(Writable)",1,2,3
"org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.deserializeField(TypedBytesWritableInput,TypeInfo,Object)",14,13,23
"org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.initialize(Configuration,Properties)",3,6,6
"org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.serialize(Object,ObjectInspector)",1,3,3
"org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.serializeField(Object,ObjectInspector,Object)",13,13,22
"org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer.S3LogDeserializer()",1,1,1
"org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer.deserialize(S3LogStruct,String)",1,1,2
"org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer.deserialize(Writable)",2,4,6
"org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer.initialize(Configuration,Properties)",1,1,1
"org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer.main(String[])",1,4,4
"org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer.toInt(String)",2,2,2
"org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer.toString()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg.UDAFExampleAvg()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg.UDAFExampleAvgEvaluator.UDAFExampleAvgEvaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg.UDAFExampleAvgEvaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg.UDAFExampleAvgEvaluator.iterate(Double)",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg.UDAFExampleAvgEvaluator.merge(UDAFAvgState)",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg.UDAFExampleAvgEvaluator.terminate()",1,2,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg.UDAFExampleAvgEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleGroupConcat.UDAFExampleGroupConcatEvaluator.UDAFExampleGroupConcatEvaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleGroupConcat.UDAFExampleGroupConcatEvaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleGroupConcat.UDAFExampleGroupConcatEvaluator.iterate(String[])",1,3,3
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleGroupConcat.UDAFExampleGroupConcatEvaluator.merge(ArrayList<String>)",1,2,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleGroupConcat.UDAFExampleGroupConcatEvaluator.terminate()",1,2,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleGroupConcat.UDAFExampleGroupConcatEvaluator.terminatePartial()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxDoubleEvaluator.MaxDoubleEvaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxDoubleEvaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxDoubleEvaluator.iterate(DoubleWritable)",1,3,3
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxDoubleEvaluator.merge(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxDoubleEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxDoubleEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxFloatEvaluator.MaxFloatEvaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxFloatEvaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxFloatEvaluator.iterate(FloatWritable)",1,3,3
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxFloatEvaluator.merge(FloatWritable)",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxFloatEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxFloatEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxIntEvaluator.MaxIntEvaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxIntEvaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxIntEvaluator.iterate(IntWritable)",1,3,3
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxIntEvaluator.merge(IntWritable)",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxIntEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxIntEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxLongEvaluator.MaxLongEvaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxLongEvaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxLongEvaluator.iterate(LongWritable)",1,3,3
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxLongEvaluator.merge(LongWritable)",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxLongEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxLongEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxShortEvaluator.MaxShortEvaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxShortEvaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxShortEvaluator.iterate(ShortWritable)",1,3,3
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxShortEvaluator.merge(ShortWritable)",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxShortEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxShortEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxStringEvaluator.MaxStringEvaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxStringEvaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxStringEvaluator.iterate(Text)",1,4,4
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxStringEvaluator.merge(Text)",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxStringEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax.MaxStringEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxMinNUtil.Evaluator.Evaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxMinNUtil.Evaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxMinNUtil.Evaluator.iterate(Double,int)",1,5,6
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxMinNUtil.Evaluator.merge(State)",1,2,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxMinNUtil.Evaluator.terminate()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxMinNUtil.Evaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxMinNUtil.UDAFExampleMaxMinNUtil()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxMinNUtil.binaryInsert(List<T>,T,boolean)",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxMinNUtil.getComparator(boolean,T)",1,2,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxMinNUtil.sortedMerge(List<T>,List<T>,boolean,int)",3,9,11
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxN.UDAFMaxNEvaluator.getAscending()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinDoubleEvaluator.MinDoubleEvaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinDoubleEvaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinDoubleEvaluator.iterate(DoubleWritable)",1,3,3
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinDoubleEvaluator.merge(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinDoubleEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinDoubleEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinFloatEvaluator.MinFloatEvaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinFloatEvaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinFloatEvaluator.iterate(FloatWritable)",1,3,3
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinFloatEvaluator.merge(FloatWritable)",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinFloatEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinFloatEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinIntEvaluator.MinIntEvaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinIntEvaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinIntEvaluator.iterate(IntWritable)",1,3,3
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinIntEvaluator.merge(IntWritable)",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinIntEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinIntEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinLongEvaluator.MinLongEvaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinLongEvaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinLongEvaluator.iterate(LongWritable)",1,3,3
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinLongEvaluator.merge(LongWritable)",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinLongEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinLongEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinShortEvaluator.MinShortEvaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinShortEvaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinShortEvaluator.iterate(ShortWritable)",1,3,3
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinShortEvaluator.merge(ShortWritable)",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinShortEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinShortEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinStringEvaluator.MinStringEvaluator()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinStringEvaluator.init()",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinStringEvaluator.iterate(Text)",1,4,4
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinStringEvaluator.merge(Text)",1,1,1
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinStringEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin.MinStringEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMinN.UDAFMinNEvaluator.getAscending()",1,1,1
"org.apache.hadoop.hive.contrib.udf.UDFRowSequence.UDFRowSequence()",1,1,1
"org.apache.hadoop.hive.contrib.udf.UDFRowSequence.evaluate()",1,1,1
"org.apache.hadoop.hive.contrib.udf.example.UDFExampleAdd.evaluate(Double...)",1,1,3
"org.apache.hadoop.hive.contrib.udf.example.UDFExampleAdd.evaluate(Integer...)",1,1,3
"org.apache.hadoop.hive.contrib.udf.example.UDFExampleArraySum.evaluate(List<Double>)",2,2,4
"org.apache.hadoop.hive.contrib.udf.example.UDFExampleFormat.evaluate(String,Object...)",1,1,1
"org.apache.hadoop.hive.contrib.udf.example.UDFExampleMapConcat.evaluate(Map<String, String>)",2,3,4
"org.apache.hadoop.hive.contrib.udf.example.UDFExampleStructPrint.evaluate(Object)",2,2,3
"org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount2.close()",1,1,1
"org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount2.initialize(ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount2.process(Object[])",1,1,1
"org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2.close()",1,1,1
"org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2.initialize(ObjectInspector[])",3,1,3
"org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2.process(Object[])",1,2,2
"org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2.toString()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.Type.Type(int)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.TypedBytesInput()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.TypedBytesInput(DataInput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.get(DataInput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.read()",13,12,14
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readBool()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readByte()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readBytes()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readDouble()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readFloat()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readInt()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readList()",1,2,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readLong()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readMap()",1,2,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readMapHeader()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readRaw()",11,11,13
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readRawBool()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readRawByte()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readRawBytes()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readRawDouble()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readRawFloat()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readRawInt()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readRawLong()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readRawString()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readShort()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readString()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readType()",3,1,4
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readVector()",1,2,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.readVectorHeader()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.setDataInput(DataInput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesInput.skipType()",1,1,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.TypedBytesOutput()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.TypedBytesOutput(DataOutput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.get(DataOutput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.setDataOutput(DataOutput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.write(Object)",11,11,11
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeBool(boolean)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeByte(byte)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeBytes(byte[])",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeBytes(byte[],int)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeDouble(double)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeEndOfRecord()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeFloat(float)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeInt(int)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeList(List)",1,2,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeListFooter()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeListHeader()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeLong(long)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeMap(Map)",1,2,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeMapHeader(int)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeNull()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeRaw(byte[])",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeRaw(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeShort(short)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeString(String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeVector(ArrayList)",1,2,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesOutput.writeVectorHeader(int)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.TypedBytesRecordInput()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.TypedBytesRecordInput(DataInput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.TypedBytesRecordInput(TypedBytesInput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.endMap(String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.endRecord(String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.endVector(String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.get(DataInput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.get(TypedBytesInput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.readBool(String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.readByte(String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.readDouble(String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.readFloat(String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.readInt(String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.readLong(String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.readString(String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.setTypedBytesInput(TypedBytesInput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordInput.startRecord(String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.TypedBytesRecordOutput()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.TypedBytesRecordOutput(DataOutput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.TypedBytesRecordOutput(TypedBytesOutput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.endMap(TreeMap,String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.endVector(ArrayList,String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.get(DataOutput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.get(TypedBytesOutput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.setTypedBytesOutput(TypedBytesOutput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.startMap(TreeMap,String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.startVector(ArrayList,String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.writeBool(boolean,String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.writeByte(byte,String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.writeDouble(double,String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.writeFloat(float,String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.writeInt(int,String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.writeLong(long,String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordOutput.writeString(String,String)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader.allocateWritable(Type)",10,2,10
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader.close()",1,2,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader.createRow()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader.getType(int)",3,1,3
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader.initialize(InputStream,Configuration,Properties)",1,2,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader.next(Writable)",7,7,16
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader.write(int,Writable)",1,9,9
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter.close()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter.initialize(OutputStream,Configuration)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter.write(Writable)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritable.TypedBytesWritable()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritable.TypedBytesWritable(byte[])",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritable.getType()",4,1,5
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritable.getValue()",1,1,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritable.setValue(Object)",1,1,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritable.toString()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.TypedBytesWritableInput()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.TypedBytesWritableInput(DataInput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.TypedBytesWritableInput(TypedBytesInput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.get(DataInput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.get(TypedBytesInput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.getConf()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.read()",17,15,17
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readArray()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readArray(ArrayWritable)",3,3,4
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readBoolean()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readBoolean(BooleanWritable)",1,1,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readByte()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readByte(ByteWritable)",1,1,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readBytes()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readBytes(BytesWritable)",1,2,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readDouble()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readDouble(DoubleWritable)",1,1,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readFloat()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readFloat(FloatWritable)",1,1,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readInt()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readInt(IntWritable)",1,1,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readLong()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readLong(LongWritable)",1,1,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readMap()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readMap(MapWritable)",1,2,3
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readShort()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readShort(ShortWritable)",1,1,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readSortedMap()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readSortedMap(SortedMapWritable)",1,2,3
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readText()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readText(Text)",1,1,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readType()",17,2,17
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readTypeCode()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readVInt()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readVInt(VIntWritable)",1,1,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readVLong()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readVLong(VLongWritable)",1,1,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readWritable()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.readWritable(Writable)",3,2,4
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableInput.setTypedBytesInput(TypedBytesInput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.TypedBytesWritableOutput()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.TypedBytesWritableOutput(DataOutput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.TypedBytesWritableOutput(TypedBytesOutput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.get(DataOutput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.get(TypedBytesOutput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.setTypedBytesOutput(TypedBytesOutput)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.write(Writable)",1,17,18
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeArray(ArrayWritable)",1,2,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeBoolean(BooleanWritable)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeByte(ByteWritable)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeBytes(BytesWritable)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeDouble(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeEndOfRecord()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeFloat(FloatWritable)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeInt(IntWritable)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeLong(LongWritable)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeMap(MapWritable)",1,2,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeNull()",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeShort(ShortWritable)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeSortedMap(SortedMapWritable)",1,2,2
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeText(Text)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeTypedBytes(TypedBytesWritable)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeVInt(VIntWritable)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeVLong(VLongWritable)",1,1,1
"org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.writeWritable(Writable)",1,1,1
"org.apache.hadoop.hive.hbase.AbstractHBaseKeyFactory.configureJobConf(TableDesc,JobConf)",1,1,1
"org.apache.hadoop.hive.hbase.AbstractHBaseKeyFactory.decomposePredicate(JobConf,Deserializer,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.hbase.AbstractHBaseKeyFactory.init(HBaseSerDeParameters,Properties)",1,1,1
"org.apache.hadoop.hive.hbase.AbstractHBaseKeyPredicateDecomposer.decomposePredicate(String,ExprNodeDesc)",2,3,3
"org.apache.hadoop.hive.hbase.AbstractHBaseKeyPredicateDecomposer.getFieldValidator()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMapping.ColumnMapping()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMapping.getBinaryStorage()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMapping.getColumnName()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMapping.getColumnType()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMapping.getFamilyName()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMapping.getFamilyNameBytes()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMapping.getMappingSpec()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMapping.getQualifierName()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMapping.getQualifierNameBytes()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMapping.getQualifierPrefix()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMapping.getQualifierPrefixBytes()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMapping.isCategory(Category)",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMapping.isHbaseRowKey()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.ColumnMappings(List<ColumnMapping>,int)",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.getColumnsMapping()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.getKeyIndex()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.getKeyMapping()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.iterator()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.parseColumnStorageTypes(String)",11,37,41
"org.apache.hadoop.hive.hbase.ColumnMappings.setHiveColumnDescription(String,List<String>,List<TypeInfo>)",5,6,7
"org.apache.hadoop.hive.hbase.ColumnMappings.size()",1,1,1
"org.apache.hadoop.hive.hbase.ColumnMappings.toTypesString()",1,5,5
"org.apache.hadoop.hive.hbase.CompositeHBaseKeyFactory.CompositeHBaseKeyFactory(Class<T>)",1,1,1
"org.apache.hadoop.hive.hbase.CompositeHBaseKeyFactory.configureJobConf(TableDesc,JobConf)",1,1,1
"org.apache.hadoop.hive.hbase.CompositeHBaseKeyFactory.createKey(ObjectInspector)",1,1,2
"org.apache.hadoop.hive.hbase.DataInputInputStream.DataInputInputStream(DataInput)",1,1,1
"org.apache.hadoop.hive.hbase.DataInputInputStream.from(DataInput)",2,1,2
"org.apache.hadoop.hive.hbase.DataInputInputStream.read()",1,1,2
"org.apache.hadoop.hive.hbase.DataOutputOutputStream.DataOutputOutputStream(DataOutput)",1,1,1
"org.apache.hadoop.hive.hbase.DataOutputOutputStream.from(DataOutput)",2,1,2
"org.apache.hadoop.hive.hbase.DataOutputOutputStream.write(int)",1,1,1
"org.apache.hadoop.hive.hbase.DefaultHBaseKeyFactory.createKey(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.hbase.DefaultHBaseKeyFactory.createKeyObjectInspector(TypeInfo)",1,1,1
"org.apache.hadoop.hive.hbase.DefaultHBaseKeyFactory.init(HBaseSerDeParameters,Properties)",1,1,1
"org.apache.hadoop.hive.hbase.DefaultHBaseKeyFactory.serializeKey(Object,StructField)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseCompositeKey.HBaseCompositeKey(LazySimpleStructObjectInspector)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseCompositeKey.getFieldsAsList()",1,2,2
"org.apache.hadoop.hive.hbase.HBaseCompositeKey.toLazyObject(int,byte[])",1,1,1
"org.apache.hadoop.hive.hbase.HBaseLazyObjectFactory.createLazyHBaseStructInspector(SerDeParameters,int,HBaseKeyFactory)",1,3,3
"org.apache.hadoop.hive.hbase.HBaseQTestUtil.HBaseQTestUtil(String,String,MiniClusterType,HBaseTestSetup,String,String)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseQTestUtil.cleanUp()",1,3,3
"org.apache.hadoop.hive.hbase.HBaseQTestUtil.createSources()",1,2,2
"org.apache.hadoop.hive.hbase.HBaseQTestUtil.hbaseTableSnapshotExists(HBaseAdmin,String)",3,2,3
"org.apache.hadoop.hive.hbase.HBaseQTestUtil.init()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseRowSerializer.HBaseRowSerializer(HBaseSerDeParameters)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(Object,ObjectInspector)",5,3,6
"org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(Object,ObjectInspector,int,Output)",5,11,15
"org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(Object,ObjectInspector,int,boolean)",3,3,4
"org.apache.hadoop.hive.hbase.HBaseRowSerializer.serializeField(Object,StructField,ColumnMapping,Put)",8,5,10
"org.apache.hadoop.hive.hbase.HBaseRowSerializer.serializeKeyField(Object,StructField,ColumnMapping)",3,3,4
"org.apache.hadoop.hive.hbase.HBaseScanRange.FilterDesc.FilterDesc(String,byte[])",1,1,1
"org.apache.hadoop.hive.hbase.HBaseScanRange.FilterDesc.getFactoryMethod(String,Configuration)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseScanRange.FilterDesc.toFilter(Configuration)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseScanRange.addFilter(Filter)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseScanRange.getStartRow()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseScanRange.getStopRow()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseScanRange.setStartRow(byte[])",1,1,1
"org.apache.hadoop.hive.hbase.HBaseScanRange.setStopRow(byte[])",1,1,1
"org.apache.hadoop.hive.hbase.HBaseScanRange.setup(Scan,Configuration)",3,5,6
"org.apache.hadoop.hive.hbase.HBaseScanRange.toString()",1,3,3
"org.apache.hadoop.hive.hbase.HBaseSerDe.HBaseSerDe()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDe.configureJobConf(TableDesc,JobConf)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDe.deserialize(Writable)",2,2,2
"org.apache.hadoop.hive.hbase.HBaseSerDe.getHBaseSerdeParam()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDe.getKeyFactory()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDe.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDe.getSerdeParams()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDe.initialize(Configuration,Properties)",1,2,2
"org.apache.hadoop.hive.hbase.HBaseSerDe.parseColumnsMapping(String)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDe.parseColumnsMapping(String,boolean)",5,8,13
"org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(Object,ObjectInspector)",1,1,3
"org.apache.hadoop.hive.hbase.HBaseSerDe.toString()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDeParameters.HBaseSerDeParameters(Configuration,Properties,String)",1,2,2
"org.apache.hadoop.hive.hbase.HBaseSerDeParameters.createKeyFactory(Configuration,Properties)",3,3,3
"org.apache.hadoop.hive.hbase.HBaseSerDeParameters.getBaseConfiguration()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDeParameters.getColumnMappings()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDeParameters.getColumnNames()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDeParameters.getColumnTypes()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDeParameters.getKeyColumnMapping()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDeParameters.getKeyFactory()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDeParameters.getKeyIndex()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDeParameters.getPutTimestamp()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDeParameters.getSerdeParams()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSerDeParameters.getTypeForName(String)",3,3,3
"org.apache.hadoop.hive.hbase.HBaseSerDeParameters.initKeyFactory(Configuration,Properties)",1,2,3
"org.apache.hadoop.hive.hbase.HBaseSerDeParameters.toString()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSplit.HBaseSplit()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSplit.HBaseSplit(InputSplit,Path)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSplit.HBaseSplit(TableSplit,Path)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSplit.getLength()",1,2,2
"org.apache.hadoop.hive.hbase.HBaseSplit.getLocations()",1,2,2
"org.apache.hadoop.hive.hbase.HBaseSplit.getSnapshotSplit()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSplit.getTableSplit()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseSplit.readFields(DataInput)",1,2,2
"org.apache.hadoop.hive.hbase.HBaseSplit.toString()",1,1,2
"org.apache.hadoop.hive.hbase.HBaseSplit.write(DataOutput)",1,2,2
"org.apache.hadoop.hive.hbase.HBaseStatsAggregator.aggregateStats(String,String)",2,4,4
"org.apache.hadoop.hive.hbase.HBaseStatsAggregator.cleanUp(String)",1,3,3
"org.apache.hadoop.hive.hbase.HBaseStatsAggregator.closeConnection()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStatsAggregator.connect(Configuration,Task)",1,2,2
"org.apache.hadoop.hive.hbase.HBaseStatsPublisher.closeConnection()",1,2,2
"org.apache.hadoop.hive.hbase.HBaseStatsPublisher.connect(Configuration)",1,2,2
"org.apache.hadoop.hive.hbase.HBaseStatsPublisher.init(Configuration)",1,3,3
"org.apache.hadoop.hive.hbase.HBaseStatsPublisher.publishStat(String,Map<String, String>)",4,5,7
"org.apache.hadoop.hive.hbase.HBaseStatsUtils.getBasicStat()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStatsUtils.getColumnName(String)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStatsUtils.getFamilyName()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStatsUtils.getStatFromMap(String,Map<String, String>)",2,1,2
"org.apache.hadoop.hive.hbase.HBaseStatsUtils.getSupportedStatistics()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStatsUtils.isValidStatistic(String)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStatsUtils.isValidStatisticSet(Collection<String>)",4,2,4
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.addHBaseDelegationToken(Configuration)",2,2,3
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.addHBaseResources(Configuration,Map<String, String>)",1,3,3
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.commitCreateTable(Table)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.commitDropTable(Table,boolean)",1,4,5
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.configureInputJobProperties(TableDesc,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.configureJobConf(TableDesc,JobConf)",1,1,2
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.configureOutputJobProperties(TableDesc,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.configureTableJobProperties(TableDesc,Map<String, String>)",6,12,16
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.decomposePredicate(JobConf,Deserializer,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.decomposePredicate(JobConf,HBaseSerDe,ExprNodeDesc)",3,3,6
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.getConf()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.getHBaseAdmin()",1,2,3
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.getHBaseTableName(Table)",1,5,5
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.getInputFormatClass()",2,2,2
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.getJobConf()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.getMetaHook()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.getOutputFormatClass()",2,1,2
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.getSerDeClass()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.isHBaseGenerateHFiles(Configuration)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.preCreateTable(Table)",8,9,13
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.preDropTable(Table)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.rollbackCreateTable(Table)",1,5,5
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.rollbackDropTable(Table)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseStorageHandler.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseTableSnapshotInputFormatUtil.assertSupportsTableSnapshots()",2,1,2
"org.apache.hadoop.hive.hbase.HBaseTableSnapshotInputFormatUtil.configureJob(Configuration,String,Path)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseTableSnapshotInputFormatUtil.createTableSnapshotRegionSplit()",1,2,7
"org.apache.hadoop.hive.hbase.HBaseTestCompositeKey.HBaseTestCompositeKey(LazySimpleStructObjectInspector,Properties,Configuration)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseTestCompositeKey.getField(int)",1,2,2
"org.apache.hadoop.hive.hbase.HBaseTestCompositeKey.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseTestSetup.HBaseTestSetup(Test)",1,1,1
"org.apache.hadoop.hive.hbase.HBaseTestSetup.createHBaseTable()",2,4,5
"org.apache.hadoop.hive.hbase.HBaseTestSetup.findFreePort()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseTestSetup.getConnection()",1,1,1
"org.apache.hadoop.hive.hbase.HBaseTestSetup.preTest(HiveConf)",1,1,2
"org.apache.hadoop.hive.hbase.HBaseTestSetup.setUpFixtures(HiveConf)",2,2,4
"org.apache.hadoop.hive.hbase.HBaseTestSetup.tearDown()",1,3,3
"org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.getScan(JobConf)",9,11,16
"org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.getStorageFormatOfKey(String,String)",6,4,7
"org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.getTable(JobConf)",1,1,1
"org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.createFilterScan(JobConf,int,boolean)",9,11,14
"org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getConstantVal(Object,PrimitiveObjectInspector,boolean)",11,11,11
"org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getNextBA(byte[])",1,1,1
"org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,2,5
"org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(JobConf,int)",4,5,8
"org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.newIndexPredicateAnalyzer(String,String,boolean)",1,3,3
"org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.newIndexPredicateAnalyzer(String,TypeInfo,boolean)",1,1,1
"org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.MyRecordWriter.MyRecordWriter(HTable,boolean)",1,1,1
"org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.MyRecordWriter.close(Reporter)",1,1,1
"org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.MyRecordWriter.write(ImmutableBytesWritable,Object)",3,5,5
"org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.checkOutputSpecs(FileSystem,JobConf)",1,1,2
"org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.getOutputCommitter(TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)",1,1,1
"org.apache.hadoop.hive.hbase.HiveHBaseTableSnapshotInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.hbase.HiveHBaseTableSnapshotInputFormat.getSplits(JobConf,int)",1,1,2
"org.apache.hadoop.hive.hbase.HiveHBaseTableSnapshotInputFormat.setColumns(JobConf)",1,3,3
"org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.checkOutputSpecs(FileSystem,JobConf)",1,1,1
"org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.getFamilyPath(Configuration,Properties)",1,1,1
"org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.getFileWriter(TaskAttemptContext)",1,1,2
"org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.getHiveRecordWriter(JobConf,Path,Class<? extends Writable>,boolean,Properties,Progressable)",14,13,22
"org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)",1,1,1
"org.apache.hadoop.hive.hbase.LazyHBaseCellMap.LazyHBaseCellMap(LazyMapObjectInspector)",1,1,1
"org.apache.hadoop.hive.hbase.LazyHBaseCellMap.getMap()",1,2,2
"org.apache.hadoop.hive.hbase.LazyHBaseCellMap.getMapSize()",1,2,2
"org.apache.hadoop.hive.hbase.LazyHBaseCellMap.getMapValueElement(Object)",4,5,6
"org.apache.hadoop.hive.hbase.LazyHBaseCellMap.init(Result,byte[],List<Boolean>)",1,1,1
"org.apache.hadoop.hive.hbase.LazyHBaseCellMap.init(Result,byte[],List<Boolean>,byte[])",1,1,1
"org.apache.hadoop.hive.hbase.LazyHBaseCellMap.parse()",5,6,8
"org.apache.hadoop.hive.hbase.LazyHBaseRow.LazyHBaseRow(LazySimpleStructObjectInspector)",1,1,1
"org.apache.hadoop.hive.hbase.LazyHBaseRow.LazyHBaseRow(LazySimpleStructObjectInspector,int,HBaseKeyFactory)",1,1,1
"org.apache.hadoop.hive.hbase.LazyHBaseRow.createLazyField(int,StructField)",3,3,4
"org.apache.hadoop.hive.hbase.LazyHBaseRow.getField(int)",1,1,1
"org.apache.hadoop.hive.hbase.LazyHBaseRow.getFieldsAsList()",1,3,3
"org.apache.hadoop.hive.hbase.LazyHBaseRow.getObject()",1,1,1
"org.apache.hadoop.hive.hbase.LazyHBaseRow.init(Result,ColumnMappings)",1,1,1
"org.apache.hadoop.hive.hbase.LazyHBaseRow.initFields()",1,3,3
"org.apache.hadoop.hive.hbase.LazyHBaseRow.uncheckedGetField(int)",5,6,6
"org.apache.hadoop.hive.hbase.PutWritable.PutWritable()",1,1,1
"org.apache.hadoop.hive.hbase.PutWritable.PutWritable(Put)",1,1,1
"org.apache.hadoop.hive.hbase.PutWritable.getPut()",1,1,1
"org.apache.hadoop.hive.hbase.PutWritable.readFields(DataInput)",2,2,3
"org.apache.hadoop.hive.hbase.PutWritable.write(DataOutput)",1,2,2
"org.apache.hadoop.hive.hbase.ResultWritable.ResultWritable()",1,1,1
"org.apache.hadoop.hive.hbase.ResultWritable.ResultWritable(Result)",1,1,1
"org.apache.hadoop.hive.hbase.ResultWritable.getResult()",1,1,1
"org.apache.hadoop.hive.hbase.ResultWritable.readFields(DataInput)",2,2,3
"org.apache.hadoop.hive.hbase.ResultWritable.setResult(Result)",1,1,1
"org.apache.hadoop.hive.hbase.ResultWritable.write(DataOutput)",1,2,2
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory.DoubleDollarSeparated.getObject()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory.DoubleDollarSeparated.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory.SlashSeparatedOI.SlashSeparatedOI(StructTypeInfo)",1,2,2
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory.SlashSeparatedOI.getStructFieldData(Object,StructField)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory.SlashSeparatedOI.getStructFieldsDataAsList(Object)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory.createKey(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory.createKeyObjectInspector(TypeInfo)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory.serializeKey(Object,StructField)",2,5,5
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory2.FixedLengthed.FixedLengthed(int)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory2.FixedLengthed.getObject()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory2.FixedLengthed.init(ByteArrayRef,int,int)",1,2,2
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory2.StringArrayOI.StringArrayOI(StructTypeInfo)",1,2,2
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory2.StringArrayOI.getStructFieldData(Object,StructField)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory2.StringArrayOI.getStructFieldsDataAsList(Object)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory2.createKey(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory2.createKeyObjectInspector(TypeInfo)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory2.decomposePredicate(JobConf,Deserializer,ExprNodeDesc)",2,2,3
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory2.serializeKey(Object,StructField)",2,4,4
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory2.setupFilter(String,List<IndexSearchCondition>)",11,21,25
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory2.toBinary(String,int,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory2.toBinary(byte[],int,boolean,boolean)",1,2,3
"org.apache.hadoop.hive.hbase.TestHBaseKeyFactory3.decomposePredicate(JobConf,Deserializer,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBasePredicateDecomposer.TestHBasePredicateDecomposer(ColumnMapping)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBasePredicateDecomposer.getScanRange(List<IndexSearchCondition>)",10,11,14
"org.apache.hadoop.hive.hbase.TestHBasePredicateDecomposer.toBinary(String,int,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBasePredicateDecomposer.toBinary(byte[],int,boolean,boolean)",1,2,3
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.TestStruct.TestStruct(String,String,String,boolean,byte)",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.TestStruct.getBytes()",1,3,3
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.TestStruct.getBytesWithDelimiters()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.createPropertiesForColumnPrefixes()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.createPropertiesForCompositeKeyWithSeparator()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.createPropertiesForCompositeKeyWithoutSeparator()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.createPropertiesForHiveMapHBaseColumnFamily()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.createPropertiesForHiveMapHBaseColumnFamilyII()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.createPropertiesForHiveMapHBaseColumnFamilyII_I()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.createPropertiesForHiveMapHBaseColumnFamilyII_II()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.createPropertiesII_I()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.createPropertiesII_II()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.createPropertiesII_III()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.createPropertiesI_I()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.createPropertiesI_II()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.createPropertiesI_III()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.createPropertiesI_IV()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.deserializeAndSerialize(HBaseSerDe,Result,Put,Object[])",1,3,3
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.deserializeAndSerializeHBaseCompositeKey(HBaseSerDe,Result,Put)",1,2,2
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.deserializeAndSerializeHiveMapHBaseColumnFamily(HBaseSerDe,Result[],Put[],Object[][],byte[][],byte[][],byte[][][])",1,6,6
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.deserializeAndSerializeHiveMapHBaseColumnFamilyII(HBaseSerDe,Result,Put,Object[],byte[][],byte[][])",1,4,4
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.deserializeAndSerializeHivePrefixColumnFamily(HBaseSerDe,Result,Put,Object[],int[],List<Object>,Object)",1,5,5
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.testHBaseSerDeCompositeKeyWithSeparator()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.testHBaseSerDeCompositeKeyWithoutSeparator()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.testHBaseSerDeI()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.testHBaseSerDeII()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.testHBaseSerDeWithColumnPrefixes()",1,1,1
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.testHBaseSerDeWithHiveMapToHBaseColumnFamily()",1,3,3
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.testHBaseSerDeWithHiveMapToHBaseColumnFamilyII()",1,2,2
"org.apache.hadoop.hive.hbase.TestHBaseSerDe.testHBaseSerDeWithTimestamp()",1,1,1
"org.apache.hadoop.hive.hbase.TestLazyHBaseObject.testLazyHBaseCellMap1()",1,1,1
"org.apache.hadoop.hive.hbase.TestLazyHBaseObject.testLazyHBaseCellMap2()",1,1,1
"org.apache.hadoop.hive.hbase.TestLazyHBaseObject.testLazyHBaseCellMap3()",1,1,1
"org.apache.hadoop.hive.hbase.TestLazyHBaseObject.testLazyHBaseRow1()",1,4,5
"org.apache.hadoop.hive.hbase.TestLazyHBaseObject.testLazyHBaseRow2()",1,4,5
"org.apache.hadoop.hive.hbase.TestLazyHBaseObject.testLazyHBaseRow3()",4,8,26
"org.apache.hadoop.hive.hbase.TestPutResultWritable.copy(T,T)",1,1,1
"org.apache.hadoop.hive.hbase.TestPutResultWritable.testPut()",1,2,2
"org.apache.hadoop.hive.hbase.TestPutResultWritable.testResult()",1,1,1
"org.apache.hadoop.hive.hooks.TestHs2Hooks.PostExecHook.run(HookContext)",1,2,2
"org.apache.hadoop.hive.hooks.TestHs2Hooks.PreExecHook.run(HookContext)",1,2,2
"org.apache.hadoop.hive.hooks.TestHs2Hooks.setUpBeforeClass()",1,1,1
"org.apache.hadoop.hive.hooks.TestHs2Hooks.tearDownAfterClass()",1,2,2
"org.apache.hadoop.hive.hooks.TestHs2Hooks.testIpUserName()",1,1,1
"org.apache.hadoop.hive.hwi.HWIAuth.HWIAuth()",1,1,1
"org.apache.hadoop.hive.hwi.HWIAuth.compareTo(Object)",3,1,3
"org.apache.hadoop.hive.hwi.HWIAuth.equals(Object)",7,2,7
"org.apache.hadoop.hive.hwi.HWIAuth.getGroups()",1,1,1
"org.apache.hadoop.hive.hwi.HWIAuth.getUser()",1,1,1
"org.apache.hadoop.hive.hwi.HWIAuth.hashCode()",1,2,2
"org.apache.hadoop.hive.hwi.HWIAuth.setGroups(String[])",1,1,1
"org.apache.hadoop.hive.hwi.HWIAuth.setUser(String)",1,1,1
"org.apache.hadoop.hive.hwi.HWIContextListener.contextDestroyed(ServletContextEvent)",1,2,2
"org.apache.hadoop.hive.hwi.HWIContextListener.contextInitialized(ServletContextEvent)",1,1,1
"org.apache.hadoop.hive.hwi.HWIException.HWIException()",1,1,1
"org.apache.hadoop.hive.hwi.HWIException.HWIException(String)",1,1,1
"org.apache.hadoop.hive.hwi.HWIException.HWIException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.hwi.HWIException.HWIException(Throwable)",1,1,1
"org.apache.hadoop.hive.hwi.HWIServer.HWIServer(String[])",1,1,1
"org.apache.hadoop.hive.hwi.HWIServer.main(String[])",1,1,1
"org.apache.hadoop.hive.hwi.HWIServer.start()",2,7,9
"org.apache.hadoop.hive.hwi.HWIServer.stop()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.HWISessionItem(HWIAuth,String)",1,2,3
"org.apache.hadoop.hive.hwi.HWISessionItem.addQuery(String)",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.clearQueries()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.clientKill()",2,1,2
"org.apache.hadoop.hive.hwi.HWISessionItem.clientRenew()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.clientStart()",2,1,2
"org.apache.hadoop.hive.hwi.HWISessionItem.compareTo(HWISessionItem)",2,1,2
"org.apache.hadoop.hive.hwi.HWISessionItem.equals(Object)",4,1,4
"org.apache.hadoop.hive.hwi.HWISessionItem.getAuth()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.getErrorFile()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.getHistoryViewer()",2,1,2
"org.apache.hadoop.hive.hwi.HWISessionItem.getHiveConfVar(ConfVars)",1,1,2
"org.apache.hadoop.hive.hwi.HWISessionItem.getHiveConfVar(String)",1,1,2
"org.apache.hadoop.hive.hwi.HWISessionItem.getJobTrackerURL(String)",1,3,3
"org.apache.hadoop.hive.hwi.HWISessionItem.getQueries()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.getQueryRet()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.getResultBucket()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.getResultBucketMaxSize()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.getResultFile()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.getSSIsSilent()",2,1,2
"org.apache.hadoop.hive.hwi.HWISessionItem.getSessionName()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.getStatus()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.itemInit()",1,3,4
"org.apache.hadoop.hive.hwi.HWISessionItem.killIt()",1,3,3
"org.apache.hadoop.hive.hwi.HWISessionItem.removeQuery(int)",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.run()",1,5,5
"org.apache.hadoop.hive.hwi.HWISessionItem.runQuery()",7,18,18
"org.apache.hadoop.hive.hwi.HWISessionItem.setAuth(HWIAuth)",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.setErrorFile(String)",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.setResultBucketMaxSize(int)",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.setResultFile(String)",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionItem.setSSIsSilent(boolean)",2,1,2
"org.apache.hadoop.hive.hwi.HWISessionItem.throwIfRunning()",2,1,2
"org.apache.hadoop.hive.hwi.HWISessionManager.HWISessionManager()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionManager.createSession(HWIAuth,String)",1,3,3
"org.apache.hadoop.hive.hwi.HWISessionManager.findAllSessionItems()",1,2,2
"org.apache.hadoop.hive.hwi.HWISessionManager.findAllSessionsForUser(HWIAuth)",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionManager.findAllUsersWithSessions()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionManager.findSessionItemByName(HWIAuth,String)",4,2,4
"org.apache.hadoop.hive.hwi.HWISessionManager.getItems()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionManager.isGoOn()",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionManager.run()",1,11,11
"org.apache.hadoop.hive.hwi.HWISessionManager.setGoOn(boolean)",1,1,1
"org.apache.hadoop.hive.hwi.HWISessionManager.setItems(TreeMap<HWIAuth, Set<HWISessionItem>>)",1,1,1
"org.apache.hadoop.hive.hwi.TestHWIServer.TestHWIServer(String)",1,1,1
"org.apache.hadoop.hive.hwi.TestHWIServer.setUp()",1,1,1
"org.apache.hadoop.hive.hwi.TestHWIServer.tearDown()",1,1,1
"org.apache.hadoop.hive.hwi.TestHWIServer.testServerInit()",1,3,3
"org.apache.hadoop.hive.hwi.TestHWISessionManager.TestHWISessionManager(String)",1,1,1
"org.apache.hadoop.hive.hwi.TestHWISessionManager.isFileContentEqual(File,File)",6,8,8
"org.apache.hadoop.hive.hwi.TestHWISessionManager.setUp()",1,1,1
"org.apache.hadoop.hive.hwi.TestHWISessionManager.tearDown()",1,1,1
"org.apache.hadoop.hive.hwi.TestHWISessionManager.testHiveDriver()",4,9,11
"org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.getHandlerChain()",1,1,1
"org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.getHiveIOExceptionHandlerChain(JobConf)",1,6,8
"org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(Exception)",4,4,5
"org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(Exception)",4,5,5
"org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.setHandlerChain(List<HiveIOExceptionHandler>)",1,1,1
"org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.get(JobConf)",1,2,2
"org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(Exception,JobConf)",2,2,2
"org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(Exception,JobConf)",2,2,2
"org.apache.hadoop.hive.io.HiveIOExceptionNextHandleResult.clear()",1,1,1
"org.apache.hadoop.hive.io.HiveIOExceptionNextHandleResult.getHandleResult()",1,1,1
"org.apache.hadoop.hive.io.HiveIOExceptionNextHandleResult.getHandled()",1,1,1
"org.apache.hadoop.hive.io.HiveIOExceptionNextHandleResult.setHandleResult(boolean)",1,1,1
"org.apache.hadoop.hive.io.HiveIOExceptionNextHandleResult.setHandled(boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.absolute(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.afterLast()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.beforeFirst()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.cancelRowUpdates()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.clearWarnings()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.close()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.deleteRow()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.findColumn(String)",2,1,2
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.first()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getArray(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getArray(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getAsciiStream(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getAsciiStream(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getBigDecimal(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getBigDecimal(String,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getBigDecimal(int)",4,2,4
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getBigDecimal(int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getBinaryStream(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getBinaryStream(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getBlob(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getBlob(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getBoolean(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getBoolean(int)",5,5,5
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getByte(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getByte(int)",3,2,3
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getBytes(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getBytes(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getCharacterStream(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getCharacterStream(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getClob(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getClob(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getConcurrency()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getCursorName()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getDate(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getDate(String,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getDate(int)",4,3,5
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getDate(int,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getDouble(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getDouble(int)",4,5,5
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getFetchDirection()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getFetchSize()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getFloat(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getFloat(int)",4,5,5
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getHoldability()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getInt(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getInt(int)",4,5,5
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getLong(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getLong(int)",4,5,5
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getMetaData()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getNCharacterStream(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getNCharacterStream(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getNClob(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getNClob(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getNString(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getNString(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getObject(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getObject(String,Class<T>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getObject(String,Map<String, Class<?>>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getObject(int)",3,2,5
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getObject(int,Class<T>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getObject(int,Map<String, Class<?>>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getRef(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getRef(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getRow()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getRowId(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getRowId(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getSQLXML(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getSQLXML(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getShort(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getShort(int)",4,5,5
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getStatement()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getString(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getString(int)",2,1,2
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getTime(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getTime(String,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getTime(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getTime(int,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getTimestamp(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getTimestamp(String,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getTimestamp(int)",4,2,4
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getTimestamp(int,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getType()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getURL(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getURL(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getUnicodeStream(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getUnicodeStream(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.getWarnings()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.insertRow()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.isAfterLast()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.isBeforeFirst()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.isClosed()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.isFirst()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.isLast()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.isWrapperFor(Class<?>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.last()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.moveToCurrentRow()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.moveToInsertRow()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.previous()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.refreshRow()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.relative(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.rowDeleted()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.rowInserted()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.rowUpdated()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.setFetchDirection(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.setFetchSize(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.unwrap(Class<T>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateArray(String,Array)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateArray(int,Array)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateAsciiStream(String,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateAsciiStream(String,InputStream,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateAsciiStream(String,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateAsciiStream(int,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateAsciiStream(int,InputStream,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateAsciiStream(int,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBigDecimal(String,BigDecimal)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBigDecimal(int,BigDecimal)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBinaryStream(String,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBinaryStream(String,InputStream,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBinaryStream(String,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBinaryStream(int,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBinaryStream(int,InputStream,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBinaryStream(int,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBlob(String,Blob)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBlob(String,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBlob(String,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBlob(int,Blob)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBlob(int,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBlob(int,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBoolean(String,boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBoolean(int,boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateByte(String,byte)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateByte(int,byte)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBytes(String,byte[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateBytes(int,byte[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateCharacterStream(String,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateCharacterStream(String,Reader,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateCharacterStream(String,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateCharacterStream(int,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateCharacterStream(int,Reader,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateCharacterStream(int,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateClob(String,Clob)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateClob(String,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateClob(String,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateClob(int,Clob)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateClob(int,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateClob(int,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateDate(String,Date)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateDate(int,Date)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateDouble(String,double)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateDouble(int,double)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateFloat(String,float)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateFloat(int,float)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateInt(String,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateInt(int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateLong(String,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateLong(int,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateNCharacterStream(String,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateNCharacterStream(String,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateNCharacterStream(int,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateNCharacterStream(int,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateNClob(String,NClob)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateNClob(String,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateNClob(String,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateNClob(int,NClob)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateNClob(int,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateNClob(int,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateNString(String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateNString(int,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateNull(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateNull(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateObject(String,Object)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateObject(String,Object,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateObject(int,Object)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateObject(int,Object,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateRef(String,Ref)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateRef(int,Ref)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateRow()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateRowId(String,RowId)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateRowId(int,RowId)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateSQLXML(String,SQLXML)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateSQLXML(int,SQLXML)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateShort(String,short)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateShort(int,short)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateString(String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateString(int,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateTime(String,Time)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateTime(int,Time)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateTimestamp(String,Timestamp)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.updateTimestamp(int,Timestamp)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveBaseResultSet.wasNull()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.HiveCallableStatement()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.addBatch()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.addBatch(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.cancel()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.clearBatch()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.clearParameters()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.clearWarnings()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.close()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.closeOnCompletion()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.execute()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.execute(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.execute(String,String[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.execute(String,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.execute(String,int[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.executeBatch()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.executeQuery()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.executeQuery(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.executeUpdate()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.executeUpdate(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.executeUpdate(String,String[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.executeUpdate(String,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.executeUpdate(String,int[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getArray(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getArray(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getBigDecimal(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getBigDecimal(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getBigDecimal(int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getBlob(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getBlob(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getBoolean(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getBoolean(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getByte(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getByte(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getBytes(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getBytes(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getCharacterStream(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getCharacterStream(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getClob(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getClob(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getConnection()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getDate(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getDate(String,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getDate(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getDate(int,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getDouble(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getDouble(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getFetchDirection()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getFetchSize()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getFloat(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getFloat(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getGeneratedKeys()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getInt(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getInt(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getLong(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getLong(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getMaxFieldSize()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getMaxRows()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getMetaData()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getMoreResults()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getMoreResults(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getNCharacterStream(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getNCharacterStream(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getNClob(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getNClob(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getNString(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getNString(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getObject(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getObject(String,Class<T>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getObject(String,Map<String, Class<?>>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getObject(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getObject(int,Class<T>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getObject(int,Map<String, Class<?>>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getParameterMetaData()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getQueryTimeout()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getRef(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getRef(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getResultSet()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getResultSetConcurrency()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getResultSetHoldability()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getResultSetType()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getRowId(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getRowId(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getSQLXML(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getSQLXML(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getShort(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getShort(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getString(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getString(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getTime(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getTime(String,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getTime(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getTime(int,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getTimestamp(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getTimestamp(String,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getTimestamp(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getTimestamp(int,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getURL(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getURL(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getUpdateCount()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.getWarnings()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.isCloseOnCompletion()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.isClosed()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.isPoolable()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.isWrapperFor(Class<?>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.registerOutParameter(String,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.registerOutParameter(String,int,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.registerOutParameter(String,int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.registerOutParameter(int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.registerOutParameter(int,int,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.registerOutParameter(int,int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setArray(int,Array)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setAsciiStream(String,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setAsciiStream(String,InputStream,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setAsciiStream(String,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setAsciiStream(int,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setAsciiStream(int,InputStream,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setAsciiStream(int,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBigDecimal(String,BigDecimal)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBigDecimal(int,BigDecimal)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBinaryStream(String,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBinaryStream(String,InputStream,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBinaryStream(String,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBinaryStream(int,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBinaryStream(int,InputStream,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBinaryStream(int,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBlob(String,Blob)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBlob(String,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBlob(String,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBlob(int,Blob)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBlob(int,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBlob(int,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBoolean(String,boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBoolean(int,boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setByte(String,byte)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setByte(int,byte)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBytes(String,byte[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setBytes(int,byte[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setCharacterStream(String,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setCharacterStream(String,Reader,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setCharacterStream(String,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setCharacterStream(int,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setCharacterStream(int,Reader,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setCharacterStream(int,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setClob(String,Clob)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setClob(String,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setClob(String,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setClob(int,Clob)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setClob(int,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setClob(int,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setCursorName(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setDate(String,Date)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setDate(String,Date,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setDate(int,Date)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setDate(int,Date,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setDouble(String,double)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setDouble(int,double)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setEscapeProcessing(boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setFetchDirection(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setFetchSize(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setFloat(String,float)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setFloat(int,float)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setInt(String,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setInt(int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setLong(String,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setLong(int,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setMaxFieldSize(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setMaxRows(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNCharacterStream(String,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNCharacterStream(String,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNCharacterStream(int,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNCharacterStream(int,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNClob(String,NClob)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNClob(String,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNClob(String,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNClob(int,NClob)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNClob(int,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNClob(int,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNString(String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNString(int,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNull(String,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNull(String,int,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNull(int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setNull(int,int,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setObject(String,Object)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setObject(String,Object,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setObject(String,Object,int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setObject(int,Object)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setObject(int,Object,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setObject(int,Object,int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setPoolable(boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setQueryTimeout(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setRef(int,Ref)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setRowId(String,RowId)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setRowId(int,RowId)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setSQLXML(String,SQLXML)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setSQLXML(int,SQLXML)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setShort(String,short)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setShort(int,short)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setString(String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setString(int,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setTime(String,Time)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setTime(String,Time,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setTime(int,Time)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setTime(int,Time,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setTimestamp(String,Timestamp)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setTimestamp(String,Timestamp,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setTimestamp(int,Timestamp)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setTimestamp(int,Timestamp,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setURL(String,URL)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setURL(int,URL)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.setUnicodeStream(int,InputStream,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.unwrap(Class<T>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveCallableStatement.wasNull()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.HiveConnection(HiveConf)",1,2,2
"org.apache.hadoop.hive.jdbc.HiveConnection.HiveConnection(String,Properties)",3,4,6
"org.apache.hadoop.hive.jdbc.HiveConnection.abort(Executor)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.clearWarnings()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.close()",2,3,4
"org.apache.hadoop.hive.jdbc.HiveConnection.commit()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.configureConnection()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.createArrayOf(String,Object[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.createBlob()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.createClob()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.createNClob()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.createSQLXML()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.createStatement()",2,1,2
"org.apache.hadoop.hive.jdbc.HiveConnection.createStatement(int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.createStatement(int,int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.createStruct(String,Object[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.getAutoCommit()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.getCatalog()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.getClientInfo()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.getClientInfo(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.getHoldability()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.getMetaData()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.getNetworkTimeout()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.getSchema()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.getTransactionIsolation()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.getTypeMap()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.getWarnings()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.isClosed()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.isReadOnly()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.isValid(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.isWrapperFor(Class<?>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.nativeSQL(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.prepareCall(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.prepareCall(String,int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.prepareCall(String,int,int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.prepareStatement(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.prepareStatement(String,String[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.prepareStatement(String,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.prepareStatement(String,int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.prepareStatement(String,int,int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.prepareStatement(String,int[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.releaseSavepoint(Savepoint)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.rollback()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.rollback(Savepoint)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.setAutoCommit(boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.setCatalog(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.setClientInfo(Properties)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.setClientInfo(String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.setHoldability(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.setNetworkTimeout(Executor,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.setReadOnly(boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.setSavepoint()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.setSavepoint(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.setSchema(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.setTransactionIsolation(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.setTypeMap(Map<String, Class<?>>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveConnection.unwrap(Class<T>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDataSource.HiveDataSource()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDataSource.getConnection()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDataSource.getConnection(String,String)",1,1,2
"org.apache.hadoop.hive.jdbc.HiveDataSource.getLogWriter()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDataSource.getLoginTimeout()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDataSource.getParentLogger()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDataSource.isWrapperFor(Class<?>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDataSource.setLogWriter(PrintWriter)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDataSource.setLoginTimeout(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDataSource.unwrap(Class<T>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.GetColumnsComparator.compare(JdbcColumn,JdbcColumn)",4,3,4
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.GetTablesComparator.compare(JdbcTable,JdbcTable)",2,2,2
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.HiveDatabaseMetaData(HiveInterface)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.allProceduresAreCallable()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.allTablesAreSelectable()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.autoCommitFailureClosesAllResultSets()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.convertPattern(String)",5,7,8
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.dataDefinitionCausesTransactionCommit()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.dataDefinitionIgnoredInTransactions()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.deletesAreDetected(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.doesMaxRowSizeIncludeBlobs()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.generatedKeyAlwaysReturned()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getAttributes(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getBestRowIdentifier(String,String,String,int,boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getCatalogSeparator()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getCatalogTerm()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getCatalogs()",2,2,3
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getClientInfoProperties()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getColumnPrivileges(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getColumns(String,String,String,String)",2,6,8
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getConnection()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getCrossReference(String,String,String,String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getDatabaseMajorVersion()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getDatabaseMinorVersion()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getDatabaseProductName()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getDatabaseProductVersion()",1,1,2
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getDefaultTransactionIsolation()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getDriverMajorVersion()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getDriverMinorVersion()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getDriverName()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getDriverVersion()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getExportedKeys(String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getExtraNameCharacters()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getFunctionColumns(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getFunctions(String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getIdentifierQuoteString()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getImportedKeys(String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getIndexInfo(String,String,String,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getJDBCMajorVersion()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getJDBCMinorVersion()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxBinaryLiteralLength()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxCatalogNameLength()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxCharLiteralLength()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxColumnNameLength()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxColumnsInGroupBy()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxColumnsInIndex()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxColumnsInOrderBy()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxColumnsInSelect()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxColumnsInTable()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxConnections()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxCursorNameLength()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxIndexLength()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxProcedureNameLength()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxRowSize()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxSchemaNameLength()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxStatementLength()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxStatements()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxTableNameLength()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxTablesInSelect()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getMaxUserNameLength()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getNumericFunctions()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getPrimaryKeys(String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getProcedureColumns(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getProcedureTerm()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getProcedures(String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getPseudoColumns(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getResultSetHoldability()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getRowIdLifetime()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getSQLKeywords()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getSQLStateType()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getSchemaTerm()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getSchemas()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getSchemas(String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getSearchStringEscape()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getStringFunctions()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getSuperTables(String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getSuperTypes(String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getSystemFunctions()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getTablePrivileges(String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getTableTypes()",2,2,2
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getTables(String,String,String,String[])",7,7,10
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getTimeDateFunctions()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getTypeInfo()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getUDTs(String,String,String,int[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getURL()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getUserName()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.getVersionColumns(String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.insertsAreDetected(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.isCatalogAtStart()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.isReadOnly()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.isWrapperFor(Class<?>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.locatorsUpdateCopy()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.main(String[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.nullPlusNonNullIsNull()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.nullsAreSortedAtEnd()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.nullsAreSortedAtStart()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.nullsAreSortedHigh()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.nullsAreSortedLow()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.othersDeletesAreVisible(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.othersInsertsAreVisible(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.othersUpdatesAreVisible(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.ownDeletesAreVisible(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.ownInsertsAreVisible(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.ownUpdatesAreVisible(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.storesLowerCaseIdentifiers()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.storesLowerCaseQuotedIdentifiers()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.storesMixedCaseIdentifiers()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.storesMixedCaseQuotedIdentifiers()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.storesUpperCaseIdentifiers()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.storesUpperCaseQuotedIdentifiers()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsANSI92EntryLevelSQL()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsANSI92FullSQL()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsANSI92IntermediateSQL()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsAlterTableWithAddColumn()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsAlterTableWithDropColumn()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsBatchUpdates()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsCatalogsInDataManipulation()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsCatalogsInIndexDefinitions()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsCatalogsInPrivilegeDefinitions()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsCatalogsInProcedureCalls()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsCatalogsInTableDefinitions()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsColumnAliasing()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsConvert()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsConvert(int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsCoreSQLGrammar()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsCorrelatedSubqueries()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsDataDefinitionAndDataManipulationTransactions()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsDataManipulationTransactionsOnly()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsDifferentTableCorrelationNames()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsExpressionsInOrderBy()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsExtendedSQLGrammar()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsFullOuterJoins()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsGetGeneratedKeys()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsGroupBy()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsGroupByBeyondSelect()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsGroupByUnrelated()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsIntegrityEnhancementFacility()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsLikeEscapeClause()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsLimitedOuterJoins()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsMinimumSQLGrammar()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsMixedCaseIdentifiers()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsMixedCaseQuotedIdentifiers()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsMultipleOpenResults()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsMultipleResultSets()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsMultipleTransactions()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsNamedParameters()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsNonNullableColumns()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsOpenCursorsAcrossCommit()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsOpenCursorsAcrossRollback()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsOpenStatementsAcrossCommit()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsOpenStatementsAcrossRollback()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsOrderByUnrelated()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsOuterJoins()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsPositionedDelete()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsPositionedUpdate()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsResultSetConcurrency(int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsResultSetHoldability(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsResultSetType(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsSavepoints()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsSchemasInDataManipulation()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsSchemasInIndexDefinitions()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsSchemasInPrivilegeDefinitions()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsSchemasInProcedureCalls()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsSchemasInTableDefinitions()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsSelectForUpdate()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsStatementPooling()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsStoredFunctionsUsingCallSyntax()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsStoredProcedures()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsSubqueriesInComparisons()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsSubqueriesInExists()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsSubqueriesInIns()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsSubqueriesInQuantifieds()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsTableCorrelationNames()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsTransactionIsolationLevel(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsTransactions()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsUnion()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.supportsUnionAll()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.toJdbcTableType(String)",5,4,5
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.unwrap(Class<T>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.updatesAreDetected(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.usesLocalFilePerTable()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.usesLocalFiles()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDriver.HiveDriver()",1,2,2
"org.apache.hadoop.hive.jdbc.HiveDriver.acceptsURL(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDriver.connect(String,Properties)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDriver.fetchManifestAttribute(Name)",1,1,2
"org.apache.hadoop.hive.jdbc.HiveDriver.getMajorDriverVersion()",1,2,5
"org.apache.hadoop.hive.jdbc.HiveDriver.getMajorVersion()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDriver.getMinorDriverVersion()",1,2,5
"org.apache.hadoop.hive.jdbc.HiveDriver.getMinorVersion()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDriver.getParentLogger()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDriver.getPropertyInfo(String,Properties)",1,3,4
"org.apache.hadoop.hive.jdbc.HiveDriver.jdbcCompliant()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveDriver.loadManifestAttributes()",2,1,2
"org.apache.hadoop.hive.jdbc.HiveDriver.parseURL(String,Properties)",3,5,8
"org.apache.hadoop.hive.jdbc.HiveMetaDataResultSet.HiveMetaDataResultSet(List<String>,List<String>,List<M>)",1,1,4
"org.apache.hadoop.hive.jdbc.HiveMetaDataResultSet.close()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.HivePreparedStatement(HiveInterface,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.addBatch()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.addBatch(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.cancel()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.clearBatch()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.clearParameters()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.clearWarnings()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.close()",1,2,2
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.closeOnCompletion()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.execute()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.execute(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.execute(String,String[])",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.execute(String,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.execute(String,int[])",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.executeBatch()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.executeImmediate(String)",2,4,5
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.executeQuery()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.executeQuery(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.executeUpdate()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.executeUpdate(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.executeUpdate(String,String[])",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.executeUpdate(String,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.executeUpdate(String,int[])",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getCharIndexFromSqlByParamLocation(String,char,int)",5,2,7
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getConnection()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getFetchDirection()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getFetchSize()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getGeneratedKeys()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getMaxFieldSize()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getMaxRows()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getMetaData()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getMoreResults()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getMoreResults(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getParameterMetaData()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getQueryTimeout()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getResultSet()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getResultSetConcurrency()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getResultSetHoldability()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getResultSetType()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getUpdateCount()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.getWarnings()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.isCloseOnCompletion()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.isClosed()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.isPoolable()",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.isWrapperFor(Class<?>)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setArray(int,Array)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setAsciiStream(int,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setAsciiStream(int,InputStream,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setAsciiStream(int,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setBigDecimal(int,BigDecimal)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setBinaryStream(int,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setBinaryStream(int,InputStream,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setBinaryStream(int,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setBlob(int,Blob)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setBlob(int,InputStream)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setBlob(int,InputStream,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setBoolean(int,boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setByte(int,byte)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setBytes(int,byte[])",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setCharacterStream(int,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setCharacterStream(int,Reader,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setCharacterStream(int,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setClob(int,Clob)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setClob(int,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setClob(int,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setCursorName(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setDate(int,Date)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setDate(int,Date,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setDouble(int,double)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setEscapeProcessing(boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setFetchDirection(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setFetchSize(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setFloat(int,float)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setInt(int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setLong(int,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setMaxFieldSize(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setMaxRows(int)",2,1,2
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setNCharacterStream(int,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setNCharacterStream(int,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setNClob(int,NClob)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setNClob(int,Reader)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setNClob(int,Reader,long)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setNString(int,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setNull(int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setNull(int,int,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setObject(int,Object)",10,10,10
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setObject(int,Object,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setObject(int,Object,int,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setPoolable(boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setQueryTimeout(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setRef(int,Ref)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setRowId(int,RowId)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setSQLXML(int,SQLXML)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setShort(int,short)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setString(int,String)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setTime(int,Time)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setTime(int,Time,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setTimestamp(int,Timestamp)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setTimestamp(int,Timestamp,Calendar)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setURL(int,URL)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.setUnicodeStream(int,InputStream,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.unwrap(Class<T>)",1,1,1
"org.apache.hadoop.hive.jdbc.HivePreparedStatement.updateSql(String,HashMap<Integer, String>)",1,3,3
"org.apache.hadoop.hive.jdbc.HiveQueryResultSet.HiveQueryResultSet(HiveInterface)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveQueryResultSet.HiveQueryResultSet(HiveInterface,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveQueryResultSet.close()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveQueryResultSet.convertLazyToJava(Object,ObjectInspector)",1,3,3
"org.apache.hadoop.hive.jdbc.HiveQueryResultSet.getFetchSize()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveQueryResultSet.getObject(String,Class<T>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveQueryResultSet.getObject(int,Class<T>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveQueryResultSet.initSerde()",1,8,8
"org.apache.hadoop.hive.jdbc.HiveQueryResultSet.next()",4,9,12
"org.apache.hadoop.hive.jdbc.HiveQueryResultSet.setFetchSize(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.HiveResultSetMetaData(List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getCatalogName(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getColumnClassName(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getColumnCount()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getColumnDisplaySize(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getColumnLabel(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getColumnName(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getColumnType(int)",3,2,4
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getColumnTypeName(int)",17,15,18
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getPrecision(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getScale(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getSchemaName(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.getTableName(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.isAutoIncrement(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.isCaseSensitive(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.isCurrency(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.isDefinitelyWritable(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.isNullable(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.isReadOnly(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.isSearchable(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.isSigned(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.isWrapperFor(Class<?>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.isWritable(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.unwrap(Class<T>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.HiveStatement(HiveInterface)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.addBatch(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.cancel()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.clearBatch()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.clearWarnings()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.close()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.closeOnCompletion()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.execute(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.execute(String,String[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.execute(String,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.execute(String,int[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.executeBatch()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.executeQuery(String)",2,3,4
"org.apache.hadoop.hive.jdbc.HiveStatement.executeUpdate(String)",1,2,2
"org.apache.hadoop.hive.jdbc.HiveStatement.executeUpdate(String,String[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.executeUpdate(String,int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.executeUpdate(String,int[])",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getConnection()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getFetchDirection()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getFetchSize()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getGeneratedKeys()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getMaxFieldSize()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getMaxRows()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getMoreResults()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getMoreResults(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getQueryTimeout()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getResultSet()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getResultSetConcurrency()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getResultSetHoldability()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getResultSetType()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getUpdateCount()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.getWarnings()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.isCloseOnCompletion()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.isClosed()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.isPoolable()",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.isWrapperFor(Class<?>)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.setCursorName(String)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.setEscapeProcessing(boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.setFetchDirection(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.setFetchSize(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.setMaxFieldSize(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.setMaxRows(int)",2,1,2
"org.apache.hadoop.hive.jdbc.HiveStatement.setPoolable(boolean)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.setQueryTimeout(int)",1,1,1
"org.apache.hadoop.hive.jdbc.HiveStatement.unwrap(Class<T>)",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcColumn.JdbcColumn(String,String,String,String,String,int)",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcColumn.columnDisplaySize(int)",10,5,10
"org.apache.hadoop.hive.jdbc.JdbcColumn.columnPrecision(int)",13,2,13
"org.apache.hadoop.hive.jdbc.JdbcColumn.columnScale(int)",7,2,7
"org.apache.hadoop.hive.jdbc.JdbcColumn.getColumnName()",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcColumn.getColumnSize()",1,1,2
"org.apache.hadoop.hive.jdbc.JdbcColumn.getComment()",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcColumn.getDecimalDigits()",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcColumn.getNumPrecRadix()",8,7,8
"org.apache.hadoop.hive.jdbc.JdbcColumn.getOrdinalPos()",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcColumn.getSqlType()",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcColumn.getTableCatalog()",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcColumn.getTableName()",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcColumn.getType()",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcTable.JdbcTable(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcTable.getComment()",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcTable.getSqlTableType()",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcTable.getTableCatalog()",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcTable.getTableName()",1,1,1
"org.apache.hadoop.hive.jdbc.JdbcTable.getType()",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.TestJdbcDriver(String)",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.assertDpi(DriverPropertyInfo,String,String)",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.assertPreparedStatementResultAsExpected(ResultSet)",1,2,2
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.createPreapredStatementUsingSetObject(String)",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.createPreapredStatementUsingSetXXX(String)",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.doTestErrorCase(String,String,String,int)",1,2,2
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.doTestSelectAll(String,int,int)",2,8,11
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.setUp()",1,6,6
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.tearDown()",1,1,2
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testConversionsBaseResultSet()",2,5,7
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testDataTypes()",1,2,2
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testDatabaseMetaData()",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testDescribeTable()",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testDriverProperties()",1,2,2
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testErrorMessages()",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testExplainStmt()",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testMetaDataGetCatalogs()",2,3,4
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testMetaDataGetColumns()",2,4,6
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testMetaDataGetColumnsMetaData()",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testMetaDataGetSchemas()",1,1,2
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testMetaDataGetTableTypes()",1,2,2
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testMetaDataGetTables()",1,4,5
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testNullType()",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testPrepareStatement()",2,2,7
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testResultSetMetaData()",1,2,2
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testSelectAll()",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testSelectAllFetchSize()",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testSelectAllMaxRows()",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testSelectAllPartioned()",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testSetCommand()",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testShowGrant()",1,1,1
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testShowRoleGrant()",1,2,2
"org.apache.hadoop.hive.jdbc.TestJdbcDriver.testShowTables()",1,2,3
"org.apache.hadoop.hive.jdbc.Utils.hiveTypeToSqlType(String)",15,14,15
"org.apache.hadoop.hive.metastore.AlternateFailurePreListener.AlternateFailurePreListener(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.AlternateFailurePreListener.getCallCount()",1,1,1
"org.apache.hadoop.hive.metastore.AlternateFailurePreListener.onEvent(PreEventContext)",2,1,2
"org.apache.hadoop.hive.metastore.DummyEndFunctionListener.DummyEndFunctionListener(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.DummyEndFunctionListener.onEndFunction(String,MetaStoreEndFunctionContext)",1,1,1
"org.apache.hadoop.hive.metastore.DummyJdoConnectionUrlHook.getJdoConnectionUrl(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.DummyJdoConnectionUrlHook.notifyBadConnectionUrl(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyListener.DummyListener(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.DummyListener.getLastEvent()",2,2,2
"org.apache.hadoop.hive.metastore.DummyListener.onAddPartition(AddPartitionEvent)",1,1,1
"org.apache.hadoop.hive.metastore.DummyListener.onAlterPartition(AlterPartitionEvent)",1,1,1
"org.apache.hadoop.hive.metastore.DummyListener.onAlterTable(AlterTableEvent)",1,1,1
"org.apache.hadoop.hive.metastore.DummyListener.onConfigChange(ConfigChangeEvent)",1,1,1
"org.apache.hadoop.hive.metastore.DummyListener.onCreateDatabase(CreateDatabaseEvent)",1,1,1
"org.apache.hadoop.hive.metastore.DummyListener.onCreateTable(CreateTableEvent)",1,1,1
"org.apache.hadoop.hive.metastore.DummyListener.onDropDatabase(DropDatabaseEvent)",1,1,1
"org.apache.hadoop.hive.metastore.DummyListener.onDropPartition(DropPartitionEvent)",1,1,1
"org.apache.hadoop.hive.metastore.DummyListener.onDropTable(DropTableEvent)",1,1,1
"org.apache.hadoop.hive.metastore.DummyListener.onLoadPartitionDone(LoadPartitionDoneEvent)",1,1,1
"org.apache.hadoop.hive.metastore.DummyMetaStoreInitListener.DummyMetaStoreInitListener(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.DummyMetaStoreInitListener.onInit(MetaStoreInitContext)",1,1,1
"org.apache.hadoop.hive.metastore.DummyPreListener.DummyPreListener(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.DummyPreListener.onEvent(PreEventContext)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.DummyRawStoreControlledCommit()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.addIndex(Index)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.addMasterKey(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.addPartition(Partition)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.addPartitions(String,String,List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.addRole(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.addToken(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.alterDatabase(String,Database)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.alterFunction(String,String,Function)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.alterIndex(String,String,String,Index)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.alterPartition(String,String,List<String>,Partition)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.alterPartitions(String,String,List<List<String>>,List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.alterTable(String,String,Table)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.cleanupEvents()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.commitTransaction()",2,2,2
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.createDatabase(Database)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.createFunction(Function)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.createTable(Table)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.createType(Type)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.deletePartitionColumnStatistics(String,String,String,List<String>,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.deleteTableColumnStatistics(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.doesPartitionExist(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.dropDatabase(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.dropFunction(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.dropIndex(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.dropPartition(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.dropPartitions(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.dropTable(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.dropType(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getAllDatabases()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getAllTables(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getAllTokenIdentifiers()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getColumnPrivilegeSet(String,String,String,String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getConf()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getDBPrivilegeSet(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getDatabase(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getDatabases(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getFunction(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getFunctions(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getIndex(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getIndexes(String,String,int)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getMasterKeys()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getMetaStoreSchemaVersion()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getPartition(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getPartitionColumnStatistics(String,String,List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getPartitionPrivilegeSet(String,String,String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getPartitionWithAuth(String,String,List<String>,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getPartitions(String,String,int)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getPartitionsByExpr(String,String,byte[],String,short,List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getPartitionsByFilter(String,String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getPartitionsByNames(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getPartitionsWithAuth(String,String,short,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getRole(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getTable(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getTableColumnStatistics(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getTableObjectsByName(String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getTablePrivilegeSet(String,String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getTables(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getToken(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getType(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.getUserPrivilegeSet(String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.get_aggr_stats_for(String,String,List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.grantPrivileges(PrivilegeBag)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.grantRole(Role,String,PrincipalType,String,PrincipalType,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.isPartitionMarkedForEvent(String,String,Map<String, String>,PartitionEventType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listAllTableGrants(String,PrincipalType,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listDBGrantsAll(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listGlobalGrantsAll()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listIndexNames(String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPartitionColumnGrantsAll(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPartitionGrantsAll(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPartitionNames(String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPartitionNamesByFilter(String,String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPartitionNamesPs(String,String,List<String>,short)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPartitionsPsWithAuth(String,String,List<String>,short,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPrincipalDBGrants(String,PrincipalType,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPrincipalDBGrantsAll(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPrincipalGlobalGrants(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPrincipalPartitionColumnGrants(String,PrincipalType,String,String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPrincipalPartitionColumnGrantsAll(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPrincipalPartitionGrants(String,PrincipalType,String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPrincipalPartitionGrantsAll(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPrincipalTableColumnGrants(String,PrincipalType,String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPrincipalTableColumnGrantsAll(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listPrincipalTableGrantsAll(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listRoleMembers(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listRoleNames()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listRoles(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listTableColumnGrantsAll(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listTableGrantsAll(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.listTableNamesByFilter(String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.markPartitionForEvent(String,String,Map<String, String>,PartitionEventType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.openTransaction()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.removeMasterKey(Integer)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.removeRole(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.removeToken(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.revokePrivileges(PrivilegeBag,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.revokeRole(Role,String,PrincipalType,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.rollbackTransaction()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.setCommitSucceed(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.setMetaStoreSchemaVersion(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.shutdown()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.updateMasterKey(Integer,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.updatePartitionColumnStatistics(ColumnStatistics,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.updatePartitionColumnStatistics(SetPartitionsStatsRequest)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.updateTableColumnStatistics(ColumnStatistics)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.verifySchema()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.addIndex(Index)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.addMasterKey(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.addPartition(Partition)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.addPartitions(String,String,List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.addRole(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.addToken(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.alterDatabase(String,Database)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.alterFunction(String,String,Function)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.alterIndex(String,String,String,Index)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.alterPartition(String,String,List<String>,Partition)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.alterPartitions(String,String,List<List<String>>,List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.alterTable(String,String,Table)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.cleanupEvents()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.commitTransaction()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.createDatabase(Database)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.createFunction(Function)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.createTable(Table)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.createType(Type)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.deletePartitionColumnStatistics(String,String,String,List<String>,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.deleteTableColumnStatistics(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.doesPartitionExist(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.dropDatabase(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.dropFunction(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.dropIndex(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.dropPartition(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.dropPartitions(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.dropTable(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.dropType(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getAllDatabases()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getAllTables(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getAllTokenIdentifiers()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getColumnPrivilegeSet(String,String,String,String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getConf()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getDBPrivilegeSet(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getDatabase(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getDatabases(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getFunction(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getFunctions(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getIndex(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getIndexes(String,String,int)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getMasterKeys()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getMetaStoreSchemaVersion()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getPartition(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getPartitionColumnStatistics(String,String,List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getPartitionPrivilegeSet(String,String,String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getPartitionWithAuth(String,String,List<String>,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getPartitions(String,String,int)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getPartitionsByExpr(String,String,byte[],String,short,List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getPartitionsByFilter(String,String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getPartitionsByNames(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getPartitionsWithAuth(String,String,short,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getRole(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getTable(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getTableColumnStatistics(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getTableObjectsByName(String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getTablePrivilegeSet(String,String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getTables(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getToken(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getType(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.getUserPrivilegeSet(String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.get_aggr_stats_for(String,String,List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.grantPrivileges(PrivilegeBag)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.grantRole(Role,String,PrincipalType,String,PrincipalType,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.isPartitionMarkedForEvent(String,String,Map<String, String>,PartitionEventType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listAllTableGrants(String,PrincipalType,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listDBGrantsAll(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listGlobalGrantsAll()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listIndexNames(String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPartitionColumnGrantsAll(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPartitionGrantsAll(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPartitionNames(String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPartitionNamesByFilter(String,String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPartitionNamesPs(String,String,List<String>,short)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPartitionsPsWithAuth(String,String,List<String>,short,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPrincipalDBGrants(String,PrincipalType,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPrincipalDBGrantsAll(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPrincipalGlobalGrants(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPrincipalPartitionColumnGrants(String,PrincipalType,String,String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPrincipalPartitionColumnGrantsAll(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPrincipalPartitionGrants(String,PrincipalType,String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPrincipalPartitionGrantsAll(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPrincipalTableColumnGrants(String,PrincipalType,String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPrincipalTableColumnGrantsAll(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listPrincipalTableGrantsAll(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listRoleMembers(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listRoleNames()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listRoles(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listTableColumnGrantsAll(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listTableGrantsAll(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.listTableNamesByFilter(String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.markPartitionForEvent(String,String,Map<String, String>,PartitionEventType)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.openTransaction()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.removeMasterKey(Integer)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.removeRole(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.removeToken(String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.revokePrivileges(PrivilegeBag,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.revokeRole(Role,String,PrincipalType,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.rollbackTransaction()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.setMetaStoreSchemaVersion(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.shutdown()",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.updateMasterKey(Integer,String)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.updatePartitionColumnStatistics(ColumnStatistics,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.updatePartitionColumnStatistics(SetPartitionsStatsRequest)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.updateTableColumnStatistics(ColumnStatistics)",1,1,1
"org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.verifySchema()",1,1,1
"org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartition(RawStore,Warehouse,String,String,List<String>,Partition)",11,23,30
"org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartitions(RawStore,Warehouse,String,String,List<Partition>)",1,6,8
"org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(RawStore,Warehouse,String,String,Table)",15,30,37
"org.apache.hadoop.hive.metastore.HiveAlterHandler.checkPartialPartKeysEqual(List<FieldSchema>,List<FieldSchema>)",5,2,6
"org.apache.hadoop.hive.metastore.HiveAlterHandler.constructRenamedPath(Path,Path)",1,1,1
"org.apache.hadoop.hive.metastore.HiveAlterHandler.getConf()",1,1,1
"org.apache.hadoop.hive.metastore.HiveAlterHandler.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaException.HiveMetaException()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaException.HiveMetaException(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaException.HiveMetaException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaException.HiveMetaException(Throwable)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.ChainedTTransportFactory.ChainedTTransportFactory(TTransportFactory,TTransportFactory)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.ChainedTTransportFactory.getTransport(TTransport)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.HMSHandler(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.HMSHandler(String,HiveConf)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.HMSHandler(String,HiveConf,boolean)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.PartValEqWrapper.PartValEqWrapper(Partition)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.PartValEqWrapper.equals(Object)",7,5,11
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.PartValEqWrapper.hashCode()",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.PathAndPartValSize.PathAndPartValSize(Path,int)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.abort_txn(AbortTxnRequest)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.addAdminUsers()",3,7,7
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.addPrefix(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.add_index(Index,Table)",5,7,7
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.add_index_core(RawStore,Index,Table)",5,4,10
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.add_partition(Partition)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.add_partition_core(RawStore,Partition,EnvironmentContext)",2,4,6
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.add_partition_with_environment_context(Partition,EnvironmentContext)",4,6,6
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.add_partitions(List<Partition>)",5,5,6
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.add_partitions_core(RawStore,String,String,List<Partition>,boolean)",6,11,15
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.add_partitions_req(AddPartitionsRequest)",2,3,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.alter_database(String,Database)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.alter_function(String,String,Function)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.alter_index(String,String,String,Index)",3,5,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.alter_partition(String,String,Partition)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.alter_partition_with_environment_context(String,String,Partition,EnvironmentContext)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.alter_partitions(String,String,List<Partition>)",7,15,15
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.alter_table(String,String,Table)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.alter_table_with_environment_context(String,String,Table,EnvironmentContext)",3,8,8
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.append_partition(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.append_partition_by_name(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.append_partition_by_name_with_environment_context(String,String,String,EnvironmentContext)",5,6,6
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.append_partition_common(RawStore,String,String,List<String>,EnvironmentContext)",6,8,13
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.append_partition_with_environment_context(String,String,List<String>,EnvironmentContext)",4,7,7
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.cancel_delegation_token(String)",3,5,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.check_lock(CheckLockRequest)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.commit_txn(CommitTxnRequest)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.compact(CompactionRequest)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.createDefaultDB()",1,2,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.createDefaultDB_core(RawStore)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.createDefaultRoles()",1,7,7
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.createLocationForAddedPartition(Table,Partition)",6,7,9
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.create_database(Database)",5,6,7
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.create_database_core(RawStore,Database)",4,8,8
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.create_function(Function)",3,4,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.create_role(Role)",2,1,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.create_table(Table)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.create_table_core(RawStore,Table,EnvironmentContext)",13,21,26
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.create_table_with_environment_context(Table,EnvironmentContext)",4,6,6
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.create_type(Type)",4,5,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.create_type_core(RawStore,Type)",3,3,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.deleteParentRecursive(Path,int)",1,5,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.deletePartitionData(List<Path>)",1,5,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.deleteTableData(Path)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.delete_partition_column_statistics(String,String,String,String)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.delete_table_column_statistics(String,String,String)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.dropPartitionsAndGetLocations(RawStore,String,String,Path,List<FieldSchema>,boolean)",7,12,13
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_database(String,boolean,boolean)",5,7,7
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_database_core(RawStore,String,boolean,boolean)",10,23,23
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_function(String,String)",2,2,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_index_by_name(String,String,String,boolean)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_index_by_name_core(RawStore,String,String,String,boolean)",8,6,10
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_partition(String,String,List<String>,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_partition_by_name(String,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_partition_by_name_core(RawStore,String,String,String,boolean,EnvironmentContext)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_partition_by_name_with_environment_context(String,String,String,boolean,EnvironmentContext)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_partition_common(RawStore,String,String,List<String>,boolean,EnvironmentContext)",3,10,14
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_partition_with_environment_context(String,String,List<String>,boolean,EnvironmentContext)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_partitions_req(DropPartitionsRequest)",14,33,36
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_role(String)",2,2,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_table(String,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_table_core(RawStore,String,String,boolean,EnvironmentContext)",8,11,17
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_table_with_environment_context(String,String,boolean,EnvironmentContext)",3,5,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_type(String)",3,4,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.drop_type_core(RawStore,String)",3,2,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.endFunction(String,MetaStoreEndFunctionContext)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.endFunction(String,boolean,Exception)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.endFunction(String,boolean,Exception,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.exchange_partition(Map<String, String>,String,String,String,String)",2,7,9
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.fireMetaStoreAddPartitionEvent(Table,List<Partition>,EnvironmentContext,boolean)",1,5,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.firePreEvent(PreEventContext)",2,4,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getAllPrivileges(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getConf()",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getCounters()",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getCpuProfile(int)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getHiveConf()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getIpAddress()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getMS()",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getMetaConf(String)",2,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getPartName(HiveObjectRef)",2,4,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getPartValsFromName(RawStore,String,String,String)",4,2,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getRawStore()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getRolePrincipalGrants(List<MRoleMap>)",1,4,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getStatus()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getTxnHandler()",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getVersion()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.getWh()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_aggr_stats_for(PartitionsStatsRequest)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_all_databases()",2,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_all_tables(String)",2,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_column_privilege_set(String,String,String,String,String,List<String>)",1,1,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_config_value(String,String)",5,5,7
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_database(String)",1,1,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_databases(String)",2,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_db_privilege_set(String,String,List<String>)",1,1,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_delegation_token(String,String)",3,6,6
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_fields(String,String)",5,9,9
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_function(String,String)",2,2,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_functions(String,String)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_index_by_name(String,String,String)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_index_by_name_core(RawStore,String,String,String)",2,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_index_names(String,String,short)",3,4,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_indexes(String,String,short)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_open_txns()",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_open_txns_info()",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partition(String,String,List<String>)",3,4,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partition_by_name(String,String,String)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partition_by_name_core(RawStore,String,String,String)",2,2,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partition_column_statistics(String,String,String,String)",3,2,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partition_names(String,String,short)",2,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partition_names_ps(String,String,List<String>,short)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partition_privilege_set(String,String,String,String,List<String>)",1,1,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partition_with_auth(String,String,List<String>,String,List<String>)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partitions(String,String,short)",3,4,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partitions_by_expr(PartitionsByExprRequest)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partitions_by_filter(String,String,String,short)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partitions_by_names(String,String,List<String>)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partitions_ps(String,String,List<String>,short)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partitions_ps_with_auth(String,String,List<String>,short,String,List<String>)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partitions_statistics_req(PartitionsStatsRequest)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_partitions_with_auth(String,String,short,String,List<String>)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_principals_in_role(GetPrincipalsInRoleRequest)",1,2,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_privilege_set(HiveObjectRef,String,List<String>)",6,6,6
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_role_grants_for_principal(GetRoleGrantsForPrincipalRequest)",1,2,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_role_names()",1,1,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_schema(String,String)",5,7,9
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_table(String,String)",4,4,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_table_column_statistics(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_table_names_by_filter(String,String,short)",6,6,8
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_table_objects_by_name(String,List<String>)",6,6,8
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_table_privilege_set(String,String,String,List<String>)",1,1,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_table_statistics_req(TableStatsRequest)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_tables(String,String)",2,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_type(String)",4,4,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_type_all(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.get_user_privilege_set(String,List<String>)",1,1,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.grant_privileges(PrivilegeBag)",1,1,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.grant_revoke_privileges(GrantRevokePrivilegeRequest)",2,5,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.grant_revoke_role(GrantRevokeRoleRequest)",2,5,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.grant_role(String,String,PrincipalType,String,PrincipalType,boolean)",4,2,6
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.heartbeat(HeartbeatRequest)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.heartbeat_txn_range(HeartbeatTxnRangeRequest)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.init()",1,9,9
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.initializeAddedPartition(Table,Partition,boolean)",1,8,8
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.isExternal(Table)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.isIndexTable(Table)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.isNewRoleAParent(String,String)",4,2,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.isPartitionMarkedForEvent(String,String,Map<String, String>,PartitionEventType)",7,8,8
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.isSubdirectory(Path,Path)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.is_table_exists(RawStore,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.is_type_exists(RawStore,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.list_db_privileges(String,PrincipalType,String)",4,5,7
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.list_global_privileges(String,PrincipalType)",3,4,6
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.list_partition_column_privileges(String,PrincipalType,String,String,List<String>,String)",4,5,7
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.list_partition_privileges(String,PrincipalType,String,String,List<String>)",4,5,7
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.list_privileges(String,PrincipalType,HiveObjectRef)",8,9,9
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.list_roles(String,PrincipalType)",1,3,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.list_table_column_privileges(String,PrincipalType,String,String,String)",4,5,7
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.list_table_privileges(String,PrincipalType,String,String)",4,5,7
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.lock(LockRequest)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.logAuditEvent(String)",2,2,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.logInfo(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.lowerCaseConvertPartName(String)",1,2,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.markPartitionForEvent(String,String,Map<String, String>,PartitionEventType)",8,10,10
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.newMetaException(Exception)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.newRawStore()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.open_txns(OpenTxnRequest)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.partition_name_has_valid_characters(List<String>,boolean)",2,4,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.partition_name_to_spec(String)",2,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.partition_name_to_vals(String)",2,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.removeRawStore()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.rename_partition(String,String,List<String>,Partition)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.rename_partition(String,String,List<String>,Partition,EnvironmentContext)",4,13,13
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.renew_delegation_token(String)",3,5,5
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.rethrowException(Exception)",4,4,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.revoke_privileges(PrivilegeBag)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.revoke_privileges(PrivilegeBag,boolean)",1,1,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.revoke_role(String,String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.revoke_role(String,String,PrincipalType,boolean)",2,1,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.setConf(Configuration)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.setIpAddress(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.setMetaConf(String,String)",3,2,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.set_aggr_stats_for(SetPartitionsStatsRequest)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.set_ugi(String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.show_compact(ShowCompactRequest)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.show_locks(ShowLocksRequest)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.shutdown()",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.startAddPartition(RawStore,Partition,boolean)",2,1,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.startFunction(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.startFunction(String,String)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.startMultiTableFunction(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.startPartitionFunction(String,String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.startPartitionFunction(String,String,String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.startTableFunction(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.unlock(UnlockRequest)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.update_partition_column_statistics(ColumnStatistics)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.update_partition_column_statistics(SetPartitionsStatsRequest)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.update_table_column_statistics(ColumnStatistics)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.validateFunctionInfo(Function)",3,2,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler.verifyIsWritablePath(Path)",2,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.HiveMetastoreCli.HiveMetastoreCli()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.HiveMetastoreCli.parse(String[])",1,4,4
"org.apache.hadoop.hive.metastore.HiveMetaStore.cancelDelegationToken(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.getDelegationToken(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.initializeAndStartThread(MetaStoreThread,HiveConf)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.instantiateThread(String)",2,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.isMetaStoreRemote()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.main(String[])",1,7,7
"org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(String,HiveConf)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(String,HiveConf,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.renewDelegationToken(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.signalOtherThreadsToStart(TServer,Lock,Condition,BooleanPointer)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStore.startCompactorCleaner(HiveConf)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.startCompactorInitiator(HiveConf)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.startCompactorWorkers(HiveConf)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(int,HadoopThriftAuthBridge)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(int,HadoopThriftAuthBridge,HiveConf)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(int,HadoopThriftAuthBridge,HiveConf,Lock,Condition,BooleanPointer)",3,5,9
"org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStoreThreads(HiveConf,Lock,Condition,BooleanPointer)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.HiveMetaStoreClient(HiveConf)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.HiveMetaStoreClient(HiveConf,HiveMetaHookLoader)",6,7,10
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.SynchronizedHandler.SynchronizedHandler(IMetaStoreClient)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.SynchronizedHandler.invoke(Object,Method,Object[])",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.add_partition(Partition)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.add_partition(Partition,EnvironmentContext)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.add_partitions(List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.add_partitions(List<Partition>,boolean,boolean)",2,2,4
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alterDatabase(String,Database)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alterFunction(String,String,Function)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_index(String,String,String,Index)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_partition(String,String,Partition)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_partitions(String,String,List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(String,String,Table)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(String,String,Table,EnvironmentContext)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.appendPartition(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.appendPartition(String,String,List<String>,EnvironmentContext)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.appendPartition(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.appendPartition(String,String,String,EnvironmentContext)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.appendPartitionByName(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.appendPartitionByName(String,String,String,EnvironmentContext)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.cancelDelegationToken(String)",2,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.checkLock(long)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.close()",1,5,5
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.commitTxn(long)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.compact(String,String,String,CompactionType)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(Database)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createFunction(Function)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createIndex(Index,Table)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(Table)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(Table,EnvironmentContext)",1,4,5
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createType(Type)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_role(Role)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(Table,EnvironmentContext)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.deepCopy(Database)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.deepCopy(FieldSchema)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.deepCopy(Function)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.deepCopy(Index)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.deepCopy(Partition)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.deepCopy(PrincipalPrivilegeSet)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.deepCopy(Table)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.deepCopy(Type)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.deepCopyFieldSchemas(List<FieldSchema>)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.deepCopyPartitions(Collection<Partition>,List<Partition>)",2,3,4
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.deepCopyPartitions(List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.deepCopyTables(List<Table>)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.deletePartitionColumnStatistics(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.deleteTableColumnStatistics(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(String,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(String,boolean,boolean,boolean)",2,3,6
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropFunction(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropIndex(String,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartition(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartition(String,String,List<String>,EnvironmentContext)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartition(String,String,List<String>,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartition(String,String,List<String>,boolean,EnvironmentContext)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartition(String,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartition(String,String,String,boolean,EnvironmentContext)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartitionByName(String,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartitionByName(String,String,String,boolean,EnvironmentContext)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartitions(String,String,List<ObjectPair<Integer, byte[]>>,boolean,boolean,boolean)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(String,String,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(String,String,boolean,boolean,EnvironmentContext)",4,4,10
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropType(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_role(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_table_with_environment_context(String,String,boolean,EnvironmentContext)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.exchange_partition(Map<String, String>,String,String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAggrColStatsFor(String,String,List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllDatabases()",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllTables(String)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getConfigValue(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabases(String)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDelegationToken(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDelegationToken(String,String)",2,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getFields(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getFunction(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getFunctions(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getHook(Table)",2,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getIndex(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getMetaConf(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getPartition(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getPartition(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getPartitionColumnStatistics(String,String,List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getPartitionWithAuthInfo(String,String,List<String>,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getPartitionsByNames(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getSchema(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTableColumnStatistics(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTableObjectsByName(String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTables(String,String)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTokenStrForm()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getType(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTypeAll(String)",1,3,3
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getValidTxns()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.get_principals_in_role(GetPrincipalsInRoleRequest)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.get_privilege_set(HiveObjectRef,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.get_role_grants_for_principal(GetRoleGrantsForPrincipalRequest)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.grant_privileges(PrivilegeBag)",2,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.grant_role(String,String,PrincipalType,String,PrincipalType,boolean)",2,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.heartbeat(long,long)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.heartbeatTxnRange(long,long)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.isCompatibleWith(HiveConf)",2,5,6
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.isPartitionMarkedForEvent(String,String,Map<String, String>,PartitionEventType)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listIndexNames(String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listIndexes(String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionNames(String,String,List<String>,short)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionNames(String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitions(String,String,List<String>,short)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitions(String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsByExpr(String,String,byte[],String,short,List<Partition>)",2,6,7
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsByFilter(String,String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsWithAuthInfo(String,String,List<String>,short,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsWithAuthInfo(String,String,short,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listRoleNames()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listTableNamesByFilter(String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.list_privileges(String,PrincipalType,HiveObjectRef)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.list_roles(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.lock(LockRequest)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.markPartitionForEvent(String,String,Map<String, String>,PartitionEventType)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.newSynchronizedClient(IMetaStoreClient)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open()",6,17,22
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.openTxn(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.openTxns(String,int)",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.partitionNameToSpec(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.partitionNameToVals(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.promoteRandomMetaStoreURI()",2,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect()",2,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.renamePartition(String,String,List<String>,Partition)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.renewDelegationToken(String)",2,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.revoke_privileges(PrivilegeBag,boolean)",2,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.revoke_role(String,String,PrincipalType,boolean)",2,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.rollbackTxn(long)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.setMetaConf(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.setPartitionColumnStatistics(SetPartitionsStatsRequest)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.showCompactions()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.showLocks()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.showTxns()",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.snapshotActiveConf()",1,2,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.tableExists(String)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.tableExists(String,String)",1,1,2
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.unlock(long)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.updatePartitionColumnStatistics(ColumnStatistics)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.updateTableColumnStatistics(ColumnStatistics)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreClient.validatePartitionNameCharacters(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl.deleteDir(FileSystem,Path,boolean,Configuration)",4,4,6
"org.apache.hadoop.hive.metastore.IMetaStoreClient.IncompatibleMetastoreException.IncompatibleMetastoreException(String)",1,1,1
"org.apache.hadoop.hive.metastore.IpAddressListener.IpAddressListener(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.IpAddressListener.checkIpAddress()",1,2,2
"org.apache.hadoop.hive.metastore.IpAddressListener.getIpFromInetAddress(String)",1,1,1
"org.apache.hadoop.hive.metastore.IpAddressListener.onAddPartition(AddPartitionEvent)",1,1,1
"org.apache.hadoop.hive.metastore.IpAddressListener.onAlterPartition(AlterPartitionEvent)",1,1,1
"org.apache.hadoop.hive.metastore.IpAddressListener.onAlterTable(AlterTableEvent)",1,1,1
"org.apache.hadoop.hive.metastore.IpAddressListener.onCreateDatabase(CreateDatabaseEvent)",1,1,1
"org.apache.hadoop.hive.metastore.IpAddressListener.onCreateTable(CreateTableEvent)",1,1,1
"org.apache.hadoop.hive.metastore.IpAddressListener.onDropDatabase(DropDatabaseEvent)",1,1,1
"org.apache.hadoop.hive.metastore.IpAddressListener.onDropPartition(DropPartitionEvent)",1,1,1
"org.apache.hadoop.hive.metastore.IpAddressListener.onDropTable(DropTableEvent)",1,1,1
"org.apache.hadoop.hive.metastore.IpAddressListener.onLoadPartitionDone(LoadPartitionDoneEvent)",1,1,1
"org.apache.hadoop.hive.metastore.LinearExtrapolatePartStatus.createThriftDecimal(String)",1,1,1
"org.apache.hadoop.hive.metastore.LinearExtrapolatePartStatus.extrapolate(Object[],Object[],int,Map<String, Integer>)",13,12,13
"org.apache.hadoop.hive.metastore.LockComponentBuilder.LockComponentBuilder()",1,1,1
"org.apache.hadoop.hive.metastore.LockComponentBuilder.build()",1,1,3
"org.apache.hadoop.hive.metastore.LockComponentBuilder.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.LockComponentBuilder.setExclusive()",1,1,1
"org.apache.hadoop.hive.metastore.LockComponentBuilder.setPartitionName(String)",1,1,1
"org.apache.hadoop.hive.metastore.LockComponentBuilder.setSemiShared()",1,1,1
"org.apache.hadoop.hive.metastore.LockComponentBuilder.setShared()",1,1,1
"org.apache.hadoop.hive.metastore.LockComponentBuilder.setTableName(String)",1,1,1
"org.apache.hadoop.hive.metastore.LockRequestBuilder.LockRequestBuilder()",1,1,1
"org.apache.hadoop.hive.metastore.LockRequestBuilder.LockTrie.LockTrie()",1,1,1
"org.apache.hadoop.hive.metastore.LockRequestBuilder.LockTrie.add(LockComponent)",1,2,2
"org.apache.hadoop.hive.metastore.LockRequestBuilder.LockTrie.addLocksToRequest(LockRequest)",1,4,4
"org.apache.hadoop.hive.metastore.LockRequestBuilder.LockTrie.setPart(LockComponent,PartTrie)",1,5,5
"org.apache.hadoop.hive.metastore.LockRequestBuilder.LockTrie.setTable(LockComponent,TableTrie)",1,2,2
"org.apache.hadoop.hive.metastore.LockRequestBuilder.addLockComponent(LockComponent)",1,1,1
"org.apache.hadoop.hive.metastore.LockRequestBuilder.build()",2,1,3
"org.apache.hadoop.hive.metastore.LockRequestBuilder.setTransactionId(long)",1,1,1
"org.apache.hadoop.hive.metastore.LockRequestBuilder.setUser(String)",1,1,2
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.MetaStoreDirectSql(PersistenceManager)",1,5,5
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.PartitionFilterGenerator.FilterType.fromClass(Object)",4,1,4
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.PartitionFilterGenerator.FilterType.fromType(String)",4,3,4
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.PartitionFilterGenerator.PartitionFilterGenerator(Table,List<Object>,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.PartitionFilterGenerator.beginTreeNode(TreeNode)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.PartitionFilterGenerator.endTreeNode(TreeNode)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.PartitionFilterGenerator.generateSqlFilter(Table,ExpressionTree,List<Object>,List<String>)",5,3,5
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.PartitionFilterGenerator.midTreeNode(TreeNode)",1,1,2
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.PartitionFilterGenerator.shouldStop()",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.PartitionFilterGenerator.visit(LeafNode)",6,13,18
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.aggrColStatsForPartitions(String,String,List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.columnStatisticsObjForPartitions(String,String,List<String>,List<String>,long)",12,33,34
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.ensureList(Object)",2,2,2
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.extractSqlBoolean(Object)",6,3,8
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.extractSqlInt(Object)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionStats(String,String,List<String>,List<String>)",5,11,11
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitions(String,String,Integer)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(String,String,List<String>,Integer)",2,1,2
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(Table,ExpressionTree,Integer)",2,1,2
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(String,String,Boolean,String,List<? extends Object>,List<String>,Integer)",10,30,46
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getTableStats(String,String,List<String>)",4,5,7
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.isCompatibleDatastore()",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.isViewTable(String,String)",1,2,2
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.isViewTable(Table)",1,2,2
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.loopJoinOrderedResult(TreeMap<Long, T>,String,int,ApplyFunc<T>)",7,9,12
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.makeColumnStats(List<Object[]>,ColumnStatisticsDesc,int)",1,5,5
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.makeParams(int)",1,2,2
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.partsFoundForPartitions(String,String,List<String>,List<String>)",1,4,5
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.prepareCSObj(Object[],int)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.prepareParams(String,String,List<String>,List<String>)",1,1,3
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.setAnsiQuotesForMysql()",1,2,2
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.timingTrace(boolean,String,long,long)",2,1,2
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.trimCommaList(StringBuilder)",1,2,2
"org.apache.hadoop.hive.metastore.MetaStoreDirectSql.trySetAnsiQuotesForMysql()",1,3,3
"org.apache.hadoop.hive.metastore.MetaStoreEndFunctionContext.MetaStoreEndFunctionContext(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEndFunctionContext.MetaStoreEndFunctionContext(boolean,Exception,String)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEndFunctionContext.getException()",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEndFunctionContext.getInputTableName()",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEndFunctionContext.isSuccess()",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEndFunctionListener.MetaStoreEndFunctionListener(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEndFunctionListener.exportCounters(AbstractMap<String, Long>)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEndFunctionListener.getConf()",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEndFunctionListener.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEventListener.MetaStoreEventListener(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEventListener.getConf()",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEventListener.onAddPartition(AddPartitionEvent)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEventListener.onAlterPartition(AlterPartitionEvent)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEventListener.onAlterTable(AlterTableEvent)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEventListener.onConfigChange(ConfigChangeEvent)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEventListener.onCreateDatabase(CreateDatabaseEvent)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEventListener.onCreateTable(CreateTableEvent)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEventListener.onDropDatabase(DropDatabaseEvent)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEventListener.onDropPartition(DropPartitionEvent)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEventListener.onDropTable(DropTableEvent)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEventListener.onLoadPartitionDone(LoadPartitionDoneEvent)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreEventListener.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreInit.getConnectionURL(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreInit.initConnectionUrlHook(HiveConf,MetaStoreInitData)",2,2,4
"org.apache.hadoop.hive.metastore.MetaStoreInit.updateConnectionURL(HiveConf,Configuration,String,MetaStoreInitData)",2,6,6
"org.apache.hadoop.hive.metastore.MetaStoreInitListener.MetaStoreInitListener(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreInitListener.getConf()",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreInitListener.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStorePreEventListener.MetaStorePreEventListener(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStorePreEventListener.getConf()",1,1,1
"org.apache.hadoop.hive.metastore.MetaStorePreEventListener.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.MetaStoreSchemaInfo(String,HiveConf,String)",1,2,4
"org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.generateInitFileName(String)",2,2,3
"org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.generateUpgradeFileName(String)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.getHiveSchemaVersion()",2,1,2
"org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.getMetaStoreScriptDir()",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.getUpgradeScripts(String)",3,3,6
"org.apache.hadoop.hive.metastore.MetaStoreThread.BooleanPointer.BooleanPointer()",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.areColTypesCompatible(String,String)",3,2,4
"org.apache.hadoop.hive.metastore.MetaStoreUtils.canDropPartition(Table,Partition)",1,1,4
"org.apache.hadoop.hive.metastore.MetaStoreUtils.compareFieldColumns(List<FieldSchema>,List<FieldSchema>)",9,4,9
"org.apache.hadoop.hive.metastore.MetaStoreUtils.containsAllFastStats(Map<String, String>)",3,2,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.createColumnsetSchema(String,List<String>,List<String>,Configuration)",2,3,4
"org.apache.hadoop.hive.metastore.MetaStoreUtils.deleteWHDirectory(Path,Configuration,boolean)",7,7,10
"org.apache.hadoop.hive.metastore.MetaStoreUtils.determineFieldComment(String)",1,1,2
"org.apache.hadoop.hive.metastore.MetaStoreUtils.findFreePort()",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getArchivingLevel(Partition)",3,2,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getClass(String)",1,1,2
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getColumnNamesFromFieldSchema(List<FieldSchema>)",1,3,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getColumnTypesFromFieldSchema(List<FieldSchema>)",1,3,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getDDLFromFieldSchema(String,List<FieldSchema>)",1,3,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(Configuration,Partition,Table)",1,2,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(Configuration,Table)",2,2,4
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getFieldSchemaFromTypeInfo(String,TypeInfo)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getFieldsFromDeserializer(String,Deserializer)",7,12,12
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getFullDDLFromFieldSchema(String,List<FieldSchema>)",1,3,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getIndexTableName(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getListType(String)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getMapType(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getMetaStoreListeners(Class<T>,HiveConf,String)",3,3,5
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getMetaStoreSaslProperties(HiveConf)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getOriginalLocation(Partition)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getPartSchemaFromTableSchema(StorageDescriptor,StorageDescriptor,Map<String, String>,String,String,List<FieldSchema>,Properties)",4,19,20
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getPartitionMetadata(Partition,Table)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getPartitionValWithInvalidCharacter(List<String>,Pattern)",4,2,4
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getProtectMode(Map<String, String>)",3,2,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getProtectMode(Partition)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getProtectMode(Table)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getPvals(List<FieldSchema>,Map<String, String>)",1,2,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getQualifiedName(String,String)",2,1,2
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(Partition,Table)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(StorageDescriptor,StorageDescriptor,Map<String, String>,String,String,List<FieldSchema>)",1,21,22
"org.apache.hadoop.hive.metastore.MetaStoreUtils.getTableMetadata(Table)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.isArchived(Partition)",2,1,2
"org.apache.hadoop.hive.metastore.MetaStoreUtils.isDirEmpty(FileSystem,Path)",3,2,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.isExternalTable(Table)",3,1,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.isImmutableTable(Table)",3,1,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.isIndexTable(Table)",2,1,2
"org.apache.hadoop.hive.metastore.MetaStoreUtils.isNonNativeTable(Table)",2,1,2
"org.apache.hadoop.hive.metastore.MetaStoreUtils.isView(Table)",2,1,2
"org.apache.hadoop.hive.metastore.MetaStoreUtils.logAndThrowMetaException(Exception)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.loopUntilHMSReady(int)",3,3,4
"org.apache.hadoop.hive.metastore.MetaStoreUtils.makeDir(Path,HiveConf)",1,2,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.makeFilterStringFromMap(Map<String, String>)",1,3,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(Class<T>,Class<?>[],Object[])",4,3,5
"org.apache.hadoop.hive.metastore.MetaStoreUtils.partitionNameHasValidCharacters(List<String>,Pattern)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.populateQuickStats(FileStatus[],Map<String, String>)",1,3,3
"org.apache.hadoop.hive.metastore.MetaStoreUtils.pvalMatches(List<String>,List<String>)",4,3,5
"org.apache.hadoop.hive.metastore.MetaStoreUtils.recursiveDelete(File)",2,4,4
"org.apache.hadoop.hive.metastore.MetaStoreUtils.requireCalStats(Configuration,Partition,Partition,Table)",9,7,13
"org.apache.hadoop.hive.metastore.MetaStoreUtils.setSerdeParam(SerDeInfo,Properties,String)",1,2,2
"org.apache.hadoop.hive.metastore.MetaStoreUtils.startMetaStore(int,HadoopThriftAuthBridge)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.startMetaStore(int,HadoopThriftAuthBridge,HiveConf)",1,2,2
"org.apache.hadoop.hive.metastore.MetaStoreUtils.throwExceptionIfIncompatibleColTypeChange(List<FieldSchema>,List<FieldSchema>)",2,4,4
"org.apache.hadoop.hive.metastore.MetaStoreUtils.typeToThriftType(String)",1,4,5
"org.apache.hadoop.hive.metastore.MetaStoreUtils.updatePartitionStatsFast(Partition,Warehouse)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.updatePartitionStatsFast(Partition,Warehouse,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.updatePartitionStatsFast(Partition,Warehouse,boolean,boolean)",1,7,8
"org.apache.hadoop.hive.metastore.MetaStoreUtils.updateUnpartitionedTableStatsFast(Database,Table,Warehouse,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.updateUnpartitionedTableStatsFast(Database,Table,Warehouse,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.updateUnpartitionedTableStatsFast(Table,FileStatus[],boolean,boolean)",1,7,8
"org.apache.hadoop.hive.metastore.MetaStoreUtils.validateColumnName(String)",1,1,1
"org.apache.hadoop.hive.metastore.MetaStoreUtils.validateColumnType(String)",4,4,5
"org.apache.hadoop.hive.metastore.MetaStoreUtils.validateName(String)",2,1,2
"org.apache.hadoop.hive.metastore.MetaStoreUtils.validatePartitionNameCharacters(List<String>,Pattern)",2,2,2
"org.apache.hadoop.hive.metastore.MetaStoreUtils.validateSkewedColNames(List<String>)",4,2,4
"org.apache.hadoop.hive.metastore.MetaStoreUtils.validateSkewedColNamesSubsetCol(List<String>,List<FieldSchema>)",3,2,4
"org.apache.hadoop.hive.metastore.MetaStoreUtils.validateTblColumns(List<FieldSchema>)",4,4,4
"org.apache.hadoop.hive.metastore.ObjectStore.GetHelper.GetHelper(String,String,boolean,boolean)",2,7,9
"org.apache.hadoop.hive.metastore.ObjectStore.GetHelper.close()",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.GetHelper.commit()",1,2,3
"org.apache.hadoop.hive.metastore.ObjectStore.GetHelper.disableDirectSql()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.GetHelper.getTable()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.GetHelper.handleDirectSqlError(Exception)",3,6,8
"org.apache.hadoop.hive.metastore.ObjectStore.GetHelper.run(boolean)",1,5,7
"org.apache.hadoop.hive.metastore.ObjectStore.GetHelper.setResult(T)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.GetHelper.start(boolean)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.GetListHelper.GetListHelper(String,String,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.GetListHelper.describeResult()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.GetStatHelper.GetStatHelper(String,String,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.GetStatHelper.describeResult()",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.LikeChecker.hasLike()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.LikeChecker.shouldStop()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.LikeChecker.visit(LeafNode)",1,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.ObjectStore()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMDatabaseURIRetVal.UpdateMDatabaseURIRetVal(List<String>,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMDatabaseURIRetVal.getBadRecords()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMDatabaseURIRetVal.getUpdateLocations()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMDatabaseURIRetVal.setBadRecords(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMDatabaseURIRetVal.setUpdateLocations(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMStorageDescriptorTblPropURIRetVal.UpdateMStorageDescriptorTblPropURIRetVal(List<String>,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMStorageDescriptorTblPropURIRetVal.getBadRecords()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMStorageDescriptorTblPropURIRetVal.getUpdateLocations()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMStorageDescriptorTblPropURIRetVal.setBadRecords(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMStorageDescriptorTblPropURIRetVal.setUpdateLocations(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMStorageDescriptorTblURIRetVal.UpdateMStorageDescriptorTblURIRetVal(List<String>,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMStorageDescriptorTblURIRetVal.getBadRecords()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMStorageDescriptorTblURIRetVal.getUpdateLocations()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMStorageDescriptorTblURIRetVal.setBadRecords(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateMStorageDescriptorTblURIRetVal.setUpdateLocations(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateSerdeURIRetVal.UpdateSerdeURIRetVal(List<String>,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateSerdeURIRetVal.getBadRecords()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateSerdeURIRetVal.getUpdateLocations()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateSerdeURIRetVal.setBadRecords(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.UpdateSerdeURIRetVal.setUpdateLocations(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.addIndex(Index)",2,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.addMasterKey(String)",2,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.addPartition(Partition)",1,8,8
"org.apache.hadoop.hive.metastore.ObjectStore.addPartitions(String,String,List<Partition>)",3,10,11
"org.apache.hadoop.hive.metastore.ObjectStore.addRole(String,String)",2,2,3
"org.apache.hadoop.hive.metastore.ObjectStore.addToken(String,String)",1,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.alterDatabase(String,Database)",2,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.alterFunction(String,String,Function)",3,2,4
"org.apache.hadoop.hive.metastore.ObjectStore.alterIndex(String,String,String,Index)",3,2,4
"org.apache.hadoop.hive.metastore.ObjectStore.alterPartition(String,String,List<String>,Partition)",2,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.alterPartitionNoTxn(String,String,List<String>,Partition)",2,4,6
"org.apache.hadoop.hive.metastore.ObjectStore.alterPartitions(String,String,List<List<String>>,List<Partition>)",2,4,5
"org.apache.hadoop.hive.metastore.ObjectStore.alterTable(String,String,Table)",3,2,4
"org.apache.hadoop.hive.metastore.ObjectStore.checkSchema()",6,5,6
"org.apache.hadoop.hive.metastore.ObjectStore.cleanupEvents()",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction()",4,6,6
"org.apache.hadoop.hive.metastore.ObjectStore.converToMSerDeInfo(SerDeInfo)",2,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.converToSerDeInfo(MSerDeInfo)",2,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertDB(List<MDBPrivilege>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertGlobal(List<MGlobalPrivilege>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertList(List<T>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertMap(Map<K, V>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertPartCols(List<MPartitionColumnPrivilege>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertPartition(List<MPartitionPrivilege>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertTable(List<MTablePrivilege>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertTableCols(List<MTableColumnPrivilege>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertToFieldSchemas(List<MFieldSchema>)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.convertToFunction(MFunction)",2,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertToIndex(MIndex)",2,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertToMFieldSchemas(List<FieldSchema>)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.convertToMFunction(Function)",2,2,3
"org.apache.hadoop.hive.metastore.ObjectStore.convertToMIndex(Index)",4,1,4
"org.apache.hadoop.hive.metastore.ObjectStore.convertToMOrders(List<Order>)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.convertToMPart(Partition,boolean)",3,7,9
"org.apache.hadoop.hive.metastore.ObjectStore.convertToMResourceUriList(List<ResourceUri>)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.convertToMStorageDescriptor(StorageDescriptor)",2,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertToMStorageDescriptor(StorageDescriptor,MColumnDescriptor)",2,4,5
"org.apache.hadoop.hive.metastore.ObjectStore.convertToMStringLists(List<List<String>>)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.convertToMTable(Table)",2,6,7
"org.apache.hadoop.hive.metastore.ObjectStore.convertToOrders(List<MOrder>)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.convertToPart(MPartition)",2,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertToPart(String,String,MPartition)",2,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertToParts(List<MPartition>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.convertToParts(List<MPartition>,List<Partition>)",2,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.convertToParts(String,String,List<MPartition>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.convertToResourceUriList(List<MResourceUri>)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.convertToSkewedValues(List<MStringList>)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.convertToStorageDescriptor(MStorageDescriptor)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.convertToStorageDescriptor(MStorageDescriptor,boolean)",2,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.convertToTable(MTable)",2,4,5
"org.apache.hadoop.hive.metastore.ObjectStore.copyMSD(MStorageDescriptor,MStorageDescriptor)",1,8,8
"org.apache.hadoop.hive.metastore.ObjectStore.covertToMapMStringList(Map<List<String>, String>)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.covertToSkewedMap(Map<MStringList, String>)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(Database)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.createExpressionProxy(Configuration)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.createFunction(Function)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.createNewMColumnDescriptor(List<MFieldSchema>)",2,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.createTable(Table)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.createType(Type)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.debugLog(String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.deletePartitionColumnStatistics(String,String,String,List<String>,String)",7,6,11
"org.apache.hadoop.hive.metastore.ObjectStore.deleteTableColumnStatistics(String,String,String)",6,6,10
"org.apache.hadoop.hive.metastore.ObjectStore.detachCdsFromSdsNoTxn(String,String,List<String>)",1,4,4
"org.apache.hadoop.hive.metastore.ObjectStore.doesPartitionExist(String,String,List<String>)",2,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.dropDatabase(String)",1,5,5
"org.apache.hadoop.hive.metastore.ObjectStore.dropFunction(String,String)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.dropIndex(String,String,String)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.dropPartition(String,String,List<String>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.dropPartitionAllColumnGrantsNoTxn(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.dropPartitionColumnStatisticsNoTxn(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.dropPartitionCommon(MPartition)",1,9,9
"org.apache.hadoop.hive.metastore.ObjectStore.dropPartitionGrantsNoTxn(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.dropPartitions(String,String,List<String>)",3,3,5
"org.apache.hadoop.hive.metastore.ObjectStore.dropPartitionsNoTxn(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.dropTable(String,String)",1,12,12
"org.apache.hadoop.hive.metastore.ObjectStore.dropType(String)",1,4,4
"org.apache.hadoop.hive.metastore.ObjectStore.ensureGetMTable(String,String)",2,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.ensureGetTable(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.executeJDOQLSelect(String)",2,2,3
"org.apache.hadoop.hive.metastore.ObjectStore.executeJDOQLUpdate(String)",2,2,3
"org.apache.hadoop.hive.metastore.ObjectStore.getAllDatabases()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.getAllRoleAncestors(Set<String>,List<MRoleMap>)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.getAllTables(String)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.getAllTokenIdentifiers()",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.getCallStack()",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.getColumnPrivilege(String,String,String,String,String,PrincipalType)",4,8,8
"org.apache.hadoop.hive.metastore.ObjectStore.getColumnPrivilegeSet(String,String,String,String,String,List<String>)",1,9,9
"org.apache.hadoop.hive.metastore.ObjectStore.getConf()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.getDBPrivilege(String,String,PrincipalType)",3,5,5
"org.apache.hadoop.hive.metastore.ObjectStore.getDBPrivilegeSet(String,String,List<String>)",1,9,9
"org.apache.hadoop.hive.metastore.ObjectStore.getDataSourceProps(Configuration)",1,12,12
"org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(String)",1,4,4
"org.apache.hadoop.hive.metastore.ObjectStore.getDatabases(String)",1,4,5
"org.apache.hadoop.hive.metastore.ObjectStore.getFilterParser(String)",2,1,3
"org.apache.hadoop.hive.metastore.ObjectStore.getFunction(String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.getFunctions(String,String)",1,4,5
"org.apache.hadoop.hive.metastore.ObjectStore.getIndex(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.getIndexes(String,String,int)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.getMDatabase(String)",2,2,3
"org.apache.hadoop.hive.metastore.ObjectStore.getMFunction(String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.getMIndex(String,String,String)",2,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.getMPartition(String,String,List<String>)",2,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.getMPartitionColumnStatistics(Table,List<String>,List<String>)",3,3,8
"org.apache.hadoop.hive.metastore.ObjectStore.getMPartitionsViaOrmFilter(String,String,List<String>,Out<Query>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.getMRole(String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion()",4,4,6
"org.apache.hadoop.hive.metastore.ObjectStore.getMSecurityUserRoleMap(String,PrincipalType,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.getMTable(String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.getMTableColumnStatistics(Table,List<String>)",4,5,7
"org.apache.hadoop.hive.metastore.ObjectStore.getMType(Type)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.getMasterKeys()",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.getMetaStoreSchemaVersion()",1,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.getPMF()",1,7,7
"org.apache.hadoop.hive.metastore.ObjectStore.getPartQueryWithParams(String,String,List<String>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.getPartition(String,String,List<String>)",2,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionColumnStatistics(String,String,List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionColumnStatisticsInternal(String,String,List<String>,List<String>,boolean,boolean)",4,7,8
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionNamesNoTxn(String,String,short)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionNamesPrunedByExprNoTxn(Table,byte[],String,short,List<String>)",1,4,4
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionPrivilege(String,String,String,String,PrincipalType)",3,5,5
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionPrivilegeSet(String,String,String,String,List<String>)",1,9,9
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionPsQueryResults(String,String,List<String>,short,String)",3,4,7
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionStr(Table,Map<String, String>)",4,4,4
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionWithAuth(String,String,List<String>,String,List<String>)",2,4,4
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(String,String,int)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(String,String,byte[],String,short,List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(String,String,byte[],String,short,List<Partition>,boolean,boolean)",1,6,6
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(String,String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(String,String,String,short,boolean,boolean)",1,4,5
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNames(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNamesInternal(String,String,List<String>,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(String,String,int,boolean,boolean)",1,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsViaOrmFilter(String,String,List<String>)",2,2,3
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsViaOrmFilter(Table,ExpressionTree,short,boolean)",2,2,3
"org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsWithAuth(String,String,short,String,List<String>)",1,6,6
"org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager()",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.getPrincipalTypeFromStr(String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.getRole(String)",2,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.getTable(String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatistics(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatisticsInternal(String,String,List<String>,boolean,boolean)",2,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.getTableObjectsByName(String,List<String>)",2,5,6
"org.apache.hadoop.hive.metastore.ObjectStore.getTablePrivilege(String,String,String,PrincipalType)",3,5,5
"org.apache.hadoop.hive.metastore.ObjectStore.getTablePrivilegeSet(String,String,String,List<String>)",1,9,9
"org.apache.hadoop.hive.metastore.ObjectStore.getTables(String,String)",1,4,5
"org.apache.hadoop.hive.metastore.ObjectStore.getToken(String)",1,4,4
"org.apache.hadoop.hive.metastore.ObjectStore.getTokenFrom(String)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.getType(MType)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.getType(String)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.getUserPrivilegeSet(String,List<String>)",1,10,10
"org.apache.hadoop.hive.metastore.ObjectStore.get_aggr_stats_for(String,String,List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.grantPrivileges(PrivilegeBag)",26,47,49
"org.apache.hadoop.hive.metastore.ObjectStore.grantRole(Role,String,PrincipalType,String,PrincipalType,boolean)",2,4,5
"org.apache.hadoop.hive.metastore.ObjectStore.initialize(Properties)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.isActiveTransaction()",2,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.isPartitionMarkedForEvent(String,String,Map<String, String>,PartitionEventType)",2,3,5
"org.apache.hadoop.hive.metastore.ObjectStore.listAllRolesInHierarchy(String,List<String>)",1,4,4
"org.apache.hadoop.hive.metastore.ObjectStore.listAllTableGrants(String,PrincipalType,String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listAllTableGrants(String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listDBGrantsAll(String)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.listDatabaseGrants(String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listFSRoots()",2,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.listGlobalGrantsAll()",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listIndexNames(String,String,short)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.listMIndexes(String,String,int)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listMPartitions(String,String,int)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.listMSecurityPrincipalMembershipRole(String,PrincipalType)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listPartitionAllColumnGrants(String,String,List<String>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listPartitionColumnGrantsAll(String,String,String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listPartitionGrants(String,String,List<String>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listPartitionGrantsAll(String,String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listPartitionNames(String,String,short)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listPartitionNamesByFilter(String,String,String,short)",2,4,5
"org.apache.hadoop.hive.metastore.ObjectStore.listPartitionNamesPs(String,String,List<String>,short)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.listPartitionsPsWithAuth(String,String,List<String>,short,String,List<String>)",1,6,6
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalAllDBGrant(String,PrincipalType)",1,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalAllPartitionColumnGrants(String,PrincipalType)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalAllPartitionGrants(String,PrincipalType)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalAllTableColumnGrants(String,PrincipalType)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalAllTableGrants(String,PrincipalType)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalDBGrants(String,PrincipalType,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalDBGrantsAll(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalGlobalGrants(String,PrincipalType)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalPartitionColumnGrants(String,PrincipalType,String,String,String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalPartitionColumnGrantsAll(String,PrincipalType)",1,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalPartitionGrants(String,PrincipalType,String,String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalPartitionGrantsAll(String,PrincipalType)",1,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalTableColumnGrants(String,PrincipalType,String,String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalTableColumnGrantsAll(String,PrincipalType)",1,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.listPrincipalTableGrantsAll(String,PrincipalType)",1,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.listRoleMembers(String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listRoleNames()",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.listRoles(String,PrincipalType)",1,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.listStorageDescriptorsWithCD(MColumnDescriptor,long)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.listTableAllColumnGrants(String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listTableAllPartitionColumnGrants(String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listTableAllPartitionGrants(String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listTableColumnGrantsAll(String,String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listTableGrantsAll(String,String)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.listTableNamesByFilter(String,String,short)",1,5,5
"org.apache.hadoop.hive.metastore.ObjectStore.makeExpressionTree(String)",3,3,6
"org.apache.hadoop.hive.metastore.ObjectStore.makeParameterDeclarationString(Map<String, String>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.makeParameterDeclarationStringObj(Map<String, Object>)",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.makeQueryByPartitionNames(String,String,List<String>,Class<?>,String,String,String)",1,1,3
"org.apache.hadoop.hive.metastore.ObjectStore.makeQueryFilterString(String,MTable,String,Map<String, Object>)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.makeQueryFilterString(String,Table,ExpressionTree,Map<String, Object>,boolean)",2,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.markPartitionForEvent(String,String,Map<String, String>,PartitionEventType)",2,2,3
"org.apache.hadoop.hive.metastore.ObjectStore.openTransaction()",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.preDropStorageDescriptor(MStorageDescriptor)",2,2,3
"org.apache.hadoop.hive.metastore.ObjectStore.putPersistentPrivObjects(MTable,List<Object>,int,Map<String, List<PrivilegeGrantInfo>>,PrincipalType)",5,4,5
"org.apache.hadoop.hive.metastore.ObjectStore.queryByPartitionNames(String,String,List<String>,Class<T>,String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.ObjectStore.removeMasterKey(Integer)",1,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.removeRole(String)",1,11,11
"org.apache.hadoop.hive.metastore.ObjectStore.removeToken(String)",1,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.removeUnusedColumnDescriptor(MColumnDescriptor)",2,4,5
"org.apache.hadoop.hive.metastore.ObjectStore.revokePrivileges(PrivilegeBag,boolean)",50,54,57
"org.apache.hadoop.hive.metastore.ObjectStore.revokeRole(Role,String,PrincipalType,boolean)",3,4,4
"org.apache.hadoop.hive.metastore.ObjectStore.rollbackTransaction()",2,4,4
"org.apache.hadoop.hive.metastore.ObjectStore.setConf(Configuration)",2,2,3
"org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(String,String)",1,2,3
"org.apache.hadoop.hive.metastore.ObjectStore.shouldUpdateURI(URI,URI)",8,3,8
"org.apache.hadoop.hive.metastore.ObjectStore.shutdown()",1,2,2
"org.apache.hadoop.hive.metastore.ObjectStore.updateMDatabaseURI(URI,URI,boolean)",1,8,9
"org.apache.hadoop.hive.metastore.ObjectStore.updateMStorageDescriptorTblPropURI(URI,URI,String,boolean)",1,9,10
"org.apache.hadoop.hive.metastore.ObjectStore.updateMStorageDescriptorTblURI(URI,URI,boolean)",1,8,9
"org.apache.hadoop.hive.metastore.ObjectStore.updateMasterKey(Integer,String)",3,3,5
"org.apache.hadoop.hive.metastore.ObjectStore.updatePartitionColumnStatistics(ColumnStatistics,List<String>)",3,3,4
"org.apache.hadoop.hive.metastore.ObjectStore.updatePartitionColumnStatistics(SetPartitionsStatsRequest)",1,4,4
"org.apache.hadoop.hive.metastore.ObjectStore.updateSerdeURI(URI,URI,String,boolean)",1,9,10
"org.apache.hadoop.hive.metastore.ObjectStore.updateTableColumnStatistics(ColumnStatistics)",1,3,3
"org.apache.hadoop.hive.metastore.ObjectStore.validateRole(String)",2,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.validateTableCols(Table,List<String>)",5,3,5
"org.apache.hadoop.hive.metastore.ObjectStore.verifySchema()",2,1,2
"org.apache.hadoop.hive.metastore.ObjectStore.writeMPartitionColumnStatistics(Table,Partition,MPartitionColumnStatistics)",4,3,5
"org.apache.hadoop.hive.metastore.ObjectStore.writeMTableColumnStatistics(Table,MTableColumnStatistics)",1,2,2
"org.apache.hadoop.hive.metastore.ProtectMode.ProtectMode()",1,1,1
"org.apache.hadoop.hive.metastore.ProtectMode.ProtectMode(String)",1,5,6
"org.apache.hadoop.hive.metastore.ProtectMode.getProtectModeFromString(String)",1,1,1
"org.apache.hadoop.hive.metastore.ProtectMode.toString()",1,1,8
"org.apache.hadoop.hive.metastore.RawStoreProxy.RawStoreProxy(HiveConf,Configuration,Class<? extends RawStore>,int)",1,1,1
"org.apache.hadoop.hive.metastore.RawStoreProxy.getAllInterfaces(Class<?>)",1,1,2
"org.apache.hadoop.hive.metastore.RawStoreProxy.getConf()",1,1,1
"org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(HiveConf,Configuration,String,int)",1,1,1
"org.apache.hadoop.hive.metastore.RawStoreProxy.init()",1,1,1
"org.apache.hadoop.hive.metastore.RawStoreProxy.initMS()",1,1,1
"org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(Object,Method,Object[])",1,3,3
"org.apache.hadoop.hive.metastore.RetryingHMSHandler.RetryingHMSHandler(HiveConf,String,boolean)",1,2,2
"org.apache.hadoop.hive.metastore.RetryingHMSHandler.getConf()",1,1,1
"org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(HiveConf,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(Object,Method,Object[])",9,21,23
"org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.RetryingMetaStoreClient(HiveConf,HiveMetaHookLoader,Class<? extends IMetaStoreClient>)",1,1,1
"org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(HiveConf,HiveMetaHookLoader,String)",1,1,1
"org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(Object,Method,Object[])",5,10,11
"org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.reloginExpiringKeytabUser()",2,2,3
"org.apache.hadoop.hive.metastore.StatObjectConverter.convertToMPartitionColumnStatistics(MPartition,ColumnStatisticsDesc,ColumnStatisticsObj)",2,29,31
"org.apache.hadoop.hive.metastore.StatObjectConverter.convertToMTableColumnStatistics(MTable,ColumnStatisticsDesc,ColumnStatisticsObj)",2,29,31
"org.apache.hadoop.hive.metastore.StatObjectConverter.createJdoDecimalString(Decimal)",1,1,1
"org.apache.hadoop.hive.metastore.StatObjectConverter.createThriftDecimal(String)",1,1,1
"org.apache.hadoop.hive.metastore.StatObjectConverter.extractSqlLong(Object)",3,2,3
"org.apache.hadoop.hive.metastore.StatObjectConverter.fillColumnStatisticsData(String,ColumnStatisticsData,Object,Object,Object,Object,Object,Object,Object,Object,Object,Object,Object,Object)",1,20,20
"org.apache.hadoop.hive.metastore.StatObjectConverter.getPartitionColumnStatisticsDesc(MPartitionColumnStatistics)",1,1,1
"org.apache.hadoop.hive.metastore.StatObjectConverter.getPartitionColumnStatisticsObj(MPartitionColumnStatistics)",1,20,20
"org.apache.hadoop.hive.metastore.StatObjectConverter.getTableColumnStatisticsDesc(MTableColumnStatistics)",1,1,1
"org.apache.hadoop.hive.metastore.StatObjectConverter.getTableColumnStatisticsObj(MTableColumnStatistics)",1,20,20
"org.apache.hadoop.hive.metastore.StatObjectConverter.setFieldsIntoOldStats(MPartitionColumnStatistics,MPartitionColumnStatistics)",1,13,13
"org.apache.hadoop.hive.metastore.StatObjectConverter.setFieldsIntoOldStats(MTableColumnStatistics,MTableColumnStatistics)",1,13,13
"org.apache.hadoop.hive.metastore.TServerSocketKeepAlive.TServerSocketKeepAlive(int)",1,1,1
"org.apache.hadoop.hive.metastore.TServerSocketKeepAlive.acceptImpl()",1,1,2
"org.apache.hadoop.hive.metastore.TSetIpAddressProcessor.TSetIpAddressProcessor(I)",1,1,1
"org.apache.hadoop.hive.metastore.TSetIpAddressProcessor.process(TProtocol,TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.TSetIpAddressProcessor.setIpAddress(Socket)",1,1,1
"org.apache.hadoop.hive.metastore.TSetIpAddressProcessor.setIpAddress(TProtocol)",2,1,2
"org.apache.hadoop.hive.metastore.TUGIBasedProcessor.TUGIBasedProcessor(I)",1,1,1
"org.apache.hadoop.hive.metastore.TUGIBasedProcessor.handleSetUGI(TUGIContainingTransport,set_ugi<Iface>,TMessage,TProtocol,TProtocol)",2,3,3
"org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TProtocol,TProtocol)",5,7,11
"org.apache.hadoop.hive.metastore.TUGIBasedProcessor.setIpAddress(TProtocol)",1,2,2
"org.apache.hadoop.hive.metastore.TestAdminUser.testCreateAdminNAddUser()",1,1,1
"org.apache.hadoop.hive.metastore.TestAuthorizationApiAuthorizer.setup()",1,2,2
"org.apache.hadoop.hive.metastore.TestAuthorizationApiAuthorizer.testCreateRole()",1,1,1
"org.apache.hadoop.hive.metastore.TestAuthorizationApiAuthorizer.testDropRole()",1,1,1
"org.apache.hadoop.hive.metastore.TestAuthorizationApiAuthorizer.testFunction(FunctionInvoker)",1,4,4
"org.apache.hadoop.hive.metastore.TestAuthorizationApiAuthorizer.testGetPrivSet()",1,1,1
"org.apache.hadoop.hive.metastore.TestAuthorizationApiAuthorizer.testGrantPriv()",1,1,1
"org.apache.hadoop.hive.metastore.TestAuthorizationApiAuthorizer.testGrantRole()",1,1,1
"org.apache.hadoop.hive.metastore.TestAuthorizationApiAuthorizer.testListPriv()",1,1,1
"org.apache.hadoop.hive.metastore.TestAuthorizationApiAuthorizer.testListRoles()",1,1,1
"org.apache.hadoop.hive.metastore.TestAuthorizationApiAuthorizer.testRevokePriv()",1,1,1
"org.apache.hadoop.hive.metastore.TestAuthorizationApiAuthorizer.testRevokeRole()",1,1,1
"org.apache.hadoop.hive.metastore.TestAuthzApiEmbedAuthorizerInEmbed.setup()",1,1,1
"org.apache.hadoop.hive.metastore.TestAuthzApiEmbedAuthorizerInRemote.setup()",1,1,1
"org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore.setUp()",1,2,2
"org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore.tearDown()",1,2,2
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.add_partition(HiveMetaStoreClient,Table,List<String>,String)",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.adjust(HiveMetaStoreClient,Partition,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.checkDbOwnerType(String,String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.checkFilter(HiveMetaStoreClient,String,String,String,int)",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.cleanUp(String,String,String)",1,4,5
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.createDb(String)",2,1,2
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.createFunction(String,String,String,String,PrincipalType,int,FunctionType,List<ResourceUri>)",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.createMultiPartitionTableSchema(String,String,String,List<List<String>>)",1,2,2
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.createPartitions(String,Table,List<List<String>>)",1,3,3
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.createStorageDescriptor(String,List<FieldSchema>,Map<String, String>,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.createTable(String,String,String,Map<String, String>,Map<String, String>,StorageDescriptor,int)",1,5,5
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.createTableForTestFilter(String,String,String,int,boolean)",1,3,3
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.createType(String,Map<String, String>)",1,2,2
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.makePartitionObject(String,String,List<String>,Table,String)",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.makeVals(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.partitionTester(HiveMetaStoreClient,HiveConf)",1,9,11
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.silentDropDatabase(String)",1,2,4
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testAlterPartition()",1,3,3
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testAlterTable()",1,9,15
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testAlterViewParititon()",1,3,3
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testColumnStatistics()",1,2,2
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testComplexTable()",1,5,5
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testComplexTypeApi()",1,2,3
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testConcurrentMetastores()",1,2,2
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testDBOwner()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testDBOwnerChange()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testDatabase()",1,2,2
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testDatabaseLocation()",1,3,5
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testDatabaseLocationWithPermissionProblems()",2,4,5
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testDropTable()",1,2,2
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testFilterLastPartition()",1,1,2
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testFilterSinglePartition()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testFunctionWithResources()",1,3,3
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testGetConfigValue()",1,5,6
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testListPartitionNames()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testListPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testNameMethods()",1,2,3
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testPartition()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testPartitionFilter()",1,1,5
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testRenamePartition()",1,5,5
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testSimpleFunction()",1,3,4
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testSimpleTable()",1,13,16
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testSimpleTypeApi()",1,2,3
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testSynchronized()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testTableDatabase()",1,2,2
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.testTableFilter()",1,2,3
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.updateTableNameInDB(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStore.verifyPartitionsPublished(HiveMetaStoreClient,String,String,List<String>,List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.TestHiveMetaStoreTxns()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.stringifyValidTxns()",1,6,6
"org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.tearDown()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.testLocks()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.testLocksWithTxn()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.testTxnRange()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.testTxns()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStoreWithEnvironmentContext.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStoreWithEnvironmentContext.tearDown()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaStoreWithEnvironmentContext.testEnvironmentContext()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaTool.dropDatabase(String)",1,1,4
"org.apache.hadoop.hive.metastore.TestHiveMetaTool.redirectOutputStream()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaTool.restoreOutputStream()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaTool.setUp()",1,2,2
"org.apache.hadoop.hive.metastore.TestHiveMetaTool.tearDown()",1,2,2
"org.apache.hadoop.hive.metastore.TestHiveMetaTool.testExecuteJDOQL()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaTool.testListFSRoot()",1,1,1
"org.apache.hadoop.hive.metastore.TestHiveMetaTool.testUpdateFSRootLocation()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.noUser()",1,2,2
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testDbTable()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testExExDb()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testExExPart()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testExExTable()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testExSRDb()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testExSRPart()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testExSRTable()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testExSWDb()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testExSWPart()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testExSWTable()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSRExDb()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSRExPart()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSRExTable()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSRSRDb()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSRSRPart()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSRSRTable()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSRSWDb()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSRSWPart()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSRSWTable()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSWExDb()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSWExPart()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSWExTable()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSWSRDb()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSWSRPart()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSWSRTable()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSWSWDb()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSWSWPart()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testSWSWTable()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testTablePartition()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testTwoSeparateDbs()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testTwoSeparatePartitions()",1,1,1
"org.apache.hadoop.hive.metastore.TestLockRequestBuilder.testTwoSeparateTables()",1,1,1
"org.apache.hadoop.hive.metastore.TestMarkPartition.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.TestMarkPartition.tearDown()",1,1,1
"org.apache.hadoop.hive.metastore.TestMarkPartition.testMarkingPartitionSet()",1,1,4
"org.apache.hadoop.hive.metastore.TestMarkPartitionRemote.RunMS.RunMS(int)",1,1,1
"org.apache.hadoop.hive.metastore.TestMarkPartitionRemote.RunMS.run()",1,2,2
"org.apache.hadoop.hive.metastore.TestMarkPartitionRemote.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.findFreePort()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.setup()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.testIsWritable()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.testMetaStoreAuthorization()",1,4,6
"org.apache.hadoop.hive.metastore.TestMetaStoreConnectionUrlHook.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreConnectionUrlHook.tearDown()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreConnectionUrlHook.testUrlHook()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEndFunctionListener.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEndFunctionListener.tearDown()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEndFunctionListener.testEndFunctionListener()",1,1,5
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.tearDown()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.testListener()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.validateAddPartition(Partition,Partition)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.validateAlterPartition(Partition,Partition,String,String,List<String>,Partition)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.validateAlterTable(Table,Table,Table,Table)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.validateAlterTableColumns(Table,Table,Table,Table)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.validateCreateDb(Database,Database)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.validateCreateTable(Table,Table)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.validateDropDb(Database,Database)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.validateDropPartition(Partition,Partition)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.validateDropTable(Table,Table)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.validateLoadPartitionDone(String,Map<String, String>,String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.validatePartition(Partition,Partition)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.validateTable(Table,Table)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.validateTableInAddPartition(Table,Table)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.validateTableInDropPartition(Table,Table)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListenerOnlyOnCommit.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreEventListenerOnlyOnCommit.testEventStatus()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreInitListener.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreInitListener.tearDown()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreInitListener.testMetaStoreInitListener()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreListenersError.ErrorEventListener.ErrorEventListener(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreListenersError.ErrorInitListener.ErrorInitListener(Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreListenersError.ErrorInitListener.onInit(MetaStoreInitContext)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetaStoreListenersError.testEventListenerException()",1,2,2
"org.apache.hadoop.hive.metastore.TestMetaStoreListenersError.testInitListenerException()",1,2,2
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.ExprBuilder.ExprBuilder(String)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.ExprBuilder.build()",2,2,2
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.ExprBuilder.colInternal(TypeInfo,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.ExprBuilder.fn(String,TypeInfo,int)",1,2,2
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.ExprBuilder.intCol(String)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.ExprBuilder.pred(String,int)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.ExprBuilder.strCol(String)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.ExprBuilder.val(String)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.ExprBuilder.val(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.ExprBuilder.val(int)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.ExprBuilder.valInternal(TypeInfo,Object)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.addPartition(HiveMetaStoreClient,Table,List<String>,String)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.addSd(ArrayList<FieldSchema>,Table)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.checkExpr(int,String,String,ExprNodeGenericFuncDesc)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.setUp()",1,2,2
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.silentDropDatabase(String)",1,2,4
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.tearDown()",1,2,2
"org.apache.hadoop.hive.metastore.TestMetastoreExpr.testPartitionExpr()",1,2,4
"org.apache.hadoop.hive.metastore.TestMetastoreVersion.getMetaStoreVersion()",1,1,2
"org.apache.hadoop.hive.metastore.TestMetastoreVersion.getVersion(HiveConf)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreVersion.setMetaStoreVersion(String,String)",1,1,2
"org.apache.hadoop.hive.metastore.TestMetastoreVersion.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreVersion.setVersion(HiveConf,String)",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreVersion.tearDown()",1,2,2
"org.apache.hadoop.hive.metastore.TestMetastoreVersion.testDefaults()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreVersion.testMetastoreVersion()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreVersion.testVersionMatching()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreVersion.testVersionMisMatch()",1,1,1
"org.apache.hadoop.hive.metastore.TestMetastoreVersion.testVersionRestriction()",1,2,2
"org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistValidation.getPartValsWithCommas()",1,1,1
"org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistValidation.getPartValsWithUnicode()",1,1,1
"org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistValidation.getPartValsWithValidCharacters()",1,1,1
"org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistValidation.runValidation(List<String>)",1,1,2
"org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistValidation.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistValidation.testAddPartitionWithCommas()",1,1,1
"org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistValidation.testAddPartitionWithUnicode()",1,1,1
"org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistValidation.testAddPartitionWithValidPartVal()",1,1,1
"org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistValidation.testAppendPartitionWithCommas()",1,1,1
"org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistValidation.testAppendPartitionWithUnicode()",1,1,1
"org.apache.hadoop.hive.metastore.TestPartitionNameWhitelistValidation.testAppendPartitionWithValidCharacters()",1,1,1
"org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.TestRemoteHiveMetaStore()",1,1,1
"org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.createClient(boolean,int)",1,1,1
"org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.setUp()",2,2,2
"org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStoreIpAddress.createClient(int)",1,1,1
"org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStoreIpAddress.setUp()",2,2,2
"org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStoreIpAddress.testIpAddress()",1,2,2
"org.apache.hadoop.hive.metastore.TestRemoteUGIHiveMetaStoreIpAddress.TestRemoteUGIHiveMetaStoreIpAddress()",1,1,1
"org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.tearDown()",1,1,1
"org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.testRetryingHMSHandler()",1,1,1
"org.apache.hadoop.hive.metastore.TestServerSpecificConfig.resetDefaults()",1,1,1
"org.apache.hadoop.hive.metastore.TestServerSpecificConfig.setHiveSiteWithRemoteMetastore()",1,1,1
"org.apache.hadoop.hive.metastore.TestServerSpecificConfig.testHiveMetastoreRemoteConfig()",1,1,1
"org.apache.hadoop.hive.metastore.TestServerSpecificConfig.testServerConfigsEmbeddedMetastore()",1,1,1
"org.apache.hadoop.hive.metastore.TestServerSpecificConfig.testSystemPropertyPrecedence()",1,1,1
"org.apache.hadoop.hive.metastore.TestServerSpecificConfig.verifyHS2ConfParams(HiveConf)",1,1,1
"org.apache.hadoop.hive.metastore.TestServerSpecificConfig.verifyMetastoreConfNotLoaded(HiveConf)",1,1,1
"org.apache.hadoop.hive.metastore.TestSetUGIOnBothClientServer.TestSetUGIOnBothClientServer()",1,1,1
"org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyClient.createClient(boolean,int)",1,1,1
"org.apache.hadoop.hive.metastore.TestSetUGIOnOnlyServer.createClient(boolean,int)",1,1,1
"org.apache.hadoop.hive.metastore.VerifyingObjectStore.VerifyingObjectStore()",1,1,1
"org.apache.hadoop.hive.metastore.VerifyingObjectStore.dumpObject(StringBuilder,String,Object,Class<?>,int)",7,13,14
"org.apache.hadoop.hive.metastore.VerifyingObjectStore.getPartitionColumnStatistics(String,String,List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.VerifyingObjectStore.getPartitions(String,String,int)",1,1,1
"org.apache.hadoop.hive.metastore.VerifyingObjectStore.getPartitionsByExpr(String,String,byte[],String,short,List<Partition>)",2,2,2
"org.apache.hadoop.hive.metastore.VerifyingObjectStore.getPartitionsByFilter(String,String,String,short)",1,1,1
"org.apache.hadoop.hive.metastore.VerifyingObjectStore.getPartitionsByNames(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.VerifyingObjectStore.getTableColumnStatistics(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.VerifyingObjectStore.verifyLists(Collection<T>,Collection<T>,Class<?>)",6,7,8
"org.apache.hadoop.hive.metastore.VerifyingObjectStore.verifyObjects(Object,Object,Class<?>)",2,2,3
"org.apache.hadoop.hive.metastore.Warehouse.Warehouse(Configuration)",2,1,2
"org.apache.hadoop.hive.metastore.Warehouse.closeFs(FileSystem)",1,3,3
"org.apache.hadoop.hive.metastore.Warehouse.deleteDir(Path,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.escapePathName(String)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.getDatabasePath(Database)",2,2,2
"org.apache.hadoop.hive.metastore.Warehouse.getDefaultDatabasePath(String)",2,2,2
"org.apache.hadoop.hive.metastore.Warehouse.getDnsPath(Path)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.getDnsPath(Path,Configuration)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.getFileStatusesForSD(StorageDescriptor)",1,2,2
"org.apache.hadoop.hive.metastore.Warehouse.getFileStatusesForUnpartitionedTable(Database,Table)",1,2,2
"org.apache.hadoop.hive.metastore.Warehouse.getFs(Path)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.getFs(Path,Configuration)",1,2,2
"org.apache.hadoop.hive.metastore.Warehouse.getMetaStoreFsHandler(Configuration)",1,2,2
"org.apache.hadoop.hive.metastore.Warehouse.getPartValuesFromPartName(String)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.getPartitionPath(Database,String,LinkedHashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.getPartitionPath(Path,LinkedHashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.getQualifiedName(Partition)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.getQualifiedName(Table)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.getTablePath(Database,String)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.getTablePath(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.getWhRoot()",2,1,2
"org.apache.hadoop.hive.metastore.Warehouse.isDir(Path)",2,2,4
"org.apache.hadoop.hive.metastore.Warehouse.isEmpty(Path)",2,3,4
"org.apache.hadoop.hive.metastore.Warehouse.isWritable(Path)",7,3,11
"org.apache.hadoop.hive.metastore.Warehouse.makeDynamicPartName(Map<String, String>)",3,4,4
"org.apache.hadoop.hive.metastore.Warehouse.makeEscSpecFromName(String)",2,6,7
"org.apache.hadoop.hive.metastore.Warehouse.makePartName(List<FieldSchema>,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.makePartName(List<FieldSchema>,List<String>,String)",2,5,6
"org.apache.hadoop.hive.metastore.Warehouse.makePartName(Map<String, String>,boolean)",3,5,6
"org.apache.hadoop.hive.metastore.Warehouse.makePartPath(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.makeSpecFromName(Map<String, String>,Path)",1,5,5
"org.apache.hadoop.hive.metastore.Warehouse.makeSpecFromName(String)",2,2,3
"org.apache.hadoop.hive.metastore.Warehouse.makeSpecFromValues(List<FieldSchema>,List<String>)",1,2,2
"org.apache.hadoop.hive.metastore.Warehouse.makeValsFromName(String,AbstractList<String>)",4,3,4
"org.apache.hadoop.hive.metastore.Warehouse.mkdirs(Path,boolean)",1,3,3
"org.apache.hadoop.hive.metastore.Warehouse.renameDir(Path,Path)",1,1,1
"org.apache.hadoop.hive.metastore.Warehouse.renameDir(Path,Path,boolean)",1,2,2
"org.apache.hadoop.hive.metastore.Warehouse.unescapePathName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.AbortTxnRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.AbortTxnRequest(AbortTxnRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.AbortTxnRequest(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.AbortTxnRequestStandardScheme.read(TProtocol,AbortTxnRequest)",4,4,6
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.AbortTxnRequestStandardScheme.write(TProtocol,AbortTxnRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.AbortTxnRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.AbortTxnRequestTupleScheme.read(TProtocol,AbortTxnRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.AbortTxnRequestTupleScheme.write(TProtocol,AbortTxnRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.AbortTxnRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.compareTo(AbortTxnRequest)",5,3,5
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.equals(AbortTxnRequest)",5,1,7
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.getTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.hashCode()",1,2,2
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.isSetTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.setTxnid(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.setTxnidIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.toString()",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.unsetTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.validate()",2,2,2
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.AbortTxnRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.AddPartitionsRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.AddPartitionsRequest(AddPartitionsRequest)",1,3,5
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.AddPartitionsRequest(String,String,List<Partition>,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.AddPartitionsRequestStandardScheme.read(TProtocol,AddPartitionsRequest)",4,9,15
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.AddPartitionsRequestStandardScheme.write(TProtocol,AddPartitionsRequest)",1,6,6
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.AddPartitionsRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.AddPartitionsRequestTupleScheme.read(TProtocol,AddPartitionsRequest)",1,3,3
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.AddPartitionsRequestTupleScheme.write(TProtocol,AddPartitionsRequest)",1,4,4
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.AddPartitionsRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest._Fields.findByThriftId(int)",7,2,7
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.addToParts(Partition)",1,1,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.compareTo(AddPartitionsRequest)",17,7,17
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.equals(AddPartitionsRequest)",17,12,35
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.getFieldValue(_Fields)",6,6,6
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.getParts()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.getPartsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.getPartsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.getTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.hashCode()",1,10,10
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.isIfNotExists()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.isNeedResult()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.isSet(_Fields)",7,6,7
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.isSetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.isSetIfNotExists()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.isSetNeedResult()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.isSetParts()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.isSetTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.setDbNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.setFieldValue(_Fields,Object)",2,7,11
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.setIfNotExists(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.setIfNotExistsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.setNeedResult(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.setNeedResultIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.setParts(List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.setPartsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.setTblName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.setTblNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.toString()",1,9,9
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.unsetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.unsetIfNotExists()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.unsetNeedResult()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.unsetParts()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.unsetTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.validate()",5,5,5
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.AddPartitionsResult()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.AddPartitionsResult(AddPartitionsResult)",1,3,3
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.AddPartitionsResultStandardScheme.read(TProtocol,AddPartitionsResult)",4,5,7
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.AddPartitionsResultStandardScheme.write(TProtocol,AddPartitionsResult)",1,4,4
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.AddPartitionsResultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.AddPartitionsResultTupleScheme.read(TProtocol,AddPartitionsResult)",1,3,3
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.AddPartitionsResultTupleScheme.write(TProtocol,AddPartitionsResult)",1,4,4
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.AddPartitionsResultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.addToPartitions(Partition)",1,1,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.compareTo(AddPartitionsResult)",5,3,5
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.equals(AddPartitionsResult)",5,4,9
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.getPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.getPartitionsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.getPartitionsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.isSetPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.setPartitions(List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.setPartitionsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.toString()",1,3,3
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.unsetPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.AddPartitionsResult.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.AggrStats.AggrStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.AggrStats(AggrStats)",1,3,3
"org.apache.hadoop.hive.metastore.api.AggrStats.AggrStats(List<ColumnStatisticsObj>,long)",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.AggrStatsStandardScheme.read(TProtocol,AggrStats)",4,6,9
"org.apache.hadoop.hive.metastore.api.AggrStats.AggrStatsStandardScheme.write(TProtocol,AggrStats)",1,3,3
"org.apache.hadoop.hive.metastore.api.AggrStats.AggrStatsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.AggrStatsTupleScheme.read(TProtocol,AggrStats)",1,2,2
"org.apache.hadoop.hive.metastore.api.AggrStats.AggrStatsTupleScheme.write(TProtocol,AggrStats)",1,2,2
"org.apache.hadoop.hive.metastore.api.AggrStats.AggrStatsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.AggrStats._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.AggrStats._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.addToColStats(ColumnStatisticsObj)",1,1,2
"org.apache.hadoop.hive.metastore.api.AggrStats.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.compareTo(AggrStats)",8,4,8
"org.apache.hadoop.hive.metastore.api.AggrStats.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.equals(AggrStats)",8,4,14
"org.apache.hadoop.hive.metastore.api.AggrStats.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.AggrStats.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.getColStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.getColStatsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.AggrStats.getColStatsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.AggrStats.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.AggrStats.getPartsFound()",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.hashCode()",1,4,4
"org.apache.hadoop.hive.metastore.api.AggrStats.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.AggrStats.isSetColStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.isSetPartsFound()",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.AggrStats.setColStats(List<ColumnStatisticsObj>)",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.setColStatsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.AggrStats.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.AggrStats.setPartsFound(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.setPartsFoundIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.toString()",1,3,3
"org.apache.hadoop.hive.metastore.api.AggrStats.unsetColStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.unsetPartsFound()",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.validate()",3,3,3
"org.apache.hadoop.hive.metastore.api.AggrStats.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.AggrStats.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.AlreadyExistsException()",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.AlreadyExistsException(AlreadyExistsException)",1,1,2
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.AlreadyExistsException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.AlreadyExistsExceptionStandardScheme.read(TProtocol,AlreadyExistsException)",4,4,6
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.AlreadyExistsExceptionStandardScheme.write(TProtocol,AlreadyExistsException)",1,2,2
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.AlreadyExistsExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.AlreadyExistsExceptionTupleScheme.read(TProtocol,AlreadyExistsException)",1,2,2
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.AlreadyExistsExceptionTupleScheme.write(TProtocol,AlreadyExistsException)",1,3,3
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.AlreadyExistsExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.compareTo(AlreadyExistsException)",5,3,5
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.equals(AlreadyExistsException)",5,4,9
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.AlreadyExistsException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.BinaryColumnStatsData()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.BinaryColumnStatsData(BinaryColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.BinaryColumnStatsData(long,double,long)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.BinaryColumnStatsDataStandardScheme.read(TProtocol,BinaryColumnStatsData)",4,6,10
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.BinaryColumnStatsDataStandardScheme.write(TProtocol,BinaryColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.BinaryColumnStatsDataStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.BinaryColumnStatsDataTupleScheme.read(TProtocol,BinaryColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.BinaryColumnStatsDataTupleScheme.write(TProtocol,BinaryColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.BinaryColumnStatsDataTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.compareTo(BinaryColumnStatsData)",11,5,11
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.equals(BinaryColumnStatsData)",11,1,17
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.getAvgColLen()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.getMaxColLen()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.getNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.hashCode()",1,4,4
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.isSetAvgColLen()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.isSetMaxColLen()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.isSetNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.setAvgColLen(double)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.setAvgColLenIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.setMaxColLen(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.setMaxColLenIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.setNumNulls(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.setNumNullsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.toString()",1,3,3
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.unsetAvgColLen()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.unsetMaxColLen()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.unsetNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.validate()",4,4,4
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.BooleanColumnStatsData()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.BooleanColumnStatsData(BooleanColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.BooleanColumnStatsData(long,long,long)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.BooleanColumnStatsDataStandardScheme.read(TProtocol,BooleanColumnStatsData)",4,6,10
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.BooleanColumnStatsDataStandardScheme.write(TProtocol,BooleanColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.BooleanColumnStatsDataStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.BooleanColumnStatsDataTupleScheme.read(TProtocol,BooleanColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.BooleanColumnStatsDataTupleScheme.write(TProtocol,BooleanColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.BooleanColumnStatsDataTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.compareTo(BooleanColumnStatsData)",11,5,11
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.equals(BooleanColumnStatsData)",11,1,17
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.getNumFalses()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.getNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.getNumTrues()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.hashCode()",1,4,4
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.isSetNumFalses()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.isSetNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.isSetNumTrues()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.setNumFalses(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.setNumFalsesIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.setNumNulls(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.setNumNullsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.setNumTrues(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.setNumTruesIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.toString()",1,3,3
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.unsetNumFalses()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.unsetNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.unsetNumTrues()",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.validate()",4,4,4
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.CheckLockRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.CheckLockRequest(CheckLockRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.CheckLockRequest(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.CheckLockRequestStandardScheme.read(TProtocol,CheckLockRequest)",4,4,6
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.CheckLockRequestStandardScheme.write(TProtocol,CheckLockRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.CheckLockRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.CheckLockRequestTupleScheme.read(TProtocol,CheckLockRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.CheckLockRequestTupleScheme.write(TProtocol,CheckLockRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.CheckLockRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.CheckLockRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.CheckLockRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.compareTo(CheckLockRequest)",5,3,5
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.equals(CheckLockRequest)",5,1,7
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.getLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.hashCode()",1,2,2
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.isSetLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.setLockid(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.setLockidIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.toString()",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.unsetLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.validate()",2,2,2
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.CheckLockRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.ColumnStatistics()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.ColumnStatistics(ColumnStatistics)",1,3,4
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.ColumnStatistics(ColumnStatisticsDesc,List<ColumnStatisticsObj>)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.ColumnStatisticsStandardScheme.read(TProtocol,ColumnStatistics)",4,6,9
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.ColumnStatisticsStandardScheme.write(TProtocol,ColumnStatistics)",1,4,4
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.ColumnStatisticsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.ColumnStatisticsTupleScheme.read(TProtocol,ColumnStatistics)",1,2,2
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.ColumnStatisticsTupleScheme.write(TProtocol,ColumnStatistics)",1,2,2
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.ColumnStatisticsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.ColumnStatistics._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatistics._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.addToStatsObj(ColumnStatisticsObj)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.compareTo(ColumnStatistics)",8,4,8
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.equals(ColumnStatistics)",8,7,16
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.getStatsDesc()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.getStatsObj()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.getStatsObjIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.getStatsObjSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.hashCode()",1,5,5
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.isSetStatsDesc()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.isSetStatsObj()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.setStatsDesc(ColumnStatisticsDesc)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.setStatsDescIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.setStatsObj(List<ColumnStatisticsObj>)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.setStatsObjIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.toString()",1,4,4
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.unsetStatsDesc()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.unsetStatsObj()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.validate()",3,4,4
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatistics.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.ColumnStatisticsData()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.ColumnStatisticsData(ColumnStatisticsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.ColumnStatisticsData(_Fields,Object)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData._Fields.findByThriftId(int)",8,2,8
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.binaryStats(BinaryColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.booleanStats(BooleanColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.checkType(_Fields,Object)",2,2,14
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.compareTo(ColumnStatisticsData)",2,2,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.decimalStats(DecimalColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.doubleStats(DoubleColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.enumForId(short)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.equals(ColumnStatisticsData)",1,3,3
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.equals(Object)",2,2,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getBinaryStats()",2,2,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getBooleanStats()",2,2,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getDecimalStats()",2,2,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getDoubleStats()",2,2,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc(_Fields)",8,2,8
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getLongStats()",2,2,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getStringStats()",2,2,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getStructDesc()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.isSetBinaryStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.isSetBooleanStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.isSetDecimalStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.isSetDoubleStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.isSetLongStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.isSetStringStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.longStats(LongColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.setBinaryStats(BinaryColumnStatsData)",2,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.setBooleanStats(BooleanColumnStatsData)",2,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.setDecimalStats(DecimalColumnStatsData)",2,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.setDoubleStats(DoubleColumnStatsData)",2,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.setLongStats(LongColumnStatsData)",2,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.setStringStats(StringColumnStatsData)",2,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.standardSchemeReadValue(TProtocol,TField)",15,15,15
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.standardSchemeWriteValue(TProtocol)",2,2,8
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.stringStats(StringColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.tupleSchemeReadValue(TProtocol,short)",3,3,9
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.tupleSchemeWriteValue(TProtocol)",2,2,8
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.ColumnStatisticsDesc()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.ColumnStatisticsDesc(ColumnStatisticsDesc)",1,1,4
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.ColumnStatisticsDesc(boolean,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.ColumnStatisticsDescStandardScheme.read(TProtocol,ColumnStatisticsDesc)",4,8,14
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.ColumnStatisticsDescStandardScheme.write(TProtocol,ColumnStatisticsDesc)",1,6,6
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.ColumnStatisticsDescStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.ColumnStatisticsDescTupleScheme.read(TProtocol,ColumnStatisticsDesc)",1,3,3
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.ColumnStatisticsDescTupleScheme.write(TProtocol,ColumnStatisticsDesc)",1,5,5
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.ColumnStatisticsDescTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc._Fields.findByThriftId(int)",7,2,7
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.compareTo(ColumnStatisticsDesc)",17,7,17
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.equals(ColumnStatisticsDesc)",17,12,35
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.getFieldValue(_Fields)",6,6,6
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.getLastAnalyzed()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.getPartName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.hashCode()",1,10,10
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.isIsTblLevel()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.isSet(_Fields)",7,6,7
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.isSetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.isSetIsTblLevel()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.isSetLastAnalyzed()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.isSetPartName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.isSetTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.setDbNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.setFieldValue(_Fields,Object)",2,7,11
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.setIsTblLevel(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.setIsTblLevelIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.setLastAnalyzed(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.setLastAnalyzedIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.setPartName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.setPartNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.setTableNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.toString()",1,10,10
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.unsetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.unsetIsTblLevel()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.unsetLastAnalyzed()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.unsetPartName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.unsetTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.validate()",4,4,4
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.ColumnStatisticsObj()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.ColumnStatisticsObj(ColumnStatisticsObj)",1,1,4
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.ColumnStatisticsObj(String,String,ColumnStatisticsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.ColumnStatisticsObjStandardScheme.read(TProtocol,ColumnStatisticsObj)",4,6,10
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.ColumnStatisticsObjStandardScheme.write(TProtocol,ColumnStatisticsObj)",1,4,4
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.ColumnStatisticsObjStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.ColumnStatisticsObjTupleScheme.read(TProtocol,ColumnStatisticsObj)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.ColumnStatisticsObjTupleScheme.write(TProtocol,ColumnStatisticsObj)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.ColumnStatisticsObjTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.compareTo(ColumnStatisticsObj)",11,5,11
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.equals(ColumnStatisticsObj)",11,10,23
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.getColName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.getColType()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.getStatsData()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.hashCode()",1,7,7
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.isSetColName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.isSetColType()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.isSetStatsData()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.setColName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.setColNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.setColType(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.setColTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.setStatsData(ColumnStatisticsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.setStatsDataIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.toString()",1,6,6
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.unsetColName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.unsetColType()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.unsetStatsData()",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.validate()",4,4,4
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.CommitTxnRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.CommitTxnRequest(CommitTxnRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.CommitTxnRequest(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.CommitTxnRequestStandardScheme.read(TProtocol,CommitTxnRequest)",4,4,6
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.CommitTxnRequestStandardScheme.write(TProtocol,CommitTxnRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.CommitTxnRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.CommitTxnRequestTupleScheme.read(TProtocol,CommitTxnRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.CommitTxnRequestTupleScheme.write(TProtocol,CommitTxnRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.CommitTxnRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.compareTo(CommitTxnRequest)",5,3,5
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.equals(CommitTxnRequest)",5,1,7
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.getTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.hashCode()",1,2,2
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.isSetTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.setTxnid(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.setTxnidIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.toString()",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.unsetTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.validate()",2,2,2
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.CommitTxnRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.CompactionRequest.CompactionRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.CompactionRequest(CompactionRequest)",1,1,6
"org.apache.hadoop.hive.metastore.api.CompactionRequest.CompactionRequest(String,String,CompactionType)",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.CompactionRequestStandardScheme.read(TProtocol,CompactionRequest)",4,8,14
"org.apache.hadoop.hive.metastore.api.CompactionRequest.CompactionRequestStandardScheme.write(TProtocol,CompactionRequest)",1,8,8
"org.apache.hadoop.hive.metastore.api.CompactionRequest.CompactionRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.CompactionRequestTupleScheme.read(TProtocol,CompactionRequest)",1,3,3
"org.apache.hadoop.hive.metastore.api.CompactionRequest.CompactionRequestTupleScheme.write(TProtocol,CompactionRequest)",1,5,5
"org.apache.hadoop.hive.metastore.api.CompactionRequest.CompactionRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest._Fields.findByThriftId(int)",7,2,7
"org.apache.hadoop.hive.metastore.api.CompactionRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.CompactionRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.compareTo(CompactionRequest)",17,7,17
"org.apache.hadoop.hive.metastore.api.CompactionRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.equals(CompactionRequest)",17,16,37
"org.apache.hadoop.hive.metastore.api.CompactionRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.CompactionRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.getDbname()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.getFieldValue(_Fields)",6,6,6
"org.apache.hadoop.hive.metastore.api.CompactionRequest.getPartitionname()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.getRunas()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.getTablename()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.getType()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.hashCode()",1,11,11
"org.apache.hadoop.hive.metastore.api.CompactionRequest.isSet(_Fields)",7,6,7
"org.apache.hadoop.hive.metastore.api.CompactionRequest.isSetDbname()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.isSetPartitionname()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.isSetRunas()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.isSetTablename()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.isSetType()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.CompactionRequest.setDbname(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.setDbnameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.CompactionRequest.setFieldValue(_Fields,Object)",2,7,11
"org.apache.hadoop.hive.metastore.api.CompactionRequest.setPartitionname(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.setPartitionnameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.CompactionRequest.setRunas(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.setRunasIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.CompactionRequest.setTablename(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.setTablenameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.CompactionRequest.setType(CompactionType)",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.setTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.CompactionRequest.toString()",1,12,12
"org.apache.hadoop.hive.metastore.api.CompactionRequest.unsetDbname()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.unsetPartitionname()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.unsetRunas()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.unsetTablename()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.unsetType()",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.validate()",4,4,4
"org.apache.hadoop.hive.metastore.api.CompactionRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.CompactionType.CompactionType(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.CompactionType.findByValue(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.CompactionType.getValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.ConfigValSecurityException()",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.ConfigValSecurityException(ConfigValSecurityException)",1,1,2
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.ConfigValSecurityException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.ConfigValSecurityExceptionStandardScheme.read(TProtocol,ConfigValSecurityException)",4,4,6
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.ConfigValSecurityExceptionStandardScheme.write(TProtocol,ConfigValSecurityException)",1,2,2
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.ConfigValSecurityExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.ConfigValSecurityExceptionTupleScheme.read(TProtocol,ConfigValSecurityException)",1,2,2
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.ConfigValSecurityExceptionTupleScheme.write(TProtocol,ConfigValSecurityException)",1,3,3
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.ConfigValSecurityExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.compareTo(ConfigValSecurityException)",5,3,5
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.equals(ConfigValSecurityException)",5,4,9
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ConfigValSecurityException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Database.Database()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.Database(Database)",1,3,9
"org.apache.hadoop.hive.metastore.api.Database.Database(String,String,String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.DatabaseStandardScheme.read(TProtocol,Database)",4,11,19
"org.apache.hadoop.hive.metastore.api.Database.DatabaseStandardScheme.write(TProtocol,Database)",1,12,12
"org.apache.hadoop.hive.metastore.api.Database.DatabaseStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.DatabaseTupleScheme.read(TProtocol,Database)",1,9,9
"org.apache.hadoop.hive.metastore.api.Database.DatabaseTupleScheme.write(TProtocol,Database)",1,16,16
"org.apache.hadoop.hive.metastore.api.Database.DatabaseTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Database._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Database._Fields.findByThriftId(int)",9,2,9
"org.apache.hadoop.hive.metastore.api.Database._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.Database._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.compareTo(Database)",23,9,23
"org.apache.hadoop.hive.metastore.api.Database.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.equals(Database)",23,22,51
"org.apache.hadoop.hive.metastore.api.Database.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.Database.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.getDescription()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.getFieldValue(_Fields)",8,8,8
"org.apache.hadoop.hive.metastore.api.Database.getLocationUri()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.getName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.getOwnerName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.getOwnerType()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.getParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.getParametersSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.Database.getPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.hashCode()",1,15,15
"org.apache.hadoop.hive.metastore.api.Database.isSet(_Fields)",9,8,9
"org.apache.hadoop.hive.metastore.api.Database.isSetDescription()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.isSetLocationUri()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.isSetName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.isSetOwnerName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.isSetOwnerType()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.isSetParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.isSetPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.putToParameters(String,String)",1,1,2
"org.apache.hadoop.hive.metastore.api.Database.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Database.setDescription(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.setDescriptionIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Database.setFieldValue(_Fields,Object)",2,9,15
"org.apache.hadoop.hive.metastore.api.Database.setLocationUri(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.setLocationUriIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Database.setName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.setNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Database.setOwnerName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.setOwnerNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Database.setOwnerType(PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.setOwnerTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Database.setParameters(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.setParametersIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Database.setPrivileges(PrincipalPrivilegeSet)",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.setPrivilegesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Database.toString()",1,17,17
"org.apache.hadoop.hive.metastore.api.Database.unsetDescription()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.unsetLocationUri()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.unsetName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.unsetOwnerName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.unsetOwnerType()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.unsetParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.unsetPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.validate()",1,2,2
"org.apache.hadoop.hive.metastore.api.Database.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Database.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Decimal.Decimal()",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.Decimal(ByteBuffer,short)",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.Decimal(Decimal)",1,2,2
"org.apache.hadoop.hive.metastore.api.Decimal.DecimalStandardScheme.read(TProtocol,Decimal)",4,5,8
"org.apache.hadoop.hive.metastore.api.Decimal.DecimalStandardScheme.write(TProtocol,Decimal)",1,2,2
"org.apache.hadoop.hive.metastore.api.Decimal.DecimalStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.DecimalTupleScheme.read(TProtocol,Decimal)",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.DecimalTupleScheme.write(TProtocol,Decimal)",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.DecimalTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.Decimal._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.Decimal._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.bufferForUnscaled()",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.compareTo(Decimal)",8,4,8
"org.apache.hadoop.hive.metastore.api.Decimal.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.equals(Decimal)",8,4,14
"org.apache.hadoop.hive.metastore.api.Decimal.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.Decimal.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.Decimal.getScale()",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.getUnscaled()",1,2,2
"org.apache.hadoop.hive.metastore.api.Decimal.hashCode()",1,4,4
"org.apache.hadoop.hive.metastore.api.Decimal.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.Decimal.isSetScale()",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.isSetUnscaled()",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Decimal.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.Decimal.setScale(short)",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.setScaleIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.setUnscaled(ByteBuffer)",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.setUnscaled(byte[])",1,2,2
"org.apache.hadoop.hive.metastore.api.Decimal.setUnscaledIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Decimal.toString()",1,3,3
"org.apache.hadoop.hive.metastore.api.Decimal.unsetScale()",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.unsetUnscaled()",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.validate()",3,3,3
"org.apache.hadoop.hive.metastore.api.Decimal.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Decimal.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.DecimalColumnStatsData()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.DecimalColumnStatsData(DecimalColumnStatsData)",1,1,3
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.DecimalColumnStatsData(long,long)",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.DecimalColumnStatsDataStandardScheme.read(TProtocol,DecimalColumnStatsData)",4,7,12
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.DecimalColumnStatsDataStandardScheme.write(TProtocol,DecimalColumnStatsData)",1,5,5
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.DecimalColumnStatsDataStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.DecimalColumnStatsDataTupleScheme.read(TProtocol,DecimalColumnStatsData)",1,3,3
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.DecimalColumnStatsDataTupleScheme.write(TProtocol,DecimalColumnStatsData)",1,5,5
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.DecimalColumnStatsDataTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData._Fields.findByThriftId(int)",6,2,6
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.compareTo(DecimalColumnStatsData)",14,6,14
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.equals(DecimalColumnStatsData)",14,7,26
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.getFieldValue(_Fields)",5,5,5
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.getHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.getLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.getNumDVs()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.getNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.hashCode()",1,7,7
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.isSet(_Fields)",6,5,6
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.isSetHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.isSetLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.isSetNumDVs()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.isSetNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.setHighValue(Decimal)",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.setHighValueIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.setLowValue(Decimal)",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.setLowValueIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.setNumDVs(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.setNumDVsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.setNumNulls(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.setNumNullsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.toString()",1,8,8
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.unsetHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.unsetLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.unsetNumDVs()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.unsetNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.validate()",3,5,5
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.DoubleColumnStatsData()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.DoubleColumnStatsData(DoubleColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.DoubleColumnStatsData(long,long)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.DoubleColumnStatsDataStandardScheme.read(TProtocol,DoubleColumnStatsData)",4,7,12
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.DoubleColumnStatsDataStandardScheme.write(TProtocol,DoubleColumnStatsData)",1,3,3
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.DoubleColumnStatsDataStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.DoubleColumnStatsDataTupleScheme.read(TProtocol,DoubleColumnStatsData)",1,3,3
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.DoubleColumnStatsDataTupleScheme.write(TProtocol,DoubleColumnStatsData)",1,5,5
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.DoubleColumnStatsDataTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData._Fields.findByThriftId(int)",6,2,6
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.compareTo(DoubleColumnStatsData)",14,6,14
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.equals(DoubleColumnStatsData)",14,5,26
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.getFieldValue(_Fields)",5,5,5
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.getHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.getLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.getNumDVs()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.getNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.hashCode()",1,7,7
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.isSet(_Fields)",6,5,6
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.isSetHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.isSetLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.isSetNumDVs()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.isSetNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.setHighValue(double)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.setHighValueIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.setLowValue(double)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.setLowValueIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.setNumDVs(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.setNumDVsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.setNumNulls(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.setNumNullsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.toString()",1,6,6
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.unsetHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.unsetLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.unsetNumDVs()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.unsetNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.validate()",3,3,3
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.DropPartitionsExpr()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.DropPartitionsExpr(ByteBuffer)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.DropPartitionsExpr(DropPartitionsExpr)",1,2,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.DropPartitionsExprStandardScheme.read(TProtocol,DropPartitionsExpr)",4,5,8
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.DropPartitionsExprStandardScheme.write(TProtocol,DropPartitionsExpr)",1,3,3
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.DropPartitionsExprStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.DropPartitionsExprTupleScheme.read(TProtocol,DropPartitionsExpr)",1,2,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.DropPartitionsExprTupleScheme.write(TProtocol,DropPartitionsExpr)",1,3,3
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.DropPartitionsExprTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.bufferForExpr()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.compareTo(DropPartitionsExpr)",8,4,8
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.equals(DropPartitionsExpr)",8,6,16
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.getExpr()",1,2,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.getPartArchiveLevel()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.hashCode()",1,5,5
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.isSetExpr()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.isSetPartArchiveLevel()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.setExpr(ByteBuffer)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.setExpr(byte[])",1,2,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.setExprIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.setPartArchiveLevel(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.setPartArchiveLevelIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.toString()",1,4,4
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.unsetExpr()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.unsetPartArchiveLevel()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.validate()",2,2,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsExpr.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.DropPartitionsRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.DropPartitionsRequest(DropPartitionsRequest)",1,1,5
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.DropPartitionsRequest(String,String,RequestPartsSpec)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.DropPartitionsRequestStandardScheme.read(TProtocol,DropPartitionsRequest)",4,11,20
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.DropPartitionsRequestStandardScheme.write(TProtocol,DropPartitionsRequest)",1,10,10
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.DropPartitionsRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.DropPartitionsRequestTupleScheme.read(TProtocol,DropPartitionsRequest)",1,6,6
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.DropPartitionsRequestTupleScheme.write(TProtocol,DropPartitionsRequest)",1,11,11
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.DropPartitionsRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest._Fields.findByThriftId(int)",10,2,10
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.compareTo(DropPartitionsRequest)",26,10,26
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.equals(DropPartitionsRequest)",26,21,58
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.getEnvironmentContext()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.getFieldValue(_Fields)",9,9,9
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.getParts()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.getTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.hashCode()",1,17,17
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.isDeleteData()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.isIfExists()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.isIgnoreProtection()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.isNeedResult()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.isSet(_Fields)",10,9,10
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.isSetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.isSetDeleteData()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.isSetEnvironmentContext()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.isSetIfExists()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.isSetIgnoreProtection()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.isSetNeedResult()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.isSetParts()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.isSetTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setDbNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setDeleteData(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setDeleteDataIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setEnvironmentContext(EnvironmentContext)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setEnvironmentContextIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setFieldValue(_Fields,Object)",2,10,17
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setIfExists(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setIfExistsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setIgnoreProtection(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setIgnoreProtectionIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setNeedResult(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setNeedResultIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setParts(RequestPartsSpec)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setPartsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setTblName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.setTblNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.toString()",1,17,17
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.unsetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.unsetDeleteData()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.unsetEnvironmentContext()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.unsetIfExists()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.unsetIgnoreProtection()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.unsetNeedResult()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.unsetParts()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.unsetTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.validate()",4,5,5
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.DropPartitionsResult()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.DropPartitionsResult(DropPartitionsResult)",1,3,3
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.DropPartitionsResultStandardScheme.read(TProtocol,DropPartitionsResult)",4,5,7
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.DropPartitionsResultStandardScheme.write(TProtocol,DropPartitionsResult)",1,4,4
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.DropPartitionsResultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.DropPartitionsResultTupleScheme.read(TProtocol,DropPartitionsResult)",1,3,3
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.DropPartitionsResultTupleScheme.write(TProtocol,DropPartitionsResult)",1,4,4
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.DropPartitionsResultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.addToPartitions(Partition)",1,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.compareTo(DropPartitionsResult)",5,3,5
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.equals(DropPartitionsResult)",5,4,9
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.getPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.getPartitionsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.getPartitionsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.isSetPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.setPartitions(List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.setPartitionsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.toString()",1,3,3
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.unsetPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.DropPartitionsResult.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.EnvironmentContext()",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.EnvironmentContext(EnvironmentContext)",1,3,3
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.EnvironmentContext(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.EnvironmentContextStandardScheme.read(TProtocol,EnvironmentContext)",4,5,7
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.EnvironmentContextStandardScheme.write(TProtocol,EnvironmentContext)",1,3,3
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.EnvironmentContextStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.EnvironmentContextTupleScheme.read(TProtocol,EnvironmentContext)",1,3,3
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.EnvironmentContextTupleScheme.write(TProtocol,EnvironmentContext)",1,4,4
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.EnvironmentContextTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.EnvironmentContext._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.EnvironmentContext._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.compareTo(EnvironmentContext)",5,3,5
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.equals(EnvironmentContext)",5,4,9
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.getProperties()",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.getPropertiesSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.isSetProperties()",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.putToProperties(String,String)",1,1,2
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.setProperties(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.setPropertiesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.unsetProperties()",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.EnvironmentContext.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.FieldSchema.FieldSchema()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.FieldSchema(FieldSchema)",1,1,4
"org.apache.hadoop.hive.metastore.api.FieldSchema.FieldSchema(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.FieldSchemaStandardScheme.read(TProtocol,FieldSchema)",4,6,10
"org.apache.hadoop.hive.metastore.api.FieldSchema.FieldSchemaStandardScheme.write(TProtocol,FieldSchema)",1,4,4
"org.apache.hadoop.hive.metastore.api.FieldSchema.FieldSchemaStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.FieldSchemaTupleScheme.read(TProtocol,FieldSchema)",1,4,4
"org.apache.hadoop.hive.metastore.api.FieldSchema.FieldSchemaTupleScheme.write(TProtocol,FieldSchema)",1,7,7
"org.apache.hadoop.hive.metastore.api.FieldSchema.FieldSchemaTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.FieldSchema._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.FieldSchema._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.compareTo(FieldSchema)",11,5,11
"org.apache.hadoop.hive.metastore.api.FieldSchema.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.equals(FieldSchema)",11,10,23
"org.apache.hadoop.hive.metastore.api.FieldSchema.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.FieldSchema.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.getComment()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.metastore.api.FieldSchema.getName()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.getType()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.hashCode()",1,7,7
"org.apache.hadoop.hive.metastore.api.FieldSchema.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.metastore.api.FieldSchema.isSetComment()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.isSetName()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.isSetType()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.FieldSchema.setComment(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.setCommentIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.FieldSchema.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.metastore.api.FieldSchema.setName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.setNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.FieldSchema.setType(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.setTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.FieldSchema.toString()",1,6,6
"org.apache.hadoop.hive.metastore.api.FieldSchema.unsetComment()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.unsetName()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.unsetType()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.FieldSchema.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Function.Function()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.Function(Function)",1,3,9
"org.apache.hadoop.hive.metastore.api.Function.Function(String,String,String,String,PrincipalType,int,FunctionType,List<ResourceUri>)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.FunctionStandardScheme.read(TProtocol,Function)",4,12,21
"org.apache.hadoop.hive.metastore.api.Function.FunctionStandardScheme.write(TProtocol,Function)",1,9,9
"org.apache.hadoop.hive.metastore.api.Function.FunctionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.FunctionTupleScheme.read(TProtocol,Function)",1,10,10
"org.apache.hadoop.hive.metastore.api.Function.FunctionTupleScheme.write(TProtocol,Function)",1,18,18
"org.apache.hadoop.hive.metastore.api.Function.FunctionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function._Fields.findByThriftId(int)",10,2,10
"org.apache.hadoop.hive.metastore.api.Function._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.Function._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.addToResourceUris(ResourceUri)",1,1,2
"org.apache.hadoop.hive.metastore.api.Function.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.compareTo(Function)",26,10,26
"org.apache.hadoop.hive.metastore.api.Function.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.equals(Function)",26,22,56
"org.apache.hadoop.hive.metastore.api.Function.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.Function.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.getClassName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.getFieldValue(_Fields)",9,9,9
"org.apache.hadoop.hive.metastore.api.Function.getFunctionName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.getFunctionType()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.getOwnerName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.getOwnerType()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.getResourceUris()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.getResourceUrisIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.Function.getResourceUrisSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.Function.hashCode()",1,16,16
"org.apache.hadoop.hive.metastore.api.Function.isSet(_Fields)",10,9,10
"org.apache.hadoop.hive.metastore.api.Function.isSetClassName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.isSetCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.isSetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.isSetFunctionName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.isSetFunctionType()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.isSetOwnerName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.isSetOwnerType()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.isSetResourceUris()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Function.setClassName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.setClassNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Function.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.setCreateTimeIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.setDbNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Function.setFieldValue(_Fields,Object)",2,10,17
"org.apache.hadoop.hive.metastore.api.Function.setFunctionName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.setFunctionNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Function.setFunctionType(FunctionType)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.setFunctionTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Function.setOwnerName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.setOwnerNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Function.setOwnerType(PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.setOwnerTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Function.setResourceUris(List<ResourceUri>)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.setResourceUrisIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Function.toString()",1,15,15
"org.apache.hadoop.hive.metastore.api.Function.unsetClassName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.unsetCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.unsetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.unsetFunctionName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.unsetFunctionType()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.unsetOwnerName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.unsetOwnerType()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.unsetResourceUris()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Function.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.FunctionType.FunctionType(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.FunctionType.findByValue(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.FunctionType.getValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.GetOpenTxnsInfoResponse()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.GetOpenTxnsInfoResponse(GetOpenTxnsInfoResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.GetOpenTxnsInfoResponse(long,List<TxnInfo>)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.GetOpenTxnsInfoResponseStandardScheme.read(TProtocol,GetOpenTxnsInfoResponse)",4,6,9
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.GetOpenTxnsInfoResponseStandardScheme.write(TProtocol,GetOpenTxnsInfoResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.GetOpenTxnsInfoResponseStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.GetOpenTxnsInfoResponseTupleScheme.read(TProtocol,GetOpenTxnsInfoResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.GetOpenTxnsInfoResponseTupleScheme.write(TProtocol,GetOpenTxnsInfoResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.GetOpenTxnsInfoResponseTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.addToOpen_txns(TxnInfo)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.compareTo(GetOpenTxnsInfoResponse)",8,4,8
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.equals(GetOpenTxnsInfoResponse)",8,4,14
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.getOpen_txns()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.getOpen_txnsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.getOpen_txnsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.getTxn_high_water_mark()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.hashCode()",1,4,4
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.isSetOpen_txns()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.isSetTxn_high_water_mark()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.setOpen_txns(List<TxnInfo>)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.setOpen_txnsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.setTxn_high_water_mark(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.setTxn_high_water_markIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.toString()",1,3,3
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.unsetOpen_txns()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.unsetTxn_high_water_mark()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.validate()",3,3,3
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.GetOpenTxnsResponse()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.GetOpenTxnsResponse(GetOpenTxnsResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.GetOpenTxnsResponse(long,Set<Long>)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.GetOpenTxnsResponseStandardScheme.read(TProtocol,GetOpenTxnsResponse)",4,6,9
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.GetOpenTxnsResponseStandardScheme.write(TProtocol,GetOpenTxnsResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.GetOpenTxnsResponseStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.GetOpenTxnsResponseTupleScheme.read(TProtocol,GetOpenTxnsResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.GetOpenTxnsResponseTupleScheme.write(TProtocol,GetOpenTxnsResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.GetOpenTxnsResponseTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.addToOpen_txns(long)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.compareTo(GetOpenTxnsResponse)",8,4,8
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.equals(GetOpenTxnsResponse)",8,4,14
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.getOpen_txns()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.getOpen_txnsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.getOpen_txnsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.getTxn_high_water_mark()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.hashCode()",1,4,4
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.isSetOpen_txns()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.isSetTxn_high_water_mark()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.setOpen_txns(Set<Long>)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.setOpen_txnsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.setTxn_high_water_mark(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.setTxn_high_water_markIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.toString()",1,3,3
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.unsetOpen_txns()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.unsetTxn_high_water_mark()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.validate()",3,3,3
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.GetPrincipalsInRoleRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.GetPrincipalsInRoleRequest(GetPrincipalsInRoleRequest)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.GetPrincipalsInRoleRequest(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.GetPrincipalsInRoleRequestStandardScheme.read(TProtocol,GetPrincipalsInRoleRequest)",4,4,6
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.GetPrincipalsInRoleRequestStandardScheme.write(TProtocol,GetPrincipalsInRoleRequest)",1,2,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.GetPrincipalsInRoleRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.GetPrincipalsInRoleRequestTupleScheme.read(TProtocol,GetPrincipalsInRoleRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.GetPrincipalsInRoleRequestTupleScheme.write(TProtocol,GetPrincipalsInRoleRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.GetPrincipalsInRoleRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.compareTo(GetPrincipalsInRoleRequest)",5,3,5
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.equals(GetPrincipalsInRoleRequest)",5,4,9
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.getRoleName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.isSetRoleName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.setRoleName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.setRoleNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.unsetRoleName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.validate()",2,2,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.GetPrincipalsInRoleResponse()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.GetPrincipalsInRoleResponse(GetPrincipalsInRoleResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.GetPrincipalsInRoleResponse(List<RolePrincipalGrant>)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.GetPrincipalsInRoleResponseStandardScheme.read(TProtocol,GetPrincipalsInRoleResponse)",4,5,7
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.GetPrincipalsInRoleResponseStandardScheme.write(TProtocol,GetPrincipalsInRoleResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.GetPrincipalsInRoleResponseStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.GetPrincipalsInRoleResponseTupleScheme.read(TProtocol,GetPrincipalsInRoleResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.GetPrincipalsInRoleResponseTupleScheme.write(TProtocol,GetPrincipalsInRoleResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.GetPrincipalsInRoleResponseTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.addToPrincipalGrants(RolePrincipalGrant)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.compareTo(GetPrincipalsInRoleResponse)",5,3,5
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.equals(GetPrincipalsInRoleResponse)",5,4,9
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.getPrincipalGrants()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.getPrincipalGrantsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.getPrincipalGrantsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.isSetPrincipalGrants()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.setPrincipalGrants(List<RolePrincipalGrant>)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.setPrincipalGrantsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.unsetPrincipalGrants()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.validate()",2,2,2
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetPrincipalsInRoleResponse.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.GetRoleGrantsForPrincipalRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.GetRoleGrantsForPrincipalRequest(GetRoleGrantsForPrincipalRequest)",1,1,3
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.GetRoleGrantsForPrincipalRequest(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.GetRoleGrantsForPrincipalRequestStandardScheme.read(TProtocol,GetRoleGrantsForPrincipalRequest)",4,5,8
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.GetRoleGrantsForPrincipalRequestStandardScheme.write(TProtocol,GetRoleGrantsForPrincipalRequest)",1,3,3
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.GetRoleGrantsForPrincipalRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.GetRoleGrantsForPrincipalRequestTupleScheme.read(TProtocol,GetRoleGrantsForPrincipalRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.GetRoleGrantsForPrincipalRequestTupleScheme.write(TProtocol,GetRoleGrantsForPrincipalRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.GetRoleGrantsForPrincipalRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.compareTo(GetRoleGrantsForPrincipalRequest)",8,4,8
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.equals(GetRoleGrantsForPrincipalRequest)",8,7,16
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.getPrincipal_name()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.getPrincipal_type()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.hashCode()",1,5,5
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.isSetPrincipal_name()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.isSetPrincipal_type()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.setPrincipal_name(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.setPrincipal_nameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.setPrincipal_type(PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.setPrincipal_typeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.toString()",1,4,4
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.unsetPrincipal_name()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.unsetPrincipal_type()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.validate()",3,3,3
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.GetRoleGrantsForPrincipalResponse()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.GetRoleGrantsForPrincipalResponse(GetRoleGrantsForPrincipalResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.GetRoleGrantsForPrincipalResponse(List<RolePrincipalGrant>)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.GetRoleGrantsForPrincipalResponseStandardScheme.read(TProtocol,GetRoleGrantsForPrincipalResponse)",4,5,7
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.GetRoleGrantsForPrincipalResponseStandardScheme.write(TProtocol,GetRoleGrantsForPrincipalResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.GetRoleGrantsForPrincipalResponseStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.GetRoleGrantsForPrincipalResponseTupleScheme.read(TProtocol,GetRoleGrantsForPrincipalResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.GetRoleGrantsForPrincipalResponseTupleScheme.write(TProtocol,GetRoleGrantsForPrincipalResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.GetRoleGrantsForPrincipalResponseTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.addToPrincipalGrants(RolePrincipalGrant)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.compareTo(GetRoleGrantsForPrincipalResponse)",5,3,5
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.equals(GetRoleGrantsForPrincipalResponse)",5,4,9
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.getPrincipalGrants()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.getPrincipalGrantsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.getPrincipalGrantsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.isSetPrincipalGrants()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.setPrincipalGrants(List<RolePrincipalGrant>)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.setPrincipalGrantsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.unsetPrincipalGrants()",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.validate()",2,2,2
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GetRoleGrantsForPrincipalResponse.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.GrantRevokePrivilegeRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.GrantRevokePrivilegeRequest(GrantRevokePrivilegeRequest)",1,1,3
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.GrantRevokePrivilegeRequest(GrantRevokeType,PrivilegeBag)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.GrantRevokePrivilegeRequestStandardScheme.read(TProtocol,GrantRevokePrivilegeRequest)",4,6,10
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.GrantRevokePrivilegeRequestStandardScheme.write(TProtocol,GrantRevokePrivilegeRequest)",1,4,4
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.GrantRevokePrivilegeRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.GrantRevokePrivilegeRequestTupleScheme.read(TProtocol,GrantRevokePrivilegeRequest)",1,4,4
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.GrantRevokePrivilegeRequestTupleScheme.write(TProtocol,GrantRevokePrivilegeRequest)",1,7,7
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.GrantRevokePrivilegeRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.compareTo(GrantRevokePrivilegeRequest)",11,5,11
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.equals(GrantRevokePrivilegeRequest)",11,9,23
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.getPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.getRequestType()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.hashCode()",1,7,7
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.isRevokeGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.isSetPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.isSetRequestType()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.isSetRevokeGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.setPrivileges(PrivilegeBag)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.setPrivilegesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.setRequestType(GrantRevokeType)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.setRequestTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.setRevokeGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.setRevokeGrantOptionIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.toString()",1,6,6
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.unsetPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.unsetRequestType()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.unsetRevokeGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.validate()",1,2,2
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.GrantRevokePrivilegeResponse()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.GrantRevokePrivilegeResponse(GrantRevokePrivilegeResponse)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.GrantRevokePrivilegeResponseStandardScheme.read(TProtocol,GrantRevokePrivilegeResponse)",4,4,6
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.GrantRevokePrivilegeResponseStandardScheme.write(TProtocol,GrantRevokePrivilegeResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.GrantRevokePrivilegeResponseStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.GrantRevokePrivilegeResponseTupleScheme.read(TProtocol,GrantRevokePrivilegeResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.GrantRevokePrivilegeResponseTupleScheme.write(TProtocol,GrantRevokePrivilegeResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.GrantRevokePrivilegeResponseTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.compareTo(GrantRevokePrivilegeResponse)",5,3,5
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.equals(GrantRevokePrivilegeResponse)",5,3,9
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.isSetSuccess()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.isSuccess()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.setSuccess(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.setSuccessIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.unsetSuccess()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokePrivilegeResponse.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.GrantRevokeRoleRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.GrantRevokeRoleRequest(GrantRevokeRoleRequest)",1,1,7
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.GrantRevokeRoleRequest(GrantRevokeType,String,String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.GrantRevokeRoleRequestStandardScheme.read(TProtocol,GrantRevokeRoleRequest)",4,10,18
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.GrantRevokeRoleRequestStandardScheme.write(TProtocol,GrantRevokeRoleRequest)",1,10,10
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.GrantRevokeRoleRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.GrantRevokeRoleRequestTupleScheme.read(TProtocol,GrantRevokeRoleRequest)",1,8,8
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.GrantRevokeRoleRequestTupleScheme.write(TProtocol,GrantRevokeRoleRequest)",1,15,15
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.GrantRevokeRoleRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest._Fields.findByThriftId(int)",9,2,9
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.compareTo(GrantRevokeRoleRequest)",23,9,23
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.equals(GrantRevokeRoleRequest)",23,21,51
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.getFieldValue(_Fields)",8,8,8
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.getGrantor()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.getGrantorType()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.getPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.getPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.getRequestType()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.getRoleName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.hashCode()",1,15,15
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.isGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.isSet(_Fields)",9,8,9
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.isSetGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.isSetGrantor()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.isSetGrantorType()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.isSetPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.isSetPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.isSetRequestType()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.isSetRoleName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setFieldValue(_Fields,Object)",2,9,15
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setGrantOptionIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setGrantor(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setGrantorIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setGrantorType(PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setGrantorTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setPrincipalName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setPrincipalNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setPrincipalType(PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setPrincipalTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setRequestType(GrantRevokeType)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setRequestTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setRoleName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.setRoleNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.toString()",1,16,16
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.unsetGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.unsetGrantor()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.unsetGrantorType()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.unsetPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.unsetPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.unsetRequestType()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.unsetRoleName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.GrantRevokeRoleResponse()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.GrantRevokeRoleResponse(GrantRevokeRoleResponse)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.GrantRevokeRoleResponseStandardScheme.read(TProtocol,GrantRevokeRoleResponse)",4,4,6
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.GrantRevokeRoleResponseStandardScheme.write(TProtocol,GrantRevokeRoleResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.GrantRevokeRoleResponseStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.GrantRevokeRoleResponseTupleScheme.read(TProtocol,GrantRevokeRoleResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.GrantRevokeRoleResponseTupleScheme.write(TProtocol,GrantRevokeRoleResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.GrantRevokeRoleResponseTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.compareTo(GrantRevokeRoleResponse)",5,3,5
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.equals(GrantRevokeRoleResponse)",5,3,9
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.isSetSuccess()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.isSuccess()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.setSuccess(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.setSuccessIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.unsetSuccess()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeRoleResponse.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.GrantRevokeType.GrantRevokeType(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.GrantRevokeType.findByValue(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.GrantRevokeType.getValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.HeartbeatRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.HeartbeatRequest(HeartbeatRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.HeartbeatRequestStandardScheme.read(TProtocol,HeartbeatRequest)",4,5,8
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.HeartbeatRequestStandardScheme.write(TProtocol,HeartbeatRequest)",1,3,3
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.HeartbeatRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.HeartbeatRequestTupleScheme.read(TProtocol,HeartbeatRequest)",1,3,3
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.HeartbeatRequestTupleScheme.write(TProtocol,HeartbeatRequest)",1,5,5
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.HeartbeatRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.compareTo(HeartbeatRequest)",8,4,8
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.equals(HeartbeatRequest)",8,5,16
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.getLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.getTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.hashCode()",1,5,5
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.isSetLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.isSetTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.setLockid(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.setLockidIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.setTxnid(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.setTxnidIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.toString()",1,4,4
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.unsetLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.unsetTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.HeartbeatTxnRangeRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.HeartbeatTxnRangeRequest(HeartbeatTxnRangeRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.HeartbeatTxnRangeRequest(long,long)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.HeartbeatTxnRangeRequestStandardScheme.read(TProtocol,HeartbeatTxnRangeRequest)",4,5,8
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.HeartbeatTxnRangeRequestStandardScheme.write(TProtocol,HeartbeatTxnRangeRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.HeartbeatTxnRangeRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.HeartbeatTxnRangeRequestTupleScheme.read(TProtocol,HeartbeatTxnRangeRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.HeartbeatTxnRangeRequestTupleScheme.write(TProtocol,HeartbeatTxnRangeRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.HeartbeatTxnRangeRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.compareTo(HeartbeatTxnRangeRequest)",8,4,8
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.equals(HeartbeatTxnRangeRequest)",8,1,12
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.getMax()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.getMin()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.isSetMax()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.isSetMin()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.setMax(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.setMaxIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.setMin(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.setMinIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.unsetMax()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.unsetMin()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.validate()",3,3,3
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.HeartbeatTxnRangeResponse()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.HeartbeatTxnRangeResponse(HeartbeatTxnRangeResponse)",1,5,5
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.HeartbeatTxnRangeResponse(Set<Long>,Set<Long>)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.HeartbeatTxnRangeResponseStandardScheme.read(TProtocol,HeartbeatTxnRangeResponse)",4,7,10
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.HeartbeatTxnRangeResponseStandardScheme.write(TProtocol,HeartbeatTxnRangeResponse)",1,5,5
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.HeartbeatTxnRangeResponseStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.HeartbeatTxnRangeResponseTupleScheme.read(TProtocol,HeartbeatTxnRangeResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.HeartbeatTxnRangeResponseTupleScheme.write(TProtocol,HeartbeatTxnRangeResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.HeartbeatTxnRangeResponseTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.addToAborted(long)",1,1,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.addToNosuch(long)",1,1,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.compareTo(HeartbeatTxnRangeResponse)",8,4,8
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.equals(HeartbeatTxnRangeResponse)",8,7,16
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.getAborted()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.getAbortedIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.getAbortedSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.getNosuch()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.getNosuchIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.getNosuchSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.hashCode()",1,5,5
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.isSetAborted()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.isSetNosuch()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.setAborted(Set<Long>)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.setAbortedIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.setNosuch(Set<Long>)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.setNosuchIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.toString()",1,4,4
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.unsetAborted()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.unsetNosuch()",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.validate()",3,3,3
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.HiveObjectPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.HiveObjectPrivilege(HiveObjectPrivilege)",1,1,5
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.HiveObjectPrivilege(HiveObjectRef,String,PrincipalType,PrivilegeGrantInfo)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.HiveObjectPrivilegeStandardScheme.read(TProtocol,HiveObjectPrivilege)",4,7,12
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.HiveObjectPrivilegeStandardScheme.write(TProtocol,HiveObjectPrivilege)",1,5,5
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.HiveObjectPrivilegeStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.HiveObjectPrivilegeTupleScheme.read(TProtocol,HiveObjectPrivilege)",1,5,5
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.HiveObjectPrivilegeTupleScheme.write(TProtocol,HiveObjectPrivilege)",1,9,9
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.HiveObjectPrivilegeTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege._Fields.findByThriftId(int)",6,2,6
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.compareTo(HiveObjectPrivilege)",14,6,14
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.equals(HiveObjectPrivilege)",14,13,30
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.getFieldValue(_Fields)",5,5,5
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.getGrantInfo()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.getHiveObject()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.getPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.getPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.hashCode()",1,9,9
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.isSet(_Fields)",6,5,6
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.isSetGrantInfo()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.isSetHiveObject()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.isSetPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.isSetPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.setGrantInfo(PrivilegeGrantInfo)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.setGrantInfoIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.setHiveObject(HiveObjectRef)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.setHiveObjectIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.setPrincipalName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.setPrincipalNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.setPrincipalType(PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.setPrincipalTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.toString()",1,8,8
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.unsetGrantInfo()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.unsetHiveObject()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.unsetPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.unsetPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.validate()",1,3,3
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.HiveObjectRef()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.HiveObjectRef(HiveObjectRef)",1,3,7
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.HiveObjectRef(HiveObjectType,String,String,List<String>,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.HiveObjectRefStandardScheme.read(TProtocol,HiveObjectRef)",4,9,15
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.HiveObjectRefStandardScheme.write(TProtocol,HiveObjectRef)",1,7,7
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.HiveObjectRefStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.HiveObjectRefTupleScheme.read(TProtocol,HiveObjectRef)",1,7,7
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.HiveObjectRefTupleScheme.write(TProtocol,HiveObjectRef)",1,12,12
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.HiveObjectRefTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef._Fields.findByThriftId(int)",7,2,7
"org.apache.hadoop.hive.metastore.api.HiveObjectRef._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectRef._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.addToPartValues(String)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.compareTo(HiveObjectRef)",17,7,17
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.equals(HiveObjectRef)",17,16,37
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.getColumnName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.getFieldValue(_Fields)",6,6,6
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.getObjectName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.getObjectType()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.getPartValues()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.getPartValuesIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.getPartValuesSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.hashCode()",1,11,11
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.isSet(_Fields)",7,6,7
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.isSetColumnName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.isSetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.isSetObjectName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.isSetObjectType()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.isSetPartValues()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.setColumnName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.setColumnNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.setDbNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.setFieldValue(_Fields,Object)",2,7,11
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.setObjectName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.setObjectNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.setObjectType(HiveObjectType)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.setObjectTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.setPartValues(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.setPartValuesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.toString()",1,10,10
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.unsetColumnName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.unsetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.unsetObjectName()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.unsetObjectType()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.unsetPartValues()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectRef.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.HiveObjectType.HiveObjectType(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.HiveObjectType.findByValue(int)",7,2,7
"org.apache.hadoop.hive.metastore.api.HiveObjectType.getValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.Index()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.Index(Index)",1,3,9
"org.apache.hadoop.hive.metastore.api.Index.Index(String,String,String,String,int,int,String,StorageDescriptor,Map<String, String>,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.IndexStandardScheme.read(TProtocol,Index)",4,14,25
"org.apache.hadoop.hive.metastore.api.Index.IndexStandardScheme.write(TProtocol,Index)",1,9,9
"org.apache.hadoop.hive.metastore.api.Index.IndexStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.IndexTupleScheme.read(TProtocol,Index)",1,12,12
"org.apache.hadoop.hive.metastore.api.Index.IndexTupleScheme.write(TProtocol,Index)",1,22,22
"org.apache.hadoop.hive.metastore.api.Index.IndexTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index._Fields.findByThriftId(int)",12,2,12
"org.apache.hadoop.hive.metastore.api.Index._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.Index._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.compareTo(Index)",32,12,32
"org.apache.hadoop.hive.metastore.api.Index.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.equals(Index)",32,22,66
"org.apache.hadoop.hive.metastore.api.Index.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.Index.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.getFieldValue(_Fields)",11,11,11
"org.apache.hadoop.hive.metastore.api.Index.getIndexHandlerClass()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.getIndexName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.getIndexTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.getLastAccessTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.getOrigTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.getParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.getParametersSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.Index.getSd()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.hashCode()",1,18,18
"org.apache.hadoop.hive.metastore.api.Index.isDeferredRebuild()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.isSet(_Fields)",12,11,12
"org.apache.hadoop.hive.metastore.api.Index.isSetCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.isSetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.isSetDeferredRebuild()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.isSetIndexHandlerClass()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.isSetIndexName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.isSetIndexTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.isSetLastAccessTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.isSetOrigTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.isSetParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.isSetSd()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.putToParameters(String,String)",1,1,2
"org.apache.hadoop.hive.metastore.api.Index.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Index.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.setCreateTimeIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.setDbNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Index.setDeferredRebuild(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.setDeferredRebuildIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.setFieldValue(_Fields,Object)",2,12,21
"org.apache.hadoop.hive.metastore.api.Index.setIndexHandlerClass(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.setIndexHandlerClassIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Index.setIndexName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.setIndexNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Index.setIndexTableName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.setIndexTableNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Index.setLastAccessTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.setLastAccessTimeIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.setOrigTableName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.setOrigTableNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Index.setParameters(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.setParametersIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Index.setSd(StorageDescriptor)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.setSdIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Index.toString()",1,17,17
"org.apache.hadoop.hive.metastore.api.Index.unsetCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.unsetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.unsetDeferredRebuild()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.unsetIndexHandlerClass()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.unsetIndexName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.unsetIndexTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.unsetLastAccessTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.unsetOrigTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.unsetParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.unsetSd()",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.validate()",1,2,2
"org.apache.hadoop.hive.metastore.api.Index.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Index.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.IndexAlreadyExistsException()",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.IndexAlreadyExistsException(IndexAlreadyExistsException)",1,1,2
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.IndexAlreadyExistsException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.IndexAlreadyExistsExceptionStandardScheme.read(TProtocol,IndexAlreadyExistsException)",4,4,6
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.IndexAlreadyExistsExceptionStandardScheme.write(TProtocol,IndexAlreadyExistsException)",1,2,2
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.IndexAlreadyExistsExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.IndexAlreadyExistsExceptionTupleScheme.read(TProtocol,IndexAlreadyExistsException)",1,2,2
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.IndexAlreadyExistsExceptionTupleScheme.write(TProtocol,IndexAlreadyExistsException)",1,3,3
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.IndexAlreadyExistsExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.compareTo(IndexAlreadyExistsException)",5,3,5
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.equals(IndexAlreadyExistsException)",5,4,9
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.IndexAlreadyExistsException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidInputException.InvalidInputException()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException.InvalidInputException(InvalidInputException)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidInputException.InvalidInputException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException.InvalidInputExceptionStandardScheme.read(TProtocol,InvalidInputException)",4,4,6
"org.apache.hadoop.hive.metastore.api.InvalidInputException.InvalidInputExceptionStandardScheme.write(TProtocol,InvalidInputException)",1,2,2
"org.apache.hadoop.hive.metastore.api.InvalidInputException.InvalidInputExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException.InvalidInputExceptionTupleScheme.read(TProtocol,InvalidInputException)",1,2,2
"org.apache.hadoop.hive.metastore.api.InvalidInputException.InvalidInputExceptionTupleScheme.write(TProtocol,InvalidInputException)",1,3,3
"org.apache.hadoop.hive.metastore.api.InvalidInputException.InvalidInputExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.InvalidInputException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.InvalidInputException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException.compareTo(InvalidInputException)",5,3,5
"org.apache.hadoop.hive.metastore.api.InvalidInputException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException.equals(InvalidInputException)",5,4,9
"org.apache.hadoop.hive.metastore.api.InvalidInputException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.InvalidInputException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.InvalidInputException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.InvalidInputException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.InvalidInputException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidInputException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.InvalidInputException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidInputException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.InvalidInputException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidInputException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.InvalidObjectException()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.InvalidObjectException(InvalidObjectException)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.InvalidObjectException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.InvalidObjectExceptionStandardScheme.read(TProtocol,InvalidObjectException)",4,4,6
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.InvalidObjectExceptionStandardScheme.write(TProtocol,InvalidObjectException)",1,2,2
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.InvalidObjectExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.InvalidObjectExceptionTupleScheme.read(TProtocol,InvalidObjectException)",1,2,2
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.InvalidObjectExceptionTupleScheme.write(TProtocol,InvalidObjectException)",1,3,3
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.InvalidObjectExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.InvalidObjectException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.InvalidObjectException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.compareTo(InvalidObjectException)",5,3,5
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.equals(InvalidObjectException)",5,4,9
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidObjectException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.InvalidOperationException()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.InvalidOperationException(InvalidOperationException)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.InvalidOperationException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.InvalidOperationExceptionStandardScheme.read(TProtocol,InvalidOperationException)",4,4,6
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.InvalidOperationExceptionStandardScheme.write(TProtocol,InvalidOperationException)",1,2,2
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.InvalidOperationExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.InvalidOperationExceptionTupleScheme.read(TProtocol,InvalidOperationException)",1,2,2
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.InvalidOperationExceptionTupleScheme.write(TProtocol,InvalidOperationException)",1,3,3
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.InvalidOperationExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.InvalidOperationException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.InvalidOperationException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.compareTo(InvalidOperationException)",5,3,5
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.equals(InvalidOperationException)",5,4,9
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidOperationException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.InvalidPartitionException()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.InvalidPartitionException(InvalidPartitionException)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.InvalidPartitionException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.InvalidPartitionExceptionStandardScheme.read(TProtocol,InvalidPartitionException)",4,4,6
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.InvalidPartitionExceptionStandardScheme.write(TProtocol,InvalidPartitionException)",1,2,2
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.InvalidPartitionExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.InvalidPartitionExceptionTupleScheme.read(TProtocol,InvalidPartitionException)",1,2,2
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.InvalidPartitionExceptionTupleScheme.write(TProtocol,InvalidPartitionException)",1,3,3
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.InvalidPartitionExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.compareTo(InvalidPartitionException)",5,3,5
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.equals(InvalidPartitionException)",5,4,9
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.InvalidPartitionException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockComponent.LockComponent()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.LockComponent(LockComponent)",1,1,6
"org.apache.hadoop.hive.metastore.api.LockComponent.LockComponent(LockType,LockLevel,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.LockComponentStandardScheme.read(TProtocol,LockComponent)",4,8,14
"org.apache.hadoop.hive.metastore.api.LockComponent.LockComponentStandardScheme.write(TProtocol,LockComponent)",1,8,8
"org.apache.hadoop.hive.metastore.api.LockComponent.LockComponentStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.LockComponentTupleScheme.read(TProtocol,LockComponent)",1,3,3
"org.apache.hadoop.hive.metastore.api.LockComponent.LockComponentTupleScheme.write(TProtocol,LockComponent)",1,5,5
"org.apache.hadoop.hive.metastore.api.LockComponent.LockComponentTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent._Fields.findByThriftId(int)",7,2,7
"org.apache.hadoop.hive.metastore.api.LockComponent._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.LockComponent._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.compareTo(LockComponent)",17,7,17
"org.apache.hadoop.hive.metastore.api.LockComponent.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.equals(LockComponent)",17,16,37
"org.apache.hadoop.hive.metastore.api.LockComponent.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.LockComponent.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.getDbname()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.getFieldValue(_Fields)",6,6,6
"org.apache.hadoop.hive.metastore.api.LockComponent.getLevel()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.getPartitionname()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.getTablename()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.getType()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.hashCode()",1,11,11
"org.apache.hadoop.hive.metastore.api.LockComponent.isSet(_Fields)",7,6,7
"org.apache.hadoop.hive.metastore.api.LockComponent.isSetDbname()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.isSetLevel()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.isSetPartitionname()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.isSetTablename()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.isSetType()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockComponent.setDbname(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.setDbnameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockComponent.setFieldValue(_Fields,Object)",2,7,11
"org.apache.hadoop.hive.metastore.api.LockComponent.setLevel(LockLevel)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.setLevelIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockComponent.setPartitionname(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.setPartitionnameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockComponent.setTablename(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.setTablenameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockComponent.setType(LockType)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.setTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockComponent.toString()",1,12,12
"org.apache.hadoop.hive.metastore.api.LockComponent.unsetDbname()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.unsetLevel()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.unsetPartitionname()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.unsetTablename()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.unsetType()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.validate()",4,4,4
"org.apache.hadoop.hive.metastore.api.LockComponent.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockComponent.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockLevel.LockLevel(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockLevel.findByValue(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.LockLevel.getValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.LockRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.LockRequest(List<LockComponent>,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.LockRequest(LockRequest)",1,3,5
"org.apache.hadoop.hive.metastore.api.LockRequest.LockRequestStandardScheme.read(TProtocol,LockRequest)",4,8,13
"org.apache.hadoop.hive.metastore.api.LockRequest.LockRequestStandardScheme.write(TProtocol,LockRequest)",1,6,6
"org.apache.hadoop.hive.metastore.api.LockRequest.LockRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.LockRequestTupleScheme.read(TProtocol,LockRequest)",1,3,3
"org.apache.hadoop.hive.metastore.api.LockRequest.LockRequestTupleScheme.write(TProtocol,LockRequest)",1,4,4
"org.apache.hadoop.hive.metastore.api.LockRequest.LockRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest._Fields.findByThriftId(int)",6,2,6
"org.apache.hadoop.hive.metastore.api.LockRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.LockRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.addToComponent(LockComponent)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.compareTo(LockRequest)",14,6,14
"org.apache.hadoop.hive.metastore.api.LockRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.equals(LockRequest)",14,12,30
"org.apache.hadoop.hive.metastore.api.LockRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.LockRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.getComponent()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.getComponentIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.LockRequest.getComponentSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.LockRequest.getFieldValue(_Fields)",5,5,5
"org.apache.hadoop.hive.metastore.api.LockRequest.getHostname()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.getTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.getUser()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.hashCode()",1,9,9
"org.apache.hadoop.hive.metastore.api.LockRequest.isSet(_Fields)",6,5,6
"org.apache.hadoop.hive.metastore.api.LockRequest.isSetComponent()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.isSetHostname()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.isSetTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.isSetUser()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockRequest.setComponent(List<LockComponent>)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.setComponentIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockRequest.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hadoop.hive.metastore.api.LockRequest.setHostname(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.setHostnameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockRequest.setTxnid(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.setTxnidIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.setUser(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.setUserIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockRequest.toString()",1,8,8
"org.apache.hadoop.hive.metastore.api.LockRequest.unsetComponent()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.unsetHostname()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.unsetTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.unsetUser()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.validate()",4,4,4
"org.apache.hadoop.hive.metastore.api.LockRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockResponse.LockResponse()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.LockResponse(LockResponse)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockResponse.LockResponse(long,LockState)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.LockResponseStandardScheme.read(TProtocol,LockResponse)",4,5,8
"org.apache.hadoop.hive.metastore.api.LockResponse.LockResponseStandardScheme.write(TProtocol,LockResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.LockResponse.LockResponseStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.LockResponseTupleScheme.read(TProtocol,LockResponse)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.LockResponseTupleScheme.write(TProtocol,LockResponse)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.LockResponseTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.LockResponse._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.LockResponse._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.compareTo(LockResponse)",8,4,8
"org.apache.hadoop.hive.metastore.api.LockResponse.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.equals(LockResponse)",8,4,14
"org.apache.hadoop.hive.metastore.api.LockResponse.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.LockResponse.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.LockResponse.getLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.getState()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.hashCode()",1,4,4
"org.apache.hadoop.hive.metastore.api.LockResponse.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.LockResponse.isSetLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.isSetState()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockResponse.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.LockResponse.setLockid(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.setLockidIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.setState(LockState)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.setStateIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockResponse.toString()",1,3,3
"org.apache.hadoop.hive.metastore.api.LockResponse.unsetLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.unsetState()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.validate()",3,3,3
"org.apache.hadoop.hive.metastore.api.LockResponse.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockResponse.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.LockState.LockState(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockState.findByValue(int)",6,2,6
"org.apache.hadoop.hive.metastore.api.LockState.getValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.LockType.LockType(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.LockType.findByValue(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.LockType.getValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.LongColumnStatsData()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.LongColumnStatsData(LongColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.LongColumnStatsData(long,long)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.LongColumnStatsDataStandardScheme.read(TProtocol,LongColumnStatsData)",4,7,12
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.LongColumnStatsDataStandardScheme.write(TProtocol,LongColumnStatsData)",1,3,3
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.LongColumnStatsDataStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.LongColumnStatsDataTupleScheme.read(TProtocol,LongColumnStatsData)",1,3,3
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.LongColumnStatsDataTupleScheme.write(TProtocol,LongColumnStatsData)",1,5,5
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.LongColumnStatsDataTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData._Fields.findByThriftId(int)",6,2,6
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.compareTo(LongColumnStatsData)",14,6,14
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.equals(LongColumnStatsData)",14,5,26
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.getFieldValue(_Fields)",5,5,5
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.getHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.getLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.getNumDVs()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.getNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.hashCode()",1,7,7
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.isSet(_Fields)",6,5,6
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.isSetHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.isSetLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.isSetNumDVs()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.isSetNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.setHighValue(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.setHighValueIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.setLowValue(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.setLowValueIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.setNumDVs(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.setNumDVsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.setNumNulls(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.setNumNullsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.toString()",1,6,6
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.unsetHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.unsetLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.unsetNumDVs()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.unsetNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.validate()",3,3,3
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.LongColumnStatsData.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.MetaException.MetaException()",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException.MetaException(MetaException)",1,1,2
"org.apache.hadoop.hive.metastore.api.MetaException.MetaException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException.MetaExceptionStandardScheme.read(TProtocol,MetaException)",4,4,6
"org.apache.hadoop.hive.metastore.api.MetaException.MetaExceptionStandardScheme.write(TProtocol,MetaException)",1,2,2
"org.apache.hadoop.hive.metastore.api.MetaException.MetaExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException.MetaExceptionTupleScheme.read(TProtocol,MetaException)",1,2,2
"org.apache.hadoop.hive.metastore.api.MetaException.MetaExceptionTupleScheme.write(TProtocol,MetaException)",1,3,3
"org.apache.hadoop.hive.metastore.api.MetaException.MetaExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.MetaException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.MetaException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException.compareTo(MetaException)",5,3,5
"org.apache.hadoop.hive.metastore.api.MetaException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException.equals(MetaException)",5,4,9
"org.apache.hadoop.hive.metastore.api.MetaException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.MetaException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.MetaException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.MetaException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.MetaException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.MetaException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.MetaException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.MetaException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.MetaException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.MetaException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.NoSuchLockException()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.NoSuchLockException(NoSuchLockException)",1,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.NoSuchLockException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.NoSuchLockExceptionStandardScheme.read(TProtocol,NoSuchLockException)",4,4,6
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.NoSuchLockExceptionStandardScheme.write(TProtocol,NoSuchLockException)",1,2,2
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.NoSuchLockExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.NoSuchLockExceptionTupleScheme.read(TProtocol,NoSuchLockException)",1,2,2
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.NoSuchLockExceptionTupleScheme.write(TProtocol,NoSuchLockException)",1,3,3
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.NoSuchLockExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.NoSuchLockException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchLockException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.compareTo(NoSuchLockException)",5,3,5
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.equals(NoSuchLockException)",5,4,9
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchLockException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.NoSuchObjectException()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.NoSuchObjectException(NoSuchObjectException)",1,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.NoSuchObjectException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.NoSuchObjectExceptionStandardScheme.read(TProtocol,NoSuchObjectException)",4,4,6
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.NoSuchObjectExceptionStandardScheme.write(TProtocol,NoSuchObjectException)",1,2,2
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.NoSuchObjectExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.NoSuchObjectExceptionTupleScheme.read(TProtocol,NoSuchObjectException)",1,2,2
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.NoSuchObjectExceptionTupleScheme.write(TProtocol,NoSuchObjectException)",1,3,3
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.NoSuchObjectExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.compareTo(NoSuchObjectException)",5,3,5
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.equals(NoSuchObjectException)",5,4,9
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchObjectException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.NoSuchTxnException()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.NoSuchTxnException(NoSuchTxnException)",1,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.NoSuchTxnException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.NoSuchTxnExceptionStandardScheme.read(TProtocol,NoSuchTxnException)",4,4,6
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.NoSuchTxnExceptionStandardScheme.write(TProtocol,NoSuchTxnException)",1,2,2
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.NoSuchTxnExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.NoSuchTxnExceptionTupleScheme.read(TProtocol,NoSuchTxnException)",1,2,2
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.NoSuchTxnExceptionTupleScheme.write(TProtocol,NoSuchTxnException)",1,3,3
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.NoSuchTxnExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.compareTo(NoSuchTxnException)",5,3,5
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.equals(NoSuchTxnException)",5,4,9
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.NoSuchTxnException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.OpenTxnRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.OpenTxnRequest(OpenTxnRequest)",1,1,3
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.OpenTxnRequest(int,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.OpenTxnRequestStandardScheme.read(TProtocol,OpenTxnRequest)",4,6,10
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.OpenTxnRequestStandardScheme.write(TProtocol,OpenTxnRequest)",1,3,3
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.OpenTxnRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.OpenTxnRequestTupleScheme.read(TProtocol,OpenTxnRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.OpenTxnRequestTupleScheme.write(TProtocol,OpenTxnRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.OpenTxnRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.compareTo(OpenTxnRequest)",11,5,11
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.equals(OpenTxnRequest)",11,7,21
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.getHostname()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.getNum_txns()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.getUser()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.hashCode()",1,6,6
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.isSetHostname()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.isSetNum_txns()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.isSetUser()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.setHostname(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.setHostnameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.setNum_txns(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.setNum_txnsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.setUser(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.setUserIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.toString()",1,5,5
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.unsetHostname()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.unsetNum_txns()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.unsetUser()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.validate()",4,4,4
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.OpenTxnsResponse()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.OpenTxnsResponse(List<Long>)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.OpenTxnsResponse(OpenTxnsResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.OpenTxnsResponseStandardScheme.read(TProtocol,OpenTxnsResponse)",4,5,7
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.OpenTxnsResponseStandardScheme.write(TProtocol,OpenTxnsResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.OpenTxnsResponseStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.OpenTxnsResponseTupleScheme.read(TProtocol,OpenTxnsResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.OpenTxnsResponseTupleScheme.write(TProtocol,OpenTxnsResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.OpenTxnsResponseTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.addToTxn_ids(long)",1,1,2
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.compareTo(OpenTxnsResponse)",5,3,5
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.equals(OpenTxnsResponse)",5,4,9
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.getTxn_ids()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.getTxn_idsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.getTxn_idsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.isSetTxn_ids()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.setTxn_ids(List<Long>)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.setTxn_idsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.unsetTxn_ids()",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.validate()",2,2,2
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Order.Order()",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.Order(Order)",1,1,2
"org.apache.hadoop.hive.metastore.api.Order.Order(String,int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.OrderStandardScheme.read(TProtocol,Order)",4,5,8
"org.apache.hadoop.hive.metastore.api.Order.OrderStandardScheme.write(TProtocol,Order)",1,2,2
"org.apache.hadoop.hive.metastore.api.Order.OrderStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.OrderTupleScheme.read(TProtocol,Order)",1,3,3
"org.apache.hadoop.hive.metastore.api.Order.OrderTupleScheme.write(TProtocol,Order)",1,5,5
"org.apache.hadoop.hive.metastore.api.Order.OrderTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Order._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Order._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Order._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.Order._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.Order._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Order._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.compareTo(Order)",8,4,8
"org.apache.hadoop.hive.metastore.api.Order.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.Order.equals(Order)",8,4,14
"org.apache.hadoop.hive.metastore.api.Order.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.getCol()",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.Order.getOrder()",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.hashCode()",1,4,4
"org.apache.hadoop.hive.metastore.api.Order.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.Order.isSetCol()",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.isSetOrder()",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Order.setCol(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.setColIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Order.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.Order.setOrder(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.setOrderIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.toString()",1,3,3
"org.apache.hadoop.hive.metastore.api.Order.unsetCol()",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.unsetOrder()",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Order.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Partition.Partition()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.Partition(List<String>,String,String,int,int,StorageDescriptor,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.Partition(Partition)",1,5,9
"org.apache.hadoop.hive.metastore.api.Partition.PartitionStandardScheme.read(TProtocol,Partition)",4,13,22
"org.apache.hadoop.hive.metastore.api.Partition.PartitionStandardScheme.write(TProtocol,Partition)",1,10,10
"org.apache.hadoop.hive.metastore.api.Partition.PartitionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.PartitionTupleScheme.read(TProtocol,Partition)",1,11,11
"org.apache.hadoop.hive.metastore.api.Partition.PartitionTupleScheme.write(TProtocol,Partition)",1,19,19
"org.apache.hadoop.hive.metastore.api.Partition.PartitionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition._Fields.findByThriftId(int)",10,2,10
"org.apache.hadoop.hive.metastore.api.Partition._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.Partition._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.addToValues(String)",1,1,2
"org.apache.hadoop.hive.metastore.api.Partition.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.compareTo(Partition)",26,10,26
"org.apache.hadoop.hive.metastore.api.Partition.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.Partition.equals(Partition)",26,19,54
"org.apache.hadoop.hive.metastore.api.Partition.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.getFieldValue(_Fields)",9,9,9
"org.apache.hadoop.hive.metastore.api.Partition.getLastAccessTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.getParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.getParametersSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.Partition.getPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.getSd()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.getTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.getValues()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.getValuesIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.Partition.getValuesSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.Partition.hashCode()",1,15,15
"org.apache.hadoop.hive.metastore.api.Partition.isSet(_Fields)",10,9,10
"org.apache.hadoop.hive.metastore.api.Partition.isSetCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.isSetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.isSetLastAccessTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.isSetParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.isSetPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.isSetSd()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.isSetTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.isSetValues()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.putToParameters(String,String)",1,1,2
"org.apache.hadoop.hive.metastore.api.Partition.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Partition.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.setCreateTimeIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.setDbNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Partition.setFieldValue(_Fields,Object)",2,10,17
"org.apache.hadoop.hive.metastore.api.Partition.setLastAccessTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.setLastAccessTimeIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.setParameters(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.setParametersIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Partition.setPrivileges(PrincipalPrivilegeSet)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.setPrivilegesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Partition.setSd(StorageDescriptor)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.setSdIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Partition.setTableName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.setTableNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Partition.setValues(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.setValuesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Partition.toString()",1,15,15
"org.apache.hadoop.hive.metastore.api.Partition.unsetCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.unsetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.unsetLastAccessTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.unsetParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.unsetPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.unsetSd()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.unsetTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.unsetValues()",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.validate()",1,3,3
"org.apache.hadoop.hive.metastore.api.Partition.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Partition.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionEventType.PartitionEventType(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionEventType.findByValue(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.PartitionEventType.getValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.PartitionsByExprRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.PartitionsByExprRequest(PartitionsByExprRequest)",1,2,5
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.PartitionsByExprRequest(String,String,ByteBuffer)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.PartitionsByExprRequestStandardScheme.read(TProtocol,PartitionsByExprRequest)",4,8,14
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.PartitionsByExprRequestStandardScheme.write(TProtocol,PartitionsByExprRequest)",1,7,7
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.PartitionsByExprRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.PartitionsByExprRequestTupleScheme.read(TProtocol,PartitionsByExprRequest)",1,3,3
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.PartitionsByExprRequestTupleScheme.write(TProtocol,PartitionsByExprRequest)",1,5,5
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.PartitionsByExprRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest._Fields.findByThriftId(int)",7,2,7
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.bufferForExpr()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.compareTo(PartitionsByExprRequest)",17,7,17
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.equals(PartitionsByExprRequest)",17,15,37
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.getDefaultPartitionName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.getExpr()",1,2,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.getFieldValue(_Fields)",6,6,6
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.getMaxParts()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.getTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.hashCode()",1,11,11
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.isSet(_Fields)",7,6,7
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.isSetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.isSetDefaultPartitionName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.isSetExpr()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.isSetMaxParts()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.isSetTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.setDbNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.setDefaultPartitionName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.setDefaultPartitionNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.setExpr(ByteBuffer)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.setExpr(byte[])",1,2,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.setExprIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.setFieldValue(_Fields,Object)",2,7,11
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.setMaxParts(short)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.setMaxPartsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.setTblName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.setTblNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.toString()",1,11,11
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.unsetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.unsetDefaultPartitionName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.unsetExpr()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.unsetMaxParts()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.unsetTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.validate()",4,4,4
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.PartitionsByExprResult()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.PartitionsByExprResult(List<Partition>,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.PartitionsByExprResult(PartitionsByExprResult)",1,3,3
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.PartitionsByExprResultStandardScheme.read(TProtocol,PartitionsByExprResult)",4,6,9
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.PartitionsByExprResultStandardScheme.write(TProtocol,PartitionsByExprResult)",1,3,3
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.PartitionsByExprResultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.PartitionsByExprResultTupleScheme.read(TProtocol,PartitionsByExprResult)",1,2,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.PartitionsByExprResultTupleScheme.write(TProtocol,PartitionsByExprResult)",1,2,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.PartitionsByExprResultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.addToPartitions(Partition)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.compareTo(PartitionsByExprResult)",8,4,8
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.equals(PartitionsByExprResult)",8,4,14
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.getPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.getPartitionsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.getPartitionsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.hashCode()",1,4,4
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.isHasUnknownPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.isSetHasUnknownPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.isSetPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.setHasUnknownPartitions(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.setHasUnknownPartitionsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.setPartitions(List<Partition>)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.setPartitionsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.toString()",1,3,3
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.unsetHasUnknownPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.unsetPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.validate()",3,3,3
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.PartitionsStatsRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.PartitionsStatsRequest(PartitionsStatsRequest)",1,5,7
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.PartitionsStatsRequest(String,String,List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.PartitionsStatsRequestStandardScheme.read(TProtocol,PartitionsStatsRequest)",4,9,14
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.PartitionsStatsRequestStandardScheme.write(TProtocol,PartitionsStatsRequest)",1,7,7
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.PartitionsStatsRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.PartitionsStatsRequestTupleScheme.read(TProtocol,PartitionsStatsRequest)",1,3,3
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.PartitionsStatsRequestTupleScheme.write(TProtocol,PartitionsStatsRequest)",1,3,3
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.PartitionsStatsRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest._Fields.findByThriftId(int)",6,2,6
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.addToColNames(String)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.addToPartNames(String)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.compareTo(PartitionsStatsRequest)",14,6,14
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.equals(PartitionsStatsRequest)",14,13,30
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.getColNames()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.getColNamesIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.getColNamesSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.getFieldValue(_Fields)",5,5,5
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.getPartNames()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.getPartNamesIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.getPartNamesSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.getTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.hashCode()",1,9,9
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.isSet(_Fields)",6,5,6
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.isSetColNames()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.isSetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.isSetPartNames()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.isSetTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.setColNames(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.setColNamesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.setDbNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.setPartNames(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.setPartNamesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.setTblName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.setTblNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.toString()",1,8,8
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.unsetColNames()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.unsetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.unsetPartNames()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.unsetTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.validate()",5,5,5
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.PartitionsStatsResult()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.PartitionsStatsResult(Map<String, List<ColumnStatisticsObj>>)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.PartitionsStatsResult(PartitionsStatsResult)",1,4,4
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.PartitionsStatsResultStandardScheme.read(TProtocol,PartitionsStatsResult)",4,6,8
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.PartitionsStatsResultStandardScheme.write(TProtocol,PartitionsStatsResult)",1,4,4
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.PartitionsStatsResultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.PartitionsStatsResultTupleScheme.read(TProtocol,PartitionsStatsResult)",1,3,3
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.PartitionsStatsResultTupleScheme.write(TProtocol,PartitionsStatsResult)",1,3,3
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.PartitionsStatsResultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.compareTo(PartitionsStatsResult)",5,3,5
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.equals(PartitionsStatsResult)",5,4,9
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.getPartStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.getPartStatsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.isSetPartStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.putToPartStats(String,List<ColumnStatisticsObj>)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.setPartStats(Map<String, List<ColumnStatisticsObj>>)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.setPartStatsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.unsetPartStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.validate()",2,2,2
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.PrincipalPrivilegeSet()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.PrincipalPrivilegeSet(Map<String, List<PrivilegeGrantInfo>>,Map<String, List<PrivilegeGrantInfo>>,Map<String, List<PrivilegeGrantInfo>>)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.PrincipalPrivilegeSet(PrincipalPrivilegeSet)",1,10,10
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.PrincipalPrivilegeSetStandardScheme.read(TProtocol,PrincipalPrivilegeSet)",4,12,16
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.PrincipalPrivilegeSetStandardScheme.write(TProtocol,PrincipalPrivilegeSet)",1,10,10
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.PrincipalPrivilegeSetStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.PrincipalPrivilegeSetTupleScheme.read(TProtocol,PrincipalPrivilegeSet)",1,10,10
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.PrincipalPrivilegeSetTupleScheme.write(TProtocol,PrincipalPrivilegeSet)",1,13,13
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.PrincipalPrivilegeSetTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.compareTo(PrincipalPrivilegeSet)",11,5,11
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.equals(PrincipalPrivilegeSet)",11,10,23
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.getGroupPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.getGroupPrivilegesSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.getRolePrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.getRolePrivilegesSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.getUserPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.getUserPrivilegesSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.hashCode()",1,7,7
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.isSetGroupPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.isSetRolePrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.isSetUserPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.putToGroupPrivileges(String,List<PrivilegeGrantInfo>)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.putToRolePrivileges(String,List<PrivilegeGrantInfo>)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.putToUserPrivileges(String,List<PrivilegeGrantInfo>)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.setGroupPrivileges(Map<String, List<PrivilegeGrantInfo>>)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.setGroupPrivilegesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.setRolePrivileges(Map<String, List<PrivilegeGrantInfo>>)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.setRolePrivilegesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.setUserPrivileges(Map<String, List<PrivilegeGrantInfo>>)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.setUserPrivilegesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.toString()",1,6,6
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.unsetGroupPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.unsetRolePrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.unsetUserPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrincipalType.PrincipalType(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrincipalType.findByValue(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.PrincipalType.getValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.PrivilegeBag()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.PrivilegeBag(List<HiveObjectPrivilege>)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.PrivilegeBag(PrivilegeBag)",1,3,3
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.PrivilegeBagStandardScheme.read(TProtocol,PrivilegeBag)",4,5,7
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.PrivilegeBagStandardScheme.write(TProtocol,PrivilegeBag)",1,3,3
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.PrivilegeBagStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.PrivilegeBagTupleScheme.read(TProtocol,PrivilegeBag)",1,3,3
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.PrivilegeBagTupleScheme.write(TProtocol,PrivilegeBag)",1,4,4
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.PrivilegeBagTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.PrivilegeBag._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.PrivilegeBag._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.addToPrivileges(HiveObjectPrivilege)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.compareTo(PrivilegeBag)",5,3,5
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.equals(PrivilegeBag)",5,4,9
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.getPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.getPrivilegesIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.getPrivilegesSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.isSetPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.setPrivileges(List<HiveObjectPrivilege>)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.setPrivilegesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.unsetPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeBag.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.PrivilegeGrantInfo()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.PrivilegeGrantInfo(PrivilegeGrantInfo)",1,1,4
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.PrivilegeGrantInfo(String,int,String,PrincipalType,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.PrivilegeGrantInfoStandardScheme.read(TProtocol,PrivilegeGrantInfo)",4,8,14
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.PrivilegeGrantInfoStandardScheme.write(TProtocol,PrivilegeGrantInfo)",1,4,4
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.PrivilegeGrantInfoStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.PrivilegeGrantInfoTupleScheme.read(TProtocol,PrivilegeGrantInfo)",1,6,6
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.PrivilegeGrantInfoTupleScheme.write(TProtocol,PrivilegeGrantInfo)",1,11,11
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.PrivilegeGrantInfoTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo._Fields.findByThriftId(int)",7,2,7
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.compareTo(PrivilegeGrantInfo)",17,7,17
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.equals(PrivilegeGrantInfo)",17,10,33
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.getFieldValue(_Fields)",6,6,6
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.getGrantor()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.getGrantorType()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.getPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.hashCode()",1,9,9
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.isGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.isSet(_Fields)",7,6,7
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.isSetCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.isSetGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.isSetGrantor()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.isSetGrantorType()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.isSetPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.setCreateTimeIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.setFieldValue(_Fields,Object)",2,7,11
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.setGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.setGrantOptionIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.setGrantor(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.setGrantorIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.setGrantorType(PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.setGrantorTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.setPrivilege(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.setPrivilegeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.toString()",1,8,8
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.unsetCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.unsetGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.unsetGrantor()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.unsetGrantorType()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.unsetPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.RequestPartsSpec()",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.RequestPartsSpec(RequestPartsSpec)",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.RequestPartsSpec(_Fields,Object)",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.checkType(_Fields,Object)",2,2,6
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.compareTo(RequestPartsSpec)",2,2,2
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.enumForId(short)",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.equals(Object)",2,2,2
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.equals(RequestPartsSpec)",1,3,3
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.exprs(List<DropPartitionsExpr>)",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.getExprs()",2,2,2
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.getFieldDesc(_Fields)",4,2,4
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.getNames()",2,2,2
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.getStructDesc()",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.isSetExprs()",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.isSetNames()",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.names(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.setExprs(List<DropPartitionsExpr>)",2,1,2
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.setNames(List<String>)",2,1,2
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.standardSchemeReadValue(TProtocol,TField)",7,9,9
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.standardSchemeWriteValue(TProtocol)",2,4,6
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.tupleSchemeReadValue(TProtocol,short)",3,5,7
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.tupleSchemeWriteValue(TProtocol)",2,4,6
"org.apache.hadoop.hive.metastore.api.RequestPartsSpec.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ResourceType.ResourceType(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceType.findByValue(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.ResourceType.getValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.ResourceUri()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.ResourceUri(ResourceType,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.ResourceUri(ResourceUri)",1,1,3
"org.apache.hadoop.hive.metastore.api.ResourceUri.ResourceUriStandardScheme.read(TProtocol,ResourceUri)",4,5,8
"org.apache.hadoop.hive.metastore.api.ResourceUri.ResourceUriStandardScheme.write(TProtocol,ResourceUri)",1,3,3
"org.apache.hadoop.hive.metastore.api.ResourceUri.ResourceUriStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.ResourceUriTupleScheme.read(TProtocol,ResourceUri)",1,3,3
"org.apache.hadoop.hive.metastore.api.ResourceUri.ResourceUriTupleScheme.write(TProtocol,ResourceUri)",1,5,5
"org.apache.hadoop.hive.metastore.api.ResourceUri.ResourceUriTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.ResourceUri._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.ResourceUri._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.compareTo(ResourceUri)",8,4,8
"org.apache.hadoop.hive.metastore.api.ResourceUri.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.ResourceUri.equals(ResourceUri)",8,7,16
"org.apache.hadoop.hive.metastore.api.ResourceUri.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.ResourceUri.getResourceType()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.getUri()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.hashCode()",1,5,5
"org.apache.hadoop.hive.metastore.api.ResourceUri.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.ResourceUri.isSetResourceType()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.isSetUri()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ResourceUri.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.ResourceUri.setResourceType(ResourceType)",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.setResourceTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ResourceUri.setUri(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.setUriIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ResourceUri.toString()",1,4,4
"org.apache.hadoop.hive.metastore.api.ResourceUri.unsetResourceType()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.unsetUri()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ResourceUri.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Role.Role()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.Role(Role)",1,1,3
"org.apache.hadoop.hive.metastore.api.Role.Role(String,int,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.RoleStandardScheme.read(TProtocol,Role)",4,6,10
"org.apache.hadoop.hive.metastore.api.Role.RoleStandardScheme.write(TProtocol,Role)",1,3,3
"org.apache.hadoop.hive.metastore.api.Role.RoleStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.RoleTupleScheme.read(TProtocol,Role)",1,4,4
"org.apache.hadoop.hive.metastore.api.Role.RoleTupleScheme.write(TProtocol,Role)",1,7,7
"org.apache.hadoop.hive.metastore.api.Role.RoleTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Role._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Role._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.Role._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.Role._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.compareTo(Role)",11,5,11
"org.apache.hadoop.hive.metastore.api.Role.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.Role.equals(Role)",11,7,21
"org.apache.hadoop.hive.metastore.api.Role.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.metastore.api.Role.getOwnerName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.getRoleName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.hashCode()",1,6,6
"org.apache.hadoop.hive.metastore.api.Role.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.metastore.api.Role.isSetCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.isSetOwnerName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.isSetRoleName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Role.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.setCreateTimeIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.metastore.api.Role.setOwnerName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.setOwnerNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Role.setRoleName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.setRoleNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Role.toString()",1,5,5
"org.apache.hadoop.hive.metastore.api.Role.unsetCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.unsetOwnerName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.unsetRoleName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Role.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.RolePrincipalGrant()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.RolePrincipalGrant(RolePrincipalGrant)",1,1,6
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.RolePrincipalGrant(String,String,PrincipalType,boolean,int,String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.RolePrincipalGrantStandardScheme.read(TProtocol,RolePrincipalGrant)",4,10,18
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.RolePrincipalGrantStandardScheme.write(TProtocol,RolePrincipalGrant)",1,6,6
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.RolePrincipalGrantStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.RolePrincipalGrantTupleScheme.read(TProtocol,RolePrincipalGrant)",1,8,8
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.RolePrincipalGrantTupleScheme.write(TProtocol,RolePrincipalGrant)",1,15,15
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.RolePrincipalGrantTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant._Fields.findByThriftId(int)",9,2,9
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.compareTo(RolePrincipalGrant)",23,9,23
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.equals(RolePrincipalGrant)",23,16,47
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.getFieldValue(_Fields)",8,8,8
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.getGrantTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.getGrantorName()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.getGrantorPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.getPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.getPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.getRoleName()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.hashCode()",1,13,13
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.isGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.isSet(_Fields)",9,8,9
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.isSetGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.isSetGrantTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.isSetGrantorName()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.isSetGrantorPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.isSetPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.isSetPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.isSetRoleName()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setFieldValue(_Fields,Object)",2,9,15
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setGrantOptionIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setGrantTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setGrantTimeIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setGrantorName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setGrantorNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setGrantorPrincipalType(PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setGrantorPrincipalTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setPrincipalName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setPrincipalNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setPrincipalType(PrincipalType)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setPrincipalTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setRoleName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.setRoleNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.toString()",1,12,12
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.unsetGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.unsetGrantTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.unsetGrantorName()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.unsetGrantorPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.unsetPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.unsetPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.unsetRoleName()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.RolePrincipalGrant.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Schema.Schema()",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.Schema(List<FieldSchema>,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.Schema(Schema)",1,5,5
"org.apache.hadoop.hive.metastore.api.Schema.SchemaStandardScheme.read(TProtocol,Schema)",4,7,10
"org.apache.hadoop.hive.metastore.api.Schema.SchemaStandardScheme.write(TProtocol,Schema)",1,5,5
"org.apache.hadoop.hive.metastore.api.Schema.SchemaStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.SchemaTupleScheme.read(TProtocol,Schema)",1,5,5
"org.apache.hadoop.hive.metastore.api.Schema.SchemaTupleScheme.write(TProtocol,Schema)",1,7,7
"org.apache.hadoop.hive.metastore.api.Schema.SchemaTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.Schema._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.Schema._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.addToFieldSchemas(FieldSchema)",1,1,2
"org.apache.hadoop.hive.metastore.api.Schema.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.compareTo(Schema)",8,4,8
"org.apache.hadoop.hive.metastore.api.Schema.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.Schema.equals(Schema)",8,7,16
"org.apache.hadoop.hive.metastore.api.Schema.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.getFieldSchemas()",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.getFieldSchemasIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.Schema.getFieldSchemasSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.Schema.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.Schema.getProperties()",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.getPropertiesSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.Schema.hashCode()",1,5,5
"org.apache.hadoop.hive.metastore.api.Schema.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.Schema.isSetFieldSchemas()",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.isSetProperties()",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.putToProperties(String,String)",1,1,2
"org.apache.hadoop.hive.metastore.api.Schema.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Schema.setFieldSchemas(List<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.setFieldSchemasIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Schema.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.Schema.setProperties(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.setPropertiesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Schema.toString()",1,4,4
"org.apache.hadoop.hive.metastore.api.Schema.unsetFieldSchemas()",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.unsetProperties()",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Schema.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.SerDeInfo.SerDeInfo()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.SerDeInfo(SerDeInfo)",1,3,5
"org.apache.hadoop.hive.metastore.api.SerDeInfo.SerDeInfo(String,String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.SerDeInfoStandardScheme.read(TProtocol,SerDeInfo)",4,7,11
"org.apache.hadoop.hive.metastore.api.SerDeInfo.SerDeInfoStandardScheme.write(TProtocol,SerDeInfo)",1,5,5
"org.apache.hadoop.hive.metastore.api.SerDeInfo.SerDeInfoStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.SerDeInfoTupleScheme.read(TProtocol,SerDeInfo)",1,5,5
"org.apache.hadoop.hive.metastore.api.SerDeInfo.SerDeInfoTupleScheme.write(TProtocol,SerDeInfo)",1,8,8
"org.apache.hadoop.hive.metastore.api.SerDeInfo.SerDeInfoTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.SerDeInfo._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.SerDeInfo._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.compareTo(SerDeInfo)",11,5,11
"org.apache.hadoop.hive.metastore.api.SerDeInfo.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.SerDeInfo.equals(SerDeInfo)",11,10,23
"org.apache.hadoop.hive.metastore.api.SerDeInfo.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.metastore.api.SerDeInfo.getName()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.getParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.getParametersSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.SerDeInfo.getSerializationLib()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.hashCode()",1,7,7
"org.apache.hadoop.hive.metastore.api.SerDeInfo.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.metastore.api.SerDeInfo.isSetName()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.isSetParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.isSetSerializationLib()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.putToParameters(String,String)",1,1,2
"org.apache.hadoop.hive.metastore.api.SerDeInfo.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.SerDeInfo.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.metastore.api.SerDeInfo.setName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.setNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.SerDeInfo.setParameters(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.setParametersIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.SerDeInfo.setSerializationLib(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.setSerializationLibIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.SerDeInfo.toString()",1,6,6
"org.apache.hadoop.hive.metastore.api.SerDeInfo.unsetName()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.unsetParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.unsetSerializationLib()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.SerDeInfo.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.SetPartitionsStatsRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.SetPartitionsStatsRequest(List<ColumnStatistics>)",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.SetPartitionsStatsRequest(SetPartitionsStatsRequest)",1,3,3
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.SetPartitionsStatsRequestStandardScheme.read(TProtocol,SetPartitionsStatsRequest)",4,5,7
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.SetPartitionsStatsRequestStandardScheme.write(TProtocol,SetPartitionsStatsRequest)",1,3,3
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.SetPartitionsStatsRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.SetPartitionsStatsRequestTupleScheme.read(TProtocol,SetPartitionsStatsRequest)",1,2,2
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.SetPartitionsStatsRequestTupleScheme.write(TProtocol,SetPartitionsStatsRequest)",1,2,2
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.SetPartitionsStatsRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.addToColStats(ColumnStatistics)",1,1,2
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.compareTo(SetPartitionsStatsRequest)",5,3,5
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.equals(SetPartitionsStatsRequest)",5,4,9
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.getColStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.getColStatsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.getColStatsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.isSetColStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.setColStats(List<ColumnStatistics>)",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.setColStatsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.unsetColStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.validate()",2,2,2
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.ShowCompactRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.ShowCompactRequest(ShowCompactRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.ShowCompactRequestStandardScheme.read(TProtocol,ShowCompactRequest)",4,3,4
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.ShowCompactRequestStandardScheme.write(TProtocol,ShowCompactRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.ShowCompactRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.ShowCompactRequestTupleScheme.read(TProtocol,ShowCompactRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.ShowCompactRequestTupleScheme.write(TProtocol,ShowCompactRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.ShowCompactRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest._Fields.findByThriftId(int)",2,2,2
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.compareTo(ShowCompactRequest)",2,2,2
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.equals(ShowCompactRequest)",2,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.getFieldValue(_Fields)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.hashCode()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.isSet(_Fields)",2,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.setFieldValue(_Fields,Object)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.toString()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.ShowCompactResponse()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.ShowCompactResponse(List<ShowCompactResponseElement>)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.ShowCompactResponse(ShowCompactResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.ShowCompactResponseStandardScheme.read(TProtocol,ShowCompactResponse)",4,5,7
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.ShowCompactResponseStandardScheme.write(TProtocol,ShowCompactResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.ShowCompactResponseStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.ShowCompactResponseTupleScheme.read(TProtocol,ShowCompactResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.ShowCompactResponseTupleScheme.write(TProtocol,ShowCompactResponse)",1,2,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.ShowCompactResponseTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.addToCompacts(ShowCompactResponseElement)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.compareTo(ShowCompactResponse)",5,3,5
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.equals(ShowCompactResponse)",5,4,9
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.getCompacts()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.getCompactsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.getCompactsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.isSetCompacts()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.setCompacts(List<ShowCompactResponseElement>)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.setCompactsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.unsetCompacts()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.validate()",2,2,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponse.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.ShowCompactResponseElement()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.ShowCompactResponseElement(ShowCompactResponseElement)",1,1,8
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.ShowCompactResponseElement(String,String,String,CompactionType,String,String,long,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.ShowCompactResponseElementStandardScheme.read(TProtocol,ShowCompactResponseElement)",4,11,20
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.ShowCompactResponseElementStandardScheme.write(TProtocol,ShowCompactResponseElement)",1,8,8
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.ShowCompactResponseElementStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.ShowCompactResponseElementTupleScheme.read(TProtocol,ShowCompactResponseElement)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.ShowCompactResponseElementTupleScheme.write(TProtocol,ShowCompactResponseElement)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.ShowCompactResponseElementTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement._Fields.findByThriftId(int)",10,2,10
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.compareTo(ShowCompactResponseElement)",26,10,26
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.equals(ShowCompactResponseElement)",26,22,56
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.getDbname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.getFieldValue(_Fields)",9,9,9
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.getPartitionname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.getRunAs()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.getStart()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.getState()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.getTablename()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.getType()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.getWorkerid()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.hashCode()",1,16,16
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.isSet(_Fields)",10,9,10
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.isSetDbname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.isSetPartitionname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.isSetRunAs()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.isSetStart()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.isSetState()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.isSetTablename()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.isSetType()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.isSetWorkerid()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setDbname(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setDbnameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setFieldValue(_Fields,Object)",2,10,17
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setPartitionname(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setPartitionnameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setRunAs(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setRunAsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setStart(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setStartIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setState(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setStateIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setTablename(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setTablenameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setType(CompactionType)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setWorkerid(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.setWorkeridIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.toString()",1,15,15
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.unsetDbname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.unsetPartitionname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.unsetRunAs()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.unsetStart()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.unsetState()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.unsetTablename()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.unsetType()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.unsetWorkerid()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.validate()",9,9,9
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.ShowLocksRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.ShowLocksRequest(ShowLocksRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.ShowLocksRequestStandardScheme.read(TProtocol,ShowLocksRequest)",4,3,4
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.ShowLocksRequestStandardScheme.write(TProtocol,ShowLocksRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.ShowLocksRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.ShowLocksRequestTupleScheme.read(TProtocol,ShowLocksRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.ShowLocksRequestTupleScheme.write(TProtocol,ShowLocksRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.ShowLocksRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest._Fields.findByThriftId(int)",2,2,2
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.compareTo(ShowLocksRequest)",2,2,2
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.equals(ShowLocksRequest)",2,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.getFieldValue(_Fields)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.hashCode()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.isSet(_Fields)",2,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.setFieldValue(_Fields,Object)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.toString()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.ShowLocksResponse()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.ShowLocksResponse(List<ShowLocksResponseElement>)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.ShowLocksResponse(ShowLocksResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.ShowLocksResponseStandardScheme.read(TProtocol,ShowLocksResponse)",4,5,7
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.ShowLocksResponseStandardScheme.write(TProtocol,ShowLocksResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.ShowLocksResponseStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.ShowLocksResponseTupleScheme.read(TProtocol,ShowLocksResponse)",1,3,3
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.ShowLocksResponseTupleScheme.write(TProtocol,ShowLocksResponse)",1,4,4
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.ShowLocksResponseTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.addToLocks(ShowLocksResponseElement)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.compareTo(ShowLocksResponse)",5,3,5
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.equals(ShowLocksResponse)",5,4,9
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.getLocks()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.getLocksIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.getLocksSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.isSetLocks()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.setLocks(List<ShowLocksResponseElement>)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.setLocksIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.unsetLocks()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponse.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.ShowLocksResponseElement()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.ShowLocksResponseElement(ShowLocksResponseElement)",1,1,8
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.ShowLocksResponseElement(long,String,LockState,LockType,long,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.ShowLocksResponseElementStandardScheme.read(TProtocol,ShowLocksResponseElement)",4,14,26
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.ShowLocksResponseElementStandardScheme.write(TProtocol,ShowLocksResponseElement)",1,12,12
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.ShowLocksResponseElementStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.ShowLocksResponseElementTupleScheme.read(TProtocol,ShowLocksResponseElement)",1,5,5
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.ShowLocksResponseElementTupleScheme.write(TProtocol,ShowLocksResponseElement)",1,9,9
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.ShowLocksResponseElementTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement._Fields.findByThriftId(int)",13,2,13
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.compareTo(ShowLocksResponseElement)",35,13,35
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.equals(ShowLocksResponseElement)",35,26,75
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.getAcquiredat()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.getDbname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.getFieldValue(_Fields)",12,12,12
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.getHostname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.getLastheartbeat()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.getLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.getPartname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.getState()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.getTablename()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.getTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.getType()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.getUser()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.hashCode()",1,21,21
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.isSet(_Fields)",13,12,13
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.isSetAcquiredat()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.isSetDbname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.isSetHostname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.isSetLastheartbeat()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.isSetLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.isSetPartname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.isSetState()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.isSetTablename()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.isSetTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.isSetType()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.isSetUser()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setAcquiredat(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setAcquiredatIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setDbname(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setDbnameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setFieldValue(_Fields,Object)",2,13,23
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setHostname(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setHostnameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setLastheartbeat(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setLastheartbeatIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setLockid(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setLockidIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setPartname(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setPartnameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setState(LockState)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setStateIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setTablename(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setTablenameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setTxnid(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setTxnidIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setType(LockType)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setUser(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.setUserIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.toString()",1,22,22
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.unsetAcquiredat()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.unsetDbname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.unsetHostname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.unsetLastheartbeat()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.unsetLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.unsetPartname()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.unsetState()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.unsetTablename()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.unsetTxnid()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.unsetType()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.unsetUser()",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.validate()",8,8,8
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.SkewedInfo.SkewedInfo()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.SkewedInfo(List<String>,List<List<String>>,Map<List<String>, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.SkewedInfo(SkewedInfo)",1,9,9
"org.apache.hadoop.hive.metastore.api.SkewedInfo.SkewedInfoStandardScheme.read(TProtocol,SkewedInfo)",4,11,15
"org.apache.hadoop.hive.metastore.api.SkewedInfo.SkewedInfoStandardScheme.write(TProtocol,SkewedInfo)",1,9,9
"org.apache.hadoop.hive.metastore.api.SkewedInfo.SkewedInfoStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.SkewedInfoTupleScheme.read(TProtocol,SkewedInfo)",1,9,9
"org.apache.hadoop.hive.metastore.api.SkewedInfo.SkewedInfoTupleScheme.write(TProtocol,SkewedInfo)",1,12,12
"org.apache.hadoop.hive.metastore.api.SkewedInfo.SkewedInfoTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.SkewedInfo._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.SkewedInfo._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.addToSkewedColNames(String)",1,1,2
"org.apache.hadoop.hive.metastore.api.SkewedInfo.addToSkewedColValues(List<String>)",1,1,2
"org.apache.hadoop.hive.metastore.api.SkewedInfo.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.compareTo(SkewedInfo)",11,5,11
"org.apache.hadoop.hive.metastore.api.SkewedInfo.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.SkewedInfo.equals(SkewedInfo)",11,10,23
"org.apache.hadoop.hive.metastore.api.SkewedInfo.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.metastore.api.SkewedInfo.getSkewedColNames()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.getSkewedColNamesIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.SkewedInfo.getSkewedColNamesSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.SkewedInfo.getSkewedColValueLocationMaps()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.getSkewedColValueLocationMapsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.SkewedInfo.getSkewedColValues()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.getSkewedColValuesIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.SkewedInfo.getSkewedColValuesSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.SkewedInfo.hashCode()",1,7,7
"org.apache.hadoop.hive.metastore.api.SkewedInfo.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.metastore.api.SkewedInfo.isSetSkewedColNames()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.isSetSkewedColValueLocationMaps()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.isSetSkewedColValues()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.putToSkewedColValueLocationMaps(List<String>,String)",1,1,2
"org.apache.hadoop.hive.metastore.api.SkewedInfo.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.SkewedInfo.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.metastore.api.SkewedInfo.setSkewedColNames(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.setSkewedColNamesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.SkewedInfo.setSkewedColValueLocationMaps(Map<List<String>, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.setSkewedColValueLocationMapsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.SkewedInfo.setSkewedColValues(List<List<String>>)",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.setSkewedColValuesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.SkewedInfo.toString()",1,6,6
"org.apache.hadoop.hive.metastore.api.SkewedInfo.unsetSkewedColNames()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.unsetSkewedColValueLocationMaps()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.unsetSkewedColValues()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.SkewedInfo.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.StorageDescriptor()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.StorageDescriptor(List<FieldSchema>,String,String,String,boolean,int,SerDeInfo,List<String>,List<Order>,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.StorageDescriptor(StorageDescriptor)",1,9,14
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.StorageDescriptorStandardScheme.read(TProtocol,StorageDescriptor)",4,19,32
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.StorageDescriptorStandardScheme.write(TProtocol,StorageDescriptor)",1,16,16
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.StorageDescriptorStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.StorageDescriptorTupleScheme.read(TProtocol,StorageDescriptor)",1,17,17
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.StorageDescriptorTupleScheme.write(TProtocol,StorageDescriptor)",1,29,29
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.StorageDescriptorTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor._Fields.findByThriftId(int)",14,2,14
"org.apache.hadoop.hive.metastore.api.StorageDescriptor._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.addToBucketCols(String)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.addToCols(FieldSchema)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.addToSortCols(Order)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.compareTo(StorageDescriptor)",38,14,38
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.equals(StorageDescriptor)",38,30,82
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getBucketCols()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getBucketColsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getBucketColsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getCols()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getColsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getColsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getFieldValue(_Fields)",13,13,13
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getInputFormat()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getLocation()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getNumBuckets()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getOutputFormat()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getParametersSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getSerdeInfo()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getSkewedInfo()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getSortCols()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getSortColsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.getSortColsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.hashCode()",1,23,23
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isCompressed()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isSet(_Fields)",14,13,14
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isSetBucketCols()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isSetCols()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isSetCompressed()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isSetInputFormat()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isSetLocation()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isSetNumBuckets()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isSetOutputFormat()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isSetParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isSetSerdeInfo()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isSetSkewedInfo()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isSetSortCols()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isSetStoredAsSubDirectories()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.isStoredAsSubDirectories()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.putToParameters(String,String)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setBucketCols(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setBucketColsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setCols(List<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setColsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setCompressed(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setCompressedIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setFieldValue(_Fields,Object)",2,14,25
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setInputFormat(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setInputFormatIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setLocation(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setLocationIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setNumBuckets(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setNumBucketsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setOutputFormat(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setOutputFormatIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setParameters(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setParametersIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setSerdeInfo(SerDeInfo)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setSerdeInfoIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setSkewedInfo(SkewedInfo)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setSkewedInfoIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setSortCols(List<Order>)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setSortColsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setStoredAsSubDirectories(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.setStoredAsSubDirectoriesIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.toString()",1,23,23
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.unsetBucketCols()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.unsetCols()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.unsetCompressed()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.unsetInputFormat()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.unsetLocation()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.unsetNumBuckets()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.unsetOutputFormat()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.unsetParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.unsetSerdeInfo()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.unsetSkewedInfo()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.unsetSortCols()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.unsetStoredAsSubDirectories()",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.validate()",1,3,3
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.StorageDescriptor.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.StringColumnStatsData()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.StringColumnStatsData(StringColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.StringColumnStatsData(long,double,long,long)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.StringColumnStatsDataStandardScheme.read(TProtocol,StringColumnStatsData)",4,7,12
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.StringColumnStatsDataStandardScheme.write(TProtocol,StringColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.StringColumnStatsDataStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.StringColumnStatsDataTupleScheme.read(TProtocol,StringColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.StringColumnStatsDataTupleScheme.write(TProtocol,StringColumnStatsData)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.StringColumnStatsDataTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData._Fields.findByThriftId(int)",6,2,6
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.compareTo(StringColumnStatsData)",14,6,14
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.equals(StringColumnStatsData)",14,1,22
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.getAvgColLen()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.getFieldValue(_Fields)",5,5,5
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.getMaxColLen()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.getNumDVs()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.getNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.hashCode()",1,5,5
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.isSet(_Fields)",6,5,6
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.isSetAvgColLen()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.isSetMaxColLen()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.isSetNumDVs()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.isSetNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.setAvgColLen(double)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.setAvgColLenIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.setMaxColLen(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.setMaxColLenIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.setNumDVs(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.setNumDVsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.setNumNulls(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.setNumNullsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.toString()",1,4,4
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.unsetAvgColLen()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.unsetMaxColLen()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.unsetNumDVs()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.unsetNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.validate()",5,5,5
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.StringColumnStatsData.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Table.Table()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.Table(String,String,String,int,int,int,StorageDescriptor,List<FieldSchema>,Map<String, String>,String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.Table(Table)",1,5,13
"org.apache.hadoop.hive.metastore.api.Table.TableStandardScheme.read(TProtocol,Table)",4,19,34
"org.apache.hadoop.hive.metastore.api.Table.TableStandardScheme.write(TProtocol,Table)",1,15,15
"org.apache.hadoop.hive.metastore.api.Table.TableStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.TableTupleScheme.read(TProtocol,Table)",1,17,17
"org.apache.hadoop.hive.metastore.api.Table.TableTupleScheme.write(TProtocol,Table)",1,31,31
"org.apache.hadoop.hive.metastore.api.Table.TableTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table._Fields.findByThriftId(int)",16,2,16
"org.apache.hadoop.hive.metastore.api.Table._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.Table._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.addToPartitionKeys(FieldSchema)",1,1,2
"org.apache.hadoop.hive.metastore.api.Table.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.compareTo(Table)",44,16,44
"org.apache.hadoop.hive.metastore.api.Table.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.Table.equals(Table)",44,33,94
"org.apache.hadoop.hive.metastore.api.Table.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.getFieldValue(_Fields)",15,15,15
"org.apache.hadoop.hive.metastore.api.Table.getLastAccessTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.getOwner()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.getParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.getParametersSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.Table.getPartitionKeys()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.getPartitionKeysIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.Table.getPartitionKeysSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.Table.getPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.getRetention()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.getSd()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.getTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.getTableType()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.getViewExpandedText()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.getViewOriginalText()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.hashCode()",1,26,26
"org.apache.hadoop.hive.metastore.api.Table.isSet(_Fields)",16,15,16
"org.apache.hadoop.hive.metastore.api.Table.isSetCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.isSetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.isSetLastAccessTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.isSetOwner()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.isSetParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.isSetPartitionKeys()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.isSetPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.isSetRetention()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.isSetSd()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.isSetTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.isSetTableType()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.isSetTemporary()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.isSetViewExpandedText()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.isSetViewOriginalText()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.isTemporary()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.putToParameters(String,String)",1,1,2
"org.apache.hadoop.hive.metastore.api.Table.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Table.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setCreateTimeIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setDbNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Table.setFieldValue(_Fields,Object)",2,16,29
"org.apache.hadoop.hive.metastore.api.Table.setLastAccessTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setLastAccessTimeIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setOwner(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setOwnerIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Table.setParameters(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setParametersIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Table.setPartitionKeys(List<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setPartitionKeysIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Table.setPrivileges(PrincipalPrivilegeSet)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setPrivilegesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Table.setRetention(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setRetentionIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setSd(StorageDescriptor)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setSdIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Table.setTableName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setTableNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Table.setTableType(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setTableTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Table.setTemporary(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setTemporaryIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setViewExpandedText(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setViewExpandedTextIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Table.setViewOriginalText(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.setViewOriginalTextIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Table.toString()",1,26,26
"org.apache.hadoop.hive.metastore.api.Table.unsetCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.unsetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.unsetLastAccessTime()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.unsetOwner()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.unsetParameters()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.unsetPartitionKeys()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.unsetPrivileges()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.unsetRetention()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.unsetSd()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.unsetTableName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.unsetTableType()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.unsetTemporary()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.unsetViewExpandedText()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.unsetViewOriginalText()",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.validate()",1,3,3
"org.apache.hadoop.hive.metastore.api.Table.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Table.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.TableStatsRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.TableStatsRequest(String,String,List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.TableStatsRequest(TableStatsRequest)",1,3,5
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.TableStatsRequestStandardScheme.read(TProtocol,TableStatsRequest)",4,7,11
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.TableStatsRequestStandardScheme.write(TProtocol,TableStatsRequest)",1,5,5
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.TableStatsRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.TableStatsRequestTupleScheme.read(TProtocol,TableStatsRequest)",1,2,2
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.TableStatsRequestTupleScheme.write(TProtocol,TableStatsRequest)",1,2,2
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.TableStatsRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.TableStatsRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.TableStatsRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.addToColNames(String)",1,1,2
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.compareTo(TableStatsRequest)",11,5,11
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.equals(TableStatsRequest)",11,10,23
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.getColNames()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.getColNamesIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.getColNamesSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.getTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.hashCode()",1,7,7
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.isSetColNames()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.isSetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.isSetTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.setColNames(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.setColNamesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.setDbNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.setTblName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.setTblNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.toString()",1,6,6
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.unsetColNames()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.unsetDbName()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.unsetTblName()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.validate()",4,4,4
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.TableStatsResult.TableStatsResult()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult.TableStatsResult(List<ColumnStatisticsObj>)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult.TableStatsResult(TableStatsResult)",1,3,3
"org.apache.hadoop.hive.metastore.api.TableStatsResult.TableStatsResultStandardScheme.read(TProtocol,TableStatsResult)",4,5,7
"org.apache.hadoop.hive.metastore.api.TableStatsResult.TableStatsResultStandardScheme.write(TProtocol,TableStatsResult)",1,3,3
"org.apache.hadoop.hive.metastore.api.TableStatsResult.TableStatsResultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult.TableStatsResultTupleScheme.read(TProtocol,TableStatsResult)",1,2,2
"org.apache.hadoop.hive.metastore.api.TableStatsResult.TableStatsResultTupleScheme.write(TProtocol,TableStatsResult)",1,2,2
"org.apache.hadoop.hive.metastore.api.TableStatsResult.TableStatsResultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.TableStatsResult._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.TableStatsResult._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult.addToTableStats(ColumnStatisticsObj)",1,1,2
"org.apache.hadoop.hive.metastore.api.TableStatsResult.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult.compareTo(TableStatsResult)",5,3,5
"org.apache.hadoop.hive.metastore.api.TableStatsResult.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.TableStatsResult.equals(TableStatsResult)",5,4,9
"org.apache.hadoop.hive.metastore.api.TableStatsResult.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.TableStatsResult.getTableStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult.getTableStatsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.TableStatsResult.getTableStatsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.TableStatsResult.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.TableStatsResult.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.TableStatsResult.isSetTableStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.TableStatsResult.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.TableStatsResult.setTableStats(List<ColumnStatisticsObj>)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult.setTableStatsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.TableStatsResult.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.TableStatsResult.unsetTableStats()",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult.validate()",2,2,2
"org.apache.hadoop.hive.metastore.api.TableStatsResult.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.TableStatsResult.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.TxnAbortedException()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.TxnAbortedException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.TxnAbortedException(TxnAbortedException)",1,1,2
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.TxnAbortedExceptionStandardScheme.read(TProtocol,TxnAbortedException)",4,4,6
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.TxnAbortedExceptionStandardScheme.write(TProtocol,TxnAbortedException)",1,2,2
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.TxnAbortedExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.TxnAbortedExceptionTupleScheme.read(TProtocol,TxnAbortedException)",1,2,2
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.TxnAbortedExceptionTupleScheme.write(TProtocol,TxnAbortedException)",1,3,3
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.TxnAbortedExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.TxnAbortedException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.TxnAbortedException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.compareTo(TxnAbortedException)",5,3,5
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.equals(TxnAbortedException)",5,4,9
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnAbortedException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.TxnInfo.TxnInfo()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.TxnInfo(TxnInfo)",1,1,4
"org.apache.hadoop.hive.metastore.api.TxnInfo.TxnInfo(long,TxnState,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.TxnInfoStandardScheme.read(TProtocol,TxnInfo)",4,7,12
"org.apache.hadoop.hive.metastore.api.TxnInfo.TxnInfoStandardScheme.write(TProtocol,TxnInfo)",1,4,4
"org.apache.hadoop.hive.metastore.api.TxnInfo.TxnInfoStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.TxnInfoTupleScheme.read(TProtocol,TxnInfo)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.TxnInfoTupleScheme.write(TProtocol,TxnInfo)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.TxnInfoTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo._Fields.findByThriftId(int)",6,2,6
"org.apache.hadoop.hive.metastore.api.TxnInfo._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.TxnInfo._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.compareTo(TxnInfo)",14,6,14
"org.apache.hadoop.hive.metastore.api.TxnInfo.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.TxnInfo.equals(TxnInfo)",14,10,28
"org.apache.hadoop.hive.metastore.api.TxnInfo.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.getFieldValue(_Fields)",5,5,5
"org.apache.hadoop.hive.metastore.api.TxnInfo.getHostname()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.getId()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.getState()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.getUser()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.hashCode()",1,8,8
"org.apache.hadoop.hive.metastore.api.TxnInfo.isSet(_Fields)",6,5,6
"org.apache.hadoop.hive.metastore.api.TxnInfo.isSetHostname()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.isSetId()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.isSetState()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.isSetUser()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.TxnInfo.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hadoop.hive.metastore.api.TxnInfo.setHostname(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.setHostnameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.TxnInfo.setId(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.setIdIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.setState(TxnState)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.setStateIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.TxnInfo.setUser(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.setUserIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.TxnInfo.toString()",1,7,7
"org.apache.hadoop.hive.metastore.api.TxnInfo.unsetHostname()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.unsetId()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.unsetState()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.unsetUser()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.validate()",5,5,5
"org.apache.hadoop.hive.metastore.api.TxnInfo.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnInfo.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.TxnOpenException.TxnOpenException()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException.TxnOpenException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException.TxnOpenException(TxnOpenException)",1,1,2
"org.apache.hadoop.hive.metastore.api.TxnOpenException.TxnOpenExceptionStandardScheme.read(TProtocol,TxnOpenException)",4,4,6
"org.apache.hadoop.hive.metastore.api.TxnOpenException.TxnOpenExceptionStandardScheme.write(TProtocol,TxnOpenException)",1,2,2
"org.apache.hadoop.hive.metastore.api.TxnOpenException.TxnOpenExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException.TxnOpenExceptionTupleScheme.read(TProtocol,TxnOpenException)",1,2,2
"org.apache.hadoop.hive.metastore.api.TxnOpenException.TxnOpenExceptionTupleScheme.write(TProtocol,TxnOpenException)",1,3,3
"org.apache.hadoop.hive.metastore.api.TxnOpenException.TxnOpenExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.TxnOpenException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.TxnOpenException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException.compareTo(TxnOpenException)",5,3,5
"org.apache.hadoop.hive.metastore.api.TxnOpenException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.TxnOpenException.equals(TxnOpenException)",5,4,9
"org.apache.hadoop.hive.metastore.api.TxnOpenException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.TxnOpenException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.TxnOpenException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.TxnOpenException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.TxnOpenException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.TxnOpenException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.TxnOpenException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.TxnOpenException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnOpenException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.TxnState.TxnState(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.TxnState.findByValue(int)",5,2,5
"org.apache.hadoop.hive.metastore.api.TxnState.getValue()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.Type()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.Type(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.Type(Type)",1,3,6
"org.apache.hadoop.hive.metastore.api.Type.TypeStandardScheme.read(TProtocol,Type)",4,8,13
"org.apache.hadoop.hive.metastore.api.Type.TypeStandardScheme.write(TProtocol,Type)",1,9,9
"org.apache.hadoop.hive.metastore.api.Type.TypeStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.TypeTupleScheme.read(TProtocol,Type)",1,6,6
"org.apache.hadoop.hive.metastore.api.Type.TypeTupleScheme.write(TProtocol,Type)",1,10,10
"org.apache.hadoop.hive.metastore.api.Type.TypeTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Type._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Type._Fields.findByThriftId(int)",6,2,6
"org.apache.hadoop.hive.metastore.api.Type._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.Type._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.addToFields(FieldSchema)",1,1,2
"org.apache.hadoop.hive.metastore.api.Type.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.compareTo(Type)",14,6,14
"org.apache.hadoop.hive.metastore.api.Type.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.Type.equals(Type)",14,13,30
"org.apache.hadoop.hive.metastore.api.Type.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.getFieldValue(_Fields)",5,5,5
"org.apache.hadoop.hive.metastore.api.Type.getFields()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.getFieldsIterator()",1,2,2
"org.apache.hadoop.hive.metastore.api.Type.getFieldsSize()",1,2,2
"org.apache.hadoop.hive.metastore.api.Type.getName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.getType1()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.getType2()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.hashCode()",1,9,9
"org.apache.hadoop.hive.metastore.api.Type.isSet(_Fields)",6,5,6
"org.apache.hadoop.hive.metastore.api.Type.isSetFields()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.isSetName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.isSetType1()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.isSetType2()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Type.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hadoop.hive.metastore.api.Type.setFields(List<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.setFieldsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Type.setName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.setNameIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Type.setType1(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.setType1IsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Type.setType2(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.setType2IsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Type.toString()",1,11,11
"org.apache.hadoop.hive.metastore.api.Type.unsetFields()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.unsetName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.unsetType1()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.unsetType2()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Type.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.UnknownDBException.UnknownDBException()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException.UnknownDBException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException.UnknownDBException(UnknownDBException)",1,1,2
"org.apache.hadoop.hive.metastore.api.UnknownDBException.UnknownDBExceptionStandardScheme.read(TProtocol,UnknownDBException)",4,4,6
"org.apache.hadoop.hive.metastore.api.UnknownDBException.UnknownDBExceptionStandardScheme.write(TProtocol,UnknownDBException)",1,2,2
"org.apache.hadoop.hive.metastore.api.UnknownDBException.UnknownDBExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException.UnknownDBExceptionTupleScheme.read(TProtocol,UnknownDBException)",1,2,2
"org.apache.hadoop.hive.metastore.api.UnknownDBException.UnknownDBExceptionTupleScheme.write(TProtocol,UnknownDBException)",1,3,3
"org.apache.hadoop.hive.metastore.api.UnknownDBException.UnknownDBExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.UnknownDBException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.UnknownDBException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException.compareTo(UnknownDBException)",5,3,5
"org.apache.hadoop.hive.metastore.api.UnknownDBException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.UnknownDBException.equals(UnknownDBException)",5,4,9
"org.apache.hadoop.hive.metastore.api.UnknownDBException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.UnknownDBException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.UnknownDBException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.UnknownDBException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.UnknownDBException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.UnknownDBException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.UnknownDBException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.UnknownDBException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownDBException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.UnknownPartitionException()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.UnknownPartitionException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.UnknownPartitionException(UnknownPartitionException)",1,1,2
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.UnknownPartitionExceptionStandardScheme.read(TProtocol,UnknownPartitionException)",4,4,6
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.UnknownPartitionExceptionStandardScheme.write(TProtocol,UnknownPartitionException)",1,2,2
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.UnknownPartitionExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.UnknownPartitionExceptionTupleScheme.read(TProtocol,UnknownPartitionException)",1,2,2
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.UnknownPartitionExceptionTupleScheme.write(TProtocol,UnknownPartitionException)",1,3,3
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.UnknownPartitionExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.compareTo(UnknownPartitionException)",5,3,5
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.equals(UnknownPartitionException)",5,4,9
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownPartitionException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.UnknownTableException.UnknownTableException()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException.UnknownTableException(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException.UnknownTableException(UnknownTableException)",1,1,2
"org.apache.hadoop.hive.metastore.api.UnknownTableException.UnknownTableExceptionStandardScheme.read(TProtocol,UnknownTableException)",4,4,6
"org.apache.hadoop.hive.metastore.api.UnknownTableException.UnknownTableExceptionStandardScheme.write(TProtocol,UnknownTableException)",1,2,2
"org.apache.hadoop.hive.metastore.api.UnknownTableException.UnknownTableExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException.UnknownTableExceptionTupleScheme.read(TProtocol,UnknownTableException)",1,2,2
"org.apache.hadoop.hive.metastore.api.UnknownTableException.UnknownTableExceptionTupleScheme.write(TProtocol,UnknownTableException)",1,3,3
"org.apache.hadoop.hive.metastore.api.UnknownTableException.UnknownTableExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.UnknownTableException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.UnknownTableException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException.compareTo(UnknownTableException)",5,3,5
"org.apache.hadoop.hive.metastore.api.UnknownTableException.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.UnknownTableException.equals(UnknownTableException)",5,4,9
"org.apache.hadoop.hive.metastore.api.UnknownTableException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.UnknownTableException.getMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException.hashCode()",1,3,3
"org.apache.hadoop.hive.metastore.api.UnknownTableException.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.UnknownTableException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.UnknownTableException.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.UnknownTableException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.UnknownTableException.toString()",1,2,2
"org.apache.hadoop.hive.metastore.api.UnknownTableException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnknownTableException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.UnlockRequest.UnlockRequest()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.UnlockRequest(UnlockRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.UnlockRequest(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.UnlockRequestStandardScheme.read(TProtocol,UnlockRequest)",4,4,6
"org.apache.hadoop.hive.metastore.api.UnlockRequest.UnlockRequestStandardScheme.write(TProtocol,UnlockRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.UnlockRequestStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.UnlockRequestTupleScheme.read(TProtocol,UnlockRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.UnlockRequestTupleScheme.write(TProtocol,UnlockRequest)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.UnlockRequestTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.metastore.api.UnlockRequest._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.UnlockRequest._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.compareTo(UnlockRequest)",5,3,5
"org.apache.hadoop.hive.metastore.api.UnlockRequest.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.UnlockRequest.equals(UnlockRequest)",5,1,7
"org.apache.hadoop.hive.metastore.api.UnlockRequest.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.metastore.api.UnlockRequest.getLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.hashCode()",1,2,2
"org.apache.hadoop.hive.metastore.api.UnlockRequest.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.metastore.api.UnlockRequest.isSetLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.UnlockRequest.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.metastore.api.UnlockRequest.setLockid(long)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.setLockidIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.toString()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.unsetLockid()",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.validate()",2,2,2
"org.apache.hadoop.hive.metastore.api.UnlockRequest.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.UnlockRequest.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Version.Version()",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.Version(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.Version(Version)",1,1,3
"org.apache.hadoop.hive.metastore.api.Version.VersionStandardScheme.read(TProtocol,Version)",4,5,8
"org.apache.hadoop.hive.metastore.api.Version.VersionStandardScheme.write(TProtocol,Version)",1,3,3
"org.apache.hadoop.hive.metastore.api.Version.VersionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.VersionTupleScheme.read(TProtocol,Version)",1,3,3
"org.apache.hadoop.hive.metastore.api.Version.VersionTupleScheme.write(TProtocol,Version)",1,5,5
"org.apache.hadoop.hive.metastore.api.Version.VersionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.metastore.api.Version._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Version._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Version._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.metastore.api.Version._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.metastore.api.Version._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.metastore.api.Version._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.clear()",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.compareTo(Version)",8,4,8
"org.apache.hadoop.hive.metastore.api.Version.deepCopy()",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.equals(Object)",3,2,3
"org.apache.hadoop.hive.metastore.api.Version.equals(Version)",8,7,16
"org.apache.hadoop.hive.metastore.api.Version.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.getComments()",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.metastore.api.Version.getVersion()",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.hashCode()",1,5,5
"org.apache.hadoop.hive.metastore.api.Version.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.metastore.api.Version.isSetComments()",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.isSetVersion()",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.metastore.api.Version.setComments(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.setCommentsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Version.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.metastore.api.Version.setVersion(String)",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.setVersionIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.metastore.api.Version.toString()",1,4,4
"org.apache.hadoop.hive.metastore.api.Version.unsetComments()",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.unsetVersion()",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.validate()",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.metastore.api.Version.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.metastore.events.AddPartitionEvent.AddPartitionEvent(Table,List<Partition>,boolean,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.AddPartitionEvent.AddPartitionEvent(Table,Partition,boolean,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.AddPartitionEvent.getPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.events.AddPartitionEvent.getTable()",1,1,1
"org.apache.hadoop.hive.metastore.events.AlterPartitionEvent.AlterPartitionEvent(Partition,Partition,boolean,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.AlterPartitionEvent.getNewPartition()",1,1,1
"org.apache.hadoop.hive.metastore.events.AlterPartitionEvent.getOldPartition()",1,1,1
"org.apache.hadoop.hive.metastore.events.AlterTableEvent.AlterTableEvent(Table,Table,boolean,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.AlterTableEvent.getNewTable()",1,1,1
"org.apache.hadoop.hive.metastore.events.AlterTableEvent.getOldTable()",1,1,1
"org.apache.hadoop.hive.metastore.events.ConfigChangeEvent.ConfigChangeEvent(HMSHandler,String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.events.ConfigChangeEvent.getKey()",1,1,1
"org.apache.hadoop.hive.metastore.events.ConfigChangeEvent.getNewValue()",1,1,1
"org.apache.hadoop.hive.metastore.events.ConfigChangeEvent.getOldValue()",1,1,1
"org.apache.hadoop.hive.metastore.events.CreateDatabaseEvent.CreateDatabaseEvent(Database,boolean,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.CreateDatabaseEvent.getDatabase()",1,1,1
"org.apache.hadoop.hive.metastore.events.CreateTableEvent.CreateTableEvent(Table,boolean,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.CreateTableEvent.getTable()",1,1,1
"org.apache.hadoop.hive.metastore.events.DropDatabaseEvent.DropDatabaseEvent(Database,boolean,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.DropDatabaseEvent.getDatabase()",1,1,1
"org.apache.hadoop.hive.metastore.events.DropPartitionEvent.DropPartitionEvent(Table,Partition,boolean,boolean,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.DropPartitionEvent.getDeleteData()",1,1,1
"org.apache.hadoop.hive.metastore.events.DropPartitionEvent.getPartition()",1,1,1
"org.apache.hadoop.hive.metastore.events.DropPartitionEvent.getTable()",1,1,1
"org.apache.hadoop.hive.metastore.events.DropTableEvent.DropTableEvent(Table,boolean,boolean,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.DropTableEvent.getDeleteData()",1,1,1
"org.apache.hadoop.hive.metastore.events.DropTableEvent.getTable()",1,1,1
"org.apache.hadoop.hive.metastore.events.EventCleanerTask.EventCleanerTask(HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.EventCleanerTask.run()",1,3,3
"org.apache.hadoop.hive.metastore.events.ListenerEvent.ListenerEvent(boolean,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.ListenerEvent.getEnvironmentContext()",1,1,1
"org.apache.hadoop.hive.metastore.events.ListenerEvent.getHandler()",1,1,1
"org.apache.hadoop.hive.metastore.events.ListenerEvent.getStatus()",1,1,1
"org.apache.hadoop.hive.metastore.events.ListenerEvent.setEnvironmentContext(EnvironmentContext)",1,1,1
"org.apache.hadoop.hive.metastore.events.LoadPartitionDoneEvent.LoadPartitionDoneEvent(boolean,Table,Map<String, String>,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.LoadPartitionDoneEvent.getPartitionName()",1,1,1
"org.apache.hadoop.hive.metastore.events.LoadPartitionDoneEvent.getTable()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreAddPartitionEvent.PreAddPartitionEvent(Table,List<Partition>,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.PreAddPartitionEvent.PreAddPartitionEvent(Table,Partition,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.PreAddPartitionEvent.getPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreAddPartitionEvent.getTable()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreAlterPartitionEvent.PreAlterPartitionEvent(String,String,List<String>,Partition,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.PreAlterPartitionEvent.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreAlterPartitionEvent.getNewPartition()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreAlterPartitionEvent.getOldPartVals()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreAlterPartitionEvent.getTableName()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreAlterTableEvent.PreAlterTableEvent(Table,Table,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.PreAlterTableEvent.getNewTable()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreAlterTableEvent.getOldTable()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreAuthorizationCallEvent.PreAuthorizationCallEvent(HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.PreCreateDatabaseEvent.PreCreateDatabaseEvent(Database,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.PreCreateDatabaseEvent.getDatabase()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreCreateTableEvent.PreCreateTableEvent(Table,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.PreCreateTableEvent.getTable()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreDropDatabaseEvent.PreDropDatabaseEvent(Database,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.PreDropDatabaseEvent.getDatabase()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreDropPartitionEvent.PreDropPartitionEvent(Partition,boolean,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.PreDropPartitionEvent.PreDropPartitionEvent(Table,Partition,boolean,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.PreDropPartitionEvent.getDeleteData()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreDropPartitionEvent.getPartition()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreDropPartitionEvent.getTable()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreDropTableEvent.PreDropTableEvent(Table,boolean,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.PreDropTableEvent.getDeleteData()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreDropTableEvent.getTable()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreEventContext.PreEventContext(PreEventType,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.PreEventContext.getEventType()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreEventContext.getHandler()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreLoadPartitionDoneEvent.PreLoadPartitionDoneEvent(String,String,Map<String, String>,HMSHandler)",1,1,1
"org.apache.hadoop.hive.metastore.events.PreLoadPartitionDoneEvent.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreLoadPartitionDoneEvent.getPartitionName()",1,1,1
"org.apache.hadoop.hive.metastore.events.PreLoadPartitionDoneEvent.getTableName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MColumnDescriptor.MColumnDescriptor()",1,1,1
"org.apache.hadoop.hive.metastore.model.MColumnDescriptor.MColumnDescriptor(List<MFieldSchema>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MColumnDescriptor.getCols()",1,1,1
"org.apache.hadoop.hive.metastore.model.MColumnDescriptor.setCols(List<MFieldSchema>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.MDBPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.MDBPrivilege(String,String,MDatabase,String,int,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.getDatabase()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.getGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.getGrantor()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.getGrantorType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.getPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.getPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.getPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.setDatabase(MDatabase)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.setGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.setGrantor(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.setGrantorType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.setPrincipalName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.setPrincipalType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDBPrivilege.setPrivilege(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDatabase.MDatabase()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDatabase.MDatabase(String,String,String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDatabase.getDescription()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDatabase.getLocationUri()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDatabase.getName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDatabase.getOwnerName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDatabase.getOwnerType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDatabase.getParameters()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDatabase.setDescription(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDatabase.setLocationUri(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDatabase.setName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDatabase.setOwnerName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDatabase.setOwnerType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDatabase.setParameters(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDelegationToken.MDelegationToken(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDelegationToken.getTokenIdentifier()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDelegationToken.getTokenStr()",1,1,1
"org.apache.hadoop.hive.metastore.model.MDelegationToken.setTokenIdentifier(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MDelegationToken.setTokenStr(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MFieldSchema.MFieldSchema()",1,1,1
"org.apache.hadoop.hive.metastore.model.MFieldSchema.MFieldSchema(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MFieldSchema.getComment()",1,1,1
"org.apache.hadoop.hive.metastore.model.MFieldSchema.getName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MFieldSchema.getType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MFieldSchema.setComment(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MFieldSchema.setName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MFieldSchema.setType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.MFunction()",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.MFunction(String,MDatabase,String,String,String,int,int,List<MResourceUri>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.getClassName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.getDatabase()",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.getFunctionName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.getFunctionType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.getOwnerName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.getOwnerType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.getResourceUris()",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.setClassName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.setDatabase(MDatabase)",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.setFunctionName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.setFunctionType(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.setOwnerName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.setOwnerType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MFunction.setResourceUris(List<MResourceUri>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.MGlobalPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.MGlobalPrivilege(String,String,String,int,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.getGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.getGrantor()",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.getGrantorType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.getPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.getPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.getPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.setGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.setGrantor(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.setGrantorType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.setPrincipalName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.setPrincipalType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MGlobalPrivilege.setPrivilege(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.MIndex()",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.MIndex(String,MTable,int,int,Map<String, String>,MTable,MStorageDescriptor,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.getDeferredRebuild()",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.getIndexHandlerClass()",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.getIndexName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.getIndexTable()",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.getLastAccessTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.getOrigTable()",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.getParameters()",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.getSd()",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.isDeferredRebuild()",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.setDeferredRebuild(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.setIndexHandlerClass(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.setIndexName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.setIndexTable(MTable)",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.setLastAccessTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.setOrigTable(MTable)",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.setParameters(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MIndex.setSd(MStorageDescriptor)",1,1,1
"org.apache.hadoop.hive.metastore.model.MMasterKey.MMasterKey(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MMasterKey.MMasterKey(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MMasterKey.MMasterKey(int,String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MMasterKey.getKeyId()",1,1,1
"org.apache.hadoop.hive.metastore.model.MMasterKey.getMasterKey()",1,1,1
"org.apache.hadoop.hive.metastore.model.MMasterKey.setKeyId(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MMasterKey.setMasterKey(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MOrder.MOrder(String,int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MOrder.getCol()",1,1,1
"org.apache.hadoop.hive.metastore.model.MOrder.getOrder()",1,1,1
"org.apache.hadoop.hive.metastore.model.MOrder.setCol(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MOrder.setOrder(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.MPartition()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.MPartition(String,MTable,List<String>,int,int,MStorageDescriptor,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.getLastAccessTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.getParameters()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.getPartitionName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.getSd()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.getTable()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.getValues()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.setLastAccessTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.setParameters(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.setPartitionName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.setSd(MStorageDescriptor)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.setTable(MTable)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartition.setValues(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.MPartitionColumnPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.MPartitionColumnPrivilege(String,String,MPartition,String,String,int,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.getColumnName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.getGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.getGrantor()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.getGrantorType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.getPartition()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.getPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.getPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.getPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.setColumnName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.setGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.setGrantor(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.setGrantorType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.setPartition(MPartition)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.setPrincipalName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.setPrincipalType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege.setPrivilege(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.MPartitionColumnStatistics()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getAvgColLen()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getColName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getColType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getDecimalHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getDecimalLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getDoubleHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getDoubleLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getLastAnalyzed()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getLongHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getLongLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getMaxColLen()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getNumDVs()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getNumFalses()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getNumTrues()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getPartition()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getPartitionName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.getTableName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setAvgColLen(double)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setBinaryStats(Long,Long,Double)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setBooleanStats(Long,Long,Long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setColName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setColType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setDecimalHighValue(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setDecimalLowValue(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setDecimalStats(Long,Long,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setDoubleHighValue(Double)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setDoubleLowValue(Double)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setDoubleStats(Long,Long,Double,Double)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setLastAnalyzed(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setLongHighValue(Long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setLongLowValue(Long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setLongStats(Long,Long,Long,Long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setMaxColLen(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setNumDVs(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setNumFalses(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setNumNulls(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setNumTrues(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setPartition(MPartition)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setPartitionName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setStringStats(Long,Long,Long,Double)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.setTableName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionEvent.MPartitionEvent()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionEvent.MPartitionEvent(String,String,String,int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionEvent.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionEvent.setEventTime(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionEvent.setEventType(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionEvent.setPartName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionEvent.setTblName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionEvent.toString()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.MPartitionPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.MPartitionPrivilege(String,String,MPartition,String,int,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.getGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.getGrantor()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.getGrantorType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.getPartition()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.getPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.getPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.getPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.setGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.setGrantor(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.setGrantorType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.setPartition(MPartition)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.setPrincipalName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.setPrincipalType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPartitionPrivilege.setPrivilege(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPrincipalDesc.MPrincipalDesc()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPrincipalDesc.MPrincipalDesc(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPrincipalDesc.equals(Object)",1,2,2
"org.apache.hadoop.hive.metastore.model.MPrincipalDesc.getName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPrincipalDesc.getType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPrincipalDesc.hashCode()",1,1,1
"org.apache.hadoop.hive.metastore.model.MPrincipalDesc.setName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MPrincipalDesc.setType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MResourceUri.MResourceUri()",1,1,1
"org.apache.hadoop.hive.metastore.model.MResourceUri.MResourceUri(int,String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MResourceUri.getResourceType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MResourceUri.getUri()",1,1,1
"org.apache.hadoop.hive.metastore.model.MResourceUri.setResourceType(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MResourceUri.setUri(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MRole.MRole()",1,1,1
"org.apache.hadoop.hive.metastore.model.MRole.MRole(String,int,String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MRole.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MRole.getOwnerName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MRole.getRoleName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MRole.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MRole.setOwnerName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MRole.setRoleName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.MRoleMap()",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.MRoleMap(String,String,MRole,int,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.getAddTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.getGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.getGrantor()",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.getGrantorType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.getPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.getPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.getRole()",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.setAddTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.setGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.setGrantor(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.setGrantorType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.setPrincipalName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.setPrincipalType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MRoleMap.setRole(MRole)",1,1,1
"org.apache.hadoop.hive.metastore.model.MSerDeInfo.MSerDeInfo(String,String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MSerDeInfo.getName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MSerDeInfo.getParameters()",1,1,1
"org.apache.hadoop.hive.metastore.model.MSerDeInfo.getSerializationLib()",1,1,1
"org.apache.hadoop.hive.metastore.model.MSerDeInfo.setName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MSerDeInfo.setParameters(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MSerDeInfo.setSerializationLib(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.MStorageDescriptor()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.MStorageDescriptor(MColumnDescriptor,String,String,String,boolean,int,MSerDeInfo,List<String>,List<MOrder>,Map<String, String>,List<String>,List<MStringList>,Map<MStringList, String>,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.getBucketCols()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.getCD()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.getInputFormat()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.getLocation()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.getNumBuckets()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.getOutputFormat()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.getParameters()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.getSerDeInfo()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.getSkewedColNames()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.getSkewedColValueLocationMaps()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.getSkewedColValues()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.getSortCols()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.isCompressed()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.isStoredAsSubDirectories()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setBucketCols(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setCD(MColumnDescriptor)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setCompressed(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setInputFormat(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setLocation(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setNumBuckets(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setOutputFormat(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setParameters(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setSerDeInfo(MSerDeInfo)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setSkewedColNames(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setSkewedColValueLocationMaps(Map<MStringList, String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setSkewedColValues(List<MStringList>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setSortCols(List<MOrder>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setStoredAsSubDirectories(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStringList.MStringList(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStringList.getInternalList()",1,1,1
"org.apache.hadoop.hive.metastore.model.MStringList.setInternalList(List<String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MStringList.toString()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.MTable()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.MTable(String,MDatabase,MStorageDescriptor,String,int,int,int,List<MFieldSchema>,Map<String, String>,String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.getDatabase()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.getLastAccessTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.getOwner()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.getParameters()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.getPartitionKeys()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.getRetention()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.getSd()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.getTableName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.getTableType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.getViewExpandedText()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.getViewOriginalText()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.setDatabase(MDatabase)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.setLastAccessTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.setOwner(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.setParameters(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.setPartitionKeys(List<MFieldSchema>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.setRetention(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.setSd(MStorageDescriptor)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.setTableName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.setTableType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.setViewExpandedText(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTable.setViewOriginalText(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.MTableColumnPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.MTableColumnPrivilege(String,String,MTable,String,String,int,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.getColumnName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.getGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.getGrantor()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.getGrantorType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.getPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.getPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.getPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.getTable()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.setColumnName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.setGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.setGrantor(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.setGrantorType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.setPrincipalName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.setPrincipalType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.setPrivilege(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege.setTable(MTable)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.MTableColumnStatistics()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getAvgColLen()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getColName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getColType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getDbName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getDecimalHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getDecimalLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getDoubleHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getDoubleLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getLastAnalyzed()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getLongHighValue()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getLongLowValue()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getMaxColLen()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getNumDVs()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getNumFalses()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getNumNulls()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getNumTrues()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getTable()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.getTableName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setAvgColLen(double)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setBinaryStats(Long,Long,Double)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setBooleanStats(Long,Long,Long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setColName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setColType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setDbName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setDecimalHighValue(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setDecimalLowValue(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setDecimalStats(Long,Long,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setDoubleHighValue(double)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setDoubleLowValue(double)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setDoubleStats(Long,Long,Double,Double)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setLastAnalyzed(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setLongHighValue(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setLongLowValue(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setLongStats(Long,Long,Long,Long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setMaxColLen(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setNumDVs(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setNumFalses(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setNumNulls(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setNumTrues(long)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setStringStats(Long,Long,Long,Double)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setTable(MTable)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.setTableName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.MTablePrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.MTablePrivilege(String,String,MTable,String,int,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.getCreateTime()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.getGrantOption()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.getGrantor()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.getGrantorType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.getPrincipalName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.getPrincipalType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.getPrivilege()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.getTable()",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.setGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.setGrantor(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.setGrantorType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.setPrincipalName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.setPrincipalType(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.setPrivilege(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MTablePrivilege.setTable(MTable)",1,1,1
"org.apache.hadoop.hive.metastore.model.MType.MType()",1,1,1
"org.apache.hadoop.hive.metastore.model.MType.MType(String,String,String,List<MFieldSchema>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MType.getFields()",1,1,1
"org.apache.hadoop.hive.metastore.model.MType.getName()",1,1,1
"org.apache.hadoop.hive.metastore.model.MType.getType1()",1,1,1
"org.apache.hadoop.hive.metastore.model.MType.getType2()",1,1,1
"org.apache.hadoop.hive.metastore.model.MType.setFields(List<MFieldSchema>)",1,1,1
"org.apache.hadoop.hive.metastore.model.MType.setName(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MType.setType1(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MType.setType2(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MVersionTable.MVersionTable()",1,1,1
"org.apache.hadoop.hive.metastore.model.MVersionTable.MVersionTable(String,String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MVersionTable.getSchemaVersion()",1,1,1
"org.apache.hadoop.hive.metastore.model.MVersionTable.getVersionComment()",1,1,1
"org.apache.hadoop.hive.metastore.model.MVersionTable.setSchemaVersion(String)",1,1,1
"org.apache.hadoop.hive.metastore.model.MVersionTable.setVersionComment(String)",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.ANTLRNoCaseStringStream.ANTLRNoCaseStringStream(String)",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.ANTLRNoCaseStringStream.LA(int)",3,1,3
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.FilterBuilder.FilterBuilder(boolean)",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.FilterBuilder.append(String)",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.FilterBuilder.getErrorMessage()",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.FilterBuilder.getFilter()",2,1,2
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.FilterBuilder.hasError()",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.FilterBuilder.setError(String)",2,1,2
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.FilterBuilder.toString()",1,1,2
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.LeafNode.accept(TreeVisitor)",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.LeafNode.canJdoUseStringsWithIntegral()",1,1,3
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.LeafNode.generateJDOFilter(Configuration,Table,Map<String, Object>,FilterBuilder)",1,2,2
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.LeafNode.generateJDOFilterGeneral(Map<String, Object>,FilterBuilder)",1,4,4
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.LeafNode.generateJDOFilterOverPartitions(Configuration,Table,Map<String, Object>,FilterBuilder)",4,6,11
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.LeafNode.generateJDOFilterOverTables(Map<String, Object>,FilterBuilder)",6,6,6
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.LeafNode.getJdoFilterPushdownParam(Table,int,FilterBuilder,boolean)",3,8,12
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.LeafNode.getPartColIndexForFilter(Table,FilterBuilder)",4,3,4
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.Operator.Operator(String)",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.Operator.Operator(String,String,String)",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.Operator.fromString(String)",3,2,3
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.Operator.getJdoOp()",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.Operator.getOp()",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.Operator.getSqlOp()",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.Operator.toString()",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.TreeNode.TreeNode()",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.TreeNode.TreeNode(TreeNode,LogicalOperator,TreeNode)",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.TreeNode.accept(TreeVisitor)",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.TreeNode.generateJDOFilter(Configuration,Table,Map<String, Object>,FilterBuilder)",2,4,5
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.TreeNode.getAndOr()",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.TreeNode.getLhs()",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.TreeNode.getRhs()",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.TreeVisitor.beginTreeNode(TreeNode)",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.TreeVisitor.endTreeNode(TreeNode)",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.TreeVisitor.midTreeNode(TreeNode)",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.TreeVisitor.shouldStop()",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.TreeVisitor.visit(LeafNode)",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.TreeVisitor.visit(TreeNode)",2,3,4
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.accept(TreeVisitor)",1,2,2
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.addIntermediateNode(LogicalOperator)",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.addLeafNode(LeafNode)",1,1,2
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.generateJDOFilterFragment(Configuration,Table,Map<String, Object>,FilterBuilder)",2,1,2
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.getRoot()",1,1,1
"org.apache.hadoop.hive.metastore.parser.ExpressionTree.makeFilterForEquals(String,String,String,Map<String, Object>,int,int,boolean,FilterBuilder)",1,4,8
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.HiveMetaTool()",1,1,1
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.executeJDOQLSelect(String)",1,3,3
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.executeJDOQLUpdate(String)",1,2,2
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.init()",1,1,1
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.initObjectStore(HiveConf)",1,2,2
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.listFSRoot()",1,3,3
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.main(String[])",1,25,27
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.printAndExit(HiveMetaTool)",1,1,1
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.printDatabaseURIUpdateSummary(UpdateMDatabaseURIRetVal,boolean)",1,6,6
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.printSerdePropURIUpdateSummary(UpdateSerdeURIRetVal,String,boolean)",1,6,6
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.printTblPropURIUpdateSummary(UpdateMStorageDescriptorTblPropURIRetVal,String,boolean)",1,6,6
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.printTblURIUpdateSummary(UpdateMStorageDescriptorTblURIRetVal,boolean)",1,6,6
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.printUpdateLocations(Map<String, String>)",1,2,2
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.shutdownObjectStore()",1,2,2
"org.apache.hadoop.hive.metastore.tools.HiveMetaTool.updateFSRootLocation(URI,URI,String,String,boolean)",1,3,3
"org.apache.hadoop.hive.metastore.txn.CompactionInfo.getFullPartitionName()",1,3,3
"org.apache.hadoop.hive.metastore.txn.CompactionInfo.getFullTableName()",1,2,2
"org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.CompactionTxnHandler(HiveConf)",1,1,1
"org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.cleanEmptyAbortedTxns()",1,7,8
"org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findNextToCompact(String)",3,6,9
"org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findPotentialCompactions(int)",1,4,4
"org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findReadyToClean()",3,4,7
"org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.markCleaned(CompactionInfo)",1,10,13
"org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.markCompacted(CompactionInfo)",1,4,5
"org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.revokeFromLocalWorkers(String)",1,3,4
"org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.revokeTimedoutWorkers(long)",1,3,4
"org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.setRunAs(long,String)",1,4,5
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.TestCompactionTxnHandler()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.openTxn()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.tearDown()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testFindNextToClean()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testFindNextToCompact()",1,2,2
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testFindNextToCompact2()",1,6,6
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testFindNextToCompactNothingToCompact()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testFindPotentialCompactions()",1,6,6
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testLockNoWait()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testMarkCleaned()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testMarkCleanedCleansTxnsAndTxnComponents()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testMarkCompacted()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testRevokeFromLocalWorkers()",1,4,4
"org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testRevokeTimedOutWorkers()",1,4,4
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.TestTxnHandler()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.heartbeatTxnRange()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.heartbeatTxnRangeOneAborted()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.heartbeatTxnRangeOneCommitted()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.openTxn()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.setUp()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.showLocks()",1,11,12
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.tearDown()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testAbortInvalidTxn()",1,1,2
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testAbortTxn()",1,3,4
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testCheckLockAcquireAfterWaiting()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testCheckLockNoSuchLock()",1,1,2
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testCheckLockTxnAborted()",1,1,2
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testCompactMajorWithPartition()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testCompactMinorNoPartition()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testHeartbeatLock()",1,3,3
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testHeartbeatNoLock()",1,1,2
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testHeartbeatNoTxn()",1,1,2
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testHeartbeatTxnAborted()",1,1,2
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockDbDoesNotLockTableInDifferentDB()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockDbLocksTable()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockDifferentDBs()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockDifferentPartitions()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockDifferentTableDoesntLockPartition()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockDifferentTables()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockEESR()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockEESW()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockESRE()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockESRSR()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockESRSW()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockSRE()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockSRSR()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockSRSW()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockSWSR()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockSWSWSR()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockSWSWSW()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockSameDB()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockSamePartition()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockSameTable()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockTableLocksPartition()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testLockTimeout()",1,1,2
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testMultipleLock()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testMultipleLockWait()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testOpenTxn()",1,3,4
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testUnlockOnAbort()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testUnlockOnCommit()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testUnlockWithTxn()",1,1,2
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testValidTxnsEmpty()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testValidTxnsNoneOpen()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TestTxnHandler.testValidTxnsSomeOpen()",1,2,3
"org.apache.hadoop.hive.metastore.txn.TxnDbUtil.cleanDb()",1,8,11
"org.apache.hadoop.hive.metastore.txn.TxnDbUtil.countLockComponents(long)",2,1,2
"org.apache.hadoop.hive.metastore.txn.TxnDbUtil.findNumCurrentLocks()",2,2,3
"org.apache.hadoop.hive.metastore.txn.TxnDbUtil.getConnection()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TxnDbUtil.prepDb()",1,2,2
"org.apache.hadoop.hive.metastore.txn.TxnDbUtil.setConfValues(HiveConf)",1,1,1
"org.apache.hadoop.hive.metastore.txn.TxnHandler.LockInfo.LockInfo(ResultSet)",3,3,8
"org.apache.hadoop.hive.metastore.txn.TxnHandler.LockInfo.equals(Object)",2,1,3
"org.apache.hadoop.hive.metastore.txn.TxnHandler.LockInfo.toString()",1,3,3
"org.apache.hadoop.hive.metastore.txn.TxnHandler.LockInfoComparator.compare(LockInfo,LockInfo)",7,1,9
"org.apache.hadoop.hive.metastore.txn.TxnHandler.LockInfoComparator.equals(Object)",1,1,1
"org.apache.hadoop.hive.metastore.txn.TxnHandler.TxnHandler(HiveConf)",1,2,2
"org.apache.hadoop.hive.metastore.txn.TxnHandler.abortTxn(AbortTxnRequest)",2,4,5
"org.apache.hadoop.hive.metastore.txn.TxnHandler.abortTxns(Connection,List<Long>)",1,5,5
"org.apache.hadoop.hive.metastore.txn.TxnHandler.acquire(Connection,Statement,long,long)",2,2,2
"org.apache.hadoop.hive.metastore.txn.TxnHandler.buildJumpTable()",2,1,2
"org.apache.hadoop.hive.metastore.txn.TxnHandler.checkLock(CheckLockRequest)",1,4,5
"org.apache.hadoop.hive.metastore.txn.TxnHandler.checkLock(Connection,long,boolean)",17,26,34
"org.apache.hadoop.hive.metastore.txn.TxnHandler.checkQFileTestHack()",3,4,4
"org.apache.hadoop.hive.metastore.txn.TxnHandler.closeDbConn(Connection)",1,3,3
"org.apache.hadoop.hive.metastore.txn.TxnHandler.closeStmt(Statement)",1,3,3
"org.apache.hadoop.hive.metastore.txn.TxnHandler.commitTxn(CommitTxnRequest)",1,4,5
"org.apache.hadoop.hive.metastore.txn.TxnHandler.compact(CompactionRequest)",3,9,12
"org.apache.hadoop.hive.metastore.txn.TxnHandler.createValidTxnList(GetOpenTxnsResponse)",1,1,2
"org.apache.hadoop.hive.metastore.txn.TxnHandler.detectDeadlock(SQLException,String)",3,4,4
"org.apache.hadoop.hive.metastore.txn.TxnHandler.determineDatabaseProduct(Connection)",8,9,9
"org.apache.hadoop.hive.metastore.txn.TxnHandler.getDbConn(int)",1,2,2
"org.apache.hadoop.hive.metastore.txn.TxnHandler.getDbTime(Connection)",3,3,7
"org.apache.hadoop.hive.metastore.txn.TxnHandler.getLockInfoFromLockId(Connection,long)",2,2,3
"org.apache.hadoop.hive.metastore.txn.TxnHandler.getOpenTxns()",3,3,6
"org.apache.hadoop.hive.metastore.txn.TxnHandler.getOpenTxnsInfo()",5,4,9
"org.apache.hadoop.hive.metastore.txn.TxnHandler.getTxnIdFromLockId(Connection,long)",2,1,4
"org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeat(HeartbeatRequest)",1,3,4
"org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatLock(Connection,long)",3,2,3
"org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatTxn(Connection,long)",4,3,4
"org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatTxnRange(HeartbeatTxnRangeRequest)",1,6,7
"org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(Connection,LockRequest,boolean)",3,8,16
"org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(LockRequest)",1,3,4
"org.apache.hadoop.hive.metastore.txn.TxnHandler.lockNoWait(LockRequest)",1,3,4
"org.apache.hadoop.hive.metastore.txn.TxnHandler.numLocksInLockTable()",1,1,1
"org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(OpenTxnRequest)",2,4,7
"org.apache.hadoop.hive.metastore.txn.TxnHandler.setTimeout(long)",1,1,1
"org.apache.hadoop.hive.metastore.txn.TxnHandler.setupJdbcConnectionPool(HiveConf)",4,2,5
"org.apache.hadoop.hive.metastore.txn.TxnHandler.showCompact(ShowCompactRequest)",4,5,11
"org.apache.hadoop.hive.metastore.txn.TxnHandler.showLocks(ShowLocksRequest)",4,8,13
"org.apache.hadoop.hive.metastore.txn.TxnHandler.timeOutLocks(Connection)",1,1,1
"org.apache.hadoop.hive.metastore.txn.TxnHandler.timeOutTxns(Connection)",1,4,4
"org.apache.hadoop.hive.metastore.txn.TxnHandler.unlock(UnlockRequest)",3,5,6
"org.apache.hadoop.hive.metastore.txn.TxnHandler.wait(Connection,Savepoint)",1,1,1
"org.apache.hadoop.hive.ql.BaseTestQueries.setupQFiles(String[])",1,1,2
"org.apache.hadoop.hive.ql.CommandNeedRetryException.CommandNeedRetryException()",1,1,1
"org.apache.hadoop.hive.ql.CommandNeedRetryException.CommandNeedRetryException(String)",1,1,1
"org.apache.hadoop.hive.ql.CommandNeedRetryException.CommandNeedRetryException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.ql.CommandNeedRetryException.CommandNeedRetryException(Throwable)",1,1,1
"org.apache.hadoop.hive.ql.Context.Context(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.Context.Context(Configuration,String)",1,1,1
"org.apache.hadoop.hive.ql.Context.addCS(String,ContentSummary)",1,1,1
"org.apache.hadoop.hive.ql.Context.clear()",1,5,5
"org.apache.hadoop.hive.ql.Context.generateExecutionId()",1,1,1
"org.apache.hadoop.hive.ql.Context.getCS(Path)",1,1,1
"org.apache.hadoop.hive.ql.Context.getCS(String)",1,1,1
"org.apache.hadoop.hive.ql.Context.getCmd()",1,1,1
"org.apache.hadoop.hive.ql.Context.getConf()",1,1,1
"org.apache.hadoop.hive.ql.Context.getExplain()",1,1,1
"org.apache.hadoop.hive.ql.Context.getExplainLogical()",1,1,1
"org.apache.hadoop.hive.ql.Context.getExtTmpPathRelTo(Path)",1,1,1
"org.apache.hadoop.hive.ql.Context.getExternalScratchDir(URI)",1,1,1
"org.apache.hadoop.hive.ql.Context.getExternalTmpPath(Path)",2,2,2
"org.apache.hadoop.hive.ql.Context.getHiveLocks()",1,1,1
"org.apache.hadoop.hive.ql.Context.getHiveTxnManager()",1,1,1
"org.apache.hadoop.hive.ql.Context.getLoadTableOutputMap()",1,1,1
"org.apache.hadoop.hive.ql.Context.getLocalScratchDir(boolean)",1,1,2
"org.apache.hadoop.hive.ql.Context.getLocalTmpPath()",1,1,1
"org.apache.hadoop.hive.ql.Context.getMRScratchDir()",2,3,4
"org.apache.hadoop.hive.ql.Context.getMRTmpPath()",1,1,1
"org.apache.hadoop.hive.ql.Context.getNextStream()",2,4,6
"org.apache.hadoop.hive.ql.Context.getOutputLockObjects()",1,1,1
"org.apache.hadoop.hive.ql.Context.getPathToCS()",1,1,1
"org.apache.hadoop.hive.ql.Context.getResDir()",1,1,1
"org.apache.hadoop.hive.ql.Context.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.Context.getScratchDir(String,String,boolean,String)",4,5,6
"org.apache.hadoop.hive.ql.Context.getStream()",5,7,10
"org.apache.hadoop.hive.ql.Context.getTokenRewriteStream()",1,1,1
"org.apache.hadoop.hive.ql.Context.getTryCount()",1,1,1
"org.apache.hadoop.hive.ql.Context.isHDFSCleanup()",1,1,1
"org.apache.hadoop.hive.ql.Context.isLocalOnlyExecutionMode()",1,1,1
"org.apache.hadoop.hive.ql.Context.isMRTmpFileURI(String)",1,2,2
"org.apache.hadoop.hive.ql.Context.isNeedLockMgr()",1,1,1
"org.apache.hadoop.hive.ql.Context.nextPathId()",1,1,1
"org.apache.hadoop.hive.ql.Context.removeScratchDir()",1,3,3
"org.apache.hadoop.hive.ql.Context.resetStream()",1,1,2
"org.apache.hadoop.hive.ql.Context.restoreOriginalTracker()",1,2,2
"org.apache.hadoop.hive.ql.Context.setCmd(String)",1,1,1
"org.apache.hadoop.hive.ql.Context.setExplain(boolean)",1,1,1
"org.apache.hadoop.hive.ql.Context.setExplainLogical(boolean)",1,1,1
"org.apache.hadoop.hive.ql.Context.setHDFSCleanup(boolean)",1,1,1
"org.apache.hadoop.hive.ql.Context.setHiveLocks(List<HiveLock>)",1,1,1
"org.apache.hadoop.hive.ql.Context.setHiveTxnManager(HiveTxnManager)",1,1,1
"org.apache.hadoop.hive.ql.Context.setNeedLockMgr(boolean)",1,1,1
"org.apache.hadoop.hive.ql.Context.setOriginalTracker(String)",1,1,1
"org.apache.hadoop.hive.ql.Context.setResDir(Path)",1,1,1
"org.apache.hadoop.hive.ql.Context.setResFile(Path)",1,1,1
"org.apache.hadoop.hive.ql.Context.setTokenRewriteStream(TokenRewriteStream)",1,1,1
"org.apache.hadoop.hive.ql.Context.setTryCount(int)",1,1,1
"org.apache.hadoop.hive.ql.Context.strEquals(String,String)",1,1,1
"org.apache.hadoop.hive.ql.Driver.Driver()",1,2,2
"org.apache.hadoop.hive.ql.Driver.Driver(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.Driver.Driver(HiveConf,String)",1,1,1
"org.apache.hadoop.hive.ql.Driver.QueryState.getCmd()",1,1,1
"org.apache.hadoop.hive.ql.Driver.QueryState.getOp()",1,1,1
"org.apache.hadoop.hive.ql.Driver.QueryState.init(HiveOperation,String)",1,1,1
"org.apache.hadoop.hive.ql.Driver.QueryState.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.Driver.acquireReadWriteLocks()",1,2,2
"org.apache.hadoop.hive.ql.Driver.checkConcurrency()",2,2,2
"org.apache.hadoop.hive.ql.Driver.close()",1,9,9
"org.apache.hadoop.hive.ql.Driver.compile(String)",1,1,1
"org.apache.hadoop.hive.ql.Driver.compile(String,boolean)",2,15,15
"org.apache.hadoop.hive.ql.Driver.compileAndRespond(String)",1,1,1
"org.apache.hadoop.hive.ql.Driver.compileInternal(String)",1,3,3
"org.apache.hadoop.hive.ql.Driver.createProcessorResponse(int)",1,1,1
"org.apache.hadoop.hive.ql.Driver.createTxnManager()",2,3,3
"org.apache.hadoop.hive.ql.Driver.destroy()",2,4,5
"org.apache.hadoop.hive.ql.Driver.doAuthorization(BaseSemanticAnalyzer,String)",17,36,40
"org.apache.hadoop.hive.ql.Driver.doAuthorizationV2(SessionState,HiveOperation,HashSet<ReadEntity>,HashSet<WriteEntity>,String,Map<String, List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.Driver.execute()",9,38,42
"org.apache.hadoop.hive.ql.Driver.getClusterStatus()",1,2,2
"org.apache.hadoop.hive.ql.Driver.getErrorMsg()",1,1,1
"org.apache.hadoop.hive.ql.Driver.getHiveOperationType(HiveOperation)",1,1,1
"org.apache.hadoop.hive.ql.Driver.getHivePrivObjects(HashSet<? extends Entity>,Map<String, List<String>>)",7,7,15
"org.apache.hadoop.hive.ql.Driver.getHooks(ConfVars)",1,1,1
"org.apache.hadoop.hive.ql.Driver.getHooks(ConfVars,Class<T>)",1,2,2
"org.apache.hadoop.hive.ql.Driver.getLockObjects(Database,Table,Partition,HiveLockMode)",5,8,9
"org.apache.hadoop.hive.ql.Driver.getMaxRows()",1,1,1
"org.apache.hadoop.hive.ql.Driver.getPlan()",1,1,1
"org.apache.hadoop.hive.ql.Driver.getQueryPlan()",1,1,1
"org.apache.hadoop.hive.ql.Driver.getResults(List)",7,8,13
"org.apache.hadoop.hive.ql.Driver.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.Driver.getSchema(BaseSemanticAnalyzer,HiveConf)",1,10,12
"org.apache.hadoop.hive.ql.Driver.getTablePartitionUsedColumns(HiveOperation,BaseSemanticAnalyzer,Map<Table, List<String>>,Map<Partition, List<String>>,Map<String, Boolean>)",1,10,12
"org.apache.hadoop.hive.ql.Driver.getThriftSchema()",1,5,5
"org.apache.hadoop.hive.ql.Driver.getTryCount()",1,1,1
"org.apache.hadoop.hive.ql.Driver.init()",1,1,1
"org.apache.hadoop.hive.ql.Driver.isFetchingTable()",1,2,2
"org.apache.hadoop.hive.ql.Driver.launchTask(Task<? extends Serializable>,String,boolean,String,int,DriverContext)",1,7,7
"org.apache.hadoop.hive.ql.Driver.recordValidTxns()",1,2,2
"org.apache.hadoop.hive.ql.Driver.releaseLocks(List<HiveLock>)",1,2,2
"org.apache.hadoop.hive.ql.Driver.resetFetch()",2,3,4
"org.apache.hadoop.hive.ql.Driver.restoreSession(QueryState)",1,4,4
"org.apache.hadoop.hive.ql.Driver.run()",1,1,1
"org.apache.hadoop.hive.ql.Driver.run(String)",1,1,1
"org.apache.hadoop.hive.ql.Driver.run(String,boolean)",6,6,10
"org.apache.hadoop.hive.ql.Driver.runInternal(String,boolean)",12,20,23
"org.apache.hadoop.hive.ql.Driver.saveSession(QueryState)",1,3,3
"org.apache.hadoop.hive.ql.Driver.setErrorMsgAndDetail(int,Throwable,Task)",1,3,3
"org.apache.hadoop.hive.ql.Driver.setMaxRows(int)",1,1,1
"org.apache.hadoop.hive.ql.Driver.setTryCount(int)",1,1,1
"org.apache.hadoop.hive.ql.Driver.validateConfVariables()",1,5,5
"org.apache.hadoop.hive.ql.DriverContext.DriverContext()",1,1,1
"org.apache.hadoop.hive.ql.DriverContext.DriverContext(Context)",1,1,1
"org.apache.hadoop.hive.ql.DriverContext.addToRunnable(Task<? extends Serializable>)",2,1,2
"org.apache.hadoop.hive.ql.DriverContext.checkShutdown()",2,1,2
"org.apache.hadoop.hive.ql.DriverContext.finished(TaskRunner)",2,5,6
"org.apache.hadoop.hive.ql.DriverContext.getCtx()",1,1,1
"org.apache.hadoop.hive.ql.DriverContext.getCurJobNo()",1,1,1
"org.apache.hadoop.hive.ql.DriverContext.getRunnable(int)",2,3,3
"org.apache.hadoop.hive.ql.DriverContext.incCurJobNo(int)",1,1,1
"org.apache.hadoop.hive.ql.DriverContext.isLaunchable(Task<? extends Serializable>)",1,3,3
"org.apache.hadoop.hive.ql.DriverContext.isRunning()",1,3,3
"org.apache.hadoop.hive.ql.DriverContext.isShutdown()",1,1,1
"org.apache.hadoop.hive.ql.DriverContext.launching(TaskRunner)",1,1,1
"org.apache.hadoop.hive.ql.DriverContext.pollFinished()",4,5,5
"org.apache.hadoop.hive.ql.DriverContext.prepare(QueryPlan)",1,1,1
"org.apache.hadoop.hive.ql.DriverContext.prepare(TaskRunner)",1,1,1
"org.apache.hadoop.hive.ql.DriverContext.remove(Task<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.DriverContext.shutdown()",1,5,5
"org.apache.hadoop.hive.ql.ErrorMsg.ErrorMsg(int,String)",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.ErrorMsg(int,String,String)",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.ErrorMsg(int,String,String,boolean)",1,1,2
"org.apache.hadoop.hive.ql.ErrorMsg.ErrorMsg(int,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.findSQLState(String)",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.format(String)",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.format(String...)",4,7,8
"org.apache.hadoop.hive.ql.ErrorMsg.getCharPositionInLine(ASTNode)",2,2,2
"org.apache.hadoop.hive.ql.ErrorMsg.getErrorCode()",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.getErrorCodePattern()",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.getErrorCodedMsg()",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.getErrorCodedMsg(String...)",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.getErrorMsg(String)",8,5,9
"org.apache.hadoop.hive.ql.ErrorMsg.getErrorMsg(int)",3,2,3
"org.apache.hadoop.hive.ql.ErrorMsg.getLine(ASTNode)",2,2,2
"org.apache.hadoop.hive.ql.ErrorMsg.getMsg()",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.getMsg(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.getMsg(ASTNode,String)",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.getMsg(String)",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.getMsg(Tree)",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.getMsg(Tree,String)",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.getSQLState()",1,1,1
"org.apache.hadoop.hive.ql.ErrorMsg.getText(ASTNode)",2,2,2
"org.apache.hadoop.hive.ql.ErrorMsg.renderOrigin(StringBuilder,ASTNodeOrigin)",1,2,2
"org.apache.hadoop.hive.ql.ErrorMsg.renderPosition(StringBuilder,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.HashTableLoaderFactory.HashTableLoaderFactory()",1,1,1
"org.apache.hadoop.hive.ql.HashTableLoaderFactory.getLoader(Configuration)",2,1,2
"org.apache.hadoop.hive.ql.HiveDriverRunHookContextImpl.HiveDriverRunHookContextImpl(Configuration,String)",1,1,1
"org.apache.hadoop.hive.ql.HiveDriverRunHookContextImpl.getCommand()",1,1,1
"org.apache.hadoop.hive.ql.HiveDriverRunHookContextImpl.getConf()",1,1,1
"org.apache.hadoop.hive.ql.HiveDriverRunHookContextImpl.setCommand(String)",1,1,1
"org.apache.hadoop.hive.ql.HiveDriverRunHookContextImpl.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.MapRedStats.MapRedStats(int,int,long,boolean,String)",1,1,1
"org.apache.hadoop.hive.ql.MapRedStats.getCounters()",1,1,1
"org.apache.hadoop.hive.ql.MapRedStats.getCpuMSec()",1,1,1
"org.apache.hadoop.hive.ql.MapRedStats.getJobId()",1,1,1
"org.apache.hadoop.hive.ql.MapRedStats.getNumMap()",1,1,1
"org.apache.hadoop.hive.ql.MapRedStats.getNumReduce()",1,1,1
"org.apache.hadoop.hive.ql.MapRedStats.getTaskNumbers()",1,3,3
"org.apache.hadoop.hive.ql.MapRedStats.isSuccess()",1,1,1
"org.apache.hadoop.hive.ql.MapRedStats.setCounters(Counters)",1,1,1
"org.apache.hadoop.hive.ql.MapRedStats.setCpuMSec(long)",1,1,1
"org.apache.hadoop.hive.ql.MapRedStats.setJobId(String)",1,1,1
"org.apache.hadoop.hive.ql.MapRedStats.setNumMap(int)",1,1,1
"org.apache.hadoop.hive.ql.MapRedStats.setNumReduce(int)",1,1,1
"org.apache.hadoop.hive.ql.MapRedStats.setSuccess(boolean)",1,1,1
"org.apache.hadoop.hive.ql.MapRedStats.toString()",1,9,10
"org.apache.hadoop.hive.ql.QTestUtil.MiniClusterType.valueForString(String)",3,2,3
"org.apache.hadoop.hive.ql.QTestUtil.QTRunner.QTRunner(QTestUtil,String)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.QTRunner.run()",1,2,2
"org.apache.hadoop.hive.ql.QTestUtil.QTestSetup.QTestSetup()",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.QTestSetup.postTest(HiveConf)",2,2,3
"org.apache.hadoop.hive.ql.QTestUtil.QTestSetup.preTest(HiveConf)",1,3,3
"org.apache.hadoop.hive.ql.QTestUtil.QTestSetup.tearDown()",1,2,2
"org.apache.hadoop.hive.ql.QTestUtil.QTestUtil(String,String,MiniClusterType,String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.QTestUtil(String,String,MiniClusterType,String,String,String,String)",1,8,10
"org.apache.hadoop.hive.ql.QTestUtil.QTestUtil(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.addFile(File)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.addFile(File,boolean)",2,7,8
"org.apache.hadoop.hive.ql.QTestUtil.addFile(String)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.addFile(String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.analyzeAST(ASTNode)",1,3,3
"org.apache.hadoop.hive.ql.QTestUtil.checkCliDriverResults(String)",1,2,3
"org.apache.hadoop.hive.ql.QTestUtil.checkCompareCliDriverResults(String,List<String>)",3,3,3
"org.apache.hadoop.hive.ql.QTestUtil.checkHadoopVersionExclude(String,String)",4,7,9
"org.apache.hadoop.hive.ql.QTestUtil.checkNegativeResults(String,Exception)",3,4,5
"org.apache.hadoop.hive.ql.QTestUtil.checkOSExclude(String,String)",5,5,5
"org.apache.hadoop.hive.ql.QTestUtil.checkParseResults(String,ASTNode)",2,3,4
"org.apache.hadoop.hive.ql.QTestUtil.checkPlan(String,List<Task<? extends Serializable>>)",2,3,5
"org.apache.hadoop.hive.ql.QTestUtil.cleanUp()",1,3,6
"org.apache.hadoop.hive.ql.QTestUtil.clearPostTestEffects()",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.clearTestSideEffects()",5,15,17
"org.apache.hadoop.hive.ql.QTestUtil.cliInit(String)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.cliInit(String,boolean)",1,8,12
"org.apache.hadoop.hive.ql.QTestUtil.convertSequenceFileToTextFile()",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.copyDirectoryToLocal(Path,Path)",4,5,6
"org.apache.hadoop.hive.ql.QTestUtil.createSources()",1,2,3
"org.apache.hadoop.hive.ql.QTestUtil.deleteDirectory(File)",1,4,4
"org.apache.hadoop.hive.ql.QTestUtil.ensurePathEndsInSlash(String)",3,1,3
"org.apache.hadoop.hive.ql.QTestUtil.ensureQvFileList(String)",3,2,4
"org.apache.hadoop.hive.ql.QTestUtil.execute(String)",1,2,2
"org.apache.hadoop.hive.ql.QTestUtil.executeClient(String)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.executeClient(String,String)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.executeCmd(Collection<String>)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.executeCmd(Collection<String>,String,String)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.executeCmd(String[])",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.executeCmd(String[],String,String)",1,5,5
"org.apache.hadoop.hive.ql.QTestUtil.executeDiffCommand(String,String,boolean,boolean)",3,7,10
"org.apache.hadoop.hive.ql.QTestUtil.executeOne(String)",2,1,2
"org.apache.hadoop.hive.ql.QTestUtil.failed(String,String)",1,1,2
"org.apache.hadoop.hive.ql.QTestUtil.failed(Throwable,String,String)",1,2,4
"org.apache.hadoop.hive.ql.QTestUtil.failed(int,String,String)",1,2,4
"org.apache.hadoop.hive.ql.QTestUtil.failedDiff(int,String,String)",1,1,2
"org.apache.hadoop.hive.ql.QTestUtil.fixXml4JDK7(String)",2,7,8
"org.apache.hadoop.hive.ql.QTestUtil.getCommands(String)",1,2,2
"org.apache.hadoop.hive.ql.QTestUtil.getConf()",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.getElementValue(String,String)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.getHadoopMainVersion(String)",3,2,3
"org.apache.hadoop.hive.ql.QTestUtil.getLogDirectory()",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.getOutputDirectory()",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.getPropertyValue(String,String)",2,1,2
"org.apache.hadoop.hive.ql.QTestUtil.getQMap()",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.getQuotedString(String)",1,2,2
"org.apache.hadoop.hive.ql.QTestUtil.getVersionFiles(String,String)",1,1,2
"org.apache.hadoop.hive.ql.QTestUtil.getVersionFilesInternal(String)",5,4,8
"org.apache.hadoop.hive.ql.QTestUtil.init()",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.init(String)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.initConf()",1,5,5
"org.apache.hadoop.hive.ql.QTestUtil.maskPatterns(Pattern[],String)",1,5,5
"org.apache.hadoop.hive.ql.QTestUtil.matches(Pattern,String)",2,1,2
"org.apache.hadoop.hive.ql.QTestUtil.normalizeNames(File)",1,5,5
"org.apache.hadoop.hive.ql.QTestUtil.outPath(String,String)",3,4,4
"org.apache.hadoop.hive.ql.QTestUtil.outputTestFailureHelpMessage()",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.overwriteResults(String,String)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.parseQuery(String)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.queryListRunnerMultiThreaded(File[],QTestUtil[])",1,5,5
"org.apache.hadoop.hive.ql.QTestUtil.queryListRunnerSetup(File[],String,String)",1,2,2
"org.apache.hadoop.hive.ql.QTestUtil.queryListRunnerSingleThreaded(File[],QTestUtil[])",1,3,4
"org.apache.hadoop.hive.ql.QTestUtil.readEntireFileIntoString(File)",1,2,2
"org.apache.hadoop.hive.ql.QTestUtil.resetParser()",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.runCmd(String)",2,1,2
"org.apache.hadoop.hive.ql.QTestUtil.runCreateTableCmd(String)",2,1,2
"org.apache.hadoop.hive.ql.QTestUtil.runLoadCmd(String)",2,1,2
"org.apache.hadoop.hive.ql.QTestUtil.shouldBeSkipped(String)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.shutdown()",1,3,3
"org.apache.hadoop.hive.ql.QTestUtil.sortFiles(String,String)",1,1,1
"org.apache.hadoop.hive.ql.QTestUtil.startSessionState()",1,3,6
"org.apache.hadoop.hive.ql.QTestUtil.toPattern(String[])",1,2,2
"org.apache.hadoop.hive.ql.QueryPlan.QueryPlan()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.QueryPlan(String,BaseSemanticAnalyzer,Long)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.QueryPlan(String,BaseSemanticAnalyzer,Long,String)",1,2,2
"org.apache.hadoop.hive.ql.QueryPlan.extractCounters()",4,17,19
"org.apache.hadoop.hive.ql.QueryPlan.getColumnAccessInfo()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getCounters()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getDone()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getFetchTask()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getIdToTableNameMap()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getInputs()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getJSONAdjacency(Adjacency)",2,1,2
"org.apache.hadoop.hive.ql.QueryPlan.getJSONGraph(Graph)",2,3,4
"org.apache.hadoop.hive.ql.QueryPlan.getJSONKeyValue(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getJSONList(List)",2,2,3
"org.apache.hadoop.hive.ql.QueryPlan.getJSONMap(Map)",2,2,3
"org.apache.hadoop.hive.ql.QueryPlan.getJSONOperator(Operator)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getJSONQuery(Query)",1,3,3
"org.apache.hadoop.hive.ql.QueryPlan.getJSONStage(Stage)",1,3,3
"org.apache.hadoop.hive.ql.QueryPlan.getJSONTask(Task)",1,3,3
"org.apache.hadoop.hive.ql.QueryPlan.getJSONValue(Object)",1,3,4
"org.apache.hadoop.hive.ql.QueryPlan.getLineageInfo()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getOutputs()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getQuery()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getQueryId()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getQueryPlan()",1,2,2
"org.apache.hadoop.hive.ql.QueryPlan.getQueryProperties()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getQueryStartTime()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getQueryStr()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getQueryString()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getReducerTimeStatsPerJobList()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getRootTasks()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getStarted()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.getTableAccessInfo()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.isExplain()",1,2,2
"org.apache.hadoop.hive.ql.QueryPlan.isForExplain()",1,2,2
"org.apache.hadoop.hive.ql.QueryPlan.makeQueryId()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.populateOperatorGraph(Task,Collection<Operator<? extends OperatorDesc>>)",1,5,5
"org.apache.hadoop.hive.ql.QueryPlan.populateQueryPlan()",1,13,13
"org.apache.hadoop.hive.ql.QueryPlan.setColumnAccessInfo(ColumnAccessInfo)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.setDone()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.setFetchTask(FetchTask)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.setIdToTableNameMap(HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.setInputs(HashSet<ReadEntity>)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.setLineageInfo(LineageInfo)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.setOutputs(HashSet<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.setQuery(Query)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.setQueryId(String)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.setQueryStartTime(Long)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.setQueryString(String)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.setRootTasks(ArrayList<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.setStarted()",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.setTableAccessInfo(TableAccessInfo)",1,1,1
"org.apache.hadoop.hive.ql.QueryPlan.toBinaryString()",1,2,2
"org.apache.hadoop.hive.ql.QueryPlan.toString()",1,2,2
"org.apache.hadoop.hive.ql.QueryPlan.toThriftJSONString()",1,2,2
"org.apache.hadoop.hive.ql.QueryPlan.updateCountersInQueryPlan()",7,6,8
"org.apache.hadoop.hive.ql.QueryProperties.hasClusterBy()",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.hasDistributeBy()",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.hasGroupBy()",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.hasJoin()",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.hasJoinFollowedByGroupBy()",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.hasOrderBy()",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.hasPTF()",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.hasSortBy()",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.hasWindowing()",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.isHasMapGroupBy()",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.isMapJoinRemoved()",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.setHasClusterBy(boolean)",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.setHasDistributeBy(boolean)",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.setHasGroupBy(boolean)",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.setHasJoin(boolean)",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.setHasJoinFollowedByGroupBy(boolean)",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.setHasMapGroupBy(boolean)",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.setHasOrderBy(boolean)",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.setHasPTF(boolean)",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.setHasSortBy(boolean)",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.setHasWindowing(boolean)",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.setMapJoinRemoved(boolean)",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.setUsesScript(boolean)",1,1,1
"org.apache.hadoop.hive.ql.QueryProperties.usesScript()",1,1,1
"org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.addPartitionAndCheck(Table,String,String,String)",1,3,3
"org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.buildLocationClause(String)",1,1,2
"org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.cleanup()",1,4,4
"org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.createDatabaseAndCheck(String,String)",1,2,2
"org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.createIndexAndCheck(Table,String,String)",1,2,2
"org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.createTableAndCheck(String,String)",1,1,1
"org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.createTableAndCheck(Table,String,String)",1,3,3
"org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.executeQuery(String)",1,1,1
"org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.setUp()",5,6,10
"org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.shutdownMiniDfs()",1,2,2
"org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.tearDown()",1,2,2
"org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.testCreateDatabaseWithTableNonDefaultNameNode()",1,1,1
"org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.testCreateTableWithIndexAndPartitionsNonDefaultNameNode()",1,1,1
"org.apache.hadoop.hive.ql.TestErrorMsg.testUniqueErrorCode()",1,2,2
"org.apache.hadoop.hive.ql.TestLocationQueries.CheckResults.CheckResults(String,String,MiniClusterType,String,String)",1,1,1
"org.apache.hadoop.hive.ql.TestLocationQueries.CheckResults.checkCliDriverResults(String)",2,3,5
"org.apache.hadoop.hive.ql.TestLocationQueries.TestLocationQueries()",1,3,3
"org.apache.hadoop.hive.ql.TestLocationQueries.testAlterTablePartitionLocation_alter5()",1,3,3
"org.apache.hadoop.hive.ql.TestMTQueries.TestMTQueries()",1,3,3
"org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1()",1,3,3
"org.apache.hadoop.hive.ql.TestUtilitiesDfs.setupDfs()",1,1,1
"org.apache.hadoop.hive.ql.TestUtilitiesDfs.shutdownDfs()",1,2,2
"org.apache.hadoop.hive.ql.TestUtilitiesDfs.testCreateDirWithPermissionRecursive()",1,1,1
"org.apache.hadoop.hive.ql.WindowsPathUtil.convertPathsFromWindowsToHdfs(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.WindowsPathUtil.getHdfsUriString(String)",2,2,2
"org.apache.hadoop.hive.ql.debug.Utils.dumpHeap(String,boolean)",2,2,6
"org.apache.hadoop.hive.ql.debug.Utils.dumpHeapToTmp(String...)",1,1,3
"org.apache.hadoop.hive.ql.debug.Utils.toStringBinary(byte[],int,int)",1,6,10
"org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.AbstractMapJoinOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.AbstractMapJoinOperator(AbstractMapJoinOperator<? extends MapJoinDesc>)",1,1,1
"org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.closeOp(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.getValueObjectInspectors(byte,List<ObjectInspector>[])",2,2,3
"org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.initializeOp(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.AmbiguousMethodException.AmbiguousMethodException(Class<?>,List<TypeInfo>,List<Method>)",1,1,1
"org.apache.hadoop.hive.ql.exec.ArchiveUtils.HarPathHelper.HarPathHelper(HiveConf,URI,URI)",2,2,4
"org.apache.hadoop.hive.ql.exec.ArchiveUtils.HarPathHelper.getHarUri(URI,HadoopShims)",1,1,2
"org.apache.hadoop.hive.ql.exec.ArchiveUtils.PartSpecInfo.PartSpecInfo(List<FieldSchema>,List<String>)",1,1,1
"org.apache.hadoop.hive.ql.exec.ArchiveUtils.PartSpecInfo.create(Table,Map<String, String>)",4,2,4
"org.apache.hadoop.hive.ql.exec.ArchiveUtils.PartSpecInfo.createPath(Table)",2,1,3
"org.apache.hadoop.hive.ql.exec.ArchiveUtils.PartSpecInfo.getName()",1,1,2
"org.apache.hadoop.hive.ql.exec.ArchiveUtils.addSlash(String)",1,1,2
"org.apache.hadoop.hive.ql.exec.ArchiveUtils.addSlash(URI)",2,2,3
"org.apache.hadoop.hive.ql.exec.ArchiveUtils.conflictingArchiveNameOrNull(Hive,Table,LinkedHashMap<String, String>)",8,9,10
"org.apache.hadoop.hive.ql.exec.ArchiveUtils.getArchivingLevel(Partition)",1,2,2
"org.apache.hadoop.hive.ql.exec.ArchiveUtils.getPartialName(Partition,int)",1,2,3
"org.apache.hadoop.hive.ql.exec.ArchiveUtils.isArchived(Partition)",1,1,1
"org.apache.hadoop.hive.ql.exec.AutoProgressor.AutoProgressor(String,Reporter,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.AutoProgressor.AutoProgressor(String,Reporter,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.AutoProgressor.ReporterTask.ReporterTask(Reporter)",1,1,2
"org.apache.hadoop.hive.ql.exec.AutoProgressor.ReporterTask.run()",1,2,2
"org.apache.hadoop.hive.ql.exec.AutoProgressor.StopReporterTimerTask.StopReporterTimerTask(ReporterTask)",1,1,1
"org.apache.hadoop.hive.ql.exec.AutoProgressor.StopReporterTimerTask.run()",1,2,2
"org.apache.hadoop.hive.ql.exec.AutoProgressor.go()",1,2,2
"org.apache.hadoop.hive.ql.exec.BinaryRecordReader.close()",1,2,2
"org.apache.hadoop.hive.ql.exec.BinaryRecordReader.createRow()",1,1,1
"org.apache.hadoop.hive.ql.exec.BinaryRecordReader.initialize(InputStream,Configuration,Properties)",1,1,1
"org.apache.hadoop.hive.ql.exec.BinaryRecordReader.next(Writable)",1,2,2
"org.apache.hadoop.hive.ql.exec.ByteWritable.ByteWritable()",1,1,1
"org.apache.hadoop.hive.ql.exec.ByteWritable.ByteWritable(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.ByteWritable.Comparator.Comparator()",1,1,1
"org.apache.hadoop.hive.ql.exec.ByteWritable.Comparator.compare(byte[],int,int,byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.ByteWritable.compareTo(Object)",1,1,3
"org.apache.hadoop.hive.ql.exec.ByteWritable.equals(Object)",4,1,4
"org.apache.hadoop.hive.ql.exec.ByteWritable.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.exec.ByteWritable.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.exec.ByteWritable.set(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.ByteWritable.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.exec.CollectOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.CollectOperator.initializeOp(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.CollectOperator.processOp(Object,int)",1,3,3
"org.apache.hadoop.hive.ql.exec.CollectOperator.retrieve(InspectableObject)",1,2,2
"org.apache.hadoop.hive.ql.exec.ColumnInfo.ColumnInfo()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.ColumnInfo(ColumnInfo)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.ColumnInfo(String,Class,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.ColumnInfo(String,ObjectInspector,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.ColumnInfo(String,ObjectInspector,String,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.ColumnInfo(String,TypeInfo,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.ColumnInfo(String,TypeInfo,String,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.checkEquals(Object,Object)",1,2,2
"org.apache.hadoop.hive.ql.exec.ColumnInfo.equals(Object)",3,6,9
"org.apache.hadoop.hive.ql.exec.ColumnInfo.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.getInternalName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.getIsVirtualCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.getTabAlias()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.getTypeName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.isHiddenVirtualCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.isSkewedCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.setAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.setHiddenVirtualCol(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.setInternalName(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.setObjectinspector(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.setSkewedCol(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.setTabAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.setType(TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.setTypeName(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.setVirtualCol(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnInfo.toString()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.ColumnStatsTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.constructColumnStatsFromPackedRows()",3,7,8
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.convertToThriftDecimal(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.execute(DriverContext)",2,3,3
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.getColumnStatsDesc(String,String,String,boolean)",1,2,2
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.initialize(HiveConf,QueryPlan,DriverContext)",1,2,2
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.persistPartitionStats()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.persistTableStats()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.unpackBinaryStats(ObjectInspector,Object,String,ColumnStatisticsObj)",1,4,4
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.unpackBooleanStats(ObjectInspector,Object,String,ColumnStatisticsObj)",1,4,4
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.unpackDecimalStats(ObjectInspector,Object,String,ColumnStatisticsObj)",1,5,5
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.unpackDoubleStats(ObjectInspector,Object,String,ColumnStatisticsObj)",1,5,5
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.unpackLongStats(ObjectInspector,Object,String,ColumnStatisticsObj)",1,5,5
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.unpackPrimitiveObject(ObjectInspector,Object,String,ColumnStatisticsObj)",2,14,15
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.unpackStringStats(ObjectInspector,Object,String,ColumnStatisticsObj)",1,5,5
"org.apache.hadoop.hive.ql.exec.ColumnStatsTask.unpackStructObject(ObjectInspector,Object,String,ColumnStatisticsObj)",2,5,5
"org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.constructColumnStatsFromInput()",35,35,35
"org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.execute(DriverContext)",2,3,3
"org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.getColumnStatsDesc(String,String,String,boolean)",1,2,2
"org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.initialize(HiveConf,QueryPlan,DriverContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.persistPartitionStats()",1,1,1
"org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.persistTableStats()",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.CommonJoinOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.CommonJoinOperator(CommonJoinOperator<T>)",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject()",11,15,24
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.closeOp(boolean)",1,3,3
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.createForwardJoinObject(boolean[])",1,5,5
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.endGroup()",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genAllOneUniqueJoinObject()",1,3,3
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genJoinObject()",1,3,3
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genObject(int,boolean,boolean)",12,19,30
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObject(int,int)",1,4,4
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getFilterTag(List<Object>)",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getFilteredValue(byte,Object)",1,2,2
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getJoinOutputObjectInspector(Byte[],List<ObjectInspector>[],T)",1,4,4
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getNextSize(long)",2,1,2
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getPosToAliasMap()",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getValueObjectInspectors(byte,List<ObjectInspector>[])",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.hasAnyFiltered(int,List<Object>)",1,3,3
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.hasFilter(int)",1,1,2
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.hasLeftPairForRight(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.hasRightPairForLeft(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.initializeOp(Configuration)",1,8,10
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.innerJoin(boolean[],int,int)",2,2,2
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.internalForward(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.isInnerJoin(boolean[],int,int)",1,4,4
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.isLeftValid(int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.isRightValid(int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.leftOuterJoin(boolean[],int,int)",4,3,6
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.opAllowedAfterMapJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.opAllowedBeforeMapJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.reportProgress()",1,2,3
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.rightOuterJoin(boolean[],int,int)",4,4,6
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.setPosToAliasMap(Map<Integer, Set<String>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.CommonJoinOperator.startGroup()",1,2,2
"org.apache.hadoop.hive.ql.exec.ComparisonOpMethodResolver.ComparisonOpMethodResolver(Class<? extends UDF>)",1,1,1
"org.apache.hadoop.hive.ql.exec.ComparisonOpMethodResolver.getEvalMethod(List<TypeInfo>)",7,12,15
"org.apache.hadoop.hive.ql.exec.ConditionalTask.ConditionalTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.ConditionalTask.addDependentTask(Task<? extends Serializable>)",1,3,3
"org.apache.hadoop.hive.ql.exec.ConditionalTask.done()",1,8,9
"org.apache.hadoop.hive.ql.exec.ConditionalTask.execute(DriverContext)",1,2,2
"org.apache.hadoop.hive.ql.exec.ConditionalTask.getDependentTasks()",1,1,1
"org.apache.hadoop.hive.ql.exec.ConditionalTask.getListTasks()",1,1,1
"org.apache.hadoop.hive.ql.exec.ConditionalTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ConditionalTask.getResolver()",1,1,1
"org.apache.hadoop.hive.ql.exec.ConditionalTask.getResolverCtx()",1,1,1
"org.apache.hadoop.hive.ql.exec.ConditionalTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.ConditionalTask.hasReduce()",3,2,3
"org.apache.hadoop.hive.ql.exec.ConditionalTask.initialize(HiveConf,QueryPlan,DriverContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.ConditionalTask.isMapRedTask()",3,2,3
"org.apache.hadoop.hive.ql.exec.ConditionalTask.resolveTask(DriverContext)",1,7,7
"org.apache.hadoop.hive.ql.exec.ConditionalTask.setListTasks(List<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.ConditionalTask.setResolver(ConditionalResolver)",1,1,1
"org.apache.hadoop.hive.ql.exec.ConditionalTask.setResolverCtx(Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.CopyTask.CopyTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.CopyTask.execute(DriverContext)",6,7,8
"org.apache.hadoop.hive.ql.exec.CopyTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.CopyTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.DDLTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.addPartitions(Hive,AddPartitionDesc)",1,2,2
"org.apache.hadoop.hive.ql.exec.DDLTask.alterDatabase(AlterDatabaseDesc)",3,3,7
"org.apache.hadoop.hive.ql.exec.DDLTask.alterIndex(Hive,AlterIndexDesc)",3,10,15
"org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(Hive,AlterTableDesc)",36,77,90
"org.apache.hadoop.hive.ql.exec.DDLTask.alterTableAlterPart(Hive,AlterTableAlterPartDesc)",7,9,13
"org.apache.hadoop.hive.ql.exec.DDLTask.appendNonNull(StringBuilder,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.appendNonNull(StringBuilder,Object,boolean)",1,4,4
"org.apache.hadoop.hive.ql.exec.DDLTask.archive(Hive,AlterTableSimpleDesc,DriverContext)",12,21,28
"org.apache.hadoop.hive.ql.exec.DDLTask.checkArchiveProperty(int,boolean,Partition)",3,4,4
"org.apache.hadoop.hive.ql.exec.DDLTask.compact(Hive,AlterTableSimpleDesc)",5,3,5
"org.apache.hadoop.hive.ql.exec.DDLTask.createDatabase(Hive,CreateDatabaseDesc)",1,3,3
"org.apache.hadoop.hive.ql.exec.DDLTask.createIndex(Hive,CreateIndexDesc)",1,4,4
"org.apache.hadoop.hive.ql.exec.DDLTask.createTable(Hive,CreateTableDesc)",7,36,38
"org.apache.hadoop.hive.ql.exec.DDLTask.createTableLike(Hive,CreateTableLikeDesc)",1,12,12
"org.apache.hadoop.hive.ql.exec.DDLTask.createView(Hive,CreateViewDesc)",2,8,9
"org.apache.hadoop.hive.ql.exec.DDLTask.deleteDir(Path)",1,1,2
"org.apache.hadoop.hive.ql.exec.DDLTask.descDatabase(DescDatabaseDesc)",2,4,6
"org.apache.hadoop.hive.ql.exec.DDLTask.describeFunction(DescFunctionDesc)",1,10,11
"org.apache.hadoop.hive.ql.exec.DDLTask.describeTable(Hive,DescTableDesc)",4,10,12
"org.apache.hadoop.hive.ql.exec.DDLTask.doesTableNeedLocation(Table)",1,2,2
"org.apache.hadoop.hive.ql.exec.DDLTask.dropDatabase(Hive,DropDatabaseDesc)",1,2,2
"org.apache.hadoop.hive.ql.exec.DDLTask.dropIndex(Hive,DropIndexDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(Hive,Table,DropTableDesc)",1,2,2
"org.apache.hadoop.hive.ql.exec.DDLTask.dropTable(Hive,Table,DropTableDesc)",12,13,15
"org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(Hive,DropTableDesc)",1,2,3
"org.apache.hadoop.hive.ql.exec.DDLTask.escapeHiveCommand(String)",1,3,4
"org.apache.hadoop.hive.ql.exec.DDLTask.exchangeTablePartition(Hive,AlterTableExchangePartition)",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.execute(DriverContext)",50,51,51
"org.apache.hadoop.hive.ql.exec.DDLTask.failed(Throwable)",1,3,3
"org.apache.hadoop.hive.ql.exec.DDLTask.fixDecimalColumnTypeName(List<FieldSchema>)",1,3,3
"org.apache.hadoop.hive.ql.exec.DDLTask.getHiveObject(String,Map<String, String>)",4,2,4
"org.apache.hadoop.hive.ql.exec.DDLTask.getLocations(Hive,Table,Map<String, String>)",1,8,8
"org.apache.hadoop.hive.ql.exec.DDLTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.getOriginalLocation(Partition)",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.getOutputStream(Path)",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.getSessionAuthorizer()",1,1,2
"org.apache.hadoop.hive.ql.exec.DDLTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.grantOrRevokePrivileges(List<PrincipalDesc>,List<PrivilegeDesc>,PrivilegeObjectDesc,String,PrincipalType,boolean,boolean)",1,2,2
"org.apache.hadoop.hive.ql.exec.DDLTask.grantOrRevokeRole(GrantRevokeRoleDDL)",1,3,3
"org.apache.hadoop.hive.ql.exec.DDLTask.initialize(HiveConf,QueryPlan,DriverContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.lockDatabase(LockDatabaseDesc)",5,2,5
"org.apache.hadoop.hive.ql.exec.DDLTask.lockTable(LockTableDesc)",8,3,8
"org.apache.hadoop.hive.ql.exec.DDLTask.makeLocationQualified(CreateIndexDesc,String)",1,4,4
"org.apache.hadoop.hive.ql.exec.DDLTask.makeLocationQualified(Database)",1,2,2
"org.apache.hadoop.hive.ql.exec.DDLTask.makeLocationQualified(String,StorageDescriptor,String)",1,4,4
"org.apache.hadoop.hive.ql.exec.DDLTask.mergeFiles(Hive,AlterTablePartMergeFilesDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.moveDir(FileSystem,Path,Path)",2,1,3
"org.apache.hadoop.hive.ql.exec.DDLTask.msck(Hive,MsckDesc)",2,12,12
"org.apache.hadoop.hive.ql.exec.DDLTask.msckAddPartitionsOneByOne(Hive,Table,List<PartitionResult>,List<String>)",1,3,3
"org.apache.hadoop.hive.ql.exec.DDLTask.needToUpdateStats(Map<String, String>)",2,4,5
"org.apache.hadoop.hive.ql.exec.DDLTask.partitionInCustomLocation(Table,Partition)",3,2,4
"org.apache.hadoop.hive.ql.exec.DDLTask.pathExists(Path)",1,1,2
"org.apache.hadoop.hive.ql.exec.DDLTask.renamePartition(Hive,RenamePartitionDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.requireLock()",1,2,2
"org.apache.hadoop.hive.ql.exec.DDLTask.roleDDL(RoleDDLDesc)",2,2,9
"org.apache.hadoop.hive.ql.exec.DDLTask.setAlterProtectMode(boolean,ProtectModeType,ProtectMode)",1,1,4
"org.apache.hadoop.hive.ql.exec.DDLTask.setArchived(Partition,Path,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.setIsArchived(Partition,boolean,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.DDLTask.setOriginalLocation(Partition,String)",1,2,2
"org.apache.hadoop.hive.ql.exec.DDLTask.setUnArchived(Partition)",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.showColumns(Hive,ShowColumnsDesc)",1,1,2
"org.apache.hadoop.hive.ql.exec.DDLTask.showCompactions(ShowCompactionsDesc)",1,4,6
"org.apache.hadoop.hive.ql.exec.DDLTask.showConf(Hive,ShowConfDesc)",2,5,5
"org.apache.hadoop.hive.ql.exec.DDLTask.showCreateTable(Hive,ShowCreateTableDesc)",2,31,33
"org.apache.hadoop.hive.ql.exec.DDLTask.showDatabases(Hive,ShowDatabasesDesc)",1,2,3
"org.apache.hadoop.hive.ql.exec.DDLTask.showFunctions(ShowFunctionsDesc)",1,5,6
"org.apache.hadoop.hive.ql.exec.DDLTask.showGrants(ShowGrantDesc)",1,1,2
"org.apache.hadoop.hive.ql.exec.DDLTask.showIndexes(Hive,ShowIndexesDesc)",1,6,6
"org.apache.hadoop.hive.ql.exec.DDLTask.showLocks(ShowLocksDesc)",6,10,13
"org.apache.hadoop.hive.ql.exec.DDLTask.showLocksNewFormat(ShowLocksDesc,HiveLockManager)",2,10,11
"org.apache.hadoop.hive.ql.exec.DDLTask.showPartitions(Hive,ShowPartitionsDesc)",2,2,4
"org.apache.hadoop.hive.ql.exec.DDLTask.showTableProperties(Hive,ShowTblPropertiesDesc)",2,7,8
"org.apache.hadoop.hive.ql.exec.DDLTask.showTableStatus(Hive,ShowTableStatusDesc)",3,4,5
"org.apache.hadoop.hive.ql.exec.DDLTask.showTables(Hive,ShowTablesDesc)",2,2,4
"org.apache.hadoop.hive.ql.exec.DDLTask.showTxns(ShowTxnsDesc)",1,3,3
"org.apache.hadoop.hive.ql.exec.DDLTask.switchDatabase(Hive,SwitchDatabaseDesc)",2,4,5
"org.apache.hadoop.hive.ql.exec.DDLTask.touch(Hive,AlterTableSimpleDesc)",3,2,5
"org.apache.hadoop.hive.ql.exec.DDLTask.truncateTable(Hive,TruncateTableDesc)",2,3,4
"org.apache.hadoop.hive.ql.exec.DDLTask.unarchive(Hive,AlterTableSimpleDesc)",14,19,29
"org.apache.hadoop.hive.ql.exec.DDLTask.unlockDatabase(UnlockDatabaseDesc)",5,4,7
"org.apache.hadoop.hive.ql.exec.DDLTask.unlockTable(UnlockTableDesc)",4,4,6
"org.apache.hadoop.hive.ql.exec.DDLTask.updateModifiedParameters(Map<String, String>,HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.exec.DDLTask.validateSerDe(String)",1,2,3
"org.apache.hadoop.hive.ql.exec.DDLTask.writeGrantInfo(List<HivePrivilegeInfo>,boolean)",2,6,7
"org.apache.hadoop.hive.ql.exec.DDLTask.writeHiveRoleGrantInfo(List<HiveRoleGrant>,boolean)",2,4,5
"org.apache.hadoop.hive.ql.exec.DDLTask.writeListToFileAfterSort(List<String>,String)",1,2,2
"org.apache.hadoop.hive.ql.exec.DDLTask.writeMsckResult(List<? extends Object>,String,Writer,boolean)",2,4,4
"org.apache.hadoop.hive.ql.exec.DDLTask.writeRoleGrantsInfo(List<RolePrincipalGrant>,boolean)",2,4,5
"org.apache.hadoop.hive.ql.exec.DDLTask.writeRolesGrantedInfo(List<HiveRoleGrant>,boolean)",2,4,5
"org.apache.hadoop.hive.ql.exec.DDLTask.writeToFile(String,String)",1,3,3
"org.apache.hadoop.hive.ql.exec.DefaultBucketMatcher.DefaultBucketMatcher()",1,1,1
"org.apache.hadoop.hive.ql.exec.DefaultBucketMatcher.getAliasBucketFiles(String,String,String)",1,3,3
"org.apache.hadoop.hive.ql.exec.DefaultBucketMatcher.getBucketFileNameMapping()",1,1,1
"org.apache.hadoop.hive.ql.exec.DefaultBucketMatcher.setAliasBucketFileNameMapping(Map<String, Map<String, List<String>>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.DefaultBucketMatcher.setBucketFileNameMapping(Map<String, Integer>)",1,1,1
"org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.close()",1,1,1
"org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.initialize(Configuration,Properties)",1,1,2
"org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.initializeSerde(Configuration,Properties)",1,2,2
"org.apache.hadoop.hive.ql.exec.DefaultUDAFEvaluatorResolver.DefaultUDAFEvaluatorResolver(Class<? extends UDAF>)",1,1,1
"org.apache.hadoop.hive.ql.exec.DefaultUDAFEvaluatorResolver.getEvaluatorClass(List<TypeInfo>)",4,7,9
"org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.DefaultUDFMethodResolver(Class<? extends UDF>)",1,1,1
"org.apache.hadoop.hive.ql.exec.DefaultUDFMethodResolver.getEvalMethod(List<TypeInfo>)",1,1,1
"org.apache.hadoop.hive.ql.exec.DemuxOperator.closeOp(boolean)",1,2,2
"org.apache.hadoop.hive.ql.exec.DemuxOperator.endGroup()",2,3,4
"org.apache.hadoop.hive.ql.exec.DemuxOperator.endGroupIfNecessary(int)",1,4,4
"org.apache.hadoop.hive.ql.exec.DemuxOperator.forward(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.DemuxOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.DemuxOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.DemuxOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.DemuxOperator.initializeChildren(Configuration)",5,9,9
"org.apache.hadoop.hive.ql.exec.DemuxOperator.initializeOp(Configuration)",2,6,9
"org.apache.hadoop.hive.ql.exec.DemuxOperator.processOp(Object,int)",1,5,5
"org.apache.hadoop.hive.ql.exec.DemuxOperator.startGroup()",1,1,1
"org.apache.hadoop.hive.ql.exec.DemuxOperator.toArray(List<Integer>)",1,2,2
"org.apache.hadoop.hive.ql.exec.DemuxOperator.toArray(Map<Integer, Integer>)",1,2,2
"org.apache.hadoop.hive.ql.exec.DependencyCollectionTask.DependencyCollectionTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.DependencyCollectionTask.execute(DriverContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.DependencyCollectionTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.DependencyCollectionTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.DummyStoreOperator.DummyStoreOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.DummyStoreOperator.getResult()",1,1,1
"org.apache.hadoop.hive.ql.exec.DummyStoreOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.DummyStoreOperator.initializeOp(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.DummyStoreOperator.processOp(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.DummyStoreOperator.reset()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask.addRewrites(TokenRewriteStream,QB,String,PrintStream)",1,6,7
"org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask.addRewrites(TokenRewriteStream,QBSubQuery,String,PrintStream,String,boolean,StringBuilder)",3,4,6
"org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask.execute(DriverContext)",1,2,2
"org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask.getJoinKeyWord(QBSubQuery)",6,2,6
"org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask.getQueryASTNode(ASTNode)",1,3,3
"org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask.getResultSchema()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExplainSQRewriteTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExplainTask.ExplainTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExplainTask.MethodComparator.compare(Method,Method)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExplainTask.collectAuthRelatedEntities(PrintStream,ExplainWork)",2,8,9
"org.apache.hadoop.hive.ql.exec.ExplainTask.execute(DriverContext)",1,9,9
"org.apache.hadoop.hive.ql.exec.ExplainTask.getJSONDependencies(ExplainWork)",2,7,9
"org.apache.hadoop.hive.ql.exec.ExplainTask.getJSONLogicalPlan(PrintStream,ExplainWork)",1,8,9
"org.apache.hadoop.hive.ql.exec.ExplainTask.getJSONPlan(PrintStream,ExplainWork)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExplainTask.getJSONPlan(PrintStream,String,List<Task<?>>,Task<?>,boolean,boolean,boolean)",1,10,13
"org.apache.hadoop.hive.ql.exec.ExplainTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExplainTask.getResultSchema()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExplainTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExplainTask.indentString(int)",1,2,2
"org.apache.hadoop.hive.ql.exec.ExplainTask.isPrintable(Object)",3,2,11
"org.apache.hadoop.hive.ql.exec.ExplainTask.outputAST(String,PrintStream,boolean,int)",1,2,3
"org.apache.hadoop.hive.ql.exec.ExplainTask.outputDependencies(PrintStream,boolean,boolean,List<Task>)",1,4,7
"org.apache.hadoop.hive.ql.exec.ExplainTask.outputDependencies(Task<? extends Serializable>,PrintStream,JSONObject,boolean,boolean,int)",1,24,26
"org.apache.hadoop.hive.ql.exec.ExplainTask.outputList(List<?>,PrintStream,boolean,boolean,boolean,int)",1,9,16
"org.apache.hadoop.hive.ql.exec.ExplainTask.outputMap(Map<?, ?>,boolean,PrintStream,boolean,boolean,int)",1,28,31
"org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(Serializable,PrintStream,boolean,boolean,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(Serializable,PrintStream,boolean,boolean,int,String)",10,45,61
"org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(Task<? extends Serializable>,PrintStream,JSONObject,boolean,boolean,int)",1,4,5
"org.apache.hadoop.hive.ql.exec.ExplainTask.outputStagePlans(PrintStream,List<Task>,boolean,boolean)",1,3,5
"org.apache.hadoop.hive.ql.exec.ExplainTask.shouldPrint(Explain,Object)",2,2,3
"org.apache.hadoop.hive.ql.exec.ExplainTask.toJson(String,List<String>,PrintStream,ExplainWork)",2,2,3
"org.apache.hadoop.hive.ql.exec.ExplainTask.toJson(String,String,PrintStream,ExplainWork)",2,1,2
"org.apache.hadoop.hive.ql.exec.ExplainTask.toString(Collection<?>)",1,2,2
"org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.ExprNodeColumnEvaluator(ExprNodeColumnDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator._evaluate(Object,int)",2,4,4
"org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ObjectInspector)",2,6,7
"org.apache.hadoop.hive.ql.exec.ExprNodeConstantEvaluator.ExprNodeConstantEvaluator(ExprNodeConstantDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeConstantEvaluator._evaluate(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeConstantEvaluator.initialize(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.ExprNodeEvaluator(T)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(Object,int)",2,2,3
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.getChildren()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.getExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.getOutputOI()",2,1,2
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.isDeterministic()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.isStateful()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.toString()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.EvaluatorContext.getEvaluated(ExprNodeEvaluator)",2,2,2
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.ExprNodeEvaluatorFactory()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.get(ExprNodeDesc)",6,1,6
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.iterate(ExprNodeEvaluator,EvaluatorContext)",3,5,8
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.toCachedEval(ExprNodeEvaluator)",3,2,3
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorHead.ExprNodeEvaluatorHead(ExprNodeEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorHead._evaluate(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorHead.initialize(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorHead.next()",1,1,2
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorRef.ExprNodeEvaluatorRef(ExprNodeEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorRef._evaluate(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorRef.initialize(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator.ExprNodeFieldEvaluator(ExprNodeFieldDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator._evaluate(Object,int)",3,4,4
"org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator.initialize(ObjectInspector)",1,3,3
"org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.DeferredExprObject.DeferredExprObject(ExprNodeEvaluator,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.DeferredExprObject.get()",1,2,2
"org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.DeferredExprObject.prepare(int)",1,2,2
"org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.ExprNodeGenericFuncEvaluator(ExprNodeGenericFuncDesc)",2,3,8
"org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(Object,int)",2,4,4
"org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.compare(Object)",5,6,7
"org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.getChildren()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.initialize(ObjectInspector)",1,3,4
"org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.isDeterministic()",1,3,3
"org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.isStateful()",3,3,4
"org.apache.hadoop.hive.ql.exec.ExprNodeNullEvaluator.ExprNodeNullEvaluator(ExprNodeNullDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeNullEvaluator._evaluate(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExprNodeNullEvaluator.initialize(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExtractOperator.acceptLimitPushdown()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExtractOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExtractOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExtractOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchFormatter.ThriftFormatter.close()",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchFormatter.ThriftFormatter.convert(Object,ObjectInspector)",1,3,3
"org.apache.hadoop.hive.ql.exec.FetchFormatter.ThriftFormatter.initialize(Configuration,Properties)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.FetchInputFormatSplit.FetchInputFormatSplit(InputSplit,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.FetchOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.FetchOperator(FetchWork,JobConf)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.FetchOperator(FetchWork,JobConf,Operator<?>,List<VirtualColumn>)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.clearFetchContext()",1,5,5
"org.apache.hadoop.hive.ql.exec.FetchOperator.createPartValue(String[],Map<String, String>,String[])",1,2,2
"org.apache.hadoop.hive.ql.exec.FetchOperator.createRowInspector(StructObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.FetchOperator.createRowInspector(StructObjectInspector,String[],String[])",1,3,3
"org.apache.hadoop.hive.ql.exec.FetchOperator.getCurrPart()",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.getCurrTbl()",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.getInputFormatFromCache(Class,Configuration)",2,3,3
"org.apache.hadoop.hive.ql.exec.FetchOperator.getNextPath()",12,11,15
"org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow()",7,16,24
"org.apache.hadoop.hive.ql.exec.FetchOperator.getOutputObjectInspector()",6,7,8
"org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader()",5,14,17
"org.apache.hadoop.hive.ql.exec.FetchOperator.getRowInspectorFromPartition(PartitionDesc,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.getRowInspectorFromPartitionedTable(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.getRowInspectorFromTable(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.getSplitNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.getStructOIFrom(ObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.FetchOperator.getWork()",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.initialize()",1,5,9
"org.apache.hadoop.hive.ql.exec.FetchOperator.isEmptyTable()",1,3,3
"org.apache.hadoop.hive.ql.exec.FetchOperator.isTblDataDone()",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.listStatusUnderPath(FileSystem,Path)",2,3,3
"org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow()",2,4,4
"org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(InspectableObject)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.setCurrPart(PartitionDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.setCurrTbl(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.setSplitNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.setTblDataDone(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.setWork(FetchWork)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchOperator.setupContext(List<Path>)",1,2,2
"org.apache.hadoop.hive.ql.exec.FetchOperator.setupExecContext()",1,4,4
"org.apache.hadoop.hive.ql.exec.FetchOperator.splitSampling(SplitSample,FetchInputFormatSplit[])",3,3,5
"org.apache.hadoop.hive.ql.exec.FetchTask.FetchTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchTask.clearFetch()",1,2,2
"org.apache.hadoop.hive.ql.exec.FetchTask.execute(DriverContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchTask.fetch(List)",5,6,10
"org.apache.hadoop.hive.ql.exec.FetchTask.getMaxRows()",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchTask.getTblDesc()",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchTask.getVirtualColumns(Operator<?>)",2,3,3
"org.apache.hadoop.hive.ql.exec.FetchTask.initialize(HiveConf,QueryPlan,DriverContext)",1,3,3
"org.apache.hadoop.hive.ql.exec.FetchTask.isFetchFrom(FileSinkDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.FetchTask.setMaxRows(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.FSPaths.FSPaths()",1,1,1
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.FSPaths.FSPaths(Path)",1,1,1
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.FSPaths.abortWriters(FileSystem,boolean,boolean)",3,4,5
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.FSPaths.closeWriters(boolean)",3,3,4
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.FSPaths.commit(FileSystem)",3,4,7
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.FSPaths.getFinalPath(String,Path,String)",2,1,2
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.FSPaths.getOutWriters()",1,1,1
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.FSPaths.getStat()",1,1,1
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.FSPaths.getTaskOutPath(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.FSPaths.setOutWriters(RecordWriter[])",1,1,1
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.appendToSource(String,String)",1,2,2
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.areAllTrue(boolean[])",3,1,3
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.augmentPlan()",1,1,1
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.checkOutputSpecs(FileSystem,JobConf)",2,6,7
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(boolean)",1,12,14
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FSPaths)",6,8,10
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketForFileIdx(FSPaths,int)",1,5,7
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.createNewPaths(String)",1,2,3
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.dpSetup()",1,5,5
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.generateListBucketingDirName(Object)",2,4,5
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynOutPaths(List<String>,String)",4,12,13
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynPartDirectory(List<String>,List<String>,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.initializeOp(Configuration)",1,10,13
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.initializeSpecPath()",2,3,3
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.jobCloseOp(Configuration,boolean)",1,4,6
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.lbSetup()",1,2,2
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.lookupListBucketingPaths(String)",1,2,2
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(Object,int)",1,17,20
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.publishStats()",10,13,13
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.splitKey(String)",4,5,5
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.startGroup()",1,5,5
"org.apache.hadoop.hive.ql.exec.FileSinkOperator.updateProgress()",2,3,3
"org.apache.hadoop.hive.ql.exec.FilterOperator.FilterOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.FilterOperator.columnNamesRowResolvedCanBeObtained()",1,1,1
"org.apache.hadoop.hive.ql.exec.FilterOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.FilterOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.FilterOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.FilterOperator.initializeOp(Configuration)",1,2,3
"org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(Object,int)",5,10,13
"org.apache.hadoop.hive.ql.exec.FilterOperator.supportAutomaticSortMergeJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.FilterOperator.supportSkewJoinOptimization()",1,1,1
"org.apache.hadoop.hive.ql.exec.FilterOperator.supportUnionRemoveOptimization()",1,1,1
"org.apache.hadoop.hive.ql.exec.FooterBuffer.FooterBuffer()",1,1,1
"org.apache.hadoop.hive.ql.exec.FooterBuffer.initializeBuffer(JobConf,RecordReader,int,WritableComparable,Writable)",3,2,3
"org.apache.hadoop.hive.ql.exec.FooterBuffer.setCursor(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.FooterBuffer.updateBuffer(JobConf,RecordReader,WritableComparable,Writable)",1,2,2
"org.apache.hadoop.hive.ql.exec.ForwardOperator.acceptLimitPushdown()",1,1,1
"org.apache.hadoop.hive.ql.exec.ForwardOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ForwardOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ForwardOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.ForwardOperator.processOp(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionInfo.FunctionInfo(String,Class<? extends TableFunctionResolver>)",1,2,2
"org.apache.hadoop.hive.ql.exec.FunctionInfo.FunctionInfo(boolean,String,GenericUDAFResolver)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionInfo.FunctionInfo(boolean,String,GenericUDF)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionInfo.FunctionInfo(boolean,String,GenericUDTF)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionInfo.getDisplayName()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionInfo.getFunctionClass()",7,6,7
"org.apache.hadoop.hive.ql.exec.FunctionInfo.getGenericUDAFResolver()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionInfo.getGenericUDF()",2,1,2
"org.apache.hadoop.hive.ql.exec.FunctionInfo.getGenericUDTF()",2,1,2
"org.apache.hadoop.hive.ql.exec.FunctionInfo.isGenericUDAF()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionInfo.isGenericUDF()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionInfo.isGenericUDTF()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionInfo.isInternalTableFunction()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionInfo.isNative()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionInfo.isTableFunction()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.FunctionRegistry()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.addFunctionInfoToWindowFunctions(String,FunctionInfo)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.checkFunctionClass(CommonFunctionInfo)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.cloneGenericUDF(GenericUDF)",4,7,9
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.cloneGenericUDTF(GenericUDTF)",2,1,2
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.filterMethodsByTypeAffinity(List<Method>,List<TypeInfo>)",1,7,10
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getCommonCategory(TypeInfo,TypeInfo)",4,2,8
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getCommonClass(TypeInfo,TypeInfo)",3,1,3
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getCommonClassForComparison(TypeInfo,TypeInfo)",7,7,10
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getCommonClassForUnionAll(TypeInfo,TypeInfo)",9,9,12
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getCommonLength(int,int)",1,2,3
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionInfo(Map<String, T>,String)",1,4,4
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionInfo(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionInfoFromMetastore(String)",2,11,11
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionNames()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionNames(String)",1,3,4
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionNames(boolean)",1,5,5
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionSynonyms(String)",4,3,5
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getGenericUDAFEvaluator(String,List<ObjectInspector>,boolean,boolean)",2,3,4
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getGenericUDAFResolver(String)",2,2,3
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getGenericUDFClassFromExprDesc(ExprNodeDesc)",2,1,2
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getGenericUDFForAnd()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getGenericUDFForIndex()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getGenericWindowingEvaluator(String,List<ObjectInspector>,boolean,boolean)",3,3,4
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getHive()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getMethodInternal(Class<? extends T>,String,boolean,List<TypeInfo>)",1,3,3
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getMethodInternal(Class<?>,List<Method>,boolean,List<TypeInfo>)",13,15,26
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getQualifiedFunctionInfo(Map<String, T>,String)",1,4,4
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getTableFunctionResolver(String)",2,2,2
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getTypeInfoForPrimitiveCategory(PrimitiveTypeInfo,PrimitiveTypeInfo,PrimitiveCategory)",2,5,5
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getUDFClassFromExprDesc(ExprNodeDesc)",2,1,2
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getWindowFunctionInfo(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.getWindowingTableFunction()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.implicitConvertable(PrimitiveCategory,PrimitiveCategory)",10,1,16
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.implicitConvertable(TypeInfo,TypeInfo)",3,3,4
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.impliesOrder(String)",5,5,5
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(Method,Object,Object...)",1,7,7
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isDeterministic(GenericUDF)",6,5,8
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isExactNumericType(PrimitiveTypeInfo)",3,2,3
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isNativeFuncExpr(ExprNodeGenericFuncDesc)",1,2,2
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isNoopFunction(String)",1,4,4
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isNumericType(PrimitiveTypeInfo)",3,2,3
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isOpAnd(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isOpAndOrNot(ExprNodeDesc)",1,1,3
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isOpCast(ExprNodeDesc)",2,2,16
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isOpNot(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isOpOr(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isOpPositive(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isOpPreserveInputName(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isRankingFunction(String)",3,3,3
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isStateful(GenericUDF)",5,5,7
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.isTableFunction(String)",1,3,3
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.loadFunctionResourcesIfNecessary(String,CommonFunctionInfo)",2,3,4
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.matchCost(TypeInfo,TypeInfo,boolean)",8,7,13
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerFunctionsFromPluginJar(URL,ClassLoader)",4,2,4
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerGenericUDAF(String,GenericUDAFResolver)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerGenericUDAF(boolean,String,GenericUDAFResolver)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerGenericUDF(String,Class<? extends GenericUDF>)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerGenericUDF(boolean,String,Class<? extends GenericUDF>)",2,2,2
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerGenericUDTF(String,Class<? extends GenericUDTF>)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerGenericUDTF(boolean,String,Class<? extends GenericUDTF>)",2,2,2
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerNativeStatus(FunctionInfo)",2,1,2
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerNumericType(PrimitiveCategory,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerTableFunction(String,Class<? extends TableFunctionResolver>)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerTemporaryFunction(String,Class<?>)",2,2,8
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerTemporaryGenericUDAF(String,GenericUDAFResolver)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerTemporaryGenericUDF(String,Class<? extends GenericUDF>)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerTemporaryGenericUDTF(String,Class<? extends GenericUDTF>)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerTemporaryMacro(String,ExprNodeDesc,List<String>,List<TypeInfo>)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerTemporaryUDAF(String,Class<? extends UDAF>)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerTemporaryUDF(String,Class<? extends UDF>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerUDAF(String,Class<? extends UDAF>)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerUDAF(boolean,String,Class<? extends UDAF>)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerUDF(String,Class<? extends UDF>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerUDF(String,Class<? extends UDF>,boolean,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerUDF(boolean,String,Class<? extends UDF>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerUDF(boolean,String,Class<? extends UDF>,boolean,String)",2,2,2
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerWindowFunction(String,GenericUDAFResolver)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.registerWindowFunction(String,GenericUDAFResolver,boolean)",1,2,2
"org.apache.hadoop.hive.ql.exec.FunctionRegistry.unregisterTemporaryUDF(String)",3,3,3
"org.apache.hadoop.hive.ql.exec.FunctionTask.FunctionTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionTask.addFunctionResources(List<ResourceUri>)",4,4,4
"org.apache.hadoop.hive.ql.exec.FunctionTask.checkLocalFunctionResources(Hive,List<ResourceUri>)",5,6,8
"org.apache.hadoop.hive.ql.exec.FunctionTask.createMacro(CreateMacroDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionTask.createPermanentFunction(Hive,CreateFunctionDesc)",3,5,5
"org.apache.hadoop.hive.ql.exec.FunctionTask.createTemporaryFunction(CreateFunctionDesc)",2,3,4
"org.apache.hadoop.hive.ql.exec.FunctionTask.dropMacro(DropMacroDesc)",1,2,2
"org.apache.hadoop.hive.ql.exec.FunctionTask.dropPermanentFunction(Hive,DropFunctionDesc)",1,2,2
"org.apache.hadoop.hive.ql.exec.FunctionTask.dropTemporaryFunction(DropFunctionDesc)",1,2,2
"org.apache.hadoop.hive.ql.exec.FunctionTask.execute(DriverContext)",7,9,9
"org.apache.hadoop.hive.ql.exec.FunctionTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionTask.getResourceType(ResourceType)",5,2,5
"org.apache.hadoop.hive.ql.exec.FunctionTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionTask.getUdfClass(CreateFunctionDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionTask.initialize(HiveConf,QueryPlan,DriverContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionUtils.getQualifiedFunctionNameParts(String)",2,2,2
"org.apache.hadoop.hive.ql.exec.FunctionUtils.getUDFClassType(Class<?>)",7,6,7
"org.apache.hadoop.hive.ql.exec.FunctionUtils.isQualifiedFunctionName(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.FunctionUtils.qualifyFunctionName(String,String)",2,1,2
"org.apache.hadoop.hive.ql.exec.FunctionUtils.splitQualifiedFunctionName(String)",3,1,3
"org.apache.hadoop.hive.ql.exec.GroupByOperator.acceptLimitPushdown()",1,2,2
"org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(boolean)",2,4,8
"org.apache.hadoop.hive.ql.exec.GroupByOperator.computeMaxEntriesHashAggr(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.GroupByOperator.endGroup()",1,2,2
"org.apache.hadoop.hive.ql.exec.GroupByOperator.estimateRowSize()",3,4,5
"org.apache.hadoop.hive.ql.exec.GroupByOperator.estimateSize(AggregationBuffer,List<Field>)",1,4,5
"org.apache.hadoop.hive.ql.exec.GroupByOperator.flush()",1,5,6
"org.apache.hadoop.hive.ql.exec.GroupByOperator.flushHashTable(boolean)",4,5,5
"org.apache.hadoop.hive.ql.exec.GroupByOperator.forward(Object[],AggregationBuffer[])",1,2,4
"org.apache.hadoop.hive.ql.exec.GroupByOperator.genColLists(HashMap<Operator<? extends OperatorDesc>, OpParseContext>)",1,4,4
"org.apache.hadoop.hive.ql.exec.GroupByOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.GroupByOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.GroupByOperator.getSize(int,Class<?>,Field)",4,10,13
"org.apache.hadoop.hive.ql.exec.GroupByOperator.getSize(int,PrimitiveCategory)",4,2,6
"org.apache.hadoop.hive.ql.exec.GroupByOperator.getSize(int,TypeInfo)",2,2,2
"org.apache.hadoop.hive.ql.exec.GroupByOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.GroupByOperator.groupingSet2BitSet(int)",1,3,3
"org.apache.hadoop.hive.ql.exec.GroupByOperator.initializeOp(Configuration)",1,33,35
"org.apache.hadoop.hive.ql.exec.GroupByOperator.newAggregations()",1,2,2
"org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(Object,ObjectInspector,KeyWrapper)",1,7,11
"org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(Object,ObjectInspector,KeyWrapper)",1,5,6
"org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(Object,ObjectInspector)",1,3,5
"org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(Object,int)",1,6,12
"org.apache.hadoop.hive.ql.exec.GroupByOperator.resetAggregations(AggregationBuffer[])",1,2,2
"org.apache.hadoop.hive.ql.exec.GroupByOperator.shouldBeFlushed(KeyWrapper)",6,13,16
"org.apache.hadoop.hive.ql.exec.GroupByOperator.startGroup()",1,1,1
"org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(AggregationBuffer[],Object,ObjectInspector,boolean,boolean,Object[][])",2,25,27
"org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.closeOp(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.initializeOp(Configuration)",1,2,2
"org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.processOp(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.HashTableSinkOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.HashTableSinkOperator(MapJoinOperator)",1,1,1
"org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.closeOp(boolean)",1,2,4
"org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.flushToFile()",3,2,3
"org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.getMapJoinTables()",1,1,1
"org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.getStandardObjectInspectors(List<ObjectInspector>[],int)",3,3,4
"org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.hasFilter(int)",1,1,2
"org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.initializeOp(Configuration)",6,5,11
"org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.processOp(Object,int)",1,9,11
"org.apache.hadoop.hive.ql.exec.Heartbeater.Heartbeater(HiveTxnManager,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.Heartbeater.heartbeat()",6,6,7
"org.apache.hadoop.hive.ql.exec.HiveTotalOrderPartitioner.configure(JobConf)",1,1,1
"org.apache.hadoop.hive.ql.exec.HiveTotalOrderPartitioner.getPartition(HiveKey,Object,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.JoinOperator.closeOp(boolean)",1,2,2
"org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup()",2,3,4
"org.apache.hadoop.hive.ql.exec.JoinOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.JoinOperator.initializeOp(Configuration)",1,2,2
"org.apache.hadoop.hive.ql.exec.JoinOperator.jobCloseOp(Configuration,boolean)",9,7,10
"org.apache.hadoop.hive.ql.exec.JoinOperator.moveUpFiles(Path,Configuration,Log)",1,4,4
"org.apache.hadoop.hive.ql.exec.JoinOperator.mvFileToFinalPath(Path,Configuration,boolean,Log)",1,3,3
"org.apache.hadoop.hive.ql.exec.JoinOperator.opAllowedBeforeSortMergeJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.JoinOperator.processOp(Object,int)",1,8,12
"org.apache.hadoop.hive.ql.exec.JoinOperator.supportSkewJoinOptimization()",1,1,1
"org.apache.hadoop.hive.ql.exec.JoinUtil.computeKeys(Object,List<ExprNodeEvaluator>,List<ObjectInspector>)",1,2,2
"org.apache.hadoop.hive.ql.exec.JoinUtil.computeMapJoinValues(Object,List<ExprNodeEvaluator>,List<ObjectInspector>,List<ExprNodeEvaluator>,List<ObjectInspector>,int[])",1,3,3
"org.apache.hadoop.hive.ql.exec.JoinUtil.computeValues(Object,List<ExprNodeEvaluator>,List<ObjectInspector>,boolean)",1,3,3
"org.apache.hadoop.hive.ql.exec.JoinUtil.getObjectInspectorsFromEvaluators(List<ExprNodeEvaluator>[],ObjectInspector[],int,int)",3,3,6
"org.apache.hadoop.hive.ql.exec.JoinUtil.getRowContainer(Configuration,List<ObjectInspector>,Byte,int,TableDesc[],JoinDesc,boolean,Reporter)",1,2,3
"org.apache.hadoop.hive.ql.exec.JoinUtil.getSpillSerDe(byte,TableDesc[],JoinDesc,boolean)",2,2,3
"org.apache.hadoop.hive.ql.exec.JoinUtil.getSpillTableDesc(Byte,TableDesc[],JoinDesc,boolean)",1,2,3
"org.apache.hadoop.hive.ql.exec.JoinUtil.getStandardObjectInspectors(List<ObjectInspector>[],int,int)",3,3,5
"org.apache.hadoop.hive.ql.exec.JoinUtil.hasAnyFiltered(short)",1,1,1
"org.apache.hadoop.hive.ql.exec.JoinUtil.initSpillTables(JoinDesc,boolean)",3,4,5
"org.apache.hadoop.hive.ql.exec.JoinUtil.isFiltered(Object,List<ExprNodeEvaluator>,List<ObjectInspector>,int[])",1,4,7
"org.apache.hadoop.hive.ql.exec.JoinUtil.isFiltered(short,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.JoinUtil.populateJoinKeyValue(List<ExprNodeEvaluator>[],Map<Byte, List<ExprNodeDesc>>,Byte[],int)",3,5,6
"org.apache.hadoop.hive.ql.exec.JoinUtil.populateJoinKeyValue(List<ExprNodeEvaluator>[],Map<Byte, List<ExprNodeDesc>>,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.KeyWrapperFactory(ExprNodeEvaluator[],ObjectInspector[],ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.ListKeyWrapper.ListKeyWrapper(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.ListKeyWrapper.ListKeyWrapper(int,Object[],boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.ListKeyWrapper.copyKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.ListKeyWrapper.copyKey(KeyWrapper)",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.ListKeyWrapper.deepCopyElements(Object[],ObjectInspector[],ObjectInspectorCopyOption)",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.ListKeyWrapper.deepCopyElements(Object[],ObjectInspector[],Object[],ObjectInspectorCopyOption)",1,2,2
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.ListKeyWrapper.equals(Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.ListKeyWrapper.getKeyArray()",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.ListKeyWrapper.getNewKey(Object,ObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.ListKeyWrapper.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.ListKeyWrapper.setEqualComparer(boolean)",1,1,2
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.ListKeyWrapper.setHashKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.TextKeyWrapper.TextKeyWrapper(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.TextKeyWrapper.TextKeyWrapper(int,Object,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.TextKeyWrapper.copyKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.TextKeyWrapper.copyKey(KeyWrapper)",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.TextKeyWrapper.equals(Object)",3,4,6
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.TextKeyWrapper.getKeyArray()",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.TextKeyWrapper.getNewKey(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.TextKeyWrapper.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.TextKeyWrapper.setHashKey()",1,2,2
"org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.getKeyWrapper()",2,3,3
"org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.processOp(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.initializeOp(Configuration)",1,3,3
"org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.processOp(Object,int)",3,3,3
"org.apache.hadoop.hive.ql.exec.LimitOperator.closeOp(boolean)",2,1,3
"org.apache.hadoop.hive.ql.exec.LimitOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.LimitOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.LimitOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.LimitOperator.initializeOp(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.LimitOperator.processOp(Object,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.ListSinkOperator.getNumRows()",1,1,1
"org.apache.hadoop.hive.ql.exec.ListSinkOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.ListSinkOperator.initializeFetcher(Configuration)",1,3,3
"org.apache.hadoop.hive.ql.exec.ListSinkOperator.initializeOp(Configuration)",1,1,2
"org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(Object,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.ListSinkOperator.reset(List)",1,1,1
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.MapJoinOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.MapJoinOperator(AbstractMapJoinOperator<? extends MapJoinDesc>)",1,1,1
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp()",1,2,3
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.closeOp(boolean)",1,8,8
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.endGroup()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.generateMapMetaData()",3,3,4
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.getRefKey(byte)",4,2,4
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.getValueObjectInspectors(byte,List<ObjectInspector>[])",2,5,5
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.initializeOp(Configuration)",1,2,3
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable()",3,3,5
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(Object,int)",1,13,17
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.setMapJoinKey(ReusableGetAdaptor,Object,byte)",1,1,1
"org.apache.hadoop.hive.ql.exec.MapJoinOperator.startGroup()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapOperator.MapInputPath.MapInputPath(String,String,Operator<?>,PartitionDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.MapOperator.MapInputPath.equals(Object)",2,4,4
"org.apache.hadoop.hive.ql.exec.MapOperator.MapInputPath.hashCode()",1,4,4
"org.apache.hadoop.hive.ql.exec.MapOperator.MapOpCtx.getRowObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapOperator.MapOpCtx.hasVC()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapOperator.MapOpCtx.isPartitioned()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapOperator.MapOpCtx.readRow(Writable)",1,1,1
"org.apache.hadoop.hive.ql.exec.MapOperator.cleanUpInputFileChangedOp()",5,4,5
"org.apache.hadoop.hive.ql.exec.MapOperator.closeOp(boolean)",1,3,3
"org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(Configuration)",1,7,8
"org.apache.hadoop.hive.ql.exec.MapOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapOperator.initObjectInspector(Configuration,MapInputPath,Map<TableDesc, StructObjectInspector>)",4,12,14
"org.apache.hadoop.hive.ql.exec.MapOperator.initializeAsRoot(Configuration,MapWork)",1,1,1
"org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(Configuration)",1,4,5
"org.apache.hadoop.hive.ql.exec.MapOperator.isPartitioned(PartitionDesc)",1,2,2
"org.apache.hadoop.hive.ql.exec.MapOperator.normalizePath(String)",1,2,2
"org.apache.hadoop.hive.ql.exec.MapOperator.populateVirtualColumnValues(ExecMapperContext,List<VirtualColumn>,Object[],Deserializer)",10,15,19
"org.apache.hadoop.hive.ql.exec.MapOperator.process(Writable)",1,9,11
"org.apache.hadoop.hive.ql.exec.MapOperator.processOp(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(Configuration)",5,7,9
"org.apache.hadoop.hive.ql.exec.MapredContext.MapredContext(boolean,JobConf)",1,1,1
"org.apache.hadoop.hive.ql.exec.MapredContext.close()",1,2,2
"org.apache.hadoop.hive.ql.exec.MapredContext.closeAll()",1,3,3
"org.apache.hadoop.hive.ql.exec.MapredContext.get()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapredContext.getJobConf()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapredContext.getReporter()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapredContext.init(boolean,JobConf)",1,1,2
"org.apache.hadoop.hive.ql.exec.MapredContext.isMap()",1,1,1
"org.apache.hadoop.hive.ql.exec.MapredContext.needClose(Closeable)",1,2,3
"org.apache.hadoop.hive.ql.exec.MapredContext.needConfigure(Object)",1,3,4
"org.apache.hadoop.hive.ql.exec.MapredContext.registerCloseable(Closeable)",1,1,1
"org.apache.hadoop.hive.ql.exec.MapredContext.setReporter(Reporter)",1,1,1
"org.apache.hadoop.hive.ql.exec.MapredContext.setup(GenericUDAFEvaluator)",1,3,3
"org.apache.hadoop.hive.ql.exec.MapredContext.setup(GenericUDF)",1,3,3
"org.apache.hadoop.hive.ql.exec.MapredContext.setup(GenericUDTF)",1,2,2
"org.apache.hadoop.hive.ql.exec.MoveTask.MoveTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.MoveTask.createTargetPath(Path,FileSystem)",1,5,5
"org.apache.hadoop.hive.ql.exec.MoveTask.execute(DriverContext)",13,35,48
"org.apache.hadoop.hive.ql.exec.MoveTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.MoveTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.MoveTask.isLocal()",4,2,4
"org.apache.hadoop.hive.ql.exec.MoveTask.isSkewedStoredAsDirs(LoadTableDesc)",1,2,2
"org.apache.hadoop.hive.ql.exec.MoveTask.moveFile(Path,Path,boolean)",8,10,12
"org.apache.hadoop.hive.ql.exec.MoveTask.releaseLocks(LoadTableDesc)",3,4,6
"org.apache.hadoop.hive.ql.exec.MoveTask.updatePartitionBucketSortColumns(Table,Partition,List<BucketCol>,int,List<SortCol>)",8,11,13
"org.apache.hadoop.hive.ql.exec.MuxOperator.Handler.Handler(ObjectInspector,List<ExprNodeDesc>,List<ExprNodeDesc>,List<String>,List<String>,Integer)",1,3,3
"org.apache.hadoop.hive.ql.exec.MuxOperator.Handler.getOutputObjInspector()",1,1,1
"org.apache.hadoop.hive.ql.exec.MuxOperator.Handler.getTag()",1,1,1
"org.apache.hadoop.hive.ql.exec.MuxOperator.Handler.process(Object)",1,3,3
"org.apache.hadoop.hive.ql.exec.MuxOperator.closeOp(boolean)",1,2,2
"org.apache.hadoop.hive.ql.exec.MuxOperator.endGroup()",1,1,1
"org.apache.hadoop.hive.ql.exec.MuxOperator.forward(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.MuxOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.MuxOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.MuxOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.MuxOperator.initializeChildren(Configuration)",2,3,4
"org.apache.hadoop.hive.ql.exec.MuxOperator.initializeOp(Configuration)",2,3,4
"org.apache.hadoop.hive.ql.exec.MuxOperator.processGroup(int)",3,2,4
"org.apache.hadoop.hive.ql.exec.MuxOperator.processOp(Object,int)",1,7,7
"org.apache.hadoop.hive.ql.exec.MuxOperator.startGroup()",1,1,2
"org.apache.hadoop.hive.ql.exec.NoMatchingMethodException.NoMatchingMethodException(Class<?>,List<TypeInfo>,List<Method>)",1,1,1
"org.apache.hadoop.hive.ql.exec.NodeUtils.iterate(Collection<? extends Node>,Class<T>,Function<T>)",1,2,2
"org.apache.hadoop.hive.ql.exec.NodeUtils.iterate(Node,Class<T>,Function<T>,Set<Node>)",2,4,5
"org.apache.hadoop.hive.ql.exec.NodeUtils.iterateTask(Collection<Task<?>>,Class<T>,Function<T>)",1,2,2
"org.apache.hadoop.hive.ql.exec.NodeUtils.iterateTask(Task<?>,Class<T>,Function<T>,Set<Task>)",2,4,5
"org.apache.hadoop.hive.ql.exec.NumericOpMethodResolver.NumericOpMethodResolver(Class<? extends UDF>)",1,1,1
"org.apache.hadoop.hive.ql.exec.NumericOpMethodResolver.getEvalMethod(List<TypeInfo>)",8,14,17
"org.apache.hadoop.hive.ql.exec.NumericUDAF.NumericUDAF()",1,1,1
"org.apache.hadoop.hive.ql.exec.NumericUDAFEvaluatorResolver.NumericUDAFEvaluatorResolver(Class<? extends UDAF>)",1,1,1
"org.apache.hadoop.hive.ql.exec.NumericUDAFEvaluatorResolver.getEvaluatorClass(List<TypeInfo>)",1,4,4
"org.apache.hadoop.hive.ql.exec.ObjectCacheFactory.ObjectCacheFactory()",1,1,1
"org.apache.hadoop.hive.ql.exec.ObjectCacheFactory.getCache(Configuration)",2,1,2
"org.apache.hadoop.hive.ql.exec.Operator.DummyOperator.DummyOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.DummyOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.DummyOperator.processOp(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.Operator()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.Operator(Reporter)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.Operator(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.acceptLimitPushdown()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.allInitializedParentsAreClosed()",5,3,6
"org.apache.hadoop.hive.ql.exec.Operator.areAllParentsInitialized()",5,1,5
"org.apache.hadoop.hive.ql.exec.Operator.augmentPlan()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged()",1,3,3
"org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChangedOp()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.clone()",1,3,3
"org.apache.hadoop.hive.ql.exec.Operator.cloneOp()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.cloneRecursiveChildren()",2,5,5
"org.apache.hadoop.hive.ql.exec.Operator.close(boolean)",4,4,6
"org.apache.hadoop.hive.ql.exec.Operator.closeOp(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.columnNamesRowResolvedCanBeObtained()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.createDummy()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.defaultEndGroup()",2,2,3
"org.apache.hadoop.hive.ql.exec.Operator.defaultStartGroup()",2,2,3
"org.apache.hadoop.hive.ql.exec.Operator.dump(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.dump(int,HashSet<Integer>)",2,5,6
"org.apache.hadoop.hive.ql.exec.Operator.endGroup()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.flush()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.forward(Object,ObjectInspector)",2,5,6
"org.apache.hadoop.hive.ql.exec.Operator.getAdditionalCounters()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getChildOperators()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getChildren()",2,2,3
"org.apache.hadoop.hive.ql.exec.Operator.getColumnExprMap()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getConf()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getConfiguration()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getDone()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getExecContext()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getGroupKeyObject()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getGroupKeyObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getIdentifier()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getInputObjInspectors()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getLevelString(int)",2,2,3
"org.apache.hadoop.hive.ql.exec.Operator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getNextCntr(long)",2,1,2
"org.apache.hadoop.hive.ql.exec.Operator.getNumChild()",1,2,2
"org.apache.hadoop.hive.ql.exec.Operator.getNumParent()",1,2,2
"org.apache.hadoop.hive.ql.exec.Operator.getOpTraits()",2,2,2
"org.apache.hadoop.hive.ql.exec.Operator.getOperatorId()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getOutputObjInspector()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getParentOperators()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.getStatistics()",2,2,2
"org.apache.hadoop.hive.ql.exec.Operator.getStats()",1,2,2
"org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(ExprNodeEvaluator[],ObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(ExprNodeEvaluator[],int,int,ObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(ExprNodeEvaluator[],List<String>,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.initOperatorId()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.initialize(Configuration,ObjectInspector,int)",1,2,3
"org.apache.hadoop.hive.ql.exec.Operator.initialize(Configuration,ObjectInspector[])",9,8,14
"org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Configuration)",2,4,5
"org.apache.hadoop.hive.ql.exec.Operator.initializeLocalWork(Configuration)",1,3,3
"org.apache.hadoop.hive.ql.exec.Operator.initializeOp(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.isUseBucketizedHiveInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.jobClose(Configuration,boolean)",2,3,4
"org.apache.hadoop.hive.ql.exec.Operator.jobCloseOp(Configuration,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.logStats()",1,2,2
"org.apache.hadoop.hive.ql.exec.Operator.opAllowedAfterMapJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.opAllowedBeforeMapJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.opAllowedBeforeSortMergeJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.opAllowedConvertMapJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.passExecContext(ExecMapperContext)",1,3,3
"org.apache.hadoop.hive.ql.exec.Operator.preorderMap(OperatorFunc)",1,3,3
"org.apache.hadoop.hive.ql.exec.Operator.processGroup(int)",2,3,4
"org.apache.hadoop.hive.ql.exec.Operator.removeChild(Operator<? extends OperatorDesc>)",1,3,3
"org.apache.hadoop.hive.ql.exec.Operator.removeChildAndAdoptItsChildren(Operator<? extends OperatorDesc>)",4,4,6
"org.apache.hadoop.hive.ql.exec.Operator.removeChildren(int)",3,5,6
"org.apache.hadoop.hive.ql.exec.Operator.removeParent(Operator<? extends OperatorDesc>)",1,3,3
"org.apache.hadoop.hive.ql.exec.Operator.replaceChild(Operator<? extends OperatorDesc>,Operator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.replaceParent(Operator<? extends OperatorDesc>,Operator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.reset()",1,3,3
"org.apache.hadoop.hive.ql.exec.Operator.resetId()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.resetStats()",1,2,2
"org.apache.hadoop.hive.ql.exec.Operator.setAlias(String)",2,2,3
"org.apache.hadoop.hive.ql.exec.Operator.setChildOperators(List<Operator<? extends OperatorDesc>>)",1,1,2
"org.apache.hadoop.hive.ql.exec.Operator.setColumnExprMap(Map<String, ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.setConf(T)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.setDone(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.setExecContext(ExecMapperContext)",1,3,3
"org.apache.hadoop.hive.ql.exec.Operator.setGroupKeyObject(Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.setGroupKeyObjectInspector(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.setId(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.setInputObjInspectors(ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.setOpTraits(OpTraits)",1,3,3
"org.apache.hadoop.hive.ql.exec.Operator.setOperatorId(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.setOutputCollector(OutputCollector)",2,2,3
"org.apache.hadoop.hive.ql.exec.Operator.setParentOperators(List<Operator<? extends OperatorDesc>>)",1,1,2
"org.apache.hadoop.hive.ql.exec.Operator.setReporter(Reporter)",2,2,3
"org.apache.hadoop.hive.ql.exec.Operator.setSchema(RowSchema)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.setStatistics(Statistics)",1,3,3
"org.apache.hadoop.hive.ql.exec.Operator.setUseBucketizedHiveInputFormat(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.startGroup()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.supportAutomaticSortMergeJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.supportSkewJoinOptimization()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.supportUnionRemoveOptimization()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.toString()",1,1,1
"org.apache.hadoop.hive.ql.exec.Operator.toString(Collection<Operator<? extends OperatorDesc>>)",1,3,3
"org.apache.hadoop.hive.ql.exec.Operator.toString(StringBuilder,Set<String>,Operator<?>,int)",2,7,7
"org.apache.hadoop.hive.ql.exec.OperatorFactory.OpTuple.OpTuple(Class<T>,Class<? extends Operator<T>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.OperatorFactory.OperatorFactory()",1,1,1
"org.apache.hadoop.hive.ql.exec.OperatorFactory.get(Class<T>)",3,4,4
"org.apache.hadoop.hive.ql.exec.OperatorFactory.get(Class<T>,RowSchema)",1,1,1
"org.apache.hadoop.hive.ql.exec.OperatorFactory.get(T,Operator<? extends OperatorDesc>...)",1,1,1
"org.apache.hadoop.hive.ql.exec.OperatorFactory.get(T,RowSchema,Operator...)",1,1,1
"org.apache.hadoop.hive.ql.exec.OperatorFactory.getAndMakeChild(T,List<Operator<? extends OperatorDesc>>)",2,3,5
"org.apache.hadoop.hive.ql.exec.OperatorFactory.getAndMakeChild(T,Operator...)",2,3,5
"org.apache.hadoop.hive.ql.exec.OperatorFactory.getAndMakeChild(T,RowSchema,List<Operator<? extends OperatorDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.OperatorFactory.getAndMakeChild(T,RowSchema,Map<String, ExprNodeDesc>,List<Operator<? extends OperatorDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.OperatorFactory.getAndMakeChild(T,RowSchema,Map<String, ExprNodeDesc>,Operator...)",1,1,1
"org.apache.hadoop.hive.ql.exec.OperatorFactory.getAndMakeChild(T,RowSchema,Operator...)",1,1,1
"org.apache.hadoop.hive.ql.exec.OperatorFactory.getVectorOperator(T,VectorizationContext)",3,4,4
"org.apache.hadoop.hive.ql.exec.OperatorFactory.makeChild(Operator<? extends OperatorDesc>,Operator<? extends OperatorDesc>...)",2,3,5
"org.apache.hadoop.hive.ql.exec.OperatorUtils.findOperators(Collection<Operator<?>>,Class<T>)",1,2,2
"org.apache.hadoop.hive.ql.exec.OperatorUtils.findOperators(Operator<?>,Class<T>)",1,1,1
"org.apache.hadoop.hive.ql.exec.OperatorUtils.findOperators(Operator<?>,Class<T>,Set<T>)",1,4,4
"org.apache.hadoop.hive.ql.exec.OperatorUtils.findOperatorsUpstream(Collection<Operator<?>>,Class<T>)",1,2,2
"org.apache.hadoop.hive.ql.exec.OperatorUtils.findOperatorsUpstream(Operator<?>,Class<T>)",1,1,1
"org.apache.hadoop.hive.ql.exec.OperatorUtils.findOperatorsUpstream(Operator<?>,Class<T>,Set<T>)",1,4,4
"org.apache.hadoop.hive.ql.exec.OperatorUtils.findSingleOperator(Operator<?>,Class<T>)",1,2,2
"org.apache.hadoop.hive.ql.exec.OperatorUtils.findSingleOperatorUpstream(Operator<?>,Class<T>)",1,2,2
"org.apache.hadoop.hive.ql.exec.OperatorUtils.iterateParents(Operator<?>,Function<Operator<?>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.OperatorUtils.iterateParents(Operator<?>,Function<Operator<?>>,Set<Operator<?>>)",2,3,4
"org.apache.hadoop.hive.ql.exec.OperatorUtils.setChildrenCollector(List<Operator<? extends OperatorDesc>>,Map<String, OutputCollector>)",2,4,5
"org.apache.hadoop.hive.ql.exec.OperatorUtils.setChildrenCollector(List<Operator<? extends OperatorDesc>>,OutputCollector)",2,3,4
"org.apache.hadoop.hive.ql.exec.PTFOperator.PTFInvocation.PTFInvocation(PTFInvocation,TableFunctionEvaluator)",1,1,2
"org.apache.hadoop.hive.ql.exec.PTFOperator.PTFInvocation.close()",1,3,3
"org.apache.hadoop.hive.ql.exec.PTFOperator.PTFInvocation.createInputPartition()",1,4,4
"org.apache.hadoop.hive.ql.exec.PTFOperator.PTFInvocation.finishPartition()",1,10,10
"org.apache.hadoop.hive.ql.exec.PTFOperator.PTFInvocation.handleOutputRows(List<Object>)",1,4,4
"org.apache.hadoop.hive.ql.exec.PTFOperator.PTFInvocation.initializeStreaming(Configuration,boolean)",1,3,3
"org.apache.hadoop.hive.ql.exec.PTFOperator.PTFInvocation.isOutputIterator()",1,2,2
"org.apache.hadoop.hive.ql.exec.PTFOperator.PTFInvocation.isStreaming()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFOperator.PTFInvocation.processRow(Object)",1,2,2
"org.apache.hadoop.hive.ql.exec.PTFOperator.PTFInvocation.startPartition()",1,6,6
"org.apache.hadoop.hive.ql.exec.PTFOperator.closeOp(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFOperator.connectLeadLagFunctionsToPartition(PTFDesc,PTFPartitionIterator<Object>)",2,2,3
"org.apache.hadoop.hive.ql.exec.PTFOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(Configuration)",1,2,2
"org.apache.hadoop.hive.ql.exec.PTFOperator.processOp(Object,int)",1,7,10
"org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFOperator.setupChain()",1,3,4
"org.apache.hadoop.hive.ql.exec.PTFOperator.setupKeysWrapper(ObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.PTFPartition.PItr.PItr(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.PItr.checkForComodification()",2,1,2
"org.apache.hadoop.hive.ql.exec.PTFPartition.PItr.getAt(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.PItr.getIndex()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.PItr.getPartition()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.PItr.hasNext()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.PItr.lag(int)",1,1,2
"org.apache.hadoop.hive.ql.exec.PTFPartition.PItr.lead(int)",1,1,2
"org.apache.hadoop.hive.ql.exec.PTFPartition.PItr.next()",1,1,2
"org.apache.hadoop.hive.ql.exec.PTFPartition.PItr.remove()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.PItr.reset()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.PItr.resetToIndex(int)",2,1,3
"org.apache.hadoop.hive.ql.exec.PTFPartition.PTFPartition(Configuration,SerDe,StructObjectInspector,StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.PTFPartition(Configuration,SerDe,StructObjectInspector,StructObjectInspector,boolean)",1,2,2
"org.apache.hadoop.hive.ql.exec.PTFPartition.append(Object)",2,2,2
"org.apache.hadoop.hive.ql.exec.PTFPartition.close()",1,2,2
"org.apache.hadoop.hive.ql.exec.PTFPartition.create(Configuration,SerDe,StructObjectInspector,StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.createRolling(Configuration,SerDe,StructObjectInspector,StructObjectInspector,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.getAt(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.getInputOI()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.getOutputOI()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.getSerDe()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.iterator()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.range(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.reset()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.setupPartitionOutputOI(SerDe,StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFPartition.size()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.PTFRollingPartition(Configuration,SerDe,StructObjectInspector,StructObjectInspector,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.RollingPItr.getIndex()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.RollingPItr.getPartition()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.RollingPItr.hasNext()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.RollingPItr.lag(int)",1,1,2
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.RollingPItr.lead(int)",1,1,2
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.RollingPItr.next()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.RollingPItr.remove()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.RollingPItr.reset()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.RollingPItr.resetToIndex(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.append(Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.close()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.getAt(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.iterator()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.nextOutputRow()",1,2,2
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.processedAllRows()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.reset()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.rowToProcess(WindowFunctionDef)",1,1,2
"org.apache.hadoop.hive.ql.exec.PTFRollingPartition.size()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.Key.Key(boolean,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.Key.equals(Object)",6,1,6
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.Key.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.Key.toString()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFTopNHash._tryStoreKey(HiveKey,boolean,int)",3,5,7
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.findLargest()",1,2,2
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.flush()",2,2,4
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.getVectorizedBatchResult(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.getVectorizedKeyDistLength(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.getVectorizedKeyHashCode(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.getVectorizedKeyToForward(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.initialize(int,float,boolean,BinaryCollector)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.startVectorizedBatch(int)",3,3,6
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.storeValue(int,int,BytesWritable,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.tryStoreKey(HiveKey,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.tryStoreVectorizedKey(HiveKey,boolean,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.PTFTopNHash.updateLargest(TopNHash)",1,1,3
"org.apache.hadoop.hive.ql.exec.PTFUtils.EL.exceptionThrown(Exception)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFUtils.ReverseIterator.ReverseIterator(Iterator<T>)",1,2,2
"org.apache.hadoop.hive.ql.exec.PTFUtils.ReverseIterator.hasNext()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFUtils.ReverseIterator.next()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFUtils.ReverseIterator.remove()",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFUtils.addAntlrPersistenceDelegates(XMLEncoder)",1,2,2
"org.apache.hadoop.hive.ql.exec.PTFUtils.addEnumDelegates(XMLEncoder)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFUtils.addHivePersistenceDelegates(XMLEncoder)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFUtils.addPersistenceDelegates(XMLEncoder)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFUtils.deserialize(InputStream)",1,2,2
"org.apache.hadoop.hive.ql.exec.PTFUtils.makeTransient(Class<?>,String...)",4,3,6
"org.apache.hadoop.hive.ql.exec.PTFUtils.serialize(OutputStream,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.PTFUtils.toString(List<?>)",1,3,3
"org.apache.hadoop.hive.ql.exec.PTFUtils.toString(Map<?, ?>)",1,3,3
"org.apache.hadoop.hive.ql.exec.PTFUtils.unescapeQueryString(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.PartitionKeySampler.FetchSampler.FetchSampler(FetchWork,JobConf,Operator<?>)",1,1,1
"org.apache.hadoop.hive.ql.exec.PartitionKeySampler.FetchSampler.pushRow()",3,1,3
"org.apache.hadoop.hive.ql.exec.PartitionKeySampler.FetchSampler.pushRow(InspectableObject)",1,2,2
"org.apache.hadoop.hive.ql.exec.PartitionKeySampler.FetchSampler.setSampleNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.PartitionKeySampler.FetchSampler.setSamplePercent(float)",1,1,1
"org.apache.hadoop.hive.ql.exec.PartitionKeySampler.addSampleFile(Path,JobConf)",1,2,2
"org.apache.hadoop.hive.ql.exec.PartitionKeySampler.collect(HiveKey,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.PartitionKeySampler.createSampler(FetchWork,HiveConf,JobConf,Operator<?>)",2,1,3
"org.apache.hadoop.hive.ql.exec.PartitionKeySampler.getPartitionKeys(int)",4,3,6
"org.apache.hadoop.hive.ql.exec.PartitionKeySampler.writePartitionKeys(Path,JobConf)",1,2,2
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.closeOp(boolean)",1,2,2
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(BytesWritable,Writable)",1,2,2
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.collect(byte[],byte[],int)",1,1,1
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.computeBucketNumber(Object,int)",1,2,3
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.computeHashCode(Object,int)",1,3,5
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.computeMurmurHash(HiveKey)",1,1,1
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.getInputAliases()",1,1,1
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.getValueIndex()",1,1,1
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.initEvaluatorsAndReturnStruct(ExprNodeEvaluator[],List<List<Integer>>,List<String>,int,ObjectInspector)",1,4,5
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.initializeOp(Configuration)",1,11,13
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.makeValueWritable(Object)",1,2,2
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.opAllowedBeforeMapJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.partitionKeysAreNull(Object)",4,3,4
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.populateCachedDistinctKeys(Object,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.populateCachedDistributionKeys(Object,int)",1,2,3
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(Object,int)",2,9,15
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.setInputAliases(String[])",1,1,1
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.setOutputCollector(OutputCollector)",1,1,1
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.setSkipTag(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.setValueIndex(int[])",1,1,1
"org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.toHiveKey(Object,int,Integer)",1,2,4
"org.apache.hadoop.hive.ql.exec.RowSchema.RowSchema()",1,1,1
"org.apache.hadoop.hive.ql.exec.RowSchema.RowSchema(ArrayList<ColumnInfo>)",1,1,1
"org.apache.hadoop.hive.ql.exec.RowSchema.RowSchema(RowSchema)",1,1,1
"org.apache.hadoop.hive.ql.exec.RowSchema.getSignature()",1,1,1
"org.apache.hadoop.hive.ql.exec.RowSchema.setSignature(ArrayList<ColumnInfo>)",1,1,1
"org.apache.hadoop.hive.ql.exec.RowSchema.toString()",1,3,3
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.MergeQueue.MergeQueue(String,FetchWork,JobConf,Operator<? extends OperatorDesc>,DummyStoreOperator)",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.MergeQueue.adjustPriorityQueue(Integer)",1,2,2
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.MergeQueue.clearFetchContext()",1,4,4
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.MergeQueue.getNextRow()",2,3,3
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.MergeQueue.lessThan(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.MergeQueue.next(Integer)",3,4,5
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.MergeQueue.nextHive(Integer)",1,1,2
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.MergeQueue.nextIO(Integer)",1,1,2
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.MergeQueue.segmentsForSize(int)",1,3,4
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.MergeQueue.setupContext(List<Path>)",1,4,5
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.SMBMapJoinOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.SMBMapJoinOperator(AbstractMapJoinOperator<? extends MapJoinDesc>)",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.allFetchDone()",3,1,4
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.allInitializedParentsAreClosed()",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.cleanUpInputFileChangedOp()",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.closeOp(boolean)",3,6,11
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.compareKeys(List<Object>,List<Object>)",7,2,10
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchNextGroup(Byte)",4,4,8
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchOneRow(byte)",3,1,5
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.findSmallestKey()",4,2,6
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.initializeLocalWork(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.initializeMapredLocalWork(MapJoinDesc,Configuration,MapredLocalWork,Log)",2,2,4
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.initializeOp(Configuration)",1,3,7
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.isConvertedAutomaticallySMBJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.joinFinalLeftData()",6,10,13
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.joinObject(int[])",4,5,7
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.joinOneGroup()",1,4,4
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.opAllowedConvertMapJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.processKey(byte,List<Object>)",3,2,3
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.processOp(Object,int)",4,13,14
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.promoteNextGroupToCandidate(Byte)",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.putDummyOrEmpty(Byte)",1,1,2
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.setConvertedAutomaticallySMBJoin(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.setUpFetchContexts(String,MergeQueue)",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.smbJoinComputeKeys(Object,byte)",1,1,1
"org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.tagForAlias(String)",3,2,3
"org.apache.hadoop.hive.ql.exec.ScriptOperator.CounterStatusProcessor.CounterStatusProcessor(Configuration,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.exec.ScriptOperator.CounterStatusProcessor.incrCounter(String)",1,3,3
"org.apache.hadoop.hive.ql.exec.ScriptOperator.CounterStatusProcessor.process(String)",2,4,4
"org.apache.hadoop.hive.ql.exec.ScriptOperator.CounterStatusProcessor.setStatus(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.ScriptOperator.ErrorStreamProcessor.ErrorStreamProcessor(int)",1,1,2
"org.apache.hadoop.hive.ql.exec.ScriptOperator.ErrorStreamProcessor.close()",1,1,1
"org.apache.hadoop.hive.ql.exec.ScriptOperator.ErrorStreamProcessor.processLine(Writable)",4,8,12
"org.apache.hadoop.hive.ql.exec.ScriptOperator.OutputStreamProcessor.OutputStreamProcessor(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.ScriptOperator.OutputStreamProcessor.close()",1,1,1
"org.apache.hadoop.hive.ql.exec.ScriptOperator.OutputStreamProcessor.processLine(Writable)",1,2,2
"org.apache.hadoop.hive.ql.exec.ScriptOperator.PathFinder.PathFinder(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.ScriptOperator.PathFinder.getAbsolutePath(String)",7,7,14
"org.apache.hadoop.hive.ql.exec.ScriptOperator.PathFinder.prependPathComponent(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.ScriptOperator.StreamThread.StreamThread(RecordReader,StreamProcessor,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.ScriptOperator.StreamThread.run()",3,7,8
"org.apache.hadoop.hive.ql.exec.ScriptOperator.addJobConfToEnvironment(Configuration,Map<String, String>)",1,2,2
"org.apache.hadoop.hive.ql.exec.ScriptOperator.addWrapper(String[])",2,1,4
"org.apache.hadoop.hive.ql.exec.ScriptOperator.allowPartialConsumption()",1,1,1
"org.apache.hadoop.hive.ql.exec.ScriptOperator.close(boolean)",5,20,22
"org.apache.hadoop.hive.ql.exec.ScriptOperator.displayBrokenPipeInfo()",1,1,1
"org.apache.hadoop.hive.ql.exec.ScriptOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ScriptOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.ScriptOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.ScriptOperator.initializeOp(Configuration)",1,2,2
"org.apache.hadoop.hive.ql.exec.ScriptOperator.isBrokenPipeException(IOException)",2,4,4
"org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(Object,int)",4,14,15
"org.apache.hadoop.hive.ql.exec.ScriptOperator.safeEnvVarName(String)",1,2,8
"org.apache.hadoop.hive.ql.exec.ScriptOperator.safeEnvVarValue(String,String,boolean)",1,3,3
"org.apache.hadoop.hive.ql.exec.ScriptOperator.splitArgs(String)",1,4,14
"org.apache.hadoop.hive.ql.exec.SecureCmdDoAs.SecureCmdDoAs(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.exec.SecureCmdDoAs.addEnv(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.exec.SelectOperator.acceptLimitPushdown()",1,1,1
"org.apache.hadoop.hive.ql.exec.SelectOperator.columnNamesRowResolvedCanBeObtained()",1,1,1
"org.apache.hadoop.hive.ql.exec.SelectOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.SelectOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.SelectOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(Configuration)",2,4,4
"org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(Object,int)",2,4,5
"org.apache.hadoop.hive.ql.exec.SelectOperator.supportAutomaticSortMergeJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.SelectOperator.supportSkewJoinOptimization()",1,1,1
"org.apache.hadoop.hive.ql.exec.SelectOperator.supportUnionRemoveOptimization()",1,1,1
"org.apache.hadoop.hive.ql.exec.SkewJoinHandler.SkewJoinHandler(CommonJoinOperator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.exec.SkewJoinHandler.close(boolean)",6,4,8
"org.apache.hadoop.hive.ql.exec.SkewJoinHandler.commit()",5,3,6
"org.apache.hadoop.hive.ql.exec.SkewJoinHandler.commitOutputPathToFinalPath(Path,boolean)",3,1,4
"org.apache.hadoop.hive.ql.exec.SkewJoinHandler.delete(Path,FileSystem)",1,2,2
"org.apache.hadoop.hive.ql.exec.SkewJoinHandler.endGroup()",4,4,5
"org.apache.hadoop.hive.ql.exec.SkewJoinHandler.getOperatorFinalPath(Path)",1,1,1
"org.apache.hadoop.hive.ql.exec.SkewJoinHandler.getOperatorOutputPath(Path)",1,1,1
"org.apache.hadoop.hive.ql.exec.SkewJoinHandler.handleSkew(int)",3,5,10
"org.apache.hadoop.hive.ql.exec.SkewJoinHandler.initiliaze(Configuration)",2,7,8
"org.apache.hadoop.hive.ql.exec.SkewJoinHandler.setSkewJoinJobCounter(LongWritable)",1,1,1
"org.apache.hadoop.hive.ql.exec.SkewJoinHandler.updateSkewJoinJobCounter(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.Stat.Stat()",1,1,1
"org.apache.hadoop.hive.ql.exec.Stat.addToStat(String,long)",1,2,2
"org.apache.hadoop.hive.ql.exec.Stat.clear()",1,1,1
"org.apache.hadoop.hive.ql.exec.Stat.getBookkeepingInfo(String)",2,1,2
"org.apache.hadoop.hive.ql.exec.Stat.getStat(String)",2,1,2
"org.apache.hadoop.hive.ql.exec.Stat.getStoredStats()",1,1,1
"org.apache.hadoop.hive.ql.exec.Stat.setBookkeepingInfo(String,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.StatsNoJobTask.StatsCollection.StatsCollection(Partition)",1,1,1
"org.apache.hadoop.hive.ql.exec.StatsNoJobTask.StatsCollection.run()",1,7,7
"org.apache.hadoop.hive.ql.exec.StatsNoJobTask.StatsCollection.toString(Map<String, String>)",1,4,4
"org.apache.hadoop.hive.ql.exec.StatsNoJobTask.StatsNoJobTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.StatsNoJobTask.aggregateStats(ExecutorService)",1,9,10
"org.apache.hadoop.hive.ql.exec.StatsNoJobTask.execute(DriverContext)",1,2,2
"org.apache.hadoop.hive.ql.exec.StatsNoJobTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.StatsNoJobTask.getPartitionsList()",3,2,3
"org.apache.hadoop.hive.ql.exec.StatsNoJobTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.StatsNoJobTask.initialize(HiveConf,QueryPlan,DriverContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.StatsNoJobTask.shutdownAndAwaitTermination(ExecutorService)",1,4,4
"org.apache.hadoop.hive.ql.exec.StatsNoJobTask.toString(Map<String, String>)",1,4,4
"org.apache.hadoop.hive.ql.exec.StatsNoJobTask.updatePartitions()",3,4,4
"org.apache.hadoop.hive.ql.exec.StatsTask.StatsTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.StatsTask.aggregateStats()",7,15,19
"org.apache.hadoop.hive.ql.exec.StatsTask.clearStats(Map<String, String>)",1,3,3
"org.apache.hadoop.hive.ql.exec.StatsTask.createStatsAggregator(HiveConf)",5,5,5
"org.apache.hadoop.hive.ql.exec.StatsTask.execute(DriverContext)",1,4,7
"org.apache.hadoop.hive.ql.exec.StatsTask.existStats(Map<String, String>)",1,5,5
"org.apache.hadoop.hive.ql.exec.StatsTask.getAggregationPrefix(boolean,Table,Partition)",3,4,5
"org.apache.hadoop.hive.ql.exec.StatsTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.StatsTask.getPartitionsList()",6,9,12
"org.apache.hadoop.hive.ql.exec.StatsTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.StatsTask.receiveFeed(FeedType,Object)",1,1,2
"org.apache.hadoop.hive.ql.exec.StatsTask.toString(Map<String, String>)",1,4,4
"org.apache.hadoop.hive.ql.exec.StatsTask.updateQuickStats(Warehouse,Map<String, String>,StorageDescriptor)",1,1,1
"org.apache.hadoop.hive.ql.exec.StatsTask.updateStats(StatsAggregator,Map<String, String>,String,int,boolean)",4,8,9
"org.apache.hadoop.hive.ql.exec.TableScanOperator.cleanUpInputFileChangedOp()",1,4,4
"org.apache.hadoop.hive.ql.exec.TableScanOperator.clone()",1,1,1
"org.apache.hadoop.hive.ql.exec.TableScanOperator.closeOp(boolean)",1,4,4
"org.apache.hadoop.hive.ql.exec.TableScanOperator.gatherStats(Object)",7,10,13
"org.apache.hadoop.hive.ql.exec.TableScanOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.TableScanOperator.getNeededColumnIDs()",1,1,1
"org.apache.hadoop.hive.ql.exec.TableScanOperator.getNeededColumns()",1,1,1
"org.apache.hadoop.hive.ql.exec.TableScanOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.TableScanOperator.getReferencedColumns()",1,1,1
"org.apache.hadoop.hive.ql.exec.TableScanOperator.getTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.exec.TableScanOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(Configuration)",4,2,6
"org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(Object,int)",2,4,5
"org.apache.hadoop.hive.ql.exec.TableScanOperator.publishStats()",8,10,10
"org.apache.hadoop.hive.ql.exec.TableScanOperator.setNeededColumnIDs(List<Integer>)",1,1,1
"org.apache.hadoop.hive.ql.exec.TableScanOperator.setNeededColumns(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.exec.TableScanOperator.setReferencedColumns(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.exec.TableScanOperator.setTableDesc(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.TableScanOperator.supportAutomaticSortMergeJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.TableScanOperator.supportSkewJoinOptimization()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.Task()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.addDependentTask(Task<? extends Serializable>)",1,5,5
"org.apache.hadoop.hive.ql.exec.Task.ancestorOrSelf(Task<? extends Serializable>)",5,3,5
"org.apache.hadoop.hive.ql.exec.Task.cloneConf()",1,1,2
"org.apache.hadoop.hive.ql.exec.Task.done()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.equals(Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.executeTask()",1,4,4
"org.apache.hadoop.hive.ql.exec.Task.getAndInitBackupTask()",1,4,4
"org.apache.hadoop.hive.ql.exec.Task.getBackupChildrenTasks()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getBackupTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getChildTasks()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getChildren()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getCounters()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getDependentTasks()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getException()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getFeedSubscribers()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getId()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getInitialized()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getJobID()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getMapWork()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getNumChild()",1,2,2
"org.apache.hadoop.hive.ql.exec.Task.getNumParent()",1,2,2
"org.apache.hadoop.hive.ql.exec.Task.getParentTasks()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getQueryPlan()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getQueued()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getReducer(MapWork)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getResultSchema()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getTaskHandle()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getTaskTag()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getTopOperators()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.getWork()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.hasReduce()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.ifRetryCmdWhenFail()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.initialize(HiveConf,QueryPlan,DriverContext)",1,2,2
"org.apache.hadoop.hive.ql.exec.Task.isFetchSource()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.isLocalMode()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.isMapRedLocalTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.isMapRedTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.isRootTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.isRunnable()",4,3,4
"org.apache.hadoop.hive.ql.exec.Task.pushFeed(FeedType,Object)",1,3,3
"org.apache.hadoop.hive.ql.exec.Task.receiveFeed(FeedType,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.removeDependentTask(Task<? extends Serializable>)",1,5,5
"org.apache.hadoop.hive.ql.exec.Task.removeFromChildrenTasks()",2,4,5
"org.apache.hadoop.hive.ql.exec.Task.requireLock()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setBackupChildrenTasks(List<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setBackupTask(Task<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setChildTasks(List<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setConsole(LogHelper)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setDone()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setException(Throwable)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setFeedSubscribers(List<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setFetchSource(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setId(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setInitialized()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setLocalMode(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setParentTasks(List<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setQueryPlan(QueryPlan)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setQueued()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setRetryCmdWhenFail(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setRootTask(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setStarted()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setTaskTag(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.setWork(T)",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.shutdown()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.started()",1,1,1
"org.apache.hadoop.hive.ql.exec.Task.subscribeFeed(Task<? extends Serializable>)",1,4,4
"org.apache.hadoop.hive.ql.exec.Task.toString()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskExecutionException.TaskExecutionException(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskExecutionException.TaskExecutionException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskExecutionException.TaskExecutionException(Throwable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskFactory.TaskFactory()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskFactory.TaskTuple.TaskTuple(Class<T>,Class<? extends Task<T>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskFactory.get(Class<T>,HiveConf)",3,3,4
"org.apache.hadoop.hive.ql.exec.TaskFactory.get(T,HiveConf,Task<? extends Serializable>...)",2,2,3
"org.apache.hadoop.hive.ql.exec.TaskFactory.getAndIncrementId()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskFactory.getAndMakeChild(T,HiveConf,Task<? extends Serializable>...)",2,1,2
"org.apache.hadoop.hive.ql.exec.TaskFactory.makeChild(Task<?>,Task<? extends Serializable>...)",1,2,3
"org.apache.hadoop.hive.ql.exec.TaskFactory.resetId()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskHandle.getCounters()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskResult.TaskResult()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskResult.getExitVal()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskResult.getTaskError()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskResult.isRunning()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskResult.setExitVal(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskResult.setExitVal(int,Throwable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskResult.setRunning(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskRunner.TaskRunner(Task<? extends Serializable>,TaskResult)",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskRunner.getRunner()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskRunner.getTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskRunner.getTaskResult()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskRunner.getTaskRunnerID()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskRunner.isRunning()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskRunner.run()",1,1,1
"org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential()",1,3,3
"org.apache.hadoop.hive.ql.exec.TemporaryHashSinkOperator.TemporaryHashSinkOperator(MapJoinDesc)",1,2,2
"org.apache.hadoop.hive.ql.exec.TemporaryHashSinkOperator.flushToFile()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExecDriver.addMapWork(MapredWork,Table,String,Operator<?>)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExecDriver.executePlan()",1,2,2
"org.apache.hadoop.hive.ql.exec.TestExecDriver.fileDiff(String,String)",3,2,4
"org.apache.hadoop.hive.ql.exec.TestExecDriver.getStringColumn(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExecDriver.getTestFilterDesc(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExecDriver.populateMapPlan1(Table)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExecDriver.populateMapPlan2(Table)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExecDriver.populateMapRedPlan1(Table)",1,2,2
"org.apache.hadoop.hive.ql.exec.TestExecDriver.populateMapRedPlan2(Table)",1,2,2
"org.apache.hadoop.hive.ql.exec.TestExecDriver.populateMapRedPlan3(Table,Table)",1,2,2
"org.apache.hadoop.hive.ql.exec.TestExecDriver.populateMapRedPlan4(Table)",1,2,2
"org.apache.hadoop.hive.ql.exec.TestExecDriver.populateMapRedPlan5(Table)",1,2,2
"org.apache.hadoop.hive.ql.exec.TestExecDriver.populateMapRedPlan6(Table)",1,2,2
"org.apache.hadoop.hive.ql.exec.TestExecDriver.setUp()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan1()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapPlan2()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan1()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan2()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan3()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan4()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan5()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExecDriver.testMapRedPlan6()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.TestExpressionEvaluator()",1,2,2
"org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.getListIndexNode(ExprNodeDesc,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.getListIndexNode(ExprNodeDesc,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.measureSpeed(String,int,ExprNodeEvaluator,InspectableObject,Object)",1,2,2
"org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.setUp()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.testExprNodeColumnEvaluator()",1,2,2
"org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.testExprNodeConversionEvaluator()",1,2,2
"org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.testExprNodeFuncEvaluator()",1,2,2
"org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.testExprNodeSpeed()",1,2,2
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.TestUDF.mismatch(BytesWritable,DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.TestUDF.mismatch(DateWritable,HiveDecimalWritable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.TestUDF.mismatch(TimestampWritable,HiveDecimalWritable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.TestUDF.one(IntWritable,DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.TestUDF.one(IntWritable,HiveDecimalWritable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.TestUDF.one(IntWritable,IntWritable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.TestUDF.same(DoubleWritable,DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.TestUDF.same(HiveDecimalWritable,HiveDecimalWritable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.TestUDF.same(Text,Text)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.TestUDF.typeaffinity1(DateWritable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.TestUDF.typeaffinity1(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.TestUDF.typeaffinity1(Text)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.TestUDF.typeaffinity2(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.TestUDF.typeaffinity2(IntWritable)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.common(TypeInfo,TypeInfo,TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.comparison(TypeInfo,TypeInfo,TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.getMethods(Class<?>,String)",1,3,3
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.implicit(TypeInfo,TypeInfo,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.setUp()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.tearDown()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.testCommonClass()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.testCommonClassComparison()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.testCommonClassUnionAll()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.testGetMethodInternal()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.testGetTypeInfoForPrimitiveCategory()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.testImplicitConversion()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.testImpliesOrder()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.testIsRankingFunction()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.testPrintTypeCompatibility()",4,6,7
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.testTypeAffinity()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.typeAffinity(String,TypeInfo,int,Class)",1,2,2
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.unionAll(TypeInfo,TypeInfo,TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.verify(Class,String,TypeInfo,TypeInfo,Class,Class,boolean)",1,1,2
"org.apache.hadoop.hive.ql.exec.TestOperators.setUp()",2,2,3
"org.apache.hadoop.hive.ql.exec.TestOperators.testBaseFilterOperator()",1,3,3
"org.apache.hadoop.hive.ql.exec.TestOperators.testFileSinkOperator()",1,2,2
"org.apache.hadoop.hive.ql.exec.TestOperators.testMapOperator()",1,3,3
"org.apache.hadoop.hive.ql.exec.TestOperators.testScriptOperator()",1,5,5
"org.apache.hadoop.hive.ql.exec.TestOperators.testScriptOperatorEnvVarsProcessing()",1,2,2
"org.apache.hadoop.hive.ql.exec.TestOperators.testTaskIds(String[],String,String)",1,2,2
"org.apache.hadoop.hive.ql.exec.TestPlan.testPlan()",1,2,2
"org.apache.hadoop.hive.ql.exec.TestStatsPublisherEnhanced.TestStatsPublisherEnhanced(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestStatsPublisherEnhanced.fillStatMap(String,String)",1,2,2
"org.apache.hadoop.hive.ql.exec.TestStatsPublisherEnhanced.setUp()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestStatsPublisherEnhanced.tearDown()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestStatsPublisherEnhanced.testStatsAggregatorCleanUp()",1,2,2
"org.apache.hadoop.hive.ql.exec.TestStatsPublisherEnhanced.testStatsPublisher()",1,2,2
"org.apache.hadoop.hive.ql.exec.TestStatsPublisherEnhanced.testStatsPublisherMultipleUpdates()",1,2,2
"org.apache.hadoop.hive.ql.exec.TestStatsPublisherEnhanced.testStatsPublisherMultipleUpdatesSubsetStatistics()",1,2,2
"org.apache.hadoop.hive.ql.exec.TestStatsPublisherEnhanced.testStatsPublisherOneStat()",1,2,2
"org.apache.hadoop.hive.ql.exec.TestUtilities.checkFSUMaskReset(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.TestUtilities.testFSUmaskReset()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestUtilities.testGetFileExtension()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestUtilities.testSerializeTimestamp()",1,1,1
"org.apache.hadoop.hive.ql.exec.TestUtilities.testgetDbTableName()",1,2,2
"org.apache.hadoop.hive.ql.exec.TextRecordReader.close()",1,2,2
"org.apache.hadoop.hive.ql.exec.TextRecordReader.createRow()",1,1,1
"org.apache.hadoop.hive.ql.exec.TextRecordReader.initialize(InputStream,Configuration,Properties)",1,1,1
"org.apache.hadoop.hive.ql.exec.TextRecordReader.next(Writable)",3,2,3
"org.apache.hadoop.hive.ql.exec.TextRecordWriter.close()",1,1,1
"org.apache.hadoop.hive.ql.exec.TextRecordWriter.initialize(OutputStream,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.TextRecordWriter.write(Writable)",1,2,2
"org.apache.hadoop.hive.ql.exec.TopNHash.HashForGroup.indexes()",1,1,1
"org.apache.hadoop.hive.ql.exec.TopNHash.HashForGroup.removeBiggest()",1,1,1
"org.apache.hadoop.hive.ql.exec.TopNHash.HashForGroup.size()",1,1,1
"org.apache.hadoop.hive.ql.exec.TopNHash.HashForGroup.store(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.TopNHash.HashForRow.indexes()",1,1,1
"org.apache.hadoop.hive.ql.exec.TopNHash.HashForRow.removeBiggest()",1,1,1
"org.apache.hadoop.hive.ql.exec.TopNHash.HashForRow.size()",1,1,1
"org.apache.hadoop.hive.ql.exec.TopNHash.HashForRow.store(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.TopNHash.flush()",2,1,4
"org.apache.hadoop.hive.ql.exec.TopNHash.flushInternal()",1,3,4
"org.apache.hadoop.hive.ql.exec.TopNHash.getVectorizedBatchResult(int)",1,1,2
"org.apache.hadoop.hive.ql.exec.TopNHash.getVectorizedKeyDistLength(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.TopNHash.getVectorizedKeyHashCode(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.TopNHash.getVectorizedKeyToForward(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.TopNHash.initialize(int,float,boolean,BinaryCollector)",3,1,5
"org.apache.hadoop.hive.ql.exec.TopNHash.insertKeyIntoHeap(HiveKey)",5,4,7
"org.apache.hadoop.hive.ql.exec.TopNHash.removed(int)",1,1,2
"org.apache.hadoop.hive.ql.exec.TopNHash.startVectorizedBatch(int)",5,4,8
"org.apache.hadoop.hive.ql.exec.TopNHash.storeValue(int,int,BytesWritable,boolean)",1,1,2
"org.apache.hadoop.hive.ql.exec.TopNHash.tryStoreKey(HiveKey,boolean)",7,3,7
"org.apache.hadoop.hive.ql.exec.TopNHash.tryStoreVectorizedKey(HiveKey,boolean,int)",4,3,10
"org.apache.hadoop.hive.ql.exec.UDAF.UDAF()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDAF.UDAF(UDAFEvaluatorResolver)",1,1,1
"org.apache.hadoop.hive.ql.exec.UDAF.getResolver()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDAF.setResolver(UDAFEvaluatorResolver)",1,1,1
"org.apache.hadoop.hive.ql.exec.UDF.UDF()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDF.UDF(UDFMethodResolver)",1,1,1
"org.apache.hadoop.hive.ql.exec.UDF.getRequiredFiles()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDF.getRequiredJars()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDF.getResolver()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDF.setResolver(UDFMethodResolver)",1,1,1
"org.apache.hadoop.hive.ql.exec.UDFArgumentException.UDFArgumentException()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDFArgumentException.UDFArgumentException(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.UDFArgumentException.UDFArgumentException(String,Class<?>,List<TypeInfo>,List<Method>)",1,1,1
"org.apache.hadoop.hive.ql.exec.UDFArgumentException.UDFArgumentException(Throwable)",1,1,1
"org.apache.hadoop.hive.ql.exec.UDFArgumentException.getArgTypeList()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDFArgumentException.getFunctionClass()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDFArgumentException.getMessage(String,Class<?>,List<TypeInfo>,List<Method>)",1,5,5
"org.apache.hadoop.hive.ql.exec.UDFArgumentException.getMethods()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDFArgumentException.sortMethods(List<Method>)",4,2,5
"org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException.UDFArgumentLengthException(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.UDFArgumentTypeException()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.UDFArgumentTypeException(int,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException.getArgumentId()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDTFOperator.closeOp(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.UDTFOperator.forwardUDTFOutput(Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.UDTFOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDTFOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDTFOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.UDTFOperator.initializeOp(Configuration)",1,4,4
"org.apache.hadoop.hive.ql.exec.UDTFOperator.processOp(Object,int)",1,4,4
"org.apache.hadoop.hive.ql.exec.UnionOperator.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.UnionOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.UnionOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.UnionOperator.initializeOp(Configuration)",4,10,12
"org.apache.hadoop.hive.ql.exec.UnionOperator.opAllowedAfterMapJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.UnionOperator.opAllowedBeforeMapJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.UnionOperator.opAllowedBeforeSortMergeJoin()",1,1,1
"org.apache.hadoop.hive.ql.exec.UnionOperator.processOp(Object,int)",1,3,3
"org.apache.hadoop.hive.ql.exec.Utilities.CollectionPersistenceDelegate.initialize(Class,Object,Object,Encoder)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.CollectionPersistenceDelegate.instantiate(Object,Encoder)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.CommonTokenDelegate.instantiate(Object,Encoder)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.CommonTokenSerializer.read(Kryo,Input,Class<CommonToken>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.CommonTokenSerializer.write(Kryo,Output,CommonToken)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.DatePersistenceDelegate.instantiate(Object,Encoder)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.DatePersistenceDelegate.mutatesTo(Object,Object)",2,1,3
"org.apache.hadoop.hive.ql.exec.Utilities.EnumDelegate.instantiate(Object,Encoder)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.EnumDelegate.mutatesTo(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.ListDelegate.initialize(Class<?>,Object,Object,Encoder)",1,3,3
"org.apache.hadoop.hive.ql.exec.Utilities.ListDelegate.instantiate(Object,Encoder)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.ListDelegate.mutatesTo(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.MapDelegate.initialize(Class<?>,Object,Object,Encoder)",1,3,3
"org.apache.hadoop.hive.ql.exec.Utilities.MapDelegate.instantiate(Object,Encoder)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.MapDelegate.mutatesTo(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.PathDelegate.instantiate(Object,Encoder)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.PathSerializer.read(Kryo,Input,Class<Path>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.PathSerializer.write(Kryo,Output,Path)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.SQLCommand.run(PreparedStatement)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.SetDelegate.initialize(Class<?>,Object,Object,Encoder)",1,3,3
"org.apache.hadoop.hive.ql.exec.Utilities.SetDelegate.instantiate(Object,Encoder)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.SetDelegate.mutatesTo(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.SqlDateSerializer.read(Kryo,Input,Class<Date>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.SqlDateSerializer.write(Kryo,Output,Date)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.TimestampPersistenceDelegate.initialize(Class<?>,Object,Object,Encoder)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.TimestampSerializer.read(Kryo,Input,Class<Timestamp>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.TimestampSerializer.write(Kryo,Output,Timestamp)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.Utilities()",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.abbreviate(String,int)",2,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.addToClassPath(ClassLoader,String[])",1,5,5
"org.apache.hadoop.hive.ql.exec.Utilities.adjustBucketNumLen(String,String)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.cacheBaseWork(Configuration,String,BaseWork,Path)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.cacheMapWork(Configuration,MapWork,Path)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.clearWork(Configuration)",2,4,6
"org.apache.hadoop.hive.ql.exec.Utilities.clearWorkMap()",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.clearWorkMapForConf(Configuration)",1,3,3
"org.apache.hadoop.hive.ql.exec.Utilities.cloneOperatorTree(Configuration,Set<Operator<?>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.clonePlan(MapredWork)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.connectWithRetry(String,int,int)",3,4,6
"org.apache.hadoop.hive.ql.exec.Utilities.contentsEqual(InputStream,InputStream,boolean)",6,7,15
"org.apache.hadoop.hive.ql.exec.Utilities.copyTableJobPropertiesToConf(TableDesc,JobConf)",2,3,4
"org.apache.hadoop.hive.ql.exec.Utilities.createCompressedStream(JobConf,OutputStream)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.createCompressedStream(JobConf,OutputStream,boolean)",2,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.createDirsWithPermission(Configuration,Path,FsPermission)",1,3,3
"org.apache.hadoop.hive.ql.exec.Utilities.createDirsWithPermission(Configuration,Path,FsPermission,boolean)",1,3,4
"org.apache.hadoop.hive.ql.exec.Utilities.createDummyFileForEmptyPartition(Path,JobConf,MapWork,Path,String,int)",2,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.createDummyFileForEmptyTable(JobConf,MapWork,Path,String,int)",2,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.createEmptyBuckets(Configuration,ArrayList<String>,FileSinkDesc,Reporter)",1,2,6
"org.apache.hadoop.hive.ql.exec.Utilities.createEmptyFile(Path,Class<? extends HiveOutputFormat>,JobConf,int,Properties,boolean)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.createRCFileWriter(JobConf,FileSystem,Path,boolean,Progressable)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.createSequenceWriter(JobConf,FileSystem,Path,Class<?>,Class<?>,Progressable)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.createSequenceWriter(JobConf,FileSystem,Path,Class<?>,Class<?>,boolean,Progressable)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.createTempDir(String)",3,2,3
"org.apache.hadoop.hive.ql.exec.Utilities.createTmpDirs(Configuration,List<Operator<? extends OperatorDesc>>)",1,5,5
"org.apache.hadoop.hive.ql.exec.Utilities.createTmpDirs(Configuration,MapWork)",1,4,4
"org.apache.hadoop.hive.ql.exec.Utilities.createTmpDirs(Configuration,ReduceWork)",2,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.deserializeExpression(String)",1,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.deserializeExpressionFromKryo(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.deserializeObject(String,Class<T>)",1,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByJavaXML(InputStream)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(Kryo,InputStream,Class<T>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectFromKryo(byte[],Class<T>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(InputStream,Class<T>,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(InputStream,Class<T>,Configuration,boolean)",1,3,3
"org.apache.hadoop.hive.ql.exec.Utilities.escapeSqlLike(String)",2,4,4
"org.apache.hadoop.hive.ql.exec.Utilities.estimateNumberOfReducers(HiveConf,ContentSummary,MapWork,boolean)",1,4,4
"org.apache.hadoop.hive.ql.exec.Utilities.estimateReducers(long,long,int,boolean)",1,1,4
"org.apache.hadoop.hive.ql.exec.Utilities.executeWithRetry(SQLCommand<T>,PreparedStatement,int,int)",3,3,6
"org.apache.hadoop.hive.ql.exec.Utilities.formatBinaryString(byte[],int,int)",1,2,3
"org.apache.hadoop.hive.ql.exec.Utilities.formatMsecToStr(long)",1,5,8
"org.apache.hadoop.hive.ql.exec.Utilities.generateFileName(Byte,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.generatePath(Path,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.generatePath(Path,String,Byte,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.generateTarFileName(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.generateTarPath(Path,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.generateTmpPath(Path,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Configuration,String)",11,15,17
"org.apache.hadoop.hive.ql.exec.Utilities.getColumnNames(Properties)",1,5,5
"org.apache.hadoop.hive.ql.exec.Utilities.getColumnNamesFromFieldSchema(List<FieldSchema>)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.getColumnNamesFromSortCols(List<Order>)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.getColumnTypes(Properties)",1,5,5
"org.apache.hadoop.hive.ql.exec.Utilities.getDatabaseName(String)",2,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.getDbTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getDbTableName(String,String)",5,2,5
"org.apache.hadoop.hive.ql.exec.Utilities.getDefaultNotificationInterval(Configuration)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.getFieldSchemaString(List<FieldSchema>)",2,3,4
"org.apache.hadoop.hive.ql.exec.Utilities.getFileExtension(JobConf,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getFileExtension(JobConf,boolean,HiveOutputFormat<?, ?>)",3,2,4
"org.apache.hadoop.hive.ql.exec.Utilities.getFileNameFromDirName(String)",2,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.getFooterCount(TableDesc,JobConf)",2,1,3
"org.apache.hadoop.hive.ql.exec.Utilities.getFullDPSpecs(Configuration,DynamicPartitionCtx)",2,3,4
"org.apache.hadoop.hive.ql.exec.Utilities.getHashedStatsPrefix(String,int)",2,3,5
"org.apache.hadoop.hive.ql.exec.Utilities.getHeaderCount(TableDesc)",1,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.getHighestSamplePercentage(MapWork)",3,3,5
"org.apache.hadoop.hive.ql.exec.Utilities.getIdFromFilename(String,Pattern)",1,3,3
"org.apache.hadoop.hive.ql.exec.Utilities.getInputPaths(JobConf,MapWork,Path,Context,boolean)",5,7,9
"org.apache.hadoop.hive.ql.exec.Utilities.getInputPathsTez(JobConf,MapWork)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getInputSummary(Context,MapWork,PathFilter)",9,20,23
"org.apache.hadoop.hive.ql.exec.Utilities.getInternalColumnNamesFromSignature(List<ColumnInfo>)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.getMRTasks(List<Task<? extends Serializable>>)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.getMRTasks(List<Task<? extends Serializable>>,List<ExecDriver>)",1,5,5
"org.apache.hadoop.hive.ql.exec.Utilities.getMapRedWork(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getNameMessage(Exception)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getOpTreeSkel(Operator<?>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getOpTreeSkel_helper(Operator<?>,String)",2,3,4
"org.apache.hadoop.hive.ql.exec.Utilities.getPartitionDesc(Partition)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getPartitionDescFromTableDesc(TableDesc,Partition)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getPlanPath(Configuration)",2,2,3
"org.apache.hadoop.hive.ql.exec.Utilities.getPlanPath(Configuration,String)",2,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.getPrefixedTaskIdFromFilename(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getQualifiedPath(HiveConf,Path)",2,1,3
"org.apache.hadoop.hive.ql.exec.Utilities.getRandomWaitTime(int,int,Random)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getReduceWork(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getResourceFiles(Configuration,ResourceType)",3,5,5
"org.apache.hadoop.hive.ql.exec.Utilities.getStatsPublisher(JobConf)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.getTableDesc(String,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getTableDesc(Table)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getTableName(String)",2,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.getTaskId(Configuration)",2,4,4
"org.apache.hadoop.hive.ql.exec.Utilities.getTaskIdFromFilename(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.getTezTasks(List<Task<? extends Serializable>>)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.getTezTasks(List<Task<? extends Serializable>>,List<TezTask>)",1,5,5
"org.apache.hadoop.hive.ql.exec.Utilities.getTotalInputFileSize(ContentSummary,MapWork,double)",2,3,4
"org.apache.hadoop.hive.ql.exec.Utilities.getTotalInputNumFiles(ContentSummary,MapWork,double)",2,3,4
"org.apache.hadoop.hive.ql.exec.Utilities.isCopyFile(String)",2,3,5
"org.apache.hadoop.hive.ql.exec.Utilities.isDefaultNameNode(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.isEmptyPath(JobConf,Path)",3,2,3
"org.apache.hadoop.hive.ql.exec.Utilities.isEmptyPath(JobConf,Path,Context)",3,5,5
"org.apache.hadoop.hive.ql.exec.Utilities.isTempPath(FileStatus)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.isVectorMode(Configuration)",2,3,4
"org.apache.hadoop.hive.ql.exec.Utilities.isWhitespace(int)",2,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.join(String...)",3,4,5
"org.apache.hadoop.hive.ql.exec.Utilities.listStatusIfExists(Path,FileSystem)",1,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.makeList(Object...)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.makeMap(Object...)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.makeProperties(String...)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.mergeUniqElems(List<String>,List<String>)",3,3,5
"org.apache.hadoop.hive.ql.exec.Utilities.mvFileToFinalPath(Path,Configuration,boolean,Log,DynamicPartitionCtx,FileSinkDesc,Reporter)",1,4,4
"org.apache.hadoop.hive.ql.exec.Utilities.now()",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.prepareWithRetry(Connection,String,int,int)",3,4,6
"org.apache.hadoop.hive.ql.exec.Utilities.readColumn(DataInput,OutputStream)",5,4,8
"org.apache.hadoop.hive.ql.exec.Utilities.realFile(String,Configuration)",2,2,3
"org.apache.hadoop.hive.ql.exec.Utilities.removeField(Kryo,Class,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.removeFromClassPath(String[])",1,3,3
"org.apache.hadoop.hive.ql.exec.Utilities.removeTempOrDuplicateFiles(FileStatus[],FileSystem)",8,8,10
"org.apache.hadoop.hive.ql.exec.Utilities.removeTempOrDuplicateFiles(FileSystem,Path)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.removeTempOrDuplicateFiles(FileSystem,Path,DynamicPartitionCtx)",6,9,10
"org.apache.hadoop.hive.ql.exec.Utilities.removeValueTag(String)",2,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.rename(FileSystem,Path,Path)",2,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.renameOrMoveFiles(FileSystem,Path,Path)",6,5,8
"org.apache.hadoop.hive.ql.exec.Utilities.replaceTaskId(String,String)",2,3,3
"org.apache.hadoop.hive.ql.exec.Utilities.replaceTaskId(String,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.replaceTaskIdFromFilename(String,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.replaceTaskIdFromFilename(String,String,String)",2,4,5
"org.apache.hadoop.hive.ql.exec.Utilities.replaceTaskIdFromFilename(String,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.resetConfAndCloseFS(Configuration,boolean,String,FileSystem)",1,3,3
"org.apache.hadoop.hive.ql.exec.Utilities.reworkMapRedWork(Task<? extends Serializable>,boolean,HiveConf)",2,6,8
"org.apache.hadoop.hive.ql.exec.Utilities.serializeExpression(ExprNodeGenericFuncDesc)",1,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.serializeExpressionToKryo(ExprNodeGenericFuncDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.serializeObject(Serializable)",1,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.serializeObjectByJavaXML(Object,OutputStream)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.serializeObjectByKryo(Kryo,Object,OutputStream)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.serializeObjectToKryo(Serializable)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.serializePlan(Object,OutputStream,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.serializePlan(Object,OutputStream,Configuration,boolean)",1,3,3
"org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Configuration,BaseWork,Path,String,boolean)",1,6,6
"org.apache.hadoop.hive.ql.exec.Utilities.setBaseWork(Configuration,String,BaseWork)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.setColumnNameList(JobConf,Operator)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.setColumnNameList(JobConf,Operator,boolean)",4,4,6
"org.apache.hadoop.hive.ql.exec.Utilities.setColumnTypeList(JobConf,Operator)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.setColumnTypeList(JobConf,Operator,boolean)",4,4,6
"org.apache.hadoop.hive.ql.exec.Utilities.setInputAttributes(Configuration,MapWork)",1,3,4
"org.apache.hadoop.hive.ql.exec.Utilities.setInputPaths(JobConf,List<Path>)",1,1,3
"org.apache.hadoop.hive.ql.exec.Utilities.setMapRedWork(Configuration,MapredWork,Path)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.setMapWork(Configuration,MapWork)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.setMapWork(Configuration,MapWork,Path,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.setPlanPath(Configuration,Path)",1,2,2
"org.apache.hadoop.hive.ql.exec.Utilities.setReduceWork(Configuration,ReduceWork)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.setReduceWork(Configuration,ReduceWork,Path,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.setWorkflowAdjacencies(Configuration,QueryPlan)",5,3,7
"org.apache.hadoop.hive.ql.exec.Utilities.showTime(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.skipHeader(RecordReader<WritableComparable, Writable>,int,WritableComparable,Writable)",3,2,3
"org.apache.hadoop.hive.ql.exec.Utilities.sumOf(Map<String, Long>,Set<String>)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.sumOfExcept(Map<String, Long>,Set<String>,Set<String>)",4,3,5
"org.apache.hadoop.hive.ql.exec.Utilities.toTaskTempPath(Path)",2,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.toTempPath(Path)",2,1,2
"org.apache.hadoop.hive.ql.exec.Utilities.toTempPath(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.Utilities.urlFromPathString(String)",1,3,3
"org.apache.hadoop.hive.ql.exec.Utilities.validateColumnNames(List<String>,List<String>)",5,4,5
"org.apache.hadoop.hive.ql.exec.WindowFunctionInfo.WindowFunctionInfo(FunctionInfo)",1,2,2
"org.apache.hadoop.hive.ql.exec.WindowFunctionInfo.getFunctionClass()",1,1,1
"org.apache.hadoop.hive.ql.exec.WindowFunctionInfo.getfInfo()",1,1,1
"org.apache.hadoop.hive.ql.exec.WindowFunctionInfo.isImpliesOrder()",1,1,1
"org.apache.hadoop.hive.ql.exec.WindowFunctionInfo.isPivotResult()",1,1,1
"org.apache.hadoop.hive.ql.exec.WindowFunctionInfo.isSupportsWindow()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.DataCorruptErrorHeuristic.DataCorruptErrorHeuristic()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.DataCorruptErrorHeuristic.getErrorAndSolution()",1,4,4
"org.apache.hadoop.hive.ql.exec.errors.ErrorAndSolution.ErrorAndSolution(String,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.ErrorAndSolution.equals(Object)",2,1,3
"org.apache.hadoop.hive.ql.exec.errors.ErrorAndSolution.getError()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.ErrorAndSolution.getSolution()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.ErrorAndSolution.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.MapAggrMemErrorHeuristic.MapAggrMemErrorHeuristic()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.MapAggrMemErrorHeuristic.getErrorAndSolution()",1,4,4
"org.apache.hadoop.hive.ql.exec.errors.MapAggrMemErrorHeuristic.init(String,JobConf)",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.RegexErrorHeuristic.RegexErrorHeuristic()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.RegexErrorHeuristic.getConf()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.RegexErrorHeuristic.getLogRegexes()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.RegexErrorHeuristic.getQueryMatches()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.RegexErrorHeuristic.getQueryRegex()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.RegexErrorHeuristic.getRegexToLogLines()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.RegexErrorHeuristic.init(String,JobConf)",1,2,2
"org.apache.hadoop.hive.ql.exec.errors.RegexErrorHeuristic.processLogLine(String)",1,4,4
"org.apache.hadoop.hive.ql.exec.errors.RegexErrorHeuristic.reset()",1,2,2
"org.apache.hadoop.hive.ql.exec.errors.RegexErrorHeuristic.setQueryRegex(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.ScriptErrorHeuristic.ScriptErrorHeuristic()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.ScriptErrorHeuristic.getErrorAndSolution()",1,4,4
"org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.HeuristicStats.HeuristicStats()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.HeuristicStats.addErrorAndSolution(ErrorAndSolution)",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.HeuristicStats.getErrorAndSolutions()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.HeuristicStats.getTriggerCount()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.HeuristicStats.incTriggerCount()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.TaskLogProcessor(JobConf)",1,2,2
"org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.addTaskAttemptLogUrl(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.getErrors()",2,13,13
"org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.getStackTraces()",4,10,13
"org.apache.hadoop.hive.ql.exec.errors.TestTaskLogProcessor.after()",1,2,2
"org.apache.hadoop.hive.ql.exec.errors.TestTaskLogProcessor.checkException(String,List<String>)",1,2,2
"org.apache.hadoop.hive.ql.exec.errors.TestTaskLogProcessor.getLines(String)",3,3,3
"org.apache.hadoop.hive.ql.exec.errors.TestTaskLogProcessor.testDataCorruptErrorHeuristic()",1,2,2
"org.apache.hadoop.hive.ql.exec.errors.TestTaskLogProcessor.testGetStackTraces()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.TestTaskLogProcessor.testMapAggrMemErrorHeuristic()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.TestTaskLogProcessor.testScriptErrorHeuristic()",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.TestTaskLogProcessor.toString(Throwable)",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.TestTaskLogProcessor.writeTestLog(String,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.errors.TestTaskLogProcessor.writeThrowableAsFile(String,Throwable,String,String,TaskLogProcessor)",1,3,3
"org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionException.MapJoinMemoryExhaustionException(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionHandler.MapJoinMemoryExhaustionHandler(LogHelper,double)",1,2,2
"org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionHandler.checkMemoryStatus(long,long)",2,1,2
"org.apache.hadoop.hive.ql.exec.mapjoin.TestMapJoinMemoryExhaustionHandler.setup()",1,1,1
"org.apache.hadoop.hive.ql.exec.mapjoin.TestMapJoinMemoryExhaustionHandler.testAbort()",1,2,2
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.ExecDriver()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.ExecDriver(MapredWork,JobConf,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.checkFatalErrors(Counters,StringBuilder)",1,2,2
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(DriverContext)",5,47,50
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.generateCmdLine(HiveConf,Context)",3,6,7
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.getCurrentDB()",1,2,3
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.getMapWork()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.getTopOperators()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.handleSampling(DriverContext,MapWork,JobConf,HiveConf)",3,7,8
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.hasReduce()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.initialize(HiveConf,QueryPlan,DriverContext)",1,4,4
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.initializeFiles(String,String)",1,3,3
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.isMapRedTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.logPlanProgress(SessionState)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(String[])",1,21,23
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.mapDone()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.mapStarted()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.printUsage()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.reduceDone()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.reduceStarted()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.requireLock()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.setInputAttributes(Configuration)",1,4,4
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.setupChildLog4j(Configuration)",1,2,2
"org.apache.hadoop.hive.ql.exec.mr.ExecDriver.shutdown()",1,3,3
"org.apache.hadoop.hive.ql.exec.mr.ExecMapper.ReportStats.ReportStats(Reporter)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapper.ReportStats.func(Operator)",1,3,3
"org.apache.hadoop.hive.ql.exec.mr.ExecMapper.close()",2,10,10
"org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(JobConf)",3,4,8
"org.apache.hadoop.hive.ql.exec.mr.ExecMapper.getDone()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapper.getNextCntr(long)",2,1,2
"org.apache.hadoop.hive.ql.exec.mr.ExecMapper.isAbort()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(Object,Object,OutputCollector,Reporter)",2,7,7
"org.apache.hadoop.hive.ql.exec.mr.ExecMapper.setAbort(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapper.setDone(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.ExecMapperContext()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.clear()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.getCurrentBigBucketFile()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.getCurrentInputPath()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.getFetchOperators()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.getFileId()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.getIoCxt()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.getJc()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.getLastInputPath()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.getLocalWork()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.inputFileChanged()",1,3,3
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.resetRow()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.setCurrentBigBucketFile(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.setCurrentInputPath(Path)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.setFetchOperators(Map<String, FetchOperator>)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.setFileId(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.setIoCxt(IOContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.setJc(JobConf)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.setLastInputPath(Path)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.setLocalWork(MapredLocalWork)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close()",2,6,6
"org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(JobConf)",2,4,7
"org.apache.hadoop.hive.ql.exec.mr.ExecReducer.getNextCntr(long)",2,1,2
"org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(Object,Iterator,OutputCollector,Reporter)",5,14,15
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.ExecDriverTaskHandle.ExecDriverTaskHandle(JobClient,RunningJob,HiveTxnManager)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.ExecDriverTaskHandle.getCounters()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.ExecDriverTaskHandle.getJobClient()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.ExecDriverTaskHandle.getRunningJob()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.ExecDriverTaskHandle.getTxnManager()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.ExecDriverTaskHandle.setRunningJob(RunningJob)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.HadoopJobExecHelper(JobConf,LogHelper,Task<? extends Serializable>,HadoopJobExecHook)",1,2,2
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.checkFatalErrors(Counters,StringBuilder)",3,3,4
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.computeReducerTimeStatsPerJob(RunningJob)",1,3,3
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.extractAllCounterValues(Counters)",1,3,3
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.getClientStatPublishers()",2,3,4
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.getId()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.getJobEndMsg(JobID)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.getJobId()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.getJobStartMsg(JobID)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.jobInfo(RunningJob)",1,3,3
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.killRunningJobs()",1,3,3
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.localJobDebugger(int,String)",1,3,3
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.mapDone()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.mapStarted()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(ExecDriverTaskHandle)",7,28,33
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(RunningJob,JobClient,HiveTxnManager)",1,10,11
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progressLocal(Process,String)",1,3,4
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.reduceDone()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.reduceStarted()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.setJobId(JobID)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.updateCounters(Counters,RunningJob)",1,3,3
"org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.getBaseDir(MapredLocalWork)",5,4,5
"org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.init(ExecMapperContext,Configuration,MapJoinOperator)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(MapJoinTableContainer[],MapJoinTableContainerSerDe[])",4,4,7
"org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.loadDirectly(MapJoinTableContainer[],String)",2,5,7
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.JobDebugger(JobConf,RunningJob,LogHelper)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.JobDebugger(JobConf,RunningJob,LogHelper,Map<String, List<List<String>>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.TaskInfo.TaskInfo(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.TaskInfo.addLogUrl(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.TaskInfo.getDiagnosticMesgs()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.TaskInfo.getErrorCode()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.TaskInfo.getJobId()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.TaskInfo.getLogUrls()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.TaskInfo.setDiagnosticMesgs(String[])",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.TaskInfo.setErrorCode(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.TaskInfoGrabber.getTaskInfos()",4,10,13
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.TaskInfoGrabber.run()",1,2,2
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.computeMaxFailures()",1,3,3
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.extractErrorCode(String[])",1,3,3
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.getErrorCode()",3,3,3
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.run()",1,2,2
"org.apache.hadoop.hive.ql.exec.mr.JobDebugger.showJobFailDebugInfo()",4,16,17
"org.apache.hadoop.hive.ql.exec.mr.JobTrackerURLResolver.JobTrackerURLResolver()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.JobTrackerURLResolver.getURL(JobConf)",2,1,2
"org.apache.hadoop.hive.ql.exec.mr.MapRedTask.MapRedTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.MapRedTask.configureDebugVariablesForChildJVM(Map<String, String>)",1,7,7
"org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(DriverContext)",6,23,29
"org.apache.hadoop.hive.ql.exec.mr.MapRedTask.getReducer(MapWork)",2,3,3
"org.apache.hadoop.hive.ql.exec.mr.MapRedTask.isEligibleForLocalMode(HiveConf,int,long,long)",4,1,4
"org.apache.hadoop.hive.ql.exec.mr.MapRedTask.mapDone()",1,1,2
"org.apache.hadoop.hive.ql.exec.mr.MapRedTask.mapStarted()",1,1,2
"org.apache.hadoop.hive.ql.exec.mr.MapRedTask.reduceDone()",1,1,2
"org.apache.hadoop.hive.ql.exec.mr.MapRedTask.reduceStarted()",1,1,2
"org.apache.hadoop.hive.ql.exec.mr.MapRedTask.setNumberOfReducers()",1,6,6
"org.apache.hadoop.hive.ql.exec.mr.MapRedTask.shutdown()",1,2,2
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.MapredLocalTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.MapredLocalTask(MapredLocalWork,JobConf,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.execute(DriverContext)",2,2,2
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.executeInChildVM(DriverContext)",5,14,17
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.executeInProcess(DriverContext)",3,3,5
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.getTopOperators()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.initialize(HiveConf,QueryPlan,DriverContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.initializeOperators(Map<FetchOperator, JobConf>)",3,4,6
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.isMapRedLocalTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.now()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.requireLock()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.setExecContext(ExecMapperContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.setUpFetchOpContext(FetchOperator,String,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.shutdown()",1,2,2
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(String)",1,6,6
"org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(boolean,String)",4,7,8
"org.apache.hadoop.hive.ql.exec.mr.ObjectCache.cache(String,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.Throttle.Throttle()",1,1,1
"org.apache.hadoop.hive.ql.exec.mr.Throttle.checkJobTracker(JobConf,Log)",6,3,7
"org.apache.hadoop.hive.ql.exec.persistence.AbstractMapJoinTableContainer.AbstractMapJoinTableContainer(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.AbstractMapJoinTableContainer.createConstructorMetaData(int,float)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.AbstractMapJoinTableContainer.getMetaData()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.AbstractMapJoinTableContainer.putMetaData(String,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.BytesBytesMultiHashMap(int,float,int)",2,2,4
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.Ref.getHashBits(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.Ref.getNthHashBit(long,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.Ref.getOffset(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.Ref.getStateByte(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.Ref.hasList(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.Ref.makeFirstRef(long,byte,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.Ref.setListFlag(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.Ref.setStateByte(long,byte)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.addRecordToList(long,long)",1,1,2
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.clear()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.compareHashBits(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.createOrGetListRecord(long)",2,2,2
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.debugDumpKeyProbe(long,int,int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.debugDumpMetrics()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.debugDumpTable()",3,7,9
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.dumpRef(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.expandAndRehash()",3,2,3
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.findKeyRefToRead(byte[],int)",5,2,5
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.findKeySlotToWrite(long,int,int)",3,5,6
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.getCapacity()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.getFirstRecordLengthsOffset(long)",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.getValueRefs(byte[],int,List<ByteSegmentRef>)",3,3,6
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.isSameKey(byte[],int,long,int)",2,1,2
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.isSameKey(long,int,long,int)",3,1,3
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.nextHighestPowerOfTwo(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.populateValue(ByteSegmentRef)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.put(KvSource)",1,4,4
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.relocateKeyRef(long[],long,int)",3,1,3
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.seal()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.size()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.validateCapacity(long)",3,1,3
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.writeFirstValueRecord(KvSource,long,int,int)",1,3,3
"org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.writeValueAndLength(KvSource)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.GetAdaptor.GetAdaptor(MapJoinKey)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.GetAdaptor.getCurrentKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.GetAdaptor.getCurrentRows()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.GetAdaptor.hasAnyNulls(int,boolean[])",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.GetAdaptor.setFromOther(ReusableGetAdaptor)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.GetAdaptor.setFromRow(Object,List<ExprNodeEvaluator>,List<ObjectInspector>)",1,3,3
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.GetAdaptor.setFromVector(VectorHashKeyWrapper,VectorExpressionWriter[],VectorHashKeyWrapperBatch)",1,4,4
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.HashMapWrapper()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.HashMapWrapper(Configuration,long)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.HashMapWrapper(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.HashMapWrapper(float,int,float,boolean,boolean,long)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.calculateTableSize(float,int,float,long)",1,2,3
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.clear()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.createGetter(MapJoinKey)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.dumpMetrics()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.entrySet()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.get(MapJoinKey)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.getAnyKey()",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.put(MapJoinKey,MapJoinRowContainer)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.putRow(MapJoinObjectSerDeContext,Writable,MapJoinObjectSerDeContext,Writable)",1,2,4
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.seal()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.size()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.ByteBufferWritable.getBytes()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.ByteBufferWritable.getLength()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.ByteBufferWritable.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.ByteBufferWritable.setBytes(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.ByteBufferWritable.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.EmptyRowIterator.EmptyRowIterator(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.EmptyRowIterator.first()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.EmptyRowIterator.next()",1,1,2
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.LazyFlatRowContainer()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.ReadOnlySubList.ReadOnlySubList(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.ReadOnlySubList.get(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.ReadOnlySubList.iterator()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.ReadOnlySubList.listIterator(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.ReadOnlySubList.size()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.ReadOnlySubList.subList(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.ReadOnlySubList.toArray()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.RowIterator.first()",2,1,2
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.RowIterator.next()",2,1,2
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.add(MapJoinObjectSerDeContext,BytesWritable,boolean)",2,4,4
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.add(int,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.addAll(int,Collection<? extends Object>)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.addLazy(MapJoinObjectSerDeContext,BytesWritable)",4,2,4
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.addRow(List<Object>)",2,2,3
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.addRow(Object[])",2,1,2
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.checkSingleRow()",2,1,2
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.clearRows()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.copy()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.ensureEager()",4,2,5
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.first()",3,1,3
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.get(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.getAliasFilter()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.indexOf(Object)",5,3,5
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.isLazy()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.iterator()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.lastIndexOf(Object)",5,3,5
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.listIterator()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.listIterator(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.listIteratorInternal(int,int,int)",3,1,3
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.listRealloc(int)",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.next()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.prepareForAdd(int)",5,1,5
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.read(SerDe,Writable,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.remove(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.rowCount()",1,1,2
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.rowIter()",2,1,3
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.set(int,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.setAliasFilter(MapJoinObjectSerDeContext)",3,3,5
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.setRowLength(SerDe,int)",2,1,3
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.size()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.subList(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.toArray()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.LazyFlatRowContainer.write(MapJoinObjectSerDeContext,ObjectOutputStream)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.GetAdaptor.GetAdaptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.GetAdaptor.getCurrentKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.GetAdaptor.getCurrentRows()",1,1,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.GetAdaptor.hasAnyNulls(int,boolean[])",4,1,7
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.GetAdaptor.setFromOther(ReusableGetAdaptor)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.GetAdaptor.setFromRow(Object,List<ExprNodeEvaluator>,List<ObjectInspector>)",1,3,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.GetAdaptor.setFromVector(VectorHashKeyWrapper,VectorExpressionWriter[],VectorHashKeyWrapperBatch)",1,4,4
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.KeyValueWriter.KeyValueWriter(SerDe,SerDe,boolean)",1,3,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.KeyValueWriter.setKeyValue(Writable,Writable)",1,3,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.KeyValueWriter.updateStateByte(Byte)",2,2,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.KeyValueWriter.writeKey(RandomAccessOutput)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.KeyValueWriter.writeValue(RandomAccessOutput)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.LazyBinaryKvWriter.LazyBinaryKvWriter(SerDe,LazyBinaryStructObjectInspector,boolean)",3,3,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.LazyBinaryKvWriter.sanityCheckKeyForTag()",4,4,5
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.LazyBinaryKvWriter.setKeyValue(Writable,Writable)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.LazyBinaryKvWriter.updateStateByte(Byte)",2,2,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.LazyBinaryKvWriter.writeKey(RandomAccessOutput)",2,2,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.LazyBinaryKvWriter.writeValue(RandomAccessOutput)",2,2,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.MapJoinBytesTableContainer(Configuration,MapJoinObjectSerDeContext,long)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.MapJoinBytesTableContainer(float,int,float,int,MapJoinObjectSerDeContext,long)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.ReusableRowContainer.ReusableRowContainer()",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.ReusableRowContainer.addRow(List<Object>)",2,2,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.ReusableRowContainer.addRow(Object[])",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.ReusableRowContainer.clearRows()",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.ReusableRowContainer.copy()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.ReusableRowContainer.first()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.ReusableRowContainer.getAliasFilter()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.ReusableRowContainer.isEmpty()",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.ReusableRowContainer.next()",5,3,7
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.ReusableRowContainer.rowCount()",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.ReusableRowContainer.rowIter()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.ReusableRowContainer.setFromOutput(Output)",1,1,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.ReusableRowContainer.write(MapJoinObjectSerDeContext,ObjectOutputStream)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.clear()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.createGetter(MapJoinKey)",2,1,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.createInternalOi(MapJoinObjectSerDeContext)",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.dumpMetrics()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.getAnyKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.isSupportedKey(ObjectInspector)",3,2,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.putRow(MapJoinObjectSerDeContext,Writable,MapJoinObjectSerDeContext,Writable)",1,3,4
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.seal()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.MapJoinEagerRowContainer()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.NoCopyingArrayList.NoCopyingArrayList(Object[])",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.NoCopyingArrayList.get(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.NoCopyingArrayList.size()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.NoCopyingArrayList.toArray()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.addRow(List<Object>)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.addRow(Object[])",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.clearRows()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.copy()",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.first()",2,2,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.getAliasFilter()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.next()",2,2,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.read(MapJoinObjectSerDeContext,ObjectInputStream,Writable)",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.read(MapJoinObjectSerDeContext,Writable)",1,3,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.rowCount()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.rowIter()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.toList(Object[])",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinEagerRowContainer.write(MapJoinObjectSerDeContext,ObjectOutputStream)",3,2,4
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.isSupportedField(ObjectInspector)",3,1,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.read(Output,MapJoinKey,MapJoinObjectSerDeContext,Writable,boolean)",4,3,6
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.readFromRow(Output,MapJoinKey,Object[],List<ObjectInspector>,boolean)",4,5,7
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.readFromVector(Output,MapJoinKey,Object[],List<ObjectInspector>,boolean)",4,4,6
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.serialize(Output,Object,ObjectInspector,boolean)",6,3,8
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.serializeRow(Output,Object[],List<ObjectInspector>,boolean[])",1,4,5
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.serializeVector(Output,VectorHashKeyWrapper,VectorExpressionWriter[],VectorHashKeyWrapperBatch,boolean[],boolean[])",1,2,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.useOptimizedKeyBasedOnPrev(MapJoinKey)",1,1,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyBytes.equals(Object)",2,1,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyBytes.fromBytes(MapJoinKey,boolean,byte[])",1,1,3
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyBytes.hasAnyNulls(int,boolean[])",4,1,6
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyBytes.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyBytes.setBytes(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyBytes.write(MapJoinObjectSerDeContext,ObjectOutputStream)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.MapJoinKeyObject()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.MapJoinKeyObject(Object[])",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.equals(Object)",5,1,5
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.getKeyLength()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.getKeyObjects()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.getNulls()",1,1,4
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.hasAnyNulls(int,boolean[])",4,1,7
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.read(MapJoinObjectSerDeContext,ObjectInputStream,Writable)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.read(MapJoinObjectSerDeContext,Writable)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.read(ObjectInspector,Object)",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.readFromRow(Object[],List<ObjectInspector>)",1,2,4
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.readFromVector(VectorHashKeyWrapper,VectorExpressionWriter[],VectorHashKeyWrapperBatch)",1,2,4
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.setKeyObjects(Object[])",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinKeyObject.write(MapJoinObjectSerDeContext,ObjectOutputStream)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectSerDeContext.MapJoinObjectSerDeContext(SerDe,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectSerDeContext.getSerDe()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectSerDeContext.getStandardOI()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectSerDeContext.hasFilterTag()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectSerDeContext.toString()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.MapJoinTableContainerSerDe(MapJoinObjectSerDeContext,MapJoinObjectSerDeContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.create(String,Map<String, String>)",1,1,2
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.getKeyContext()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.getValueContext()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.load(ObjectInputStream)",1,2,6
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.persist(ObjectOutputStream,MapJoinPersistableTableContainer)",2,2,5
"org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.persistDummyTable(ObjectOutputStream)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.PTFHiveSequenceFileOutputFormat.getHiveRecordWriter(JobConf,Path,Class<? extends Writable>,boolean,Properties,Progressable)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.PTFRecordWriter.PTFRecordWriter(Writer)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.PTFRecordWriter.close(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.PTFRecordWriter.write(Writable)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.PTFRowContainer(int,Configuration,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.PTFSequenceFileInputFormat.PTFSequenceFileInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.PTFSequenceFileInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.PTFSequenceFileRecordReader.PTFSequenceFileRecordReader(Configuration,FileSplit)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.PTFSequenceFileRecordReader.seek(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.addRow(Row)",2,3,3
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.clearRows()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.close()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.createTableDesc(StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.first()",1,8,9
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.getAt(int)",1,2,3
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.getBlockNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.next()",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.numBlocks()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.readBlock(int)",3,6,7
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.RowContainer(Configuration,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.RowContainer(int,Configuration,Reporter)",1,1,3
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.addRow(ROW)",1,3,5
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.clearRows()",1,4,4
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.close()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.closeReader()",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.closeWriter()",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.copyToDFSDirecory(FileSystem,Path)",2,3,4
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.deleteLocalFile(File,boolean)",3,7,8
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.endOfCurrentReadBlock()",2,1,2
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.first()",3,4,7
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.getAddCursor()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.getBlockSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.getCurrentReadBlockSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.getInputSplits()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.getLocalFSJobConfClone(Configuration)",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.getReadBlockRow(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.getRecordWriter()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.next()",8,4,9
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.nextBlock(int)",3,8,10
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.removeKeys(ROW)",1,3,4
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.resetCurrentReadBlockToFirstReadBlock()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.resetReadBlocks()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.rowCount()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.rowIter()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.setKeyObject(List<Object>)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.setReaderAtSplit(int)",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.setSerDe(SerDe,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.setTableDesc(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.setWriteBlockAsReadBlock()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.setupWriter()",4,5,7
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(ROW[],int)",4,6,9
"org.apache.hadoop.hive.ql.exec.persistence.RowContainer.willSpill()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.FixedKeyKvSource.FixedKeyKvSource(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.FixedKeyKvSource.writeKey(RandomAccessOutput)",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.RandomKvSource.RandomKvSource(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.RandomKvSource.getLastKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.RandomKvSource.getLastValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.RandomKvSource.updateStateByte(Byte)",1,1,2
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.RandomKvSource.write(RandomAccessOutput)",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.RandomKvSource.writeKey(RandomAccessOutput)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.RandomKvSource.writeValue(RandomAccessOutput)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.UniqueKeysKvSource.UniqueKeysKvSource()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.UniqueKeysKvSource.writeKey(RandomAccessOutput)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.UniqueKeysKvSource.writeLastBuffer(RandomAccessOutput)",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.UniqueKeysKvSource.writeValue(RandomAccessOutput)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.testCapacityValidation()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.testExpand()",1,3,3
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.testGetNonExistent()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.testPutGetMultiple()",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.testPutGetOne()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.testPutWithFullMap()",1,3,3
"org.apache.hadoop.hive.ql.exec.persistence.TestBytesBytesMultiHashMap.verifyResults(BytesBytesMultiHashMap,byte[],byte[]...)",1,3,3
"org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinEqualityTableContainer.setup()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinEqualityTableContainer.testContainerBasics()",1,2,2
"org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinKey.testEqualityHashCode()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinKey.testSerialization()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinRowContainer.testSerialization()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinTableContainer.setup()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinTableContainer.testDummyContainer()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestMapJoinTableContainer.testSerialization()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestPTFRowContainer.rowContainer(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestPTFRowContainer.runTest(int,int)",1,7,8
"org.apache.hadoop.hive.ql.exec.persistence.TestPTFRowContainer.setupClass()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestPTFRowContainer.testLargeBlockSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.TestPTFRowContainer.testSmallBlockSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.UnwrapRowContainer(byte,int[],boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.addRow(List<Object>)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.addRow(Object[])",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.clearRows()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.copy()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.first()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.getAliasFilter()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.next()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.rowCount()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.rowIter()",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.setInternal(MapJoinRowContainer,Object[])",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.toString()",1,1,2
"org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.unwrap(List<Object>)",2,4,6
"org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.write(MapJoinObjectSerDeContext,ObjectOutputStream)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.Utilities.serde(MapJoinKeyObject,String,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.Utilities.serde(MapJoinRowContainer,String,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.Utilities.testEquality(MapJoinKeyObject,MapJoinKeyObject)",1,1,1
"org.apache.hadoop.hive.ql.exec.persistence.Utilities.testEquality(MapJoinRowContainer,MapJoinRowContainer)",1,2,3
"org.apache.hadoop.hive.ql.exec.tez.CustomEdgeConfiguration.CustomEdgeConfiguration()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomEdgeConfiguration.CustomEdgeConfiguration(int,Multimap<Integer, Integer>)",1,1,2
"org.apache.hadoop.hive.ql.exec.tez.CustomEdgeConfiguration.getNumBuckets()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomEdgeConfiguration.getRoutingTable()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomEdgeConfiguration.readFields(DataInput)",3,2,4
"org.apache.hadoop.hive.ql.exec.tez.CustomEdgeConfiguration.write(DataOutput)",2,3,4
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.CustomPartitionEdge()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.addAllDestinationTaskIndices(int,List<Integer>)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.getNumDestinationConsumerTasks(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.getNumDestinationTaskPhysicalInputs(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.getNumSourceTaskPhysicalOutputs(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.initialize(EdgeManagerContext)",2,1,3
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.routeDataMovementEventToDestination(DataMovementEvent,int,int,Map<Integer, List<Integer>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.routeInputErrorEventToSource(InputReadErrorEvent,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.routeInputSourceTaskFailedEventToDestination(int,int,Map<Integer, List<Integer>>)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.CustomPartitionVertex()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.getBucketSplitMapForPath(Map<Path, List<FileSplit>>)",1,3,3
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.getBytePayload(Multimap<Integer, Integer>)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.getFileSplitFromEvent(RootInputDataInformationEvent)",2,2,3
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.initialize(VertexManagerPluginContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.onRootVertexInitialized(String,InputDescriptor,List<Event>)",4,7,9
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.onSourceTaskCompleted(String,Integer)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.onVertexManagerEventReceived(VertexManagerEvent)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.onVertexStarted(Map<String, List<Integer>>)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.processAllEvents(String,Multimap<Integer, InputSplit>)",1,7,7
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.DagUtils()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.addCredentials(BaseWork,DAG)",1,3,3
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.addCredentials(MapWork,DAG)",1,5,5
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.addCredentials(ReduceWork,DAG)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.addTempFiles(Configuration,List<LocalResource>,String,String[])",3,2,3
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.checkPreExisting(Path,Path,Configuration)",2,2,2
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.createConfiguration(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.createEdge(JobConf,Vertex,JobConf,Vertex,TezEdgeProperty)",2,4,4
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.createEdge(VertexGroup,JobConf,Vertex,TezEdgeProperty)",2,4,6
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.createEdgeProperty(TezEdgeProperty)",2,3,6
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.createLocalResource(FileSystem,Path,LocalResourceType,LocalResourceVisibility)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.createPreWarmContext(TezSessionConfiguration,int,Map<String, LocalResource>)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.createTezDir(Path,Configuration)",1,1,2
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(JobConf,BaseWork,Path,LocalResource,List<LocalResource>,FileSystem,Context,boolean,TezWork)",7,8,8
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(JobConf,MapWork,LocalResource,List<LocalResource>,FileSystem,Path,Context,TezWork)",1,10,11
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(JobConf,ReduceWork,LocalResource,List<LocalResource>,FileSystem,Path,Context)",1,3,3
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.getBaseName(LocalResource)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.getContainerJavaOpts(Configuration)",2,4,4
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.getContainerResource(Configuration)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.getDefaultDestDir(Configuration)",2,2,2
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.getExecJarPathLocal()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.getHiveJarDirectory(Configuration)",2,4,4
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.getInstance()",1,1,2
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.getResourceBaseName(Path)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.getTempFilesFromConf(Configuration)",1,4,4
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.getTezDir(Path)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.initializeVertexConf(JobConf,Context,BaseWork)",3,3,3
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.initializeVertexConf(JobConf,Context,MapWork)",1,10,10
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.initializeVertexConf(JobConf,Context,ReduceWork)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeResource(Path,Path,Configuration)",5,6,7
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeTempFiles(String,Configuration,String[])",2,1,2
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeTempFilesFromConf(String,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.setupAutoReducerParallelism(TezEdgeProperty,Vertex)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.updateConfigurationForEdge(JobConf,Vertex,JobConf,Vertex)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.DagUtils.validateTargetDir(Path,Configuration)",1,2,4
"org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.describeOi(String,ObjectInspector)",1,3,3
"org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.init(ExecMapperContext,Configuration,MapJoinOperator)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.load(MapJoinTableContainer[],MapJoinTableContainerSerDe[])",6,8,13
"org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.close()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.handleEvents(List<Event>)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.initialize(TezProcessorContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.run(Map<String, LogicalInput>,Map<String, LogicalOutput>)",2,9,10
"org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.createEventList(boolean,InputSplitInfoMem)",1,4,4
"org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.generateGroupedSplits(TezRootInputInitializerContext,JobConf,Configuration,String)",1,4,5
"org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(TezRootInputInitializerContext)",1,3,3
"org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.close()",3,7,9
"org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(JobConf,TezProcessorContext,MRTaskReporter,Map<String, LogicalInput>,Map<String, LogicalOutput>)",2,8,11
"org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.processRow(Object)",3,5,5
"org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run()",3,2,3
"org.apache.hadoop.hive.ql.exec.tez.MapTezProcessor.MapTezProcessor()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.ObjectCache.cache(String,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.ObjectCache.retrieve(String)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.createOutputMap()",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.getNextUpdateRecordCounter(long)",2,1,2
"org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.init(JobConf,TezProcessorContext,MRTaskReporter,Map<String, LogicalInput>,Map<String, LogicalOutput>)",1,3,3
"org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.logCloseInfo()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.logProgress()",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close()",2,9,9
"org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.deserializeValue(BytesWritable,byte)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.getShuffleInputs(Map<String, LogicalInput>)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(JobConf,TezProcessorContext,MRTaskReporter,Map<String, LogicalInput>,Map<String, LogicalOutput>)",2,10,13
"org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processKeyValues(Iterable<Object>,byte)",2,5,5
"org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(Object,Iterable<Object>)",5,10,11
"org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(Iterable<Object>,byte)",1,9,9
"org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run()",3,5,7
"org.apache.hadoop.hive.ql.exec.tez.ReduceTezProcessor.ReduceTezProcessor()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.createTaskLocationHints(InputSplit[])",1,5,5
"org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.estimateBucketSizes(int,float,Map<Integer, Collection<InputSplit>>)",1,5,6
"org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.group(Configuration,Multimap<Integer, InputSplit>,int,float)",1,3,3
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.SessionThread.run()",1,3,3
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.TestTezSessionPoolManager.TestTezSessionPoolManager()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.TestTezSessionPoolManager.createSession(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.setUp()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.testGetNonDefaultSession()",1,3,3
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.testReturn()",1,5,5
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionPool.testSessionPoolGetInOrder()",1,6,6
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionState.TestTezSessionState(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionState.close(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionState.getConf()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionState.getSessionId()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionState.getUser()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionState.isOpen()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionState.open(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TestTezSessionState.setOpen(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TestTezTask.setUp()",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.TestTezTask.tearDown()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testBuildDag()",5,4,5
"org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testClose()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testEmptyWork()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezCacheAccess.TezCacheAccess(ObjectCache)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezCacheAccess.createInstance(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezCacheAccess.isInputCached(String)",2,2,2
"org.apache.hadoop.hive.ql.exec.tez.TezCacheAccess.registerCachedInput(String)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.TezContext.TezContext(boolean,JobConf)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezContext.getInput(String)",2,1,2
"org.apache.hadoop.hive.ql.exec.tez.TezContext.getOutput(String)",2,1,2
"org.apache.hadoop.hive.ql.exec.tez.TezContext.setInputs(Map<String, LogicalInput>)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezContext.setOutputs(Map<String, LogicalOutput>)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.TezJobMonitor()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.monitorExecution(DAGClient,HiveTxnManager,HiveConf)",4,12,22
"org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.printStatus(Map<String, Progress>,String,LogHelper)",1,10,13
"org.apache.hadoop.hive.ql.exec.tez.TezProcessor.TezKVOutputCollector.TezKVOutputCollector(LogicalOutput)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezProcessor.TezKVOutputCollector.collect(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezProcessor.TezKVOutputCollector.initialize()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezProcessor.TezProcessor(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezProcessor.close()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezProcessor.getMRInput(Map<String, LogicalInput>)",4,1,4
"org.apache.hadoop.hive.ql.exec.tez.TezProcessor.handleEvents(List<Event>)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initialize(TezProcessorContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(Map<String, LogicalInput>,Map<String, LogicalOutput>)",4,7,12
"org.apache.hadoop.hive.ql.exec.tez.TezProcessor.setupMRLegacyConfigs(TezProcessorContext)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.TezSessionPoolManager()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.canWorkWithSameSession(TezSessionState,HiveConf)",9,3,11
"org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.close(TezSessionState)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.closeAndOpen(TezSessionState,HiveConf)",1,3,3
"org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.createSession(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.getInstance()",1,1,2
"org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.getNewSessionState(HiveConf,String,boolean)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.getSession(HiveConf,boolean,boolean)",2,8,8
"org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.getSession(TezSessionState,HiveConf,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.getSession(TezSessionState,HiveConf,boolean,boolean)",2,2,3
"org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.returnSession(TezSessionState)",1,3,3
"org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.setupPool(HiveConf)",4,3,4
"org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.startPool()",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop()",2,3,5
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.TezSessionState(DagUtils)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.TezSessionState(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.cleanupScratchDir()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.close(boolean)",2,2,4
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createJarLocalResource(String)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createTezDir(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.getAppJarLr()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.getConf()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.getLocalizedResources()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.getOpenSessions()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.getQueueName()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.getSession()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.getSessionId()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.getSha(Path)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.getTezScratchDir()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.getUser()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.hasResources(String[])",5,2,6
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.isDefault()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.isOpen()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.makeSessionId()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(HiveConf,String[])",1,7,7
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.refreshLocalResourcesFromConf(HiveConf)",1,3,3
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.setDefault()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezSessionState.setQueueName(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezTask.TezTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezTask.TezTask(DagUtils)",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezTask.build(JobConf,TezWork,Path,LocalResource,List<LocalResource>,Context)",1,9,9
"org.apache.hadoop.hive.ql.exec.tez.TezTask.close(TezWork,int)",1,5,5
"org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(DriverContext)",1,13,14
"org.apache.hadoop.hive.ql.exec.tez.TezTask.getMapWork()",1,4,6
"org.apache.hadoop.hive.ql.exec.tez.TezTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezTask.getReducer(MapWork)",3,1,3
"org.apache.hadoop.hive.ql.exec.tez.TezTask.getTezCounters()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezTask.isMapRedTask()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.TezTask.submit(JobConf,DAG,Path,LocalResource,TezSessionState,List<LocalResource>)",1,4,4
"org.apache.hadoop.hive.ql.exec.tez.tools.InputMerger.InputMerger(List<? extends Input>)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.tools.InputMerger.KVReaderComparator.compare(KeyValuesReader,KeyValuesReader)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.tools.InputMerger.addToQueue(KeyValuesReader)",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.tools.InputMerger.getCurrentKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.tools.InputMerger.getCurrentValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.tools.InputMerger.next()",1,2,2
"org.apache.hadoop.hive.ql.exec.tez.tools.TezMergedLogicalInput.getReader()",1,1,1
"org.apache.hadoop.hive.ql.exec.tez.tools.TezMergedLogicalInput.setConstituentInputIsReady(Input)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.BytesColumnVector()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.BytesColumnVector(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.bufferSize()",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.copySelected(boolean,int[],int,BytesColumnVector)",2,7,8
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.fill(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.flatten(boolean,int[],int)",1,6,8
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.getWritableObject(int)",1,2,4
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.increaseBufferSpace(int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.init()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.initBuffer()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.initBuffer(int)",2,1,3
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setConcat(int,byte[],int,int,byte[],int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setElement(int,int,ColumnVector)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setRef(int,byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setVal(int,byte[],int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.ColumnVector.ColumnVector(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.ColumnVector.flattenNoNulls(boolean,int[],int)",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.ColumnVector.flattenPush()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.ColumnVector.flattenRepeatingNulls(boolean,int[],int)",1,2,4
"org.apache.hadoop.hive.ql.exec.vector.ColumnVector.init()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.ColumnVector.reset()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.ColumnVector.unFlatten()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.DecimalColumnVector(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.DecimalColumnVector(int,int,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.checkPrecisionOverflow(int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.flatten(boolean,int[],int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.getWritableObject(int)",2,2,4
"org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector.setElement(int,int,ColumnVector)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector.DoubleColumnVector()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector.DoubleColumnVector(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector.copySelected(boolean,int[],int,DoubleColumnVector)",2,4,7
"org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector.fill(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector.flatten(boolean,int[],int)",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector.getWritableObject(int)",2,2,4
"org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector.setElement(int,int,ColumnVector)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.LongColumnVector()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.LongColumnVector(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.copySelected(boolean,int[],int,DoubleColumnVector)",2,4,7
"org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.copySelected(boolean,int[],int,LongColumnVector)",2,4,7
"org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.fill(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.flatten(boolean,int[],int)",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.getWritableObject(int)",2,2,4
"org.apache.hadoop.hive.ql.exec.vector.LongColumnVector.setElement(int,int,ColumnVector)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorFilterOperator.FakeDataReader.FakeDataReader(int,int)",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.TestVectorFilterOperator.FakeDataReader.getLongVector(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorFilterOperator.FakeDataReader.getNext()",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorFilterOperator.getAVectorFilterOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorFilterOperator.testBasicFilterLargeData()",1,2,8
"org.apache.hadoop.hive.ql.exec.vector.TestVectorFilterOperator.testBasicFilterOperator()",1,1,4
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.AvgValidator.validate(String,Object,Object)",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.BaseVarianceValidator.validate(String,Object,Object)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.StdSampValidator.validateVariance(String,double,long,double,double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.StdValidator.validateVariance(String,double,long,double,double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.ValueValidator.validate(String,Object,Object)",1,9,9
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.VarianceSampValidator.validateVariance(String,double,long,double,double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.VarianceValidator.validateVariance(String,double,long,double,double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.buildAggregationDesc(VectorizationContext,String,String,TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.buildAggregationDescCountStar(VectorizationContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.buildColumnDesc(VectorizationContext,String,TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.buildGroupByDescCountStar(VectorizationContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.buildGroupByDescType(VectorizationContext,String,String,TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.buildHashMap(Object...)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.buildKeyGroupByDesc(VectorizationContext,String,String,TypeInfo,String,TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.getValidator(String)",3,3,4
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateCountStar(int,Iterable<Long>,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateCountStarIterable(Iterable<VectorizedRowBatch>,Object)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateDecimal(String,String,int,Iterable<Object>,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateDecimalIterable(String,Iterable<VectorizedRowBatch>,Object)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateDouble(String,int,Iterable<Object>,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateDoubleIterable(String,Iterable<VectorizedRowBatch>,Object)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateDoubleStringKeyAggregate(String,int,Iterable<Object>,Iterable<Object>,HashMap<Object, Object>)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateLongAggregate(String,int,Iterable<Long>,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateLongIterable(String,Iterable<VectorizedRowBatch>,Object)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateLongKeyAggregate(String,int,List<Long>,Iterable<Long>,HashMap<Object, Object>)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateLongKeyIterable(String,Iterable<VectorizedRowBatch>,HashMap<Object, Object>)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateLongRepeats(String,Long,int,int,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateString(String,int,Iterable<Object>,Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateStringIterable(String,Iterable<VectorizedRowBatch>,Object)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateStringKeyAggregate(String,int,Iterable<Object>,Iterable<Object>,HashMap<Object, Object>)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAggregateStringKeyIterable(String,Iterable<VectorizedRowBatch>,TypeInfo,HashMap<Object, Object>)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAvgDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAvgDecimalNegative()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAvgLongEmpty()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAvgLongNullKeyGroupBySingleBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAvgLongNulls()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAvgLongRepeat()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAvgLongRepeatConcatValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAvgLongRepeatNulls()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAvgLongSimple()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testBigintKeyTypeAggregate()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testBooleanKeyTypeAggregate()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testCountDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testCountLongEmpty()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testCountLongNullKeyGroupBySingleBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testCountLongNulls()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testCountLongRepeat()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testCountLongRepeatConcatValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testCountLongRepeatNulls()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testCountLongSimple()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testCountStar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testCountString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testCountStringAllNull()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testCountStringWithNull()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testDecimalKeyTypeAggregate()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testDoubleKeyTypeAggregate()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testDoubleValueTypeAvg()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testDoubleValueTypeAvgOneKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testDoubleValueTypeCount()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testDoubleValueTypeCountOneKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testDoubleValueTypeMax()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testDoubleValueTypeMaxOneKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testDoubleValueTypeMin()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testDoubleValueTypeMinOneKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testDoubleValueTypeSum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testDoubleValueTypeSumOneKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testDoubleValueTypeVariance()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testDoubleValueTypeVarianceOneKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testFloatKeyTypeAggregate()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testIntKeyTypeAggregate()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testKeyTypeAggregate(String,FakeVectorRowBatchFromObjectIterables,Map<Object, Object>)",1,12,12
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMaxDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMaxLongEmpty()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMaxLongMaxInt()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMaxLongMaxLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMaxLongNegative()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMaxLongNullKeyGroupBySingleBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMaxLongNulls()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMaxLongRepeat()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMaxLongSimple()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMaxNullString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMaxString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMemoryPressureFlush()",3,2,3
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongConcatRepeat()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongEmpty()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongGroupBy()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongKeyGroupByCompactBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongKeyGroupByCrossBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongKeyGroupBySingleBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongMinInt()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongMinLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongNegative()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongNullKeyGroupByCrossBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongNullKeyGroupBySingleBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongNullStringKeys()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongNulls()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongRepeat()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongRepeatConcatValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongRepeatNulls()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongSimple()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinLongStringKeys()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinNullLongNullKeyGroupBy()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMinString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMultiKey(String,FakeVectorRowBatchFromObjectIterables,HashMap<Object, Object>)",1,15,15
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMultiKeyDoubleShortString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMultiKeyDoubleStringInt()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMultiKeyIntStringInt()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMultiKeyIntStringString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMultiKeyStringByteString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testMultiKeyStringIntString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSmallintKeyTypeAggregate()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testStdDevLongRepeat()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testStdDevLongRepeatNulls()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testStdDevSampLongRepeat()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testStdDevSampSimple()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testStdLongEmpty()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testStdLongSimple()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testStdPopDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testStdSampDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumDecimalHive6508()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumDoubleGroupByString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumDoubleSimple()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumLong2MaxInt()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumLong2MaxLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumLong2MinInt()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumLong2MinLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumLongEmpty()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumLongMinMaxLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumLongNullKeyGroupBySingleBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumLongNulls()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumLongRepeat()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumLongRepeatConcatValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumLongRepeatNulls()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumLongSimple()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumLongZero()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testTimestampKeyTypeAggregate()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testTinyintKeyTypeAggregate()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testVarLongNullKeyGroupBySingleBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testVarPopLongRepeat()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testVarPopLongRepeatNulls()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testVarSampDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testVarSampLongEmpty()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testVarSampLongRepeat()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testVarSampLongSimple()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testVarianceDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testVarianceLongEmpty()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testVarianceLongNulls()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testVarianceLongSimple()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testVarianceLongSingle()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorLimitOperator.testLimitGreaterThanBatchSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorLimitOperator.testLimitLessThanBatchSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorLimitOperator.testLimitWithZeroBatchSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorLimitOperator.validateVectorLimitOperator(int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorSelectOperator.ValidatorVectorSelectOperator.ValidatorVectorSelectOperator(VectorizationContext,OperatorDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorSelectOperator.ValidatorVectorSelectOperator.forward(Object,ObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorSelectOperator.testSelectOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testArithmeticExpressionVectorization()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testBetweenFilters()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testBooleanColumnCompareBooleanScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testFilterBooleanColumnCompareBooleanScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testFilterScalarCompareColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testFilterStringColCompareStringColumnExpressions()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testFilterWithNegativeScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testFloatInExpressions()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testIfConditionalExprs()",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testInFiltersAndExprs()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testMathFunctions()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testNotExpression()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testNotNullExpressions()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testNullExpressions()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testStringFilterExpressions()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testTimeStampUdfs()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testUnaryMinusColumnDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testUnaryMinusColumnLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testUnaryStringExpressions()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testVectorExpressionDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testVectorizeAndOrProjectionExpression()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testVectorizeFilterAndOrExpression()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.testVectorizeScalarColumnExpression()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.addRandomNulls(ColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.addRandomNulls(VectorizedRowBatch)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.addSampleNulls(ColumnVector)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.addSampleNulls(VectorizedRowBatch)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.initColors()",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.makeBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.setRandom(VectorizedRowBatch)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.setRandomDoubleCol(DoubleColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.setRandomLongCol(LongColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.setRepeatingDoubleCol(DoubleColumnVector)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.setRepeatingLongCol(LongColumnVector)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.setSample(VectorizedRowBatch)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.setSampleDoubleCol(DoubleColumnVector)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.setSampleLongCol(LongColumnVector)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.setSampleOverwrite(VectorizedRowBatch)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.setSampleStringCol(BytesColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.testFlatten()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.testVectorizedRowBatchCreate()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatch.verifyFlatten(ColumnVector)",1,4,7
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatchCtx.GetRowBatch()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatchCtx.TestCtx()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatchCtx.ValidateRowBatch(VectorizedRowBatch)",2,5,16
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatchCtx.WriteRCFile(FileSystem,Path,Configuration)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatchCtx.initSerde()",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.TestVectorizedRowBatchCtx.openFileSystem()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.TimestampUtils.assignTimeInNanoSec(long,Timestamp)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.TimestampUtils.getTimeNanoSec(Timestamp)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferBatch.VectorAggregationBufferBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferBatch.compileAggregationBatchInfo(VectorAggregateExpression[])",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferBatch.getAggregationBuffers()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferBatch.getAggregatorsFixedSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferBatch.getDistinctBufferSetCount()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferBatch.getHasVariableSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferBatch.getVariableSize(int)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferBatch.mapAggregationBufferSet(VectorAggregationBufferRow,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferBatch.startBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow.VectorAggregationBufferRow(AggregationBuffer[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow.getAggregationBuffer(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow.getAggregationBuffers()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow.getIndex()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow.getVersion()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow.reset()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow.setVersionAndIndex(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.VectorBytesColumnAssign.assignBytes(byte[],int,int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.VectorBytesColumnAssign.reset()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.VectorColumnAssignVectorBase.assignNull(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.VectorColumnAssignVectorBase.assignObjectValue(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.VectorColumnAssignVectorBase.assignVectorValue(VectorizedRowBatch,int,int,int)",1,4,6
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.VectorColumnAssignVectorBase.copyValue(T,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.VectorColumnAssignVectorBase.init(VectorizedRowBatch,T)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.VectorColumnAssignVectorBase.reset()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.VectorDecimalColumnAssign.assignDecimal(Decimal128,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.VectorDecimalColumnAssign.assignDecimal(HiveDecimal,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.VectorDecimalColumnAssign.assignDecimal(HiveDecimalWritable,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.VectorDoubleColumnAssign.assignDouble(double,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.VectorLongColumnAssign.assignLong(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.buildAssigners(VectorizedRowBatch)",7,7,7
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.buildAssigners(VectorizedRowBatch,ObjectInspector,Map<String, Integer>,List<String>)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.buildAssigners(VectorizedRowBatch,Writable[])",13,13,13
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.buildObjectAssign(VectorizedRowBatch,int,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.buildObjectAssign(VectorizedRowBatch,int,PrimitiveCategory)",11,23,37
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.ArgumentType.ArgumentType(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.ArgumentType.getType(String)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.ArgumentType.getValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Builder.Builder()",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Builder.build()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Builder.setArgumentType(int,ArgumentType)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Builder.setArgumentType(int,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Builder.setArgumentTypes(ArgumentType...)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Builder.setArgumentTypes(String...)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Builder.setInputExpressionType(int,InputExpressionType)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Builder.setInputExpressionTypes(InputExpressionType...)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Builder.setMode(Mode)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Builder.setNumArguments(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Descriptor.Descriptor(Mode,int,ArgumentType[],InputExpressionType[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Descriptor.equals(Object)",5,5,8
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Descriptor.toString()",1,5,5
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.InputExpressionType.InputExpressionType(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.InputExpressionType.getValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Mode.Mode(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Mode.getValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.getVectorExpressionClass(Class<?>,Descriptor)",4,3,6
"org.apache.hadoop.hive.ql.exec.vector.VectorExtractOperator.VectorExtractOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExtractOperator.VectorExtractOperator(VectorizationContext,OperatorDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExtractOperator.initializeOp(Configuration)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorExtractOperator.makeStandardStructObjectInspector(StructObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorExtractOperator.processOp(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorExtractOperator.setKeyAndValueColCounts(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.VectorFileSinkOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.VectorFileSinkOperator(VectorizationContext,OperatorDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.getRowObject(VectorizedRowBatch,int)",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.initializeOp(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.processOp(Object,int)",3,19,23
"org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.VectorFilterOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.VectorFilterOperator(VectorizationContext,OperatorDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.getConditionEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.initializeOp(Configuration)",1,2,4
"org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.processOp(Object,int)",2,3,5
"org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.setConditionEvaluator(VectorExpression)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.setFilterCondition(VectorExpression)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeBase.allocateAggregationBuffer()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeBase.processAggregators(VectorizedRowBatch)",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeGlobalAggregate.close(boolean)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeGlobalAggregate.initialize(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeGlobalAggregate.processBatch(VectorizedRowBatch)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeHashAggregate.checkHashModeEfficiency()",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeHashAggregate.close(boolean)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeHashAggregate.computeMemoryLimits()",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeHashAggregate.flush(boolean)",4,7,11
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeHashAggregate.initialize(Configuration)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeHashAggregate.prepareBatchAggregationBufferSets(VectorizedRowBatch)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeHashAggregate.processBatch(VectorizedRowBatch)",3,5,7
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeHashAggregate.shouldFlush(VectorizedRowBatch)",4,2,6
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeHashAggregate.updateAvgVariableSize(VectorizedRowBatch)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeStreaming.close(boolean)",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeStreaming.initialize(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.ProcessingModeStreaming.processBatch(VectorizedRowBatch)",1,6,6
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.VectorGroupByOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.VectorGroupByOperator(VectorizationContext,OperatorDesc)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.changeToStreamingMode()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.closeOp(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.flushSingleRow(VectorHashKeyWrapper,VectorAggregationBufferRow)",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.getAggregators()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.getKeyExpressions()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.initializeOp(Configuration)",1,3,6
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.processOp(Object,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.setAggregators(VectorAggregateExpression[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.setKeyExpressions(VectorExpression[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.VectorHashKeyWrapper()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.VectorHashKeyWrapper(int,int,int,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.assignDecimal(int,Decimal128)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.assignDouble(int,double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.assignLong(int,long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.assignNullDecimal(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.assignNullDouble(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.assignNullLong(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.assignNullString(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.assignString(int,byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.bytesEquals(VectorHashKeyWrapper)",4,3,4
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.clone()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.copyKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.copyKey(KeyWrapper)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.duplicateTo(VectorHashKeyWrapper)",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.equals(Object)",2,9,9
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.getByteLength(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.getByteStart(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.getBytes(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.getDecimal(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.getDoubleValue(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.getIsBytesNull(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.getIsDecimalNull(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.getIsDoubleNull(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.getIsLongNull(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.getKeyArray()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.getLongValue(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.getNewKey(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.getVariableSize()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.setHashKey()",1,4,6
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.toString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.KeyLookupHelper.resetIndices()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.KeyLookupHelper.setDecimal(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.KeyLookupHelper.setDouble(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.KeyLookupHelper.setLong(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.KeyLookupHelper.setString(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.allocateKeyWrapper()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignDecimalNoNullsNoRepeatingNoSelection(int,int,DecimalColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignDecimalNoNullsNoRepeatingSelection(int,int,DecimalColumnVector,int[])",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignDecimalNoNullsRepeating(int,int,DecimalColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignDecimalNullsNoRepeatingNoSelection(int,int,DecimalColumnVector)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignDecimalNullsNoRepeatingSelection(int,int,DecimalColumnVector,int[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignDecimalNullsRepeating(int,int,DecimalColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignDoubleNoNullsNoRepeatingNoSelection(int,int,DoubleColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignDoubleNoNullsNoRepeatingSelection(int,int,DoubleColumnVector,int[])",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignDoubleNoNullsRepeating(int,int,DoubleColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignDoubleNullsNoRepeatingNoSelection(int,int,DoubleColumnVector)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignDoubleNullsNoRepeatingSelection(int,int,DoubleColumnVector,int[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignDoubleNullsRepeating(int,int,DoubleColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignLongNoNullsNoRepeatingNoSelection(int,int,LongColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignLongNoNullsNoRepeatingSelection(int,int,LongColumnVector,int[])",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignLongNoNullsRepeating(int,int,LongColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignLongNullsNoRepeatingNoSelection(int,int,LongColumnVector)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignLongNullsNoRepeatingSelection(int,int,LongColumnVector,int[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignLongNullsRepeating(int,int,LongColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignStringNoNullsNoRepeatingNoSelection(int,int,BytesColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignStringNoNullsNoRepeatingSelection(int,int,BytesColumnVector,int[])",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignStringNoNullsRepeating(int,int,BytesColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignStringNullsNoRepeatingNoSelection(int,int,BytesColumnVector)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignStringNullsNoRepeatingSelection(int,int,BytesColumnVector,int[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignStringNullsRepeating(int,int,BytesColumnVector)",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.compileKeyWrapperBatch(VectorExpression[])",6,8,8
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.evaluateBatch(VectorizedRowBatch)",29,31,71
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.getKeysFixedSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.getVariableSize(int)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.getVectorHashKeyWrappers()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.getWritableKeyValue(VectorHashKeyWrapper,int,VectorExpressionWriter)",5,9,9
"org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator.VectorLimitOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator.VectorLimitOperator(VectorizationContext,OperatorDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator.processOp(Object,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.VectorMapJoinOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.VectorMapJoinOperator(VectorizationContext,OperatorDesc)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.closeOp(boolean)",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.flushOutput()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.getOuputVectorizationContext()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.initializeOp(Configuration)",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.internalForward(Object,ObjectInspector)",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.processOp(Object,int)",1,6,6
"org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.setMapJoinKey(ReusableGetAdaptor,Object,byte)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(Writable)",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.VectorReduceSinkOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.VectorReduceSinkOperator(VectorizationContext,OperatorDesc)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.computeBucketNumber(VectorizedRowBatch,int,int)",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.computeHashCode(VectorizedRowBatch,int,int)",1,3,5
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.forwardExtraDistinctRows(VectorizedRowBatch,int,int,BytesWritable,int,int,int)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.getKeyEval()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.getPartitionEval()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.getValueEval()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.initializeOp(Configuration)",1,9,14
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.makeValueWritable(VectorizedRowBatch,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.partitionKeysAreNull(VectorizedRowBatch,int)",4,3,4
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.populateCachedDistinctKeys(VectorizedRowBatch,int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.populatedCachedDistributionKeys(VectorizedRowBatch,int,int)",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.processOp(Object,int)",5,16,24
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.setKeyEval(VectorExpression[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.setPartitionEval(VectorExpression[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.setValueEval(VectorExpression[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.VectorSMBMapJoinOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.VectorSMBMapJoinOperator(VectorizationContext,OperatorDesc)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.closeOp(boolean)",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.flushOutput()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.getOuputVectorizationContext()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.initializeOp(Configuration)",1,4,5
"org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.internalForward(Object,ObjectInspector)",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.processOp(Object,int)",1,7,7
"org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.smbJoinComputeKeys(Object,byte)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.VectorSelectOperator()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.VectorSelectOperator(VectorizationContext,OperatorDesc)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.getOperatorName()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.getOuputVectorizationContext()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.getVExpressions()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.getvExpressions()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.initializeOp(Configuration)",2,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.processOp(Object,int)",3,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.setVExpressions(VectorExpression[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.setvExpressions(VectorExpression[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorUtilBatchObjectPool.VectorUtilBatchObjectPool(int,IAllocator<T>)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorUtilBatchObjectPool.getFromPool()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorUtilBatchObjectPool.putInPool(T)",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.OutputColumnManager.OutputColumnManager(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.OutputColumnManager.allocateOutputColumn(String)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.OutputColumnManager.allocateOutputColumnInternal(String)",4,4,5
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.OutputColumnManager.freeOutputColumn(int)",2,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.VectorizationContext(Map<String, Integer>,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.VectorizationContext(VectorizationContext)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.addToColumnMap(String,int)",2,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.arg0Type(ExprNodeGenericFuncDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.castConstantToDecimal(Object,TypeInfo)",2,2,9
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.castConstantToDouble(Object,TypeInfo)",3,3,4
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.castConstantToLong(Object,TypeInfo)",3,3,4
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.castConstantToString(Object,TypeInfo)",3,3,4
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.createVectorExpression(Class<?>,List<ExprNodeDesc>,Mode,TypeInfo)",5,10,11
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.evaluateCastOnConstants(ExprNodeDesc)",5,8,12
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.evaluateCastOnConstants(List<ExprNodeDesc>)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.evaluateCastToTimestamp(ExprNodeDesc)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getAggregatorExpression(AggregationDesc)",3,8,12
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getBetweenFilterExpression(List<ExprNodeDesc>,Mode,TypeInfo)",4,23,28
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getCastToBoolean(List<ExprNodeDesc>)",4,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getCastToDecimal(List<ExprNodeDesc>,TypeInfo)",8,8,8
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getCastToDoubleExpression(Class<?>,List<ExprNodeDesc>,TypeInfo)",6,6,6
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getCastToLongExpression(List<ExprNodeDesc>)",4,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getCastToString(List<ExprNodeDesc>,TypeInfo)",7,7,7
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getChildExpressionsWithImplicitCast(GenericUDF,List<ExprNodeDesc>,TypeInfo)",5,8,12
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getChildrenMode(Mode,Class<?>)",2,3,4
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getCoalesceExpression(List<ExprNodeDesc>,TypeInfo)",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getColumnMap()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getColumnVectorExpression(ExprNodeColumnDesc,Mode)",2,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getCommonTypeForChildExpressions(GenericUDF,List<ExprNodeDesc>,TypeInfo)",3,3,4
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getConstantVectorExpression(Object,TypeInfo,Mode)",10,13,15
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getConstructor(Class<?>)",4,2,5
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getCustomUDFExpression(ExprNodeGenericFuncDesc)",5,8,9
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getEltExpression(List<ExprNodeDesc>,TypeInfo)",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getFileKey()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getGenericUDFBridgeVectorExpression(GenericUDFBridge,List<ExprNodeDesc>,Mode,TypeInfo)",5,5,5
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getGenericUDFForCast(TypeInfo)",4,5,16
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getGenericUdfVectorExpression(GenericUDF,List<ExprNodeDesc>,Mode,TypeInfo)",10,10,11
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getIdentityExpression(List<ExprNodeDesc>)",3,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getImplicitCastExpression(GenericUDF,ExprNodeDesc,TypeInfo)",5,4,10
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getInExpression(List<ExprNodeDesc>,Mode,TypeInfo)",1,13,19
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getInputColumnIndex(ExprNodeColumnDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getInputColumnIndex(String)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getIntFamilyScalarAsLong(ExprNodeConstantDesc)",3,1,3
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getNormalizedTypeName(String)",1,4,5
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getNumericScalarAsDouble(ExprNodeDesc)",5,1,5
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getOutputColumnTypeMap()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getPrecisionForType(PrimitiveTypeInfo)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getScalarValue(ExprNodeConstantDesc)",5,4,6
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getStringScalarAsByteArray(ExprNodeConstantDesc)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getTimestampScalar(ExprNodeDesc)",4,4,5
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(ExprNodeDesc,Mode)",2,9,9
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpressionForUdf(Class<?>,List<ExprNodeDesc>,Mode,TypeInfo)",6,7,9
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpressions(List<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpressions(List<ExprNodeDesc>,Mode)",2,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorTypeScalarValue(ExprNodeConstantDesc)",3,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.instantiateExpression(Class<?>,TypeInfo,Object...)",1,6,7
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.isCastToFloatFamily(Class<? extends UDF>)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.isCastToIntFamily(Class<? extends UDF>)",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.isCustomUDF(ExprNodeGenericFuncDesc)",3,1,3
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.isDateFamily(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.isDatetimeFamily(String)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.isDecimalFamily(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.isExcludedFromCast(GenericUDF)",3,5,6
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.isFloatFamily(String)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.isIntFamily(String)",1,6,6
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.isNonVectorizedPathUDF(ExprNodeGenericFuncDesc)",4,14,16
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.isStringFamily(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.isTimestampFamily(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.setFileKey(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.updatePrecision(TypeInfo,DecimalTypeInfo)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.addRowToBatch(Object,StructObjectInspector,int,VectorizedRowBatch,DataOutputBuffer)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.addRowToBatchFrom(Object,StructObjectInspector,int,int,VectorizedRowBatch,DataOutputBuffer)",5,15,30
"org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.allocateColumnVector(StructObjectInspector,List<ColumnVector>)",7,4,12
"org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.constructVectorizedRowBatch(StructObjectInspector)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.constructVectorizedRowBatch(StructObjectInspector,StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.setBatchSize(VectorizedRowBatch,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.setNoNullFields(VectorizedRowBatch)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.setNullColIsNullValue(ColumnVector,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.setRepeatingColumn(VectorizedRowBatch,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedColumnarSerDe.VectorizedColumnarSerDe()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedColumnarSerDe.deserialize(Writable)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedColumnarSerDe.deserializeVector(Object,int,VectorizedRowBatch)",2,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorizedColumnarSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedColumnarSerDe.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedColumnarSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedColumnarSerDe.serialize(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedColumnarSerDe.serializeVector(VectorizedRowBatch,ObjectInspector)",9,10,24
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.VectorizedRowBatch(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.VectorizedRowBatch(int,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.count()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.getMaxSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.reset()",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.setValueWriters(VectorExpressionWriter[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString()",2,12,14
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toUTF8(Object)",2,1,3
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.VectorizedRowBatchCtx()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.VectorizedRowBatchCtx(StructObjectInspector,StructObjectInspector,Deserializer,Map<String, Object>,Map<String, PrimitiveCategory>)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.addPartitionColsToBatch(VectorizedRowBatch)",4,16,29
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.addRowToBatch(int,Writable,VectorizedRowBatch,DataOutputBuffer)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.addScratchColumnsToBatch(VectorizedRowBatch)",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.allocateColumnVector(String,int)",4,4,4
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.buildObjectAssigners(VectorizedRowBatch)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.convertRowBatchBlobToVectorizedBatch(Object,int,VectorizedRowBatch)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.createVectorizedRowBatch()",7,10,14
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.getColIndexBasedOnColName(String)",3,2,3
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.getScalePrecisionFromDecimalType(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.init(Configuration,FileSplit)",3,8,9
"org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.init(Configuration,String,StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.AbstractFilterStringColLikeStringScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.AbstractFilterStringColLikeStringScalar(int,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.BeginChecker.BeginChecker(String)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.BeginChecker.check(byte[],int,int)",4,1,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.ComplexChecker.ComplexChecker(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.ComplexChecker.check(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.EndChecker.EndChecker(String)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.EndChecker.check(byte[],int,int)",4,1,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.FastUTF8Decoder.FastUTF8Decoder()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.FastUTF8Decoder.decodeUnsafely(byte[],int,int)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.MiddleChecker.MiddleChecker(String)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.MiddleChecker.check(byte[],int,int)",6,1,6
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.NoneChecker.NoneChecker(String)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.NoneChecker.check(byte[],int,int)",4,1,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.createChecker(String)",3,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.evaluate(VectorizedRowBatch)",2,15,24
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.getPattern()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.setPattern(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastBooleanToStringViaLongToString.CastBooleanToStringViaLongToString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastBooleanToStringViaLongToString.CastBooleanToStringViaLongToString(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastBooleanToStringViaLongToString.func(BytesColumnVector,long[],int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDateToString.CastDateToString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDateToString.CastDateToString(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDateToString.func(BytesColumnVector,long[],int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToBoolean.CastDecimalToBoolean()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToBoolean.CastDecimalToBoolean(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToBoolean.func(LongColumnVector,DecimalColumnVector,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.CastDecimalToDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.CastDecimalToDecimal(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.convert(DecimalColumnVector,DecimalColumnVector,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.getInputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.setInputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDecimal.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDouble.CastDecimalToDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDouble.CastDecimalToDouble(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToDouble.func(DoubleColumnVector,DecimalColumnVector,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToLong.CastDecimalToLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToLong.CastDecimalToLong(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToLong.func(LongColumnVector,DecimalColumnVector,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.CastDecimalToString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.CastDecimalToString(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.func(BytesColumnVector,DecimalColumnVector,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToTimestamp.CastDecimalToTimestamp()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToTimestamp.CastDecimalToTimestamp(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToTimestamp.func(LongColumnVector,DecimalColumnVector,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToDecimal.CastDoubleToDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToDecimal.CastDoubleToDecimal(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToDecimal.func(DecimalColumnVector,DoubleColumnVector,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDate.CastLongToDate()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDate.CastLongToDate(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDate.evaluate(VectorizedRowBatch)",3,15,17
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDate.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDate.getInputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDate.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDate.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDate.setInputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDate.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDecimal.CastLongToDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDecimal.CastLongToDecimal(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToDecimal.func(DecimalColumnVector,LongColumnVector,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToString.CastLongToString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToString.CastLongToString(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToString.func(BytesColumnVector,long[],int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.CastStringToDate()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.CastStringToDate(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.evaluate(LongColumnVector,BytesColumnVector,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.getInputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.setInputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.CastStringToDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.CastStringToDecimal(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.func(DecimalColumnVector,BytesColumnVector,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.getInputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.setInputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDecimal.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDecimal.CastTimestampToDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDecimal.CastTimestampToDecimal(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDecimal.func(DecimalColumnVector,LongColumnVector,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.ColAndCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.ColAndCol(int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.evaluate(VectorizedRowBatch)",2,2,118
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.getColNum1()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.getColNum2()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.setColNum1(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.setColNum2(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColAndCol.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.ColOrCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.ColOrCol(int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.evaluate(VectorizedRowBatch)",2,2,118
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.getColNum1()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.getColNum2()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.setColNum1(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.setColNum2(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ColOrCol.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.ConstantVectorExpression()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.ConstantVectorExpression(int,Decimal128)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.ConstantVectorExpression(int,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.ConstantVectorExpression(int,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.ConstantVectorExpression(int,byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.ConstantVectorExpression(int,double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.ConstantVectorExpression(int,long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluate(VectorizedRowBatch)",2,2,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluateBytes(VectorizedRowBatch)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluateDecimal(VectorizedRowBatch)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluateDouble(VectorizedRowBatch)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.evaluateLong(VectorizedRowBatch)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.getBytesValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.getDoubleValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.getLongValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.getTypeString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.setBytesValue(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.setDecimalValue(Decimal128)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.setDoubleValue(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.setLongValue(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.setOutputType(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.setType(Type)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.setTypeString(String)",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.CuckooSetBytes(int)",3,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.entryEqual(byte[][],int,byte[],int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.h1(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.h2(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.hash(byte[],int,int,int)",2,3,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.insert(byte[])",5,2,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.load(byte[][])",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.lookup(byte[],int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.rehash()",8,7,9
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.rot(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.tryInsert(byte[])",5,2,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetBytes.updateHashSalt()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetDouble.CuckooSetDouble(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetDouble.insert(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetDouble.load(double[])",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetDouble.lookup(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetLong.CuckooSetLong(int)",3,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetLong.findNewBlank()",1,3,6
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetLong.h1(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetLong.h2(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetLong.insert(long)",5,5,6
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetLong.load(long[])",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetLong.lookup(long)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetLong.rehash()",8,7,9
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetLong.tryInsert(long)",5,4,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.CuckooSetLong.updateHashSalt()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.DecimalColumnInList()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.DecimalColumnInList(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.evaluate(VectorizedRowBatch)",2,16,23
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.getInListValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.setInListValues(Decimal128[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalToStringUnaryUDF.DecimalToStringUnaryUDF()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalToStringUnaryUDF.DecimalToStringUnaryUDF(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalToStringUnaryUDF.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalToStringUnaryUDF.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalToStringUnaryUDF.getInputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalToStringUnaryUDF.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalToStringUnaryUDF.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalToStringUnaryUDF.setInputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalToStringUnaryUDF.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.abs(int,Decimal128,DecimalColumnVector)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.addChecked(int,Decimal128,Decimal128,DecimalColumnVector)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.ceiling(int,Decimal128,DecimalColumnVector)",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.divideChecked(int,Decimal128,Decimal128,DecimalColumnVector)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.floor(int,Decimal128,DecimalColumnVector)",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.moduloChecked(int,Decimal128,Decimal128,DecimalColumnVector)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.multiplyChecked(int,Decimal128,Decimal128,DecimalColumnVector)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.negate(int,Decimal128,DecimalColumnVector)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.round(int,Decimal128,DecimalColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.sign(int,Decimal128,LongColumnVector)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalUtil.subtractChecked(int,Decimal128,Decimal128,DecimalColumnVector)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleColumnInList.DoubleColumnInList()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleColumnInList.DoubleColumnInList(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleColumnInList.evaluate(VectorizedRowBatch)",2,15,22
"org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleColumnInList.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleColumnInList.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleColumnInList.getInListValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleColumnInList.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleColumnInList.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleColumnInList.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleColumnInList.setInListValues(double[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.DoubleColumnInList.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColAndScalar.FilterColAndScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColAndScalar.FilterColAndScalar(int,long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColAndScalar.evaluate(VectorizedRowBatch)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColAndScalar.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColAndScalar.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColAndScalar.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColAndScalar.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColAndScalar.getValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColAndScalar.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColAndScalar.setValue(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColOrScalar.FilterColOrScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColOrScalar.FilterColOrScalar(int,long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColOrScalar.evaluate(VectorizedRowBatch)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColOrScalar.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColOrScalar.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColOrScalar.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColOrScalar.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColOrScalar.getValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColOrScalar.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterColOrScalar.setValue(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterConstantBooleanVectorExpression.FilterConstantBooleanVectorExpression()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterConstantBooleanVectorExpression.FilterConstantBooleanVectorExpression(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterConstantBooleanVectorExpression.evaluate(VectorizedRowBatch)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.FilterDecimalColumnInList()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.FilterDecimalColumnInList(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.evaluate(VectorizedRowBatch)",2,16,25
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.getInListValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.setInListValues(Decimal128[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.FilterDoubleColumnInList()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.FilterDoubleColumnInList(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.evaluate(VectorizedRowBatch)",2,15,24
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.getInListValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.setInListValues(double[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr.FilterExprAndExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr.evaluate(VectorizedRowBatch)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprOrExpr.FilterExprOrExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprOrExpr.evaluate(VectorizedRowBatch)",2,2,12
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprOrExpr.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprOrExpr.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprOrExpr.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.FilterLongColumnInList()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.FilterLongColumnInList(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.evaluate(VectorizedRowBatch)",2,15,24
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.getInListValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.setInListValues(long[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarAndColumn.FilterScalarAndColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarAndColumn.FilterScalarAndColumn(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarAndColumn.evaluate(VectorizedRowBatch)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarAndColumn.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarAndColumn.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarAndColumn.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarAndColumn.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarAndColumn.getValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarAndColumn.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarAndColumn.setValue(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarOrColumn.FilterScalarOrColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarOrColumn.FilterScalarOrColumn(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarOrColumn.evaluate(VectorizedRowBatch)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarOrColumn.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarOrColumn.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarOrColumn.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarOrColumn.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarOrColumn.getValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarOrColumn.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterScalarOrColumn.setValue(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.BeginCheckerFactory.tryCreate(String)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.ComplexCheckerFactory.tryCreate(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.EndCheckerFactory.tryCreate(String)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.FilterStringColLikeStringScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.FilterStringColLikeStringScalar(int,byte[])",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.MiddleCheckerFactory.tryCreate(String)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.NoneCheckerFactory.tryCreate(String)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColLikeStringScalar.getCheckerFactories()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColRegExpStringScalar.BeginCheckerFactory.tryCreate(String)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColRegExpStringScalar.ComplexCheckerFactory.tryCreate(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColRegExpStringScalar.EndCheckerFactory.tryCreate(String)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColRegExpStringScalar.FilterStringColRegExpStringScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColRegExpStringScalar.FilterStringColRegExpStringScalar(int,byte[])",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColRegExpStringScalar.MiddleCheckerFactory.tryCreate(String)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColRegExpStringScalar.NoneCheckerFactory.tryCreate(String)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColRegExpStringScalar.PhoneNumberChecker.PhoneNumberChecker(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColRegExpStringScalar.PhoneNumberChecker.check(byte[],int,int)",5,2,8
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColRegExpStringScalar.PhoneNumberCheckerFactory.tryCreate(String)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColRegExpStringScalar.getCheckerFactories()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColumnInList.FilterStringColumnInList()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColumnInList.FilterStringColumnInList(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColumnInList.evaluate(VectorizedRowBatch)",2,15,24
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColumnInList.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColumnInList.getInListValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColumnInList.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColumnInList.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FilterStringColumnInList.setInListValues(byte[][])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncBin.FuncBin()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncBin.FuncBin(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncBin.prepareResult(int,long[],BytesColumnVector)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToDouble.FuncDecimalToDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToDouble.FuncDecimalToDouble(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToDouble.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToDouble.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToDouble.getInputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToDouble.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToDouble.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToDouble.setInputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToDouble.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToLong.FuncDecimalToLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToLong.FuncDecimalToLong(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToLong.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToLong.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToLong.getInputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToLong.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToLong.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToLong.setInputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDecimalToLong.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDoubleToDecimal.FuncDoubleToDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDoubleToDecimal.FuncDoubleToDecimal(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDoubleToDecimal.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDoubleToDecimal.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDoubleToDecimal.getInputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDoubleToDecimal.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDoubleToDecimal.setInputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncDoubleToDecimal.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncHex.FuncHex()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncHex.FuncHex(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncHex.prepareResult(int,long[],BytesColumnVector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseDoubleToDouble.FuncLogWithBaseDoubleToDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseDoubleToDouble.FuncLogWithBaseDoubleToDouble(double,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseDoubleToDouble.func(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseDoubleToDouble.getBase()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseDoubleToDouble.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseDoubleToDouble.setArg(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseDoubleToDouble.setBase(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseLongToDouble.FuncLogWithBaseLongToDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseLongToDouble.FuncLogWithBaseLongToDouble(double,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseLongToDouble.func(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseLongToDouble.getBase()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseLongToDouble.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseLongToDouble.setArg(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLogWithBaseLongToDouble.setBase(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToDecimal.FuncLongToDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToDecimal.FuncLongToDecimal(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToDecimal.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToDecimal.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToDecimal.getInputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToDecimal.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToDecimal.setInputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToDecimal.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.FuncLongToString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.FuncLongToString(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.getInputCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.getOutputCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.setInputCol(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.setOutputCol(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerDoubleToDouble.FuncPowerDoubleToDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerDoubleToDouble.FuncPowerDoubleToDouble(int,double,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerDoubleToDouble.cleanup(DoubleColumnVector,int[],boolean,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerDoubleToDouble.func(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerDoubleToDouble.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerDoubleToDouble.getPower()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerDoubleToDouble.setArg(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerDoubleToDouble.setPower(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerLongToDouble.FuncPowerLongToDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerLongToDouble.FuncPowerLongToDouble(int,double,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerLongToDouble.cleanup(DoubleColumnVector,int[],boolean,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerLongToDouble.func(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerLongToDouble.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerLongToDouble.getPower()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerLongToDouble.setArg(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncPowerLongToDouble.setPower(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRand.FuncRand()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRand.FuncRand(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRand.evaluate(VectorizedRowBatch)",2,5,7
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRand.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRand.getOutputCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRand.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRand.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRand.getRandom()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRand.setOutputCol(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRand.setRandom(Random)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRandNoSeed.FuncRandNoSeed()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRandNoSeed.FuncRandNoSeed(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRandNoSeed.evaluate(VectorizedRowBatch)",2,5,6
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRandNoSeed.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRandNoSeed.getOutputCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRandNoSeed.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRandNoSeed.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRandNoSeed.getRandom()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRandNoSeed.setOutputCol(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRandNoSeed.setRandom(Random)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.FuncRoundWithNumDigitsDecimalToDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.FuncRoundWithNumDigitsDecimalToDecimal(int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.evaluate(VectorizedRowBatch)",2,10,11
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.getDecimalPlaces()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.setDecimalPlaces(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.FuncRoundWithNumDigitsDecimalToDecimal.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression.IdentityExpression()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression.IdentityExpression(int,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression.evaluate(VectorizedRowBatch)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression.setType(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringColumn.IfExprStringColumnStringColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringColumn.IfExprStringColumnStringColumn(int,int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringColumn.evaluate(VectorizedRowBatch)",3,23,33
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringColumn.getArg1Column()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringColumn.getArg2Column()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringColumn.getArg3Column()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringColumn.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringColumn.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringColumn.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringColumn.setArg1Column(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringColumn.setArg2Column(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringColumn.setArg3Column(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringColumn.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringScalar.IfExprStringColumnStringScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringScalar.IfExprStringColumnStringScalar(int,int,byte[],int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringScalar.evaluate(VectorizedRowBatch)",3,19,28
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringScalar.getArg1Column()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringScalar.getArg2Column()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringScalar.getArg3Scalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringScalar.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringScalar.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringScalar.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringScalar.setArg1Column(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringScalar.setArg2Column(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringScalar.setArg3Scalar(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringColumnStringScalar.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringColumn.IfExprStringScalarStringColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringColumn.IfExprStringScalarStringColumn(int,byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringColumn.evaluate(VectorizedRowBatch)",3,19,28
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringColumn.getArg1Column()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringColumn.getArg2Scalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringColumn.getArg3Column()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringColumn.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringColumn.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringColumn.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringColumn.setArg1Column(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringColumn.setArg2Scalar(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringColumn.setArg3Column(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringColumn.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.IfExprStringScalarStringScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.IfExprStringScalarStringScalar(int,byte[],byte[],int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.evaluate(VectorizedRowBatch)",3,15,18
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.getArg1Column()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.getArg2Scalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.getArg3Scalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.setArg1Column(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.setArg2Scalar(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.setArg3Scalar(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull.IsNotNull()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull.IsNotNull(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull.evaluate(VectorizedRowBatch)",2,2,11
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNotNull.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull.IsNull()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull.IsNull(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull.evaluate(VectorizedRowBatch)",2,2,11
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.IsNull.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.LongColDivideLongColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.LongColDivideLongColumn(int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.evaluate(VectorizedRowBatch)",2,3,24
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.getColNum1()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.getColNum2()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.setColNum1(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.setColNum2(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongColumn.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.LongColDivideLongScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.LongColDivideLongScalar(int,long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.evaluate(VectorizedRowBatch)",2,6,12
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.getValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColDivideLongScalar.setValue(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList.LongColumnInList()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList.LongColumnInList(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList.evaluate(VectorizedRowBatch)",2,15,22
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList.getInListValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList.setInListValues(long[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongColumnInList.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.LongScalarDivideLongColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.LongScalarDivideLongColumn(long,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.evaluate(VectorizedRowBatch)",2,6,17
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.getValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongScalarDivideLongColumn.setValue(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongToStringUnaryUDF.LongToStringUnaryUDF()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongToStringUnaryUDF.LongToStringUnaryUDF(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongToStringUnaryUDF.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongToStringUnaryUDF.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongToStringUnaryUDF.getInputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongToStringUnaryUDF.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongToStringUnaryUDF.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongToStringUnaryUDF.setInputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.LongToStringUnaryUDF.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.NaNToNull(DoubleColumnVector,int[],boolean,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.NaNToNull(DoubleColumnVector,int[],boolean,int,boolean)",2,19,24
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.abs(long)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.doubleToTimestamp(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.fromTimestamp(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.fromTimestampToDouble(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.log2(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.longToTimestamp(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.round(double)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.sign(double)",3,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.sign(long)",3,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.toBool(double)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.toBool(long)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathExpr.writeLongToUTF8(byte[],long)",2,1,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncDoubleToDouble.MathFuncDoubleToDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncDoubleToDouble.MathFuncDoubleToDouble(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncDoubleToDouble.cleanup(DoubleColumnVector,int[],boolean,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncDoubleToDouble.evaluate(VectorizedRowBatch)",2,10,11
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncDoubleToDouble.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncDoubleToDouble.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncDoubleToDouble.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncDoubleToDouble.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncDoubleToDouble.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToDouble.MathFuncLongToDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToDouble.MathFuncLongToDouble(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToDouble.cleanup(DoubleColumnVector,int[],boolean,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToDouble.evaluate(VectorizedRowBatch)",2,10,11
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToDouble.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToDouble.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToDouble.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToDouble.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToDouble.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToLong.MathFuncLongToLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToLong.MathFuncLongToLong(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToLong.evaluate(VectorizedRowBatch)",2,10,11
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToLong.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToLong.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToLong.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToLong.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.MathFuncLongToLong.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.NotCol.NotCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.NotCol.NotCol(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.NotCol.evaluate(VectorizedRowBatch)",2,2,12
"org.apache.hadoop.hive.ql.exec.vector.expressions.NotCol.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.NotCol.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.NotCol.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.NotCol.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.NotCol.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.NotCol.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.filterNulls(ColumnVector,boolean,int[],int)",3,1,9
"org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.initOutputNullsToFalse(ColumnVector,boolean,boolean,int[],int)",2,2,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.propagateNullsColCol(ColumnVector,ColumnVector,ColumnVector,int[],int,boolean)",10,15,33
"org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.setNullAndDivBy0DataEntriesDouble(DoubleColumnVector,boolean,int[],int,DoubleColumnVector)",1,1,12
"org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.setNullAndDivBy0DataEntriesDouble(DoubleColumnVector,boolean,int[],int,LongColumnVector)",1,1,12
"org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.setNullAndDivBy0DataEntriesLong(LongColumnVector,boolean,int[],int,DoubleColumnVector)",1,1,12
"org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.setNullAndDivBy0DataEntriesLong(LongColumnVector,boolean,int[],int,LongColumnVector)",1,1,12
"org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.setNullDataEntriesDecimal(DecimalColumnVector,boolean,int[],int)",2,8,9
"org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.setNullDataEntriesDouble(DoubleColumnVector,boolean,int[],int)",2,1,9
"org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.setNullDataEntriesLong(LongColumnVector,boolean,int[],int)",2,1,9
"org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.setNullOutputEntriesColScalar(ColumnVector,boolean,int[],int)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.PosModDoubleToDouble.PosModDoubleToDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.PosModDoubleToDouble.PosModDoubleToDouble(int,double,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.PosModDoubleToDouble.func(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.PosModDoubleToDouble.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.PosModDoubleToDouble.getDivisor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.PosModDoubleToDouble.setArg(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.PosModDoubleToDouble.setDivisor(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.PosModLongToLong.PosModLongToLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.PosModLongToLong.PosModLongToLong(int,long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.PosModLongToLong.func(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.PosModLongToLong.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.PosModLongToLong.getDivisor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.PosModLongToLong.setArg(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.PosModLongToLong.setDivisor(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.RoundWithNumDigitsDoubleToDouble.RoundWithNumDigitsDoubleToDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.RoundWithNumDigitsDoubleToDouble.RoundWithNumDigitsDoubleToDouble(int,long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.RoundWithNumDigitsDoubleToDouble.func(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.RoundWithNumDigitsDoubleToDouble.getDecimalPlaces()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.RoundWithNumDigitsDoubleToDouble.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.RoundWithNumDigitsDoubleToDouble.setArg(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.RoundWithNumDigitsDoubleToDouble.setDecimalPlaces(IntWritable)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsFalse.SelectColumnIsFalse()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsFalse.SelectColumnIsFalse(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsFalse.evaluate(VectorizedRowBatch)",7,2,23
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsFalse.getColNum1()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsFalse.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsFalse.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsFalse.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsFalse.setColNum1(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNotNull.SelectColumnIsNotNull()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNotNull.SelectColumnIsNotNull(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNotNull.evaluate(VectorizedRowBatch)",5,2,12
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNotNull.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNotNull.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNotNull.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNotNull.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNotNull.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNull.SelectColumnIsNull()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNull.SelectColumnIsNull(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNull.evaluate(VectorizedRowBatch)",5,2,12
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNull.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNull.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNull.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNull.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNull.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsTrue.SelectColumnIsTrue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsTrue.SelectColumnIsTrue(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsTrue.evaluate(VectorizedRowBatch)",7,2,23
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsTrue.getColNum1()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsTrue.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsTrue.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsTrue.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsTrue.setColNum1(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.StringColumnInList()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.StringColumnInList(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.evaluate(VectorizedRowBatch)",2,15,22
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.getInListValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.getInputCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.setInListValues(byte[][])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.setInputCol(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringColumnInList.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.StringConcatColCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.StringConcatColCol(int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.evaluate(VectorizedRowBatch)",14,81,100
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.getColNum1()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.getColNum2()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.propagateNulls(boolean,int,int[],ColumnVector,ColumnVector)",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.propagateNullsCombine(boolean,int,int[],ColumnVector,ColumnVector,BytesColumnVector)",1,1,6
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.setColNum1(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.setColNum2(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColCol.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColScalar.StringConcatColScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColScalar.StringConcatColScalar(int,byte[],int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColScalar.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColScalar.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColScalar.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColScalar.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColScalar.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColScalar.getValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColScalar.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColScalar.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatColScalar.setValue(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatScalarCol.StringConcatScalarCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatScalarCol.StringConcatScalarCol(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatScalarCol.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatScalarCol.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatScalarCol.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatScalarCol.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatScalarCol.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatScalarCol.getValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatScalarCol.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatScalarCol.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringConcatScalarCol.setValue(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr.compare(byte[],int,int,byte[],int,int)",3,1,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringHex.StringHex(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLTrim.StringLTrim()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLTrim.StringLTrim(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLTrim.func(BytesColumnVector,byte[][],int[],int[],int)",1,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLength.StringLength()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLength.StringLength(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLength.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLength.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLength.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLength.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLength.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLength.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLength.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLength.utf8StringLength(byte[],int,int)",1,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLower.StringLower()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringLower.StringLower(int,int)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringRTrim.StringRTrim()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringRTrim.StringRTrim(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringRTrim.func(BytesColumnVector,byte[][],int[],int[],int)",1,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.StringSubstrColStart()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.StringSubstrColStart(int,int,int)",1,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.evaluate(VectorizedRowBatch)",4,18,20
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.getStartIdx()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.getSubstrStartOffset(byte[],int,int,int)",6,1,8
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.setStartIdx(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.StringSubstrColStartLen()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.StringSubstrColStartLen(int,int,int,int)",1,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.evaluate(VectorizedRowBatch)",4,18,20
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.getLength()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.getStartIdx()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.populateSubstrOffsets(byte[],int,int,int,int,int[])",4,1,11
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.setLength(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.setStartIdx(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringTrim.StringTrim()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringTrim.StringTrim(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringTrim.func(BytesColumnVector,byte[][],int[],int[],int)",1,1,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.StringUnaryUDF()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.StringUnaryUDF(int,int,IUDFUnaryString)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.getFunc()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.setFunc(IUDFUnaryString)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDF.setString(BytesColumnVector,int,Text)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDFDirect.StringUnaryUDFDirect()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDFDirect.StringUnaryUDFDirect(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDFDirect.evaluate(VectorizedRowBatch)",2,14,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDFDirect.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDFDirect.getInputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDFDirect.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDFDirect.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDFDirect.setInputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUnaryUDFDirect.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUpper.StringUpper()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.StringUpper.StringUpper(int,int)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestConstantVectorExpression.sameFirstKBytes(byte[],byte[],int)",3,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestConstantVectorExpression.testConstantExpression()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestCuckooSet.getByteArrays(String[])",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestCuckooSet.getUTF8Bytes(String)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestCuckooSet.loadRandom(long[],Random)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestCuckooSet.loadRandomBytes(byte[][],Random)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestCuckooSet.loadSet(CuckooSetBytes,byte[][])",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestCuckooSet.loadSet(CuckooSetLong,long[])",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestCuckooSet.testSetBytes()",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestCuckooSet.testSetBytesLargeRandom()",3,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestCuckooSet.testSetDouble()",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestCuckooSet.testSetLong()",1,7,7
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestCuckooSet.testSetLongRandom()",3,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.testAbs()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.testCeiling()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.testFloor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.testNegate()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.testRound()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.testRoundWithDigits()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.testSign()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestUnaryMinus.testUnaryMinus()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.getVectorizedRowBatch2LongInDoubleOut()",1,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.getVectorizedRowBatch3DecimalCols()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.getVectorizedRowBatchSingleLongVector(int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalColAddDecimalColumn()",1,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalColAddDecimalScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalColDivideDecimalColumn()",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalColDivideDecimalScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalColModuloDecimalColumn()",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalColModuloDecimalScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalColMultiplyDecimalColumn()",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalColMultiplyDecimalScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalColSubtractDecimalColumn()",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalColSubtractDecimalScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalScalarAddDecimalColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalScalarDivideDecimalColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalScalarModuloDecimalColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalScalarMultiplyDecimalColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testDecimalScalarSubtractDecimalColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testLongColAddLongColumn()",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testLongColAddLongScalarNoNulls()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testLongColAddLongScalarWithNulls()",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testLongColAddLongScalarWithRepeating()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.testLongColDivideLongColumn()",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmeticExpressions.verifyLongNullDataVectorEntries(LongColumnVector,int[],boolean,int)",2,9,10
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.getBatch1Long3BytesVectors()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.getBatch1Long3DoubleVectors()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.getBatch4LongVectors()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.getString(BytesColumnVector,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.getUTF8Bytes(String)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.setString(BytesColumnVector,int,String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.testDoubleColumnColumnIfExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.testDoubleColumnScalarIfExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.testDoubleScalarColumnIfExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.testDoubleScalarScalarIfExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.testIfExprStringColumnStringColumn()",1,1,10
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.testIfExprStringColumnStringScalar()",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.testIfExprStringScalarStringColumn()",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.testIfExprStringScalarStringScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.testLongColumnColumnIfExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.testLongColumnScalarIfExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.testLongScalarColumnIfExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.testLongScalarScalarIfExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.compareToUDFDayOfMonthDate(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.compareToUDFMonthDate(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.compareToUDFUnixTimeStampDate(long,long)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.compareToUDFWeekOfYearDate(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.compareToUDFYearDate(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.getAllBoundaries()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.getLongWritable(TimestampWritable)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.getVectorizedRandomRowBatch(int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.getVectorizedRowBatch(int[],int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.main(String[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.testVectorUDFDayOfMonth()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.testVectorUDFMonth()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.testVectorUDFUnixTimeStamp()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.testVectorUDFWeekOfYear()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.testVectorUDFYear()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.toTimestampWritable(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.verifyUDFDayOfMonth(VectorizedRowBatch)",1,4,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.verifyUDFMonth(VectorizedRowBatch)",1,4,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.verifyUDFUnixTimeStamp(VectorizedRowBatch)",1,4,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.verifyUDFWeekOfYear(VectorizedRowBatch)",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.verifyUDFYear(VectorizedRowBatch)",1,4,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.genStructOI()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.getWritableValue(TypeInfo,Decimal128)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.getWritableValue(TypeInfo,byte[])",4,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.getWritableValue(TypeInfo,double)",3,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.getWritableValue(TypeInfo,long)",7,7,8
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.getWriter(TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testSetterDecimal(DecimalTypeInfo)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testSetterDouble(TypeInfo)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testSetterLong(TypeInfo)",4,4,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testSetterText(TypeInfo)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testStructLong(TypeInfo)",1,4,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testTimeStampUtils()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionSetterBinary()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionSetterBoolean()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionSetterByte()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionSetterDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionSetterDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionSetterFloat()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionSetterInt()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionSetterLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionSetterShort()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionSetterString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionSetterTimestamp()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionSetterVarchar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionStructLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionWriterBinary()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionWriterBoolean()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionWriterByte()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionWriterDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionWriterDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionWriterFloat()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionWriterInt()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionWriterLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionWriterShort()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionWriterString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionWriterTimestamp()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testVectorExpressionWriterVarchar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testWriterDecimal(DecimalTypeInfo)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testWriterDouble(TypeInfo)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testWriterLong(TypeInfo)",4,4,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.testWriterText(TypeInfo)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.getSimpleLongBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.getVectorizedRowBatch1DecimalCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.getVectorizedRowBatch2DecimalCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testColOpScalarNumericFilterNullAndRepeatingLogic()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterDecimalColEqualDecimalScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterDecimalColGreaterEqualCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterDecimalColLessScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterDecimalColumnEqualDecimalColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterDecimalScalarEqualDecimalColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterDecimalScalarGreaterThanColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterDoubleBetween()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterDoubleIn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterDoubleNotBetween()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterLongBetween()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterLongColEqualLongScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterLongColGreaterLongColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterLongColLessLongColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterLongIn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterLongNotBetween()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterLongScalarLessLongColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterStringBetween()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterStringIn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterStringNotBetween()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterTimestampBetween()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.testFilterTimestampNotBetween()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.castTo(LongColumnVector,Type)",4,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.newRandom(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.newRandomLongColumnVector(int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.testDate()",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.testDateAddColCol()",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.testDateAddColCol(Type,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.testDateAddColScalar()",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.testDateAddColScalar(Type,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.testDateAddScalarCol()",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.testDateAddScalarCol(Type,boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.testDateDiffColCol()",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.testDateDiffColScalar()",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.testDateDiffScalarCol()",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.testDateSubColCol()",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.testDateSubColScalar()",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.testDateSubScalarCol()",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.testToDate()",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.toString(LongColumnVector)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.toString(long)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.toTimestamp(LongColumnVector)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.toTimestamp(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.validateDate(VectorizedRowBatch,Type,LongColumnVector)",3,3,6
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.validateDateAdd(VectorizedRowBatch,LongColumnVector,LongColumnVector,Type,boolean)",1,4,7
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.validateDateAdd(VectorizedRowBatch,Type,long,boolean,LongColumnVector)",1,4,6
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.validateDateAdd(VectorizedRowBatch,long,LongColumnVector,Type,boolean)",3,7,12
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.validateDateDiff(VectorizedRowBatch,LongColumnVector,LongColumnVector,Type,Type)",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.validateDateDiff(VectorizedRowBatch,LongColumnVector,long,Type,Type)",2,3,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.validateDateDiff(VectorizedRowBatch,long,Type,Type,LongColumnVector)",2,3,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.validateToDate(VectorizedRowBatch,Type,LongColumnVector)",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.getBatchThreeBooleanCols()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.testBooleanFiltersOnColumns()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.testBooleanNot()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.testDoubleInExpr()",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.testFilterExprAndExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.testFilterExprOrExpr()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.testFilterExprOrExprWithBatchReuse()",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.testFilterExprOrExprWithSelectInUse()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.testIsNotNullExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.testIsNullExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.testLongColAndLongCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.testLongColOrLongCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.testLongInExpr()",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.testSelectColumnIsNotNull()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorLogicalExpressions.testSelectColumnIsNull()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.equalsWithinTolerance(double,double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.getBatchForStringMath()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.getVectorizedRowBatchDoubleInDoubleOut()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.getVectorizedRowBatchDoubleInLongOut()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.getVectorizedRowBatchLongInDoubleOut()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.getVectorizedRowBatchLongInLongOut()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testRoundToDecimalPlaces()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorACos()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorASin()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorATan()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorAbs()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorBin()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorCeil()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorCos()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorDegrees()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorExp()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorFloor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorHex()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorLn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorLog10()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorLog2()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorLogBase()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorPosMod()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorPower()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorRadians()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorRand()",1,3,7
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorRound()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorSign()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorSin()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorSqrt()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.testVectorTan()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.equalsWithinTolerance(double,double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.getBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.getBatchSingleLongVectorPositiveNonZero()",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.getVectorizedRowBatchSingleLongVector(int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.testBooleanValuedLongIn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.testLongScalarDivide()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.testLongScalarModuloLongColNoNulls()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.testLongScalarSubtractLongColNoNulls()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.testLongScalarSubtractLongColWithNulls()",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.testLongScalarSubtractLongColWithRepeating()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorScalarColArithmetic.testScalarLongDivide()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.addMultiByteChars(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.makeStringBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.makeStringBatch2In1Out()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.makeStringBatchForColColCompare()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.makeStringBatchMixedCase()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.makeStringBatchMixedCharSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.makeTrimBatch()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testColConcatCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testColConcatScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testColLower()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testColUpper()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testLoadBytesColumnVectorByRef()",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testLoadBytesColumnVectorByValueLargeData()",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testRegex()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testScalarConcatCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringColCompareStringColFilter()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringColCompareStringColProjection()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringColCompareStringScalarFilter()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringColCompareStringScalarProjection()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringInExpr()",1,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLength()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLike()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikeMultiByte()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringLikePatternType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringScalarCompareStringCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testStringScalarCompareStringColProjection()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testSubstrStart()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testSubstrStartLen()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testVectorLTrim()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testVectorRTrim()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.testVectorTrim()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.compareToUDFDayOfMonthLong(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.compareToUDFHourLong(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.compareToUDFMinuteLong(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.compareToUDFMonthLong(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.compareToUDFSecondLong(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.compareToUDFUnixTimeStampLong(long,long)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.compareToUDFWeekOfYearLong(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.compareToUDFYearLong(long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.decodeTime(byte[])",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.encodeTime(long)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.getAllBoundaries()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.getLongWritable(TimestampWritable)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.getVectorizedRandomRowBatch(int,int,TestType)",4,4,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.getVectorizedRandomRowBatchLong2(int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.getVectorizedRandomRowBatchStringLong(int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.getVectorizedRowBatch(long[],int,TestType)",4,4,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.getVectorizedRowBatchLong2(long[],int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.getVectorizedRowBatchStringLong(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.getVectorizedRowBatchStringLong(long[],int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.main(String[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.readVectorElementAt(ColumnVector,int)",3,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFDayOfMonth(TestType)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFDayOfMonthLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFDayOfMonthString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFHour(TestType)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFHourLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFHourString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFMinute(TestType)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFMinuteLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFMinuteString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFMonth(TestType)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFMonthLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFMonthString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFSecond(TestType)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFSecondLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFSecondString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFUnixTimeStamp(TestType)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFUnixTimeStampLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFUnixTimeStampString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFWeekOfYear(TestType)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFWeekOfYearLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFWeekOfYearString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFYear(TestType)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFYearLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFYearString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.toTimestampWritable(long)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.verifyUDFDayOfMonth(VectorizedRowBatch,TestType)",1,5,6
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.verifyUDFHour(VectorizedRowBatch,TestType)",1,5,6
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.verifyUDFMinute(VectorizedRowBatch,TestType)",1,5,6
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.verifyUDFMonth(VectorizedRowBatch,TestType)",1,5,6
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.verifyUDFSecond(VectorizedRowBatch,TestType)",1,5,6
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.verifyUDFUnixTimeStamp(VectorizedRowBatch,TestType)",1,5,6
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.verifyUDFWeekOfYear(VectorizedRowBatch,TestType)",1,4,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.verifyUDFYear(VectorizedRowBatch,TestType)",1,5,6
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.getBatchDecimalDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.getBatchDecimalDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.getBatchDecimalLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.getBatchDecimalLong2()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.getBatchDecimalString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.getBatchDoubleDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.getBatchLongDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.getBatchLongDecimalPrec5Scale2()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.getBatchStringDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastBooleanToString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastDecimalToBoolean()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastDecimalToDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastDecimalToDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastDecimalToLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastDecimalToString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastDecimalToTimestamp()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastDoubleToBoolean()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastDoubleToDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastDoubleToTimestamp()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastLongToBoolean()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastLongToDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastLongToString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastLongToTimestamp()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastStringToDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastTimestampToDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastTimestampToDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastTimestampToLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testVectorCastDoubleToLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testVectorCastLongToDouble()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.toBytes(String)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.VectorCoalesce()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.VectorCoalesce(int[],int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.evaluate(VectorizedRowBatch)",11,11,20
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.getInputColumns()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.setInputColumns(int[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorElt.VectorElt()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorElt.VectorElt(int[],int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorElt.evaluate(VectorizedRowBatch)",2,13,19
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorElt.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorElt.getInputColumns()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorElt.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorElt.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorElt.setInputColumns(int[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorElt.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.Type.getValue(String)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.evaluateChildren(VectorizedRowBatch)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.getChildExpressions()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.getInputTypes()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.setChildExpressions(VectorExpression[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.setInputTypes(Type...)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.setOutputType(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.toString()",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterBase.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterBase.init(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterBase.setValue(Object,Decimal128)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterBase.setValue(Object,byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterBase.setValue(Object,double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterBase.setValue(Object,long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterBase.writeValue(Decimal128)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterBase.writeValue(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterBase.writeValue(double)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterBase.writeValue(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterBytes.setValue(Object,ColumnVector,int)",7,6,17
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterBytes.writeValue(ColumnVector,int)",7,6,17
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterDecimal.setValue(Object,ColumnVector,int)",7,6,17
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterDecimal.writeValue(ColumnVector,int)",7,5,17
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterDouble.setValue(Object,ColumnVector,int)",7,6,17
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterDouble.writeValue(ColumnVector,int)",7,6,17
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterLong.setValue(Object,ColumnVector,int)",7,6,17
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterLong.writeValue(ColumnVector,int)",7,6,17
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterSetter.init(SettableStructObjectInspector,StructField,VectorExpressionWriter)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterSetter.initValue(Object)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterSetter.setValue(Object,ColumnVector,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.VectorExpressionWriterSetter.writeValue(ColumnVector,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritable(ExprNodeDesc)",2,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritable(ObjectInspector)",19,19,19
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritable(SettableStructObjectInspector,StructField,VectorExpressionWriter)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritableBinary(SettableBinaryObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritableBoolean(SettableBooleanObjectInspector)",1,2,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritableByte(SettableByteObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritableDate(SettableDateObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritableDecimal(SettableHiveDecimalObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritableDouble(SettableDoubleObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritableFloat(SettableFloatObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritableInt(SettableIntObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritableLong(SettableLongObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritableShort(SettableShortObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritableString(SettableStringObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritableTimestamp(SettableTimestampObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritableVarchar(SettableHiveVarcharObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorExpressionWritableVoid(VoidObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.genVectorStructExpressionWritables(StructObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.getExpressionWriters(List<ExprNodeDesc>)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.getExpressionWriters(StructObjectInspector)",2,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.getSettableExpressionWriters(SettableStructObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.processVectorExpressions(List<ExprNodeDesc>,List<String>,SingleOIDClosure)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.processVectorExpressions(List<ExprNodeDesc>,ListOIDClosure)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.VectorUDFDateAddColCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.VectorUDFDateAddColCol(int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.evaluate(VectorizedRowBatch)",3,15,21
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.evaluateDate(ColumnVector,int,long)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.evaluateString(BytesColumnVector,LongColumnVector,BytesColumnVector,int)",1,3,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.evaluateTimestamp(ColumnVector,int,long)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.getColNum1()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.getColNum2()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.setColNum1(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.setColNum2(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.VectorUDFDateAddColScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.VectorUDFDateAddColScalar(int,long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.evaluate(VectorizedRowBatch)",3,30,34
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.evaluateDate(ColumnVector,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.evaluateString(ColumnVector,BytesColumnVector,int)",1,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.evaluateTimestamp(ColumnVector,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.getNumDays()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.setNumDay(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.VectorUDFDateAddScalarCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.VectorUDFDateAddScalarCol(Object,int,int)",1,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.evaluate(Date,long,BytesColumnVector,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.evaluate(VectorizedRowBatch)",3,12,20
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.getLongValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.getStringValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.isPositive()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.setLongValue(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.setPositive(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.setStringValue(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.VectorUDFDateDiffColCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.VectorUDFDateDiffColCol(int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.copySelected(BytesColumnVector,boolean,int[],int,LongColumnVector)",2,13,15
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.evaluate(VectorizedRowBatch)",2,2,30
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.getColNum1()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.getColNum2()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.setColNum1(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.setColNum2(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.setDays(BytesColumnVector,LongColumnVector,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.toDateArray(VectorizedRowBatch,Type,ColumnVector,LongColumnVector)",3,8,12
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.VectorUDFDateDiffColScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.VectorUDFDateDiffColScalar(int,Object,int)",1,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.evaluate(VectorizedRowBatch)",4,31,41
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.evaluateDate(ColumnVector,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.evaluateString(ColumnVector,LongColumnVector,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.evaluateTimestamp(ColumnVector,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.getLongValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.getStringValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.setLongValue(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.setStringValue(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.VectorUDFDateDiffScalarCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.VectorUDFDateDiffScalarCol(Object,int,int)",1,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.evaluate(VectorizedRowBatch)",4,31,41
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.evaluateDate(ColumnVector,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.evaluateString(ColumnVector,LongColumnVector,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.evaluateTimestamp(ColumnVector,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.getLongValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.getStringValue()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.setLongValue(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.setStringValue(byte[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateLong.VectorUDFDateLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateLong.VectorUDFDateLong(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateLong.func(BytesColumnVector,long[],int)",2,2,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateString.VectorUDFDateString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateString.VectorUDFDateString(int,int)",2,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateSubColCol.VectorUDFDateSubColCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateSubColCol.VectorUDFDateSubColCol(int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateSubColScalar.VectorUDFDateSubColScalar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateSubColScalar.VectorUDFDateSubColScalar(int,long,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateSubScalarCol.VectorUDFDateSubScalarCol()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateSubScalarCol.VectorUDFDateSubScalarCol(Object,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDayOfMonthLong.VectorUDFDayOfMonthLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDayOfMonthLong.VectorUDFDayOfMonthLong(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDayOfMonthString.VectorUDFDayOfMonthString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDayOfMonthString.VectorUDFDayOfMonthString(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFHourLong.VectorUDFHourLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFHourLong.VectorUDFHourLong(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFHourString.VectorUDFHourString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFHourString.VectorUDFHourString(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMinuteLong.VectorUDFMinuteLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMinuteLong.VectorUDFMinuteLong(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMinuteString.VectorUDFMinuteString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMinuteString.VectorUDFMinuteString(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMonthLong.VectorUDFMonthLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMonthLong.VectorUDFMonthLong(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMonthLong.getDateField(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMonthLong.getTimestampField(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMonthString.VectorUDFMonthString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFMonthString.VectorUDFMonthString(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFSecondLong.VectorUDFSecondLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFSecondLong.VectorUDFSecondLong(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFSecondString.VectorUDFSecondString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFSecondString.VectorUDFSecondString(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.VectorUDFTimestampFieldLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.VectorUDFTimestampFieldLong(int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.evaluate(VectorizedRowBatch)",3,21,24
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.getDateField(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.getField()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.getTimestamp(long)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.getTimestampField(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.setField(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldLong.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.VectorUDFTimestampFieldString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.VectorUDFTimestampFieldString(int,int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.doGetField(byte[],int,int)",2,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.evaluate(VectorizedRowBatch)",2,11,17
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.getColNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.getField(byte[],int,int)",3,3,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.setColNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampLong.VectorUDFUnixTimeStampLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampLong.VectorUDFUnixTimeStampLong(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampLong.getDateField(long)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampLong.getTimestampField(long)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampString.VectorUDFUnixTimeStampString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampString.VectorUDFUnixTimeStampString(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampString.doGetField(byte[],int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFWeekOfYearLong.VectorUDFWeekOfYearLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFWeekOfYearLong.VectorUDFWeekOfYearLong(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFWeekOfYearLong.initCalendar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFWeekOfYearString.VectorUDFWeekOfYearString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFWeekOfYearString.VectorUDFWeekOfYearString(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFWeekOfYearString.doGetField(byte[],int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFWeekOfYearString.initCalendar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFYearLong.VectorUDFYearLong()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFYearLong.VectorUDFYearLong(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFYearLong.getTimestampField(long)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFYearString.VectorUDFYearString()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFYearString.VectorUDFYearString(int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.hasVariableSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.Aggregation.getVariableSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.Aggregation.reset()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.Aggregation.sumValueNoCheck(Decimal128,short)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.Aggregation.sumValueWithCheck(Decimal128,short)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.VectorUDAFAvgDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.VectorUDAFAvgDecimal(VectorExpression)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.aggregateInput(AggregationBuffer,VectorizedRowBatch)",3,7,9
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.aggregateInputSelection(VectorAggregationBufferRow[],int,VectorizedRowBatch)",2,7,8
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.evaluateOutput(AggregationBuffer)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.getAggregationBufferFixedSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.getCurrentAggregationBuffer(VectorAggregationBufferRow[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.getInputExpression()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.getOutputObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.init(AggregationDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.initPartialResultInspector()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.iterateHasNullsRepeatingSelectionWithAggregationSelection(VectorAggregationBufferRow[],int,Decimal128,int,int[],boolean[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.iterateHasNullsRepeatingWithAggregationSelection(VectorAggregationBufferRow[],int,Decimal128,int,boolean[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.iterateHasNullsSelectionWithAggregationSelection(VectorAggregationBufferRow[],int,Decimal128[],int,int[],boolean[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.iterateHasNullsWithAggregationSelection(VectorAggregationBufferRow[],int,Decimal128[],int,boolean[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.iterateNoNullsRepeatingWithAggregationSelection(VectorAggregationBufferRow[],int,Decimal128,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.iterateNoNullsSelectionWithAggregationSelection(VectorAggregationBufferRow[],int,Decimal128[],int[],int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.iterateNoNullsWithAggregationSelection(VectorAggregationBufferRow[],int,Decimal128[],int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.iterateNoSelectionHasNulls(Aggregation,Decimal128[],int,boolean[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.iterateNoSelectionNoNulls(Aggregation,Decimal128[],int)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.iterateSelectionHasNulls(Aggregation,Decimal128[],int,boolean[],int[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.iterateSelectionNoNulls(Aggregation,Decimal128[],int,int[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.setInputExpression(VectorExpression)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.Aggregation.getVariableSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.Aggregation.initIfNull()",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.Aggregation.reset()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.VectorUDAFCount()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.VectorUDAFCount(VectorExpression)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.aggregateInput(AggregationBuffer,VectorizedRowBatch)",4,3,7
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.aggregateInputSelection(VectorAggregationBufferRow[],int,VectorizedRowBatch)",2,4,5
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.evaluateOutput(AggregationBuffer)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.getAggregationBufferFixedSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.getCurrentAggregationBuffer(VectorAggregationBufferRow[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.getInputExpression()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.getOutputObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.init(AggregationDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.iterateHasNullsSelectionWithAggregationSelection(VectorAggregationBufferRow[],int,int,int[],boolean[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.iterateHasNullsWithAggregationSelection(VectorAggregationBufferRow[],int,int,boolean[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.iterateNoNullsWithAggregationSelection(VectorAggregationBufferRow[],int,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.iterateNoSelectionHasNulls(Aggregation,int,boolean[])",1,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.iterateSelectionHasNulls(Aggregation,int,boolean[],int[])",1,1,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.setInputExpression(VectorExpression)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.Aggregation.getVariableSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.Aggregation.reset()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.VectorUDAFCountStar()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.VectorUDAFCountStar(VectorExpression)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.aggregateInput(AggregationBuffer,VectorizedRowBatch)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.aggregateInputSelection(VectorAggregationBufferRow[],int,VectorizedRowBatch)",2,2,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.evaluateOutput(AggregationBuffer)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.getAggregationBufferFixedSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.getCurrentAggregationBuffer(VectorAggregationBufferRow[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.getOutputObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.init(AggregationDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.Aggregation.getVariableSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.Aggregation.reset()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.Aggregation.sumValue(Decimal128,short)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.VectorUDAFSumDecimal()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.VectorUDAFSumDecimal(VectorExpression)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.aggregateInput(AggregationBuffer,VectorizedRowBatch)",3,7,10
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.aggregateInputSelection(VectorAggregationBufferRow[],int,VectorizedRowBatch)",2,7,8
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.evaluateOutput(AggregationBuffer)",2,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.getAggregationBufferFixedSize()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.getCurrentAggregationBuffer(VectorAggregationBufferRow[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.getInputExpression()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.getOutputObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.init(AggregationDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.iterateHasNullsRepeatingSelectionWithAggregationSelection(VectorAggregationBufferRow[],int,Decimal128,short,int,int[],boolean[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.iterateHasNullsRepeatingWithAggregationSelection(VectorAggregationBufferRow[],int,Decimal128,short,int,boolean[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.iterateHasNullsSelectionWithAggregationSelection(VectorAggregationBufferRow[],int,Decimal128[],short,int,int[],boolean[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.iterateHasNullsWithAggregationSelection(VectorAggregationBufferRow[],int,Decimal128[],short,int,boolean[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.iterateNoNullsRepeatingWithAggregationSelection(VectorAggregationBufferRow[],int,Decimal128,short,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.iterateNoNullsSelectionWithAggregationSelection(VectorAggregationBufferRow[],int,Decimal128[],short,int[],int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.iterateNoNullsWithAggregationSelection(VectorAggregationBufferRow[],int,Decimal128[],short,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.iterateNoSelectionHasNulls(Aggregation,Decimal128[],short,int,boolean[])",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.iterateNoSelectionNoNulls(Aggregation,Decimal128[],short,int)",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.iterateSelectionHasNulls(Aggregation,Decimal128[],short,int,boolean[],int[])",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.iterateSelectionNoNulls(Aggregation,Decimal128[],short,int,int[])",1,3,3
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.setInputExpression(VectorExpression)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.TestVectorUDFAdaptor.getBatchLongInLongOut()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.TestVectorUDFAdaptor.getBatchStrDblLongWithStrOut()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.TestVectorUDFAdaptor.testGenericUDF()",1,2,5
"org.apache.hadoop.hive.ql.exec.vector.udf.TestVectorUDFAdaptor.testLongUDF()",1,2,5
"org.apache.hadoop.hive.ql.exec.vector.udf.TestVectorUDFAdaptor.testMultiArgumentUDF()",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.VectorUDFAdaptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.VectorUDFAdaptor(ExprNodeGenericFuncDesc,int,String,VectorUDFArgDesc[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.allInputColsRepeating(VectorizedRowBatch)",4,3,5
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.evaluate(VectorizedRowBatch)",4,8,10
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.getArgDescs()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.getExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.getOutputColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.getOutputType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.getResultType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.init()",1,4,4
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.setArgDescs(VectorUDFArgDesc[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.setExpr(ExprNodeGenericFuncDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.setOutputCol(ColumnVector,int,Object)",12,23,25
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.setOutputColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.setResult(int,VectorizedRowBatch)",1,3,4
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.setResultType(String)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.VectorUDFArgDesc()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.getColumn()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.getColumnNum()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.getConstExpr()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.getDeferredJavaObject(int,VectorizedRowBatch,int,VectorExpressionWriter[])",2,2,3
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.getIsConstant()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.isConstant()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.isVariable()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.prepareConstant()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.setColumnNum(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.setConstExpr(ExprNodeConstantDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.setConstant(ExprNodeConstantDesc)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.setIsConstant(boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFArgDesc.setVariable(int)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.generic.GenericUDFIsNull.evaluate(DeferredObject[])",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.udf.generic.GenericUDFIsNull.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.udf.generic.GenericUDFIsNull.initialize(ObjectInspector[])",3,3,4
"org.apache.hadoop.hive.ql.exec.vector.udf.legacy.ConcatTextLongDoubleUDF.evaluate(Text,Long,Double)",2,1,4
"org.apache.hadoop.hive.ql.exec.vector.udf.legacy.LongUDF.evaluate(LongWritable)",2,1,2
"org.apache.hadoop.hive.ql.exec.vector.util.AllVectorTypesRecord.AllVectorTypesRecord(Byte,Short,Integer,Long,Float,Double,String,String,Timestamp,Timestamp,Boolean,Boolean)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.addCaptureOutputChild(Operator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.getCapturedRows()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.getOutputInspector()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.initializeOp(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.processOp(Object,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.setOutputInspector(OutputInspector)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperator.FakeVectorDataSourceOperator(Iterable<VectorizedRowBatch>)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperator.addFakeVectorDataSourceParent(Iterable<VectorizedRowBatch>,Operator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperator.getType()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperator.initializeOp(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperator.processOp(Object,int)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchBase.BatchIterator.getCurrentBatch()",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchBase.BatchIterator.hasNext()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchBase.BatchIterator.next()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchBase.BatchIterator.remove()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchBase.iterator()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromConcat.FakeVectorRowBatchFromConcat(Iterable<VectorizedRowBatch>...)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromConcat.produceNextBatch()",4,4,5
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromLongIterables.FakeVectorRowBatchFromLongIterables(int,Iterable<Long>...)",1,2,2
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromLongIterables.produceNextBatch()",4,3,8
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables.FakeVectorRowBatchFromObjectIterables(int,String[],Iterable<Object>...)",8,16,17
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables.getTypes()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables.produceNextBatch()",4,4,8
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromRepeats.FakeVectorRowBatchFromRepeats(Long[],int,int)",1,1,2
"org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromRepeats.produceNextBatch()",1,1,5
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.BatchGenerator.BatchGenerator()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.BatchGenerator.generateBatch(BatchDataDistribution)",3,7,13
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.BooleanBatchGenerator.generateRandomNonNullValue(Random)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.BooleanBatchGenerator.initializeFixedPointValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.ByteBatchGenerator.generateRandomNonNullValue(Random)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.ByteBatchGenerator.initializeFixedPointValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.DoubleBatchGenerator.generateRandomNonNullValue(Random)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.DoubleBatchGenerator.initializeFixedPointValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.FloatBatchGenerator.generateRandomNonNullValue(Random)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.FloatBatchGenerator.initializeFixedPointValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.IntegerBatchGenerator.generateRandomNonNullValue(Random)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.IntegerBatchGenerator.initializeFixedPointValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.LongBatchGenerator.generateRandomNonNullValue(Random)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.LongBatchGenerator.initializeFixedPointValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.ShortBatchGenerator.generateRandomNonNullValue(Random)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.ShortBatchGenerator.initializeFixedPointValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.StringBatchGenerator.generateRandomNonNullValue(Random)",3,3,6
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.StringBatchGenerator.initializeFixedPointValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.TimestampBatchGenerator.generateRandomNonNullValue(Random)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.TimestampBatchGenerator.initializeFixedPointValues()",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.OrcFileGenerator.generateOrcFile(Configuration,FileSystem,Path,Class)",5,5,7
"org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil.generateDecimalColumnVector(DecimalTypeInfo,boolean,boolean,int,Random)",1,5,8
"org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil.generateDoubleColumnVector(boolean,boolean,int,Random)",1,5,8
"org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil.generateLongColumnVector(boolean,boolean,int,Random)",1,5,8
"org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil.generateNullFrequency(Random)",1,1,1
"org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil.getVectorizedRowBatch(int,int,int)",1,1,3
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.HiveHistoryImpl(SessionState)",4,7,7
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.closeStream()",1,1,1
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.endQuery(String)",2,1,2
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.endTask(String,Task<? extends Serializable>)",2,1,2
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.finalize()",1,1,1
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.getHistFileName()",1,1,1
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.getRowCountTableName(String)",3,2,3
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.log(RecordTypes,Map<String, String>)",2,3,4
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.logPlanProgress(QueryPlan)",1,1,1
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.printRowCount(String)",2,2,3
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.progressTask(String,Task<? extends Serializable>)",2,1,2
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.setIdToTableMap(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.setQueryProperty(String,Keys,String)",2,1,2
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.setTaskCounters(String,String,Counters)",2,9,11
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.setTaskProperty(String,String,Keys,String)",2,1,2
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.startQuery(String,String)",2,1,2
"org.apache.hadoop.hive.ql.history.HiveHistoryImpl.startTask(String,Task<? extends Serializable>,String)",2,1,2
"org.apache.hadoop.hive.ql.history.HiveHistoryProxyHandler.getNoOpHiveHistoryProxy()",1,1,1
"org.apache.hadoop.hive.ql.history.HiveHistoryProxyHandler.invoke(Object,Method,Object[])",1,1,1
"org.apache.hadoop.hive.ql.history.HiveHistoryUtil.parseHiveHistory(String,Listener)",4,2,5
"org.apache.hadoop.hive.ql.history.HiveHistoryUtil.parseLine(String,Listener)",1,2,2
"org.apache.hadoop.hive.ql.history.HiveHistoryViewer.HiveHistoryViewer(String)",1,1,1
"org.apache.hadoop.hive.ql.history.HiveHistoryViewer.getJobInfoMap()",1,1,1
"org.apache.hadoop.hive.ql.history.HiveHistoryViewer.getSessionId()",1,1,1
"org.apache.hadoop.hive.ql.history.HiveHistoryViewer.getTaskInfoMap()",1,1,1
"org.apache.hadoop.hive.ql.history.HiveHistoryViewer.handle(RecordTypes,Map<String, String>)",1,6,9
"org.apache.hadoop.hive.ql.history.HiveHistoryViewer.init()",1,2,2
"org.apache.hadoop.hive.ql.history.TestHiveHistory.setUp()",4,6,8
"org.apache.hadoop.hive.ql.history.TestHiveHistory.testHiveHistoryConfigDisabled()",1,1,1
"org.apache.hadoop.hive.ql.history.TestHiveHistory.testHiveHistoryConfigEnabled()",1,1,1
"org.apache.hadoop.hive.ql.history.TestHiveHistory.testQueryloglocParentDirNotExist()",1,2,4
"org.apache.hadoop.hive.ql.history.TestHiveHistory.testSimpleQuery()",1,7,8
"org.apache.hadoop.hive.ql.hooks.ATSHook.ATSHook()",1,2,3
"org.apache.hadoop.hive.ql.hooks.ATSHook.createPostHookEvent(String,long,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.hooks.ATSHook.createPreHookEvent(String,String,JSONObject,long,String,int,int)",1,2,2
"org.apache.hadoop.hive.ql.hooks.ATSHook.fireAndForget(Configuration,TimelineEntity)",1,1,1
"org.apache.hadoop.hive.ql.hooks.ATSHook.run(HookContext)",4,3,8
"org.apache.hadoop.hive.ql.hooks.CheckColumnAccessHook.run(HookContext)",4,3,6
"org.apache.hadoop.hive.ql.hooks.CheckQueryPropertiesHook.run(HookContext)",2,2,3
"org.apache.hadoop.hive.ql.hooks.CheckTableAccessHook.run(HookContext)",4,6,9
"org.apache.hadoop.hive.ql.hooks.DriverTestHook.postDriverRun(HiveDriverRunHookContext)",1,1,1
"org.apache.hadoop.hive.ql.hooks.DriverTestHook.preDriverRun(HiveDriverRunHookContext)",1,1,1
"org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables.run(HookContext)",1,1,1
"org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables.run(SessionState,Set<ReadEntity>,Set<WriteEntity>,UserGroupInformation)",5,7,8
"org.apache.hadoop.hive.ql.hooks.Entity.Entity()",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.Entity(Database,String,Type)",2,1,2
"org.apache.hadoop.hive.ql.hooks.Entity.Entity(Database,boolean)",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.Entity(DummyPartition,boolean)",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.Entity(Partition,boolean)",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.Entity(String,boolean,boolean)",1,1,2
"org.apache.hadoop.hive.ql.hooks.Entity.Entity(Table,boolean)",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.computeName()",8,7,8
"org.apache.hadoop.hive.ql.hooks.Entity.equals(Object)",3,2,3
"org.apache.hadoop.hive.ql.hooks.Entity.getD()",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.getDatabase()",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.getFunctionName()",2,1,2
"org.apache.hadoop.hive.ql.hooks.Entity.getLocation()",5,6,9
"org.apache.hadoop.hive.ql.hooks.Entity.getName()",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.getP()",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.getParameters()",2,2,2
"org.apache.hadoop.hive.ql.hooks.Entity.getPartition()",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.getT()",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.getTable()",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.getTyp()",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.getType()",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.isComplete()",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.isDummy()",3,3,3
"org.apache.hadoop.hive.ql.hooks.Entity.setComplete(boolean)",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.setD(String)",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.setDatabase(Database)",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.setFunctionName(String)",2,1,2
"org.apache.hadoop.hive.ql.hooks.Entity.setName(String)",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.setP(Partition)",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.setT(Table)",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.setTyp(Type)",1,1,1
"org.apache.hadoop.hive.ql.hooks.Entity.toString()",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.HookContext(QueryPlan,HiveConf,Map<String, ContentSummary>,String,String)",1,2,2
"org.apache.hadoop.hive.ql.hooks.HookContext.addCompleteTask(TaskRunner)",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.getCompleteTaskList()",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.getConf()",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.getHookType()",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.getInputPathToContentSummary()",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.getInputs()",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.getIpAddress()",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.getLinfo()",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.getOperationName()",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.getOutputs()",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.getQueryPlan()",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.getUgi()",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.getUserName()",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.setCompleteTaskList(List<TaskRunner>)",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.setConf(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.setHookType(HookType)",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.setInputs(Set<ReadEntity>)",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.setLinfo(LineageInfo)",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.setOutputs(Set<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.setQueryPlan(QueryPlan)",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookContext.setUgi(UserGroupInformation)",1,1,1
"org.apache.hadoop.hive.ql.hooks.HookUtils.getHooks(HiveConf,ConfVars,Class<T>)",3,2,4
"org.apache.hadoop.hive.ql.hooks.LineageInfo.BaseColumnInfo.getColumn()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.BaseColumnInfo.getTabAlias()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.BaseColumnInfo.setColumn(FieldSchema)",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.BaseColumnInfo.setTabAlias(TableAliasInfo)",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.BaseColumnInfo.toString()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.DataContainer.DataContainer(Table)",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.DataContainer.DataContainer(Table,Partition)",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.DataContainer.getPartition()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.DataContainer.getTable()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.DataContainer.isPartition()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.DataContainer.toString()",1,2,2
"org.apache.hadoop.hive.ql.hooks.LineageInfo.Dependency.getBaseCols()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.Dependency.getExpr()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.Dependency.getType()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.Dependency.setBaseCols(List<BaseColumnInfo>)",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.Dependency.setExpr(String)",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.Dependency.setType(DependencyType)",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.Dependency.toString()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.DependencyKey.DependencyKey(DataContainer,FieldSchema)",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.DependencyKey.equals(Object)",6,1,6
"org.apache.hadoop.hive.ql.hooks.LineageInfo.DependencyKey.getDataContainer()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.DependencyKey.getFieldSchema()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.DependencyKey.hashCode()",1,3,3
"org.apache.hadoop.hive.ql.hooks.LineageInfo.DependencyKey.toString()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.LineageInfo()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.TableAliasInfo.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.TableAliasInfo.getTable()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.TableAliasInfo.setAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.TableAliasInfo.setTable(Table)",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.TableAliasInfo.toString()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.clear()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.entrySet()",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.getDependency(DataContainer,FieldSchema)",1,1,1
"org.apache.hadoop.hive.ql.hooks.LineageInfo.putDependency(DataContainer,FieldSchema,Dependency)",1,1,1
"org.apache.hadoop.hive.ql.hooks.MapJoinCounterHook.run(HookContext)",3,3,9
"org.apache.hadoop.hive.ql.hooks.PostExecutePrinter.DependencyKeyComp.compare(Entry<DependencyKey, Dependency>,Entry<DependencyKey, Dependency>)",9,9,15
"org.apache.hadoop.hive.ql.hooks.PostExecutePrinter.run(HookContext)",1,1,1
"org.apache.hadoop.hive.ql.hooks.PostExecutePrinter.run(SessionState,Set<ReadEntity>,Set<WriteEntity>,LineageInfo,UserGroupInformation)",5,8,10
"org.apache.hadoop.hive.ql.hooks.PreExecutePrinter.printEntities(LogHelper,Set<?>,String)",1,3,3
"org.apache.hadoop.hive.ql.hooks.PreExecutePrinter.run(HookContext)",1,3,4
"org.apache.hadoop.hive.ql.hooks.PreExecutePrinter.run(SessionState,Set<ReadEntity>,Set<WriteEntity>,UserGroupInformation)",2,2,3
"org.apache.hadoop.hive.ql.hooks.PrintCompletedTasksHook.run(HookContext)",1,5,5
"org.apache.hadoop.hive.ql.hooks.ReadEntity.ReadEntity()",1,1,1
"org.apache.hadoop.hive.ql.hooks.ReadEntity.ReadEntity(Database)",1,1,1
"org.apache.hadoop.hive.ql.hooks.ReadEntity.ReadEntity(Partition)",1,1,1
"org.apache.hadoop.hive.ql.hooks.ReadEntity.ReadEntity(Partition,ReadEntity)",1,1,1
"org.apache.hadoop.hive.ql.hooks.ReadEntity.ReadEntity(Partition,ReadEntity,boolean)",1,1,1
"org.apache.hadoop.hive.ql.hooks.ReadEntity.ReadEntity(Path,boolean)",1,1,1
"org.apache.hadoop.hive.ql.hooks.ReadEntity.ReadEntity(Table)",1,1,1
"org.apache.hadoop.hive.ql.hooks.ReadEntity.ReadEntity(Table,ReadEntity)",1,1,1
"org.apache.hadoop.hive.ql.hooks.ReadEntity.ReadEntity(Table,ReadEntity,boolean)",1,1,1
"org.apache.hadoop.hive.ql.hooks.ReadEntity.equals(Object)",3,2,3
"org.apache.hadoop.hive.ql.hooks.ReadEntity.getAccessedColumns()",1,1,1
"org.apache.hadoop.hive.ql.hooks.ReadEntity.getParents()",1,1,1
"org.apache.hadoop.hive.ql.hooks.ReadEntity.initParent(ReadEntity)",1,2,2
"org.apache.hadoop.hive.ql.hooks.ReadEntity.isDirect()",1,1,1
"org.apache.hadoop.hive.ql.hooks.ReadEntity.needsLock()",1,1,1
"org.apache.hadoop.hive.ql.hooks.ReadEntity.noLockNeeded()",1,1,1
"org.apache.hadoop.hive.ql.hooks.ReadEntity.setDirect(boolean)",1,1,1
"org.apache.hadoop.hive.ql.hooks.ShowMapredStatsHook.run(HookContext)",1,4,4
"org.apache.hadoop.hive.ql.hooks.UpdateInputAccessTimeHook.PreExec.run(SessionState,Set<ReadEntity>,Set<WriteEntity>,UserGroupInformation)",3,6,7
"org.apache.hadoop.hive.ql.hooks.VerifyCachingPrintStreamHook.run(HookContext)",1,3,3
"org.apache.hadoop.hive.ql.hooks.VerifyContentSummaryCacheHook.run(HookContext)",1,5,5
"org.apache.hadoop.hive.ql.hooks.VerifyHiveSortedInputFormatUsedHook.run(HookContext)",1,4,4
"org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder.RunFirst.run(HookContext)",2,1,3
"org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder.RunFirstDriverRunHook.postDriverRun(HiveDriverRunHookContext)",2,1,2
"org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder.RunFirstDriverRunHook.preDriverRun(HiveDriverRunHookContext)",2,1,2
"org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder.RunFirstSemanticAnalysisHook.postAnalyze(HiveSemanticAnalyzerHookContext,List<Task<? extends Serializable>>)",2,1,2
"org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder.RunFirstSemanticAnalysisHook.preAnalyze(HiveSemanticAnalyzerHookContext,ASTNode)",2,1,2
"org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder.RunSecond.run(HookContext)",2,2,3
"org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder.RunSecondDriverRunHook.postDriverRun(HiveDriverRunHookContext)",2,1,2
"org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder.RunSecondDriverRunHook.preDriverRun(HiveDriverRunHookContext)",2,1,2
"org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder.RunSecondSemanticAnalysisHook.postAnalyze(HiveSemanticAnalyzerHookContext,List<Task<? extends Serializable>>)",2,1,2
"org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder.RunSecondSemanticAnalysisHook.preAnalyze(HiveSemanticAnalyzerHookContext,ASTNode)",2,1,2
"org.apache.hadoop.hive.ql.hooks.VerifyIsLocalModeHook.run(HookContext)",1,4,4
"org.apache.hadoop.hive.ql.hooks.VerifyNumReducersHook.run(HookContext)",1,1,1
"org.apache.hadoop.hive.ql.hooks.VerifyOutputTableLocationSchemeIsFileHook.run(HookContext)",1,3,3
"org.apache.hadoop.hive.ql.hooks.VerifyOverriddenConfigsHook.run(HookContext)",2,3,5
"org.apache.hadoop.hive.ql.hooks.VerifyPartitionIsNotSubdirectoryOfTableHook.run(HookContext)",1,5,5
"org.apache.hadoop.hive.ql.hooks.VerifyPartitionIsNotSubdirectoryOfTableHook.verify(Partition,Table)",1,1,1
"org.apache.hadoop.hive.ql.hooks.VerifyPartitionIsSubdirectoryOfTableHook.run(HookContext)",1,5,5
"org.apache.hadoop.hive.ql.hooks.VerifyPartitionIsSubdirectoryOfTableHook.verify(Partition,Table)",1,1,1
"org.apache.hadoop.hive.ql.hooks.VerifySessionStateLocalErrorsHook.run(HookContext)",1,3,3
"org.apache.hadoop.hive.ql.hooks.VerifySessionStateStackTracesHook.run(HookContext)",1,3,3
"org.apache.hadoop.hive.ql.hooks.VerifyTableDirectoryIsEmptyHook.run(HookContext)",1,2,2
"org.apache.hadoop.hive.ql.hooks.WriteEntity.WriteEntity()",1,1,1
"org.apache.hadoop.hive.ql.hooks.WriteEntity.WriteEntity(Database,String,Type,WriteType)",1,1,1
"org.apache.hadoop.hive.ql.hooks.WriteEntity.WriteEntity(Database,WriteType)",1,1,1
"org.apache.hadoop.hive.ql.hooks.WriteEntity.WriteEntity(DummyPartition,WriteType,boolean)",1,1,1
"org.apache.hadoop.hive.ql.hooks.WriteEntity.WriteEntity(Partition,WriteType)",1,1,1
"org.apache.hadoop.hive.ql.hooks.WriteEntity.WriteEntity(Path,boolean)",1,1,1
"org.apache.hadoop.hive.ql.hooks.WriteEntity.WriteEntity(Path,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.hooks.WriteEntity.WriteEntity(Table,WriteType)",1,1,1
"org.apache.hadoop.hive.ql.hooks.WriteEntity.WriteEntity(Table,WriteType,boolean)",1,1,1
"org.apache.hadoop.hive.ql.hooks.WriteEntity.determineAlterTableWriteType(AlterTableTypes)",5,2,5
"org.apache.hadoop.hive.ql.hooks.WriteEntity.equals(Object)",3,2,3
"org.apache.hadoop.hive.ql.hooks.WriteEntity.getWriteType()",1,1,1
"org.apache.hadoop.hive.ql.hooks.WriteEntity.isTempURI()",1,1,1
"org.apache.hadoop.hive.ql.index.AbstractIndexHandler.checkQuerySize(long,HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.index.AbstractIndexHandler.generateIndexQuery(Index,ExprNodeDesc,ParseContext,HiveIndexQueryContext)",1,1,1
"org.apache.hadoop.hive.ql.index.AbstractIndexHandler.getColumnNames(List<FieldSchema>)",1,3,3
"org.apache.hadoop.hive.ql.index.AggregateIndexHandler.analyzeIndexDefinition(Table,Index,Table)",1,7,7
"org.apache.hadoop.hive.ql.index.AggregateIndexHandler.createAggregationFunction(List<FieldSchema>,String)",1,2,2
"org.apache.hadoop.hive.ql.index.AggregateIndexHandler.getIndexBuilderMapRedTask(Set<ReadEntity>,Set<WriteEntity>,Index,boolean,PartitionDesc,String,PartitionDesc,String,String)",1,9,10
"org.apache.hadoop.hive.ql.index.HiveIndex.IndexType.IndexType(String,String)",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndex.IndexType.getHandlerClsName()",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndex.IndexType.getName()",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndex.getIndexType(String)",3,2,3
"org.apache.hadoop.hive.ql.index.HiveIndex.getIndexTypeByClassName(String)",3,2,3
"org.apache.hadoop.hive.ql.index.HiveIndexQueryContext.HiveIndexQueryContext()",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexQueryContext.addAdditionalSemanticInputs(HashSet<ReadEntity>)",1,1,2
"org.apache.hadoop.hive.ql.index.HiveIndexQueryContext.getAdditionalSemanticInputs()",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexQueryContext.getIndexInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexQueryContext.getIndexIntermediateFile()",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexQueryContext.getQueryPartitions()",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexQueryContext.getQueryTasks()",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexQueryContext.getResidualPredicate()",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexQueryContext.setIndexInputFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexQueryContext.setIndexIntermediateFile(String)",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexQueryContext.setQueryPartitions(Set<Partition>)",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexQueryContext.setQueryTasks(List<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexQueryContext.setResidualPredicate(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexResult.HiveIndexResult(List<String>,JobConf)",5,8,10
"org.apache.hadoop.hive.ql.index.HiveIndexResult.IBucket.IBucket(String)",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexResult.IBucket.add(Long)",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexResult.IBucket.equals(Object)",2,1,2
"org.apache.hadoop.hive.ql.index.HiveIndexResult.IBucket.getName()",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexResult.IBucket.getOffsets()",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexResult.add(Text)",2,6,8
"org.apache.hadoop.hive.ql.index.HiveIndexResult.contains(FileSplit)",6,4,7
"org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.HiveIndexedInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.HiveIndexedInputFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.doGetSplits(JobConf,int)",2,3,4
"org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.getIndexFiles(String)",2,1,2
"org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.getSplits(JobConf,int)",8,12,16
"org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask.execute(DriverContext)",6,8,8
"org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.index.IndexMetadataChangeWork.IndexMetadataChangeWork()",1,1,1
"org.apache.hadoop.hive.ql.index.IndexMetadataChangeWork.IndexMetadataChangeWork(HashMap<String, String>,String,String)",1,1,1
"org.apache.hadoop.hive.ql.index.IndexMetadataChangeWork.getDbName()",1,1,1
"org.apache.hadoop.hive.ql.index.IndexMetadataChangeWork.getIndexTbl()",1,1,1
"org.apache.hadoop.hive.ql.index.IndexMetadataChangeWork.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.index.IndexMetadataChangeWork.setDbName(String)",1,1,1
"org.apache.hadoop.hive.ql.index.IndexMetadataChangeWork.setIndexTbl(String)",1,1,1
"org.apache.hadoop.hive.ql.index.IndexMetadataChangeWork.setPartSpec(HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.IndexPredicateAnalyzer()",1,1,1
"org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.addComparisonOp(String)",1,1,1
"org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.allowColumnName(String)",1,1,1
"org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.analyzeExpr(ExprNodeGenericFuncDesc,List<IndexSearchCondition>,Object...)",10,4,14
"org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.analyzePredicate(ExprNodeDesc,List<IndexSearchCondition>)",4,2,5
"org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.clearAllowedColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.createAnalyzer(boolean)",2,1,2
"org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.isValidField(ExprNodeFieldDesc)",1,2,2
"org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.setAcceptsFields(boolean)",1,1,1
"org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.setFieldValidator(FieldValidator)",1,1,1
"org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.translateSearchConditions(List<IndexSearchCondition>)",3,3,3
"org.apache.hadoop.hive.ql.index.IndexSearchCondition.IndexSearchCondition(ExprNodeColumnDesc,String,ExprNodeConstantDesc,ExprNodeGenericFuncDesc)",1,1,1
"org.apache.hadoop.hive.ql.index.IndexSearchCondition.IndexSearchCondition(ExprNodeColumnDesc,String,ExprNodeConstantDesc,ExprNodeGenericFuncDesc,String[])",1,1,1
"org.apache.hadoop.hive.ql.index.IndexSearchCondition.getColumnDesc()",1,1,1
"org.apache.hadoop.hive.ql.index.IndexSearchCondition.getComparisonExpr()",1,1,1
"org.apache.hadoop.hive.ql.index.IndexSearchCondition.getComparisonOp()",1,1,1
"org.apache.hadoop.hive.ql.index.IndexSearchCondition.getConstantDesc()",1,1,1
"org.apache.hadoop.hive.ql.index.IndexSearchCondition.getFields()",1,1,1
"org.apache.hadoop.hive.ql.index.IndexSearchCondition.setColumnDesc(ExprNodeColumnDesc)",1,1,1
"org.apache.hadoop.hive.ql.index.IndexSearchCondition.setComparisonExpr(ExprNodeGenericFuncDesc)",1,1,1
"org.apache.hadoop.hive.ql.index.IndexSearchCondition.setComparisonOp(String)",1,1,1
"org.apache.hadoop.hive.ql.index.IndexSearchCondition.setConstantDesc(ExprNodeConstantDesc)",1,1,1
"org.apache.hadoop.hive.ql.index.IndexSearchCondition.toString()",1,1,1
"org.apache.hadoop.hive.ql.index.TableBasedIndexHandler.generateIndexBuildTaskList(Table,Index,List<Partition>,List<Partition>,Table,Set<ReadEntity>,Set<WriteEntity>)",6,5,7
"org.apache.hadoop.hive.ql.index.TableBasedIndexHandler.getConf()",1,1,1
"org.apache.hadoop.hive.ql.index.TableBasedIndexHandler.getIndexBuilderMapRedTask(Set<ReadEntity>,Set<WriteEntity>,Index,boolean,PartitionDesc,String,PartitionDesc,String,String)",1,1,1
"org.apache.hadoop.hive.ql.index.TableBasedIndexHandler.getIndexBuilderMapRedTask(Set<ReadEntity>,Set<WriteEntity>,List<FieldSchema>,boolean,PartitionDesc,String,PartitionDesc,String,String)",1,1,1
"org.apache.hadoop.hive.ql.index.TableBasedIndexHandler.getPartKVPairStringArray(LinkedHashMap<String, String>)",1,2,2
"org.apache.hadoop.hive.ql.index.TableBasedIndexHandler.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.index.TableBasedIndexHandler.setStatsDir(HiveConf)",1,2,2
"org.apache.hadoop.hive.ql.index.TableBasedIndexHandler.usesIndexTable()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.analyzeIndexDefinition(Table,Index,Table)",1,3,3
"org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.checkQuerySize(long,HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.decomposePredicate(ExprNodeDesc,List<Index>,HiveIndexQueryContext)",2,3,4
"org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.generateIndexQuery(List<Index>,ExprNodeDesc,ParseContext,HiveIndexQueryContext)",2,5,5
"org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.getIndexBuilderMapRedTask(Set<ReadEntity>,Set<WriteEntity>,List<FieldSchema>,boolean,PartitionDesc,String,PartitionDesc,String,String)",2,8,10
"org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.getIndexPredicateAnalyzer(List<Index>,Set<Partition>)",3,5,6
"org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.usesIndexTable()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapInnerQuery.BitmapInnerQuery(String,ExprNodeDesc,String)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapInnerQuery.constructQueryStr()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapInnerQuery.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapInnerQuery.toString()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.BitmapObjectInput()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.BitmapObjectInput(List<LongWritable>)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.available()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.close()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.read()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.read(byte[])",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.read(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readBoolean()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readByte()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readChar()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readDouble()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readFloat()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readFromList(List<LongWritable>)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readFully(byte[])",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readFully(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readInt()",2,2,2
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readLine()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readLong()",2,2,2
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readObject()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readShort()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readUTF()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readUnsignedByte()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.readUnsignedShort()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.skip(long)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectInput.skipBytes(int)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.close()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.flush()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.list()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.write(byte[])",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.write(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.write(int)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.writeBoolean(boolean)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.writeByte(int)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.writeBytes(String)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.writeChar(int)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.writeChars(String)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.writeDouble(double)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.writeFloat(float)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.writeInt(int)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.writeLong(long)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.writeObject(Object)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.writeShort(int)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapObjectOutput.writeUTF(String)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapOuterQuery.BitmapOuterQuery(String,BitmapQuery,BitmapQuery)",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapOuterQuery.constructQueryStr()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapOuterQuery.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.index.bitmap.BitmapOuterQuery.toString()",1,1,1
"org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.analyzeIndexDefinition(Table,Index,Table)",1,3,3
"org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.checkQuerySize(long,HiveConf)",1,1,2
"org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.decomposePredicate(ExprNodeDesc,Index,Set<Partition>)",2,2,5
"org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.findIndexColumnExprNodeDesc(ExprNodeDesc)",6,7,9
"org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.findIndexColumnFilter(Collection<Operator<? extends OperatorDesc>>)",5,5,6
"org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.generateIndexQuery(List<Index>,ExprNodeDesc,ParseContext,HiveIndexQueryContext)",7,12,13
"org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.getIndexBuilderMapRedTask(Set<ReadEntity>,Set<WriteEntity>,List<FieldSchema>,boolean,PartitionDesc,String,PartitionDesc,String,String)",1,7,8
"org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.getIndexPredicateAnalyzer(Index,Set<Partition>)",3,4,5
"org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.usesIndexTable()",1,1,1
"org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat.HiveCompactIndexInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.AbstractStorageFormatDescriptor.getSerde()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidInputFormat.Options.Options(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidInputFormat.Options.getConfiguration()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidInputFormat.Options.getReporter()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidInputFormat.Options.reporter(Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.Options(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.bucket(int)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.filesystem(FileSystem)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.getBucket()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.getConfiguration()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.getDummyStream()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.getFilesystem()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.getInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.getMaximumTransactionId()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.getMinimumTransactionId()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.getOldStyle()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.getReporter()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.getTableProperties()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.inspector(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.isCompressed()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.isCompressed(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.isWritingBase()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.maximumTransactionId(long)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.minimumTransactionId(long)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.reporter(Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.setOldStyle(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.tableProperties(Properties)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.useDummy(PrintStream)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidOutputFormat.Options.writingBase(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidUtils.AcidUtils()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDelta.ParsedDelta(long,long,FileStatus)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDelta.compareTo(ParsedDelta)",5,3,5
"org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDelta.getMaxTransaction()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDelta.getMinTransaction()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDelta.getPath()",1,1,1
"org.apache.hadoop.hive.ql.io.AcidUtils.createBucketFile(Path,int)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidUtils.createFilename(Path,Options)",2,3,3
"org.apache.hadoop.hive.ql.io.AcidUtils.deltaSubdir(long,long)",1,1,1
"org.apache.hadoop.hive.ql.io.AcidUtils.deserializeDeltas(Path,List<Long>)",1,2,2
"org.apache.hadoop.hive.ql.io.AcidUtils.findOriginals(FileSystem,FileStatus,List<FileStatus>)",1,3,3
"org.apache.hadoop.hive.ql.io.AcidUtils.getAcidState(Path,Configuration,ValidTxnList)",1,14,14
"org.apache.hadoop.hive.ql.io.AcidUtils.getPaths(List<ParsedDelta>)",1,2,2
"org.apache.hadoop.hive.ql.io.AcidUtils.parseBase(Path)",2,2,2
"org.apache.hadoop.hive.ql.io.AcidUtils.parseBaseBucketFilename(Path,Configuration)",1,3,3
"org.apache.hadoop.hive.ql.io.AcidUtils.parseDelta(FileStatus)",2,2,2
"org.apache.hadoop.hive.ql.io.AcidUtils.serializeDeltas(List<ParsedDelta>)",1,2,2
"org.apache.hadoop.hive.ql.io.AvroStorageFormatDescriptor.getInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.AvroStorageFormatDescriptor.getNames()",1,1,1
"org.apache.hadoop.hive.ql.io.AvroStorageFormatDescriptor.getOutputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.AvroStorageFormatDescriptor.getSerde()",1,1,1
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,2
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.getSplits(JobConf,int)",4,6,9
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.listStatus(JobConf,Path)",2,5,6
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.BucketizedHiveInputSplit()",1,1,1
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.BucketizedHiveInputSplit(InputSplit[],String)",1,1,2
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.getInputFormatClassName()",1,1,1
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.getLength()",2,3,4
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.getLength(int)",2,2,3
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.getLocations()",1,1,2
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.getNumSplits()",1,1,1
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.getPath()",2,2,4
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.getSplit(int)",1,1,2
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.getStart()",2,2,4
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.inputFormatClassName()",1,1,1
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.readFields(DataInput)",2,3,3
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.setInputFormatClassName(String)",1,1,1
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.toString()",2,2,3
"org.apache.hadoop.hive.ql.io.BucketizedHiveInputSplit.write(DataOutput)",1,2,3
"org.apache.hadoop.hive.ql.io.BucketizedHiveRecordReader.BucketizedHiveRecordReader(InputFormat,BucketizedHiveInputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.BucketizedHiveRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.BucketizedHiveRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.ql.io.BucketizedHiveRecordReader.doClose()",1,2,2
"org.apache.hadoop.hive.ql.io.BucketizedHiveRecordReader.doNext(K,V)",3,3,4
"org.apache.hadoop.hive.ql.io.BucketizedHiveRecordReader.doNextWithExceptionHandler(K,V)",1,1,1
"org.apache.hadoop.hive.ql.io.BucketizedHiveRecordReader.getPos()",2,2,2
"org.apache.hadoop.hive.ql.io.BucketizedHiveRecordReader.getProgress()",1,3,3
"org.apache.hadoop.hive.ql.io.BucketizedHiveRecordReader.initNextRecordReader()",2,5,6
"org.apache.hadoop.hive.ql.io.CodecPool.CodecPool()",1,1,1
"org.apache.hadoop.hive.ql.io.CodecPool.borrow(Map<Class<T>, List<T>>,Class<? extends T>)",1,4,4
"org.apache.hadoop.hive.ql.io.CodecPool.getCompressor(CompressionCodec)",1,2,2
"org.apache.hadoop.hive.ql.io.CodecPool.getDecompressor(CompressionCodec)",1,2,2
"org.apache.hadoop.hive.ql.io.CodecPool.payback(Map<Class<T>, List<T>>,T)",1,3,3
"org.apache.hadoop.hive.ql.io.CodecPool.returnCompressor(Compressor)",2,1,2
"org.apache.hadoop.hive.ql.io.CodecPool.returnDecompressor(Decompressor)",2,1,2
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineFilter.CombineFilter(Path)",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineFilter.accept(Path)",3,2,4
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineFilter.addPath(Path)",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineFilter.toString()",1,2,2
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.CombineHiveInputSplit()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.CombineHiveInputSplit(InputSplitShim)",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.CombineHiveInputSplit(JobConf,InputSplitShim)",1,3,3
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.getInputSplitShim()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.getJob()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.getLength()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.getLength(int)",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.getLengths()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.getLocations()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.getNumPaths()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.getOffset(int)",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.getPath(int)",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.getPaths()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.getStartOffsets()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.inputFormatClassName()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.setInputFormatClassName(String)",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.shrinkSplit(long)",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombineHiveInputSplit.write(DataOutput)",1,2,2
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombinePathInputFormat.CombinePathInputFormat(List<Operator<? extends OperatorDesc>>,String,String)",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombinePathInputFormat.equals(Object)",3,5,6
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.CombinePathInputFormat.hashCode()",1,2,2
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",2,2,3
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(JobConf,int)",17,27,31
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.processPaths(JobConf,CombineFileInputFormatShim,List<InputSplitShim>,Path...)",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.removeScheme(Map<String, ArrayList<String>>)",1,2,2
"org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.sampleSplits(List<InputSplitShim>)",7,13,15
"org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.CombineHiveRecordReader(InputSplit,Configuration,Reporter,Integer)",1,1,2
"org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doClose()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(K,V)",2,1,2
"org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.getProgress()",2,2,2
"org.apache.hadoop.hive.ql.io.DefaultHivePartitioner.getBucket(K2,V2,int)",1,1,1
"org.apache.hadoop.hive.ql.io.FlatFileInputFormat.FlatFileRecordReader.FlatFileRecordReader(Configuration,FileSplit)",1,2,2
"org.apache.hadoop.hive.ql.io.FlatFileInputFormat.FlatFileRecordReader.close()",1,1,1
"org.apache.hadoop.hive.ql.io.FlatFileInputFormat.FlatFileRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.FlatFileInputFormat.FlatFileRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.ql.io.FlatFileInputFormat.FlatFileRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.FlatFileInputFormat.FlatFileRecordReader.getProgress()",2,2,2
"org.apache.hadoop.hive.ql.io.FlatFileInputFormat.FlatFileRecordReader.next(Void,RowContainer<R>)",3,2,5
"org.apache.hadoop.hive.ql.io.FlatFileInputFormat.SerializationContextFromConf.getConf()",1,1,1
"org.apache.hadoop.hive.ql.io.FlatFileInputFormat.SerializationContextFromConf.getRealClass()",1,1,1
"org.apache.hadoop.hive.ql.io.FlatFileInputFormat.SerializationContextFromConf.getSerialization()",1,2,2
"org.apache.hadoop.hive.ql.io.FlatFileInputFormat.SerializationContextFromConf.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.FlatFileInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.FlatFileInputFormat.isSplittable(FileSystem,Path)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveBinaryOutputFormat.getHiveRecordWriter(JobConf,Path,Class<? extends Writable>,boolean,Properties,Progressable)",1,2,2
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.HiveContextAwareRecordReader(JobConf)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.HiveContextAwareRecordReader(RecordReader)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.HiveContextAwareRecordReader(RecordReader,JobConf)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.beginLinearSearch()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.close()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(K,V)",11,20,26
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.foundAllTargets()",2,2,3
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.getIOContext()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.getProgress()",2,2,2
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.getSyncedPosition()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.initIOContext(FileSplit,JobConf,Class)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.initIOContext(FileSplit,JobConf,Class,RecordReader)",1,4,4
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.initIOContext(long,boolean,Path)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.initIOContextSortedProps(FileSplit,RecordReader,JobConf)",1,2,4
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(K,V)",2,5,6
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.setGenericUDFClassName(String)",1,9,9
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.setRecordReader(RecordReader)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.sync(long)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.updateIOContext()",2,1,5
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.HiveFileFormatUtils()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.checkInputFormat(FileSystem,HiveConf,Class<? extends InputFormat>,ArrayList<FileStatus>)",4,6,7
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.checkTextInputFormat(FileSystem,HiveConf,ArrayList<FileStatus>)",3,2,3
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.doGetAliasesFromPath(Map<String, ArrayList<String>>,Path)",2,1,2
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.doGetPartitionDescFromPath(Map<String, PartitionDesc>,Path)",4,5,6
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.doGetWorksFromPath(Map<String, ArrayList<String>>,Map<String, Operator<? extends OperatorDesc>>,Path)",1,2,2
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.foundAlias(Map<String, ArrayList<String>>,String)",2,2,3
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(JobConf,TableDesc,Class<? extends Writable>,FileSinkDesc,Path,Reporter)",1,9,10
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getInputFormatChecker(Class<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getMatchingPath(Map<String, ArrayList<String>>,Path)",6,3,8
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getOutputFormatFinalPath(Path,String,JobConf,HiveOutputFormat<?, ?>,boolean,Path)",2,2,2
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getOutputFormatSubstitute(Class<?>,boolean)",2,2,4
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getPartitionDescFromPathRecursively(Map<String, PartitionDesc>,Path,Map<Map<String, PartitionDesc>, Map<String, PartitionDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getPartitionDescFromPathRecursively(Map<String, PartitionDesc>,Path,Map<Map<String, PartitionDesc>, Map<String, PartitionDesc>>,boolean)",2,10,10
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRealOutputFormatClassName()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(JobConf,HiveOutputFormat<?, ?>,Class<? extends Writable>,boolean,Properties,Path,Reporter)",2,2,2
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.pathsContainNoScheme(Map<String, PartitionDesc>)",3,2,3
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.populateNewPartitionDesc(Map<String, PartitionDesc>,Map<String, PartitionDesc>)",1,2,2
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.registerInputFormatChecker(Class<? extends InputFormat>,Class<? extends InputFormatChecker>)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.registerOutputFormatSubstitute(Class<? extends OutputFormat>,Class<? extends HiveOutputFormat>)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.setRealOutputFormatClassName(String)",2,1,2
"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.IgnoreKeyWriter.IgnoreKeyWriter(RecordWriter<K, V>)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.IgnoreKeyWriter.close(Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.IgnoreKeyWriter.write(K,V)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.getHiveRecordWriter(JobConf,Path,Class<? extends Writable>,boolean,Properties,Progressable)",1,3,3
"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit.HiveInputSplit()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit.HiveInputSplit(InputSplit,String)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit.getConf()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit.getInputSplit()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit.getLength()",1,1,2
"org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit.getLocations()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit.getPath()",2,2,2
"org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit.getStart()",2,2,2
"org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit.inputFormatClassName()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit.readFields(DataInput)",1,2,2
"org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveInputFormat.HiveInputSplit.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(List<Path>,TableScanOperator,JobConf,InputFormat,Class<? extends InputFormat>,int,TableDesc,List<InputSplit>)",1,5,6
"org.apache.hadoop.hive.ql.io.HiveInputFormat.configure(JobConf)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveInputFormat.getInputFormatFromCache(Class,JobConf)",2,3,3
"org.apache.hadoop.hive.ql.io.HiveInputFormat.getPartitionDescFromPath(Map<String, PartitionDesc>,Path)",2,3,3
"org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,5,6
"org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(JobConf,int)",5,14,16
"org.apache.hadoop.hive.ql.io.HiveInputFormat.init(JobConf)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveInputFormat.pushFilters(JobConf,TableScanOperator)",3,3,5
"org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(JobConf,Class,String,String)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(JobConf,Class,String,String,boolean)",2,10,11
"org.apache.hadoop.hive.ql.io.HiveKey.Comparator.Comparator()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveKey.Comparator.compare(byte[],int,int,byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveKey.HiveKey()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveKey.HiveKey(byte[],int)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveKey.getDistKeyLength()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveKey.hashCode()",2,1,2
"org.apache.hadoop.hive.ql.io.HiveKey.setDistKeyLength(int)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveKey.setHashCode(int)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat.getHiveRecordWriter(JobConf,Path,Class<? extends Writable>,boolean,Properties,Progressable)",1,2,2
"org.apache.hadoop.hive.ql.io.HiveOutputFormatImpl.checkOutputSpecs(FileSystem,JobConf)",1,3,3
"org.apache.hadoop.hive.ql.io.HiveOutputFormatImpl.getRecordWriter(FileSystem,JobConf,String,Progressable)",1,1,1
"org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat.HivePassThroughOutputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat.checkOutputSpecs(FileSystem,JobConf)",1,2,2
"org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat.createActualOF()",2,2,3
"org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat.getConf()",1,1,1
"org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat.getHiveRecordWriter(JobConf,Path,Class<? extends Writable>,boolean,Properties,Progressable)",2,3,3
"org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)",1,2,2
"org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat.setConf(Configuration)",1,2,2
"org.apache.hadoop.hive.ql.io.HivePassThroughRecordWriter.HivePassThroughRecordWriter(RecordWriter<K, V>)",1,1,1
"org.apache.hadoop.hive.ql.io.HivePassThroughRecordWriter.close(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.HivePassThroughRecordWriter.write(Writable)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveRecordReader.HiveRecordReader(RecordReader)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveRecordReader.HiveRecordReader(RecordReader,JobConf)",1,1,1
"org.apache.hadoop.hive.ql.io.HiveRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveRecordReader.doClose()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(K,V)",2,1,2
"org.apache.hadoop.hive.ql.io.HiveRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.HiveRecordReader.getProgress()",2,2,2
"org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat.getHiveRecordWriter(JobConf,Path,Class<? extends Writable>,boolean,Properties,Progressable)",1,1,1
"org.apache.hadoop.hive.ql.io.IOConstants.IOConstants()",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.IOContext()",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.get()",2,1,2
"org.apache.hadoop.hive.ql.io.IOContext.getComparison()",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.getCurrentBlockStart()",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.getCurrentRow()",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.getGenericUDFClassName()",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.getIOExceptions()",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.getInputPath()",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.getNextBlockStart()",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.isBinarySearching()",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.isBlockPointer()",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.resetSortingValues()",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.setBlockPointer(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.setComparison(Integer)",1,4,6
"org.apache.hadoop.hive.ql.io.IOContext.setCurrentBlockStart(long)",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.setCurrentRow(long)",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.setEndBinarySearch(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.setGenericUDFClassName(String)",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.setIOExceptions(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.setInputPath(Path)",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.setIsBinarySearching(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.setNextBlockStart(long)",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.setUseSorted(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.shouldEndBinarySearch()",1,1,1
"org.apache.hadoop.hive.ql.io.IOContext.useSorted()",1,1,1
"org.apache.hadoop.hive.ql.io.IOPrepareCache.allocatePartitionDescMap()",1,1,2
"org.apache.hadoop.hive.ql.io.IOPrepareCache.clear()",1,2,2
"org.apache.hadoop.hive.ql.io.IOPrepareCache.get()",1,2,2
"org.apache.hadoop.hive.ql.io.IOPrepareCache.getPartitionDescMap()",1,1,1
"org.apache.hadoop.hive.ql.io.IOPrepareCache.setPartitionDescMap(Map<Map<String, PartitionDesc>, Map<String, PartitionDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat.IgnoreKeyWriter.IgnoreKeyWriter(RecordWriter<K, V>)",1,1,1
"org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat.IgnoreKeyWriter.close(Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat.IgnoreKeyWriter.write(K,V)",1,1,1
"org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.NonSyncDataInputBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.NonSyncDataInputBuffer(NonSyncByteArrayInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.convertUTF8WithBuf(byte[],char[],int,int)",9,1,10
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.decodeUTF(int)",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.decodeUTF(int,DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.getLength()",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.getPosition()",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.read(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.read(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readBoolean()",2,1,2
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readByte()",2,1,2
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readChar()",2,1,2
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readDouble()",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readFloat()",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readFully(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readFully(byte[],int,int)",7,2,9
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readInt()",2,1,2
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readLine()",8,7,11
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readLong()",2,1,2
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readShort()",2,1,2
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readToBuff(int)",3,2,3
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readUTF()",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readUTF(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readUnsignedByte()",2,1,2
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.readUnsignedShort()",2,1,2
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.reset(byte[],int)",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.reset(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.skipBytes(int)",2,2,4
"org.apache.hadoop.hive.ql.io.NonSyncDataOutputBuffer.NonSyncDataOutputBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataOutputBuffer.NonSyncDataOutputBuffer(NonSyncByteArrayOutputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataOutputBuffer.getData()",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataOutputBuffer.getLength()",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataOutputBuffer.incCount(int)",1,1,2
"org.apache.hadoop.hive.ql.io.NonSyncDataOutputBuffer.reset()",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataOutputBuffer.write(DataInput,int)",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataOutputBuffer.write(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.NonSyncDataOutputBuffer.write(int)",1,1,1
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.DummyInputSplit.DummyInputSplit()",1,1,1
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.DummyInputSplit.getLength()",1,1,1
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.DummyInputSplit.getLocations()",1,1,1
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.DummyInputSplit.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.DummyInputSplit.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.NullRowsRecordReader.NullRowsRecordReader()",1,1,1
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.NullRowsRecordReader.close()",1,1,1
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.NullRowsRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.NullRowsRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.NullRowsRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.NullRowsRecordReader.getProgress()",1,1,1
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.NullRowsRecordReader.next(NullWritable,NullWritable)",2,1,2
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.configure(JobConf)",1,1,1
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.NullRowsInputFormat.getSplits(JobConf,int)",1,1,1
"org.apache.hadoop.hive.ql.io.ORCFileStorageFormatDescriptor.getInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.ORCFileStorageFormatDescriptor.getNames()",1,1,1
"org.apache.hadoop.hive.ql.io.ORCFileStorageFormatDescriptor.getOutputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.ORCFileStorageFormatDescriptor.getSerde()",1,1,1
"org.apache.hadoop.hive.ql.io.OneNullRowInputFormat.OneNullRowRecordReader.getPos()",1,1,2
"org.apache.hadoop.hive.ql.io.OneNullRowInputFormat.OneNullRowRecordReader.getProgress()",1,1,2
"org.apache.hadoop.hive.ql.io.OneNullRowInputFormat.OneNullRowRecordReader.next(NullWritable,NullWritable)",2,1,2
"org.apache.hadoop.hive.ql.io.OneNullRowInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.ParquetFileStorageFormatDescriptor.getInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.ParquetFileStorageFormatDescriptor.getNames()",1,1,1
"org.apache.hadoop.hive.ql.io.ParquetFileStorageFormatDescriptor.getOutputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.ParquetFileStorageFormatDescriptor.getSerde()",1,1,1
"org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.PerformTestRCFileAndSeqFile(boolean,String)",1,3,3
"org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.getRandomChar(Random)",1,2,3
"org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.isLocalFileSystem()",1,1,1
"org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.main(String[])",3,4,4
"org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.nextRandomRow(byte[][],BytesRefArrayWritable)",1,3,3
"org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.performRCFileFullyReadColumnTest(FileSystem,Path,int,boolean)",3,5,6
"org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.performRCFileReadFirstAndLastColumnTest(FileSystem,Path,int,boolean)",3,6,7
"org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.performRCFileReadFirstColumnTest(FileSystem,Path,int,boolean)",3,5,6
"org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.performSequenceFileRead(FileSystem,int,Path)",1,2,2
"org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.resetRandomGenerators()",1,1,1
"org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.testWithColumnNumber(int,int,boolean,CompressionCodec)",4,5,8
"org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.writeRCFileTest(FileSystem,int,Path,int,CompressionCodec)",1,3,3
"org.apache.hadoop.hive.ql.io.PerformTestRCFileAndSeqFile.writeSeqenceFileTest(FileSystem,int,Path,int,CompressionCodec)",1,3,3
"org.apache.hadoop.hive.ql.io.RCFile.KeyBuffer.KeyBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.KeyBuffer.KeyBuffer(int)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.KeyBuffer.KeyBuffer(int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.KeyBuffer.compareTo(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.KeyBuffer.getColumnNumber()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.KeyBuffer.getEachColumnUncompressedValueLen()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.KeyBuffer.getEachColumnValueLen()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.KeyBuffer.getNumberRows()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.KeyBuffer.getSize()",1,2,2
"org.apache.hadoop.hive.ql.io.RCFile.KeyBuffer.nullColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.KeyBuffer.readFields(DataInput)",1,3,3
"org.apache.hadoop.hive.ql.io.RCFile.KeyBuffer.setColumnLenInfo(int,NonSyncDataOutputBuffer,int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.KeyBuffer.write(DataOutput)",1,2,2
"org.apache.hadoop.hive.ql.io.RCFile.Reader.Reader(FileSystem,Path,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.Reader(FileSystem,Path,int,Configuration,long,long)",1,9,16
"org.apache.hadoop.hive.ql.io.RCFile.Reader.close()",1,3,3
"org.apache.hadoop.hive.ql.io.RCFile.Reader.colAdvanceRow(int,SelectedColumn)",1,2,3
"org.apache.hadoop.hive.ql.io.RCFile.Reader.createKeyBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.currentValueBuffer()",1,2,2
"org.apache.hadoop.hive.ql.io.RCFile.Reader.getColumn(int,BytesRefArrayWritable)",2,5,7
"org.apache.hadoop.hive.ql.io.RCFile.Reader.getCompressionCodec()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.getCurrentBlockLength()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.getCurrentCompressedKeyLen()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.getCurrentKeyBufferObj()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.getCurrentKeyLength()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.getCurrentRow(BytesRefArrayWritable)",2,10,12
"org.apache.hadoop.hive.ql.io.RCFile.Reader.getCurrentValueBufferObj()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.getMetadata()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.getMetadataValueOf(Text)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.getPosition()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.handleChecksumException(ChecksumException)",2,2,2
"org.apache.hadoop.hive.ql.io.RCFile.Reader.hasRecordsInBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.init()",10,6,13
"org.apache.hadoop.hive.ql.io.RCFile.Reader.isCompressedRCFile()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.lastSeenSyncPos()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.next(LongWritable)",2,5,5
"org.apache.hadoop.hive.ql.io.RCFile.Reader.nextBlock()",2,2,2
"org.apache.hadoop.hive.ql.io.RCFile.Reader.nextColumnsBatch()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.nextKeyBuffer()",2,3,4
"org.apache.hadoop.hive.ql.io.RCFile.Reader.nextKeyValueTolerateCorruptions()",2,5,5
"org.apache.hadoop.hive.ql.io.RCFile.Reader.openFile(FileSystem,Path,int,long)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.readRecordLength()",5,2,6
"org.apache.hadoop.hive.ql.io.RCFile.Reader.resetBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.seek(long)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.seekToNextKeyBuffer()",2,2,3
"org.apache.hadoop.hive.ql.io.RCFile.Reader.sync(long)",6,8,10
"org.apache.hadoop.hive.ql.io.RCFile.Reader.syncSeen()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Reader.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.ValueBuffer.LazyDecompressionCallbackImpl.LazyDecompressionCallbackImpl(int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.ValueBuffer.LazyDecompressionCallbackImpl.decompress()",2,3,4
"org.apache.hadoop.hive.ql.io.RCFile.ValueBuffer.ValueBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.ValueBuffer.ValueBuffer(KeyBuffer)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.ValueBuffer.ValueBuffer(KeyBuffer,boolean[])",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.ValueBuffer.ValueBuffer(KeyBuffer,int,boolean[],CompressionCodec)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.ValueBuffer.ValueBuffer(KeyBuffer,int,boolean[],CompressionCodec,boolean)",3,2,11
"org.apache.hadoop.hive.ql.io.RCFile.ValueBuffer.clearColumnBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.ValueBuffer.close()",1,4,4
"org.apache.hadoop.hive.ql.io.RCFile.ValueBuffer.compareTo(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.ValueBuffer.nullColumn(int)",1,2,2
"org.apache.hadoop.hive.ql.io.RCFile.ValueBuffer.readFields(DataInput)",3,6,9
"org.apache.hadoop.hive.ql.io.RCFile.ValueBuffer.setColumnValueBuffer(NonSyncDataOutputBuffer,int)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.ValueBuffer.write(DataOutput)",1,4,4
"org.apache.hadoop.hive.ql.io.RCFile.Writer.ColumnBuffer.ColumnBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Writer.ColumnBuffer.append(BytesRefWritable)",2,3,3
"org.apache.hadoop.hive.ql.io.RCFile.Writer.ColumnBuffer.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Writer.ColumnBuffer.flushGroup()",1,3,3
"org.apache.hadoop.hive.ql.io.RCFile.Writer.ColumnBuffer.startNewGroup(int)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Writer.Writer(FileSystem,Configuration,Path)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Writer.Writer(FileSystem,Configuration,Path,Progressable,CompressionCodec)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Writer.Writer(FileSystem,Configuration,Path,Progressable,Metadata,CompressionCodec)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Writer.Writer(FileSystem,Configuration,Path,int,short,long,Progressable,Metadata,CompressionCodec)",1,1,3
"org.apache.hadoop.hive.ql.io.RCFile.Writer.append(Writable)",2,5,7
"org.apache.hadoop.hive.ql.io.RCFile.Writer.checkAndWriteSync()",1,3,3
"org.apache.hadoop.hive.ql.io.RCFile.Writer.clearColumnBuffers()",1,2,2
"org.apache.hadoop.hive.ql.io.RCFile.Writer.close()",1,4,4
"org.apache.hadoop.hive.ql.io.RCFile.Writer.finalizeFileHeader()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Writer.flushBlock(KeyBuffer,ValueBuffer,int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Writer.flushRecords()",2,8,9
"org.apache.hadoop.hive.ql.io.RCFile.Writer.getCompressionCodec()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Writer.getConf()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Writer.getLength()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Writer.init(Configuration,FSDataOutputStream,CompressionCodec,Metadata)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Writer.initializeFileHeader()",1,2,2
"org.apache.hadoop.hive.ql.io.RCFile.Writer.isCompressed()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFile.Writer.sync()",1,3,3
"org.apache.hadoop.hive.ql.io.RCFile.Writer.writeFileHeader()",1,3,3
"org.apache.hadoop.hive.ql.io.RCFile.Writer.writeKey(KeyBuffer,int,int)",1,2,2
"org.apache.hadoop.hive.ql.io.RCFile.createMetadata(Text...)",2,2,3
"org.apache.hadoop.hive.ql.io.RCFileInputFormat.RCFileInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileInputFormat.validateInput(FileSystem,HiveConf,ArrayList<FileStatus>)",3,3,5
"org.apache.hadoop.hive.ql.io.RCFileOutputFormat.getColumnNumber(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileOutputFormat.getHiveRecordWriter(JobConf,Path,Class<? extends Writable>,boolean,Properties,Progressable)",1,3,3
"org.apache.hadoop.hive.ql.io.RCFileOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)",1,2,2
"org.apache.hadoop.hive.ql.io.RCFileOutputFormat.setColumnNumber(Configuration,int)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.RCFileRecordReader(Configuration,FileSplit)",1,4,4
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.RCFileSyncCache.RCFileSyncCache()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.RCFileSyncCache.get(FileSplit)",2,1,2
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.RCFileSyncCache.put(FileSplit,long)",1,2,2
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.close()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.getKeyBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.getKeyClass()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.getProgress()",2,2,2
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.getStart()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.getValueClass()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.next(LongWritable)",3,3,4
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.next(LongWritable,BytesRefArrayWritable)",1,2,2
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.nextBlock()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.resetBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.seek(long)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileRecordReader.sync(long)",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileStorageFormatDescriptor.getInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileStorageFormatDescriptor.getNames()",1,1,1
"org.apache.hadoop.hive.ql.io.RCFileStorageFormatDescriptor.getOutputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.RecordIdentifier.Field.Field(TypeInfo,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.io.RecordIdentifier.RecordIdentifier()",1,1,1
"org.apache.hadoop.hive.ql.io.RecordIdentifier.RecordIdentifier(long,int,long)",1,1,1
"org.apache.hadoop.hive.ql.io.RecordIdentifier.StructInfo.toArray(RecordIdentifier,Object[])",2,3,3
"org.apache.hadoop.hive.ql.io.RecordIdentifier.compareTo(RecordIdentifier)",2,2,2
"org.apache.hadoop.hive.ql.io.RecordIdentifier.compareToInternal(RecordIdentifier)",5,1,8
"org.apache.hadoop.hive.ql.io.RecordIdentifier.equals(Object)",2,2,5
"org.apache.hadoop.hive.ql.io.RecordIdentifier.getBucketId()",1,1,1
"org.apache.hadoop.hive.ql.io.RecordIdentifier.getRowId()",1,1,1
"org.apache.hadoop.hive.ql.io.RecordIdentifier.getTransactionId()",1,1,1
"org.apache.hadoop.hive.ql.io.RecordIdentifier.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.RecordIdentifier.set(RecordIdentifier)",1,1,1
"org.apache.hadoop.hive.ql.io.RecordIdentifier.setRowId(long)",1,1,1
"org.apache.hadoop.hive.ql.io.RecordIdentifier.setValues(long,int,long)",1,1,1
"org.apache.hadoop.hive.ql.io.RecordIdentifier.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.RecordIdentifier.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.io.SchemaAwareCompressionInputStream.SchemaAwareCompressionInputStream(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.SchemaAwareCompressionOutputStream.SchemaAwareCompressionOutputStream(OutputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker.validateInput(FileSystem,HiveConf,ArrayList<FileStatus>)",3,2,4
"org.apache.hadoop.hive.ql.io.SequenceFileStorageFormatDescriptor.getInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.SequenceFileStorageFormatDescriptor.getNames()",1,1,1
"org.apache.hadoop.hive.ql.io.SequenceFileStorageFormatDescriptor.getOutputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.StorageFormatFactory.StorageFormatFactory()",1,4,4
"org.apache.hadoop.hive.ql.io.StorageFormatFactory.get(String)",1,1,1
"org.apache.hadoop.hive.ql.io.StorageFormats.asParameters()",1,5,5
"org.apache.hadoop.hive.ql.io.StorageFormats.createTestArguments(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.io.SymbolicInputFormat.rework(HiveConf,MapredWork)",1,7,7
"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.SymlinkTextInputSplit.SymlinkTextInputSplit()",1,1,1
"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.SymlinkTextInputSplit.SymlinkTextInputSplit(Path,FileSplit)",1,1,1
"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.SymlinkTextInputSplit.getTargetSplit()",1,1,1
"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.SymlinkTextInputSplit.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.SymlinkTextInputSplit.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.configure(JobConf)",1,1,1
"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.getContentSummary(Path,JobConf)",1,2,3
"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,2,2
"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.getSplits(JobConf,int)",3,3,6
"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat.getTargetPathsFromSymlinksDirs(Configuration,Path[],List<Path>,List<Path>)",1,4,4
"org.apache.hadoop.hive.ql.io.TestAcidUtils.testBaseDeltas()",1,1,1
"org.apache.hadoop.hive.ql.io.TestAcidUtils.testBestBase()",1,1,1
"org.apache.hadoop.hive.ql.io.TestAcidUtils.testCreateFilename()",1,1,1
"org.apache.hadoop.hive.ql.io.TestAcidUtils.testObsoleteOriginals()",1,1,1
"org.apache.hadoop.hive.ql.io.TestAcidUtils.testOriginal()",1,1,1
"org.apache.hadoop.hive.ql.io.TestAcidUtils.testOriginalDeltas()",1,1,1
"org.apache.hadoop.hive.ql.io.TestAcidUtils.testOverlapingDelta()",1,1,1
"org.apache.hadoop.hive.ql.io.TestAcidUtils.testParsing()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.TestHiveInputSplit.getLength()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.TestHiveInputSplit.getPath()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.TestHiveInputSplit.getStart()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.TestHiveRecordReader.TestHiveRecordReader(RecordReader,JobConf)",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.TestHiveRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.TestHiveRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.TestHiveRecordReader.doClose()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.TestHiveRecordReader.doNext(K,V)",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.TestHiveRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.executeDoNext(HiveContextAwareRecordReader)",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.init()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.main(String[])",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.resetIOContext()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.testEqualOpClass()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.testGreaterThanOpClass()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.testGreaterThanOrEqualOpClass()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.testHitLastBlock()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.testHitSamePositionTwice()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.testLessThanOpClass()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.testLessThanOrEqualOpClass()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.testNonLinearEqualTo()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.testNonLinearGreaterThan()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.testNonLinearLessThan()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.testResetRange()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveFileFormatUtils.testGetPartitionDescFromPathRecursively()",1,1,3
"org.apache.hadoop.hive.ql.io.TestHiveInputOutputBuffer.composeString(int,Random)",1,5,5
"org.apache.hadoop.hive.ql.io.TestHiveInputOutputBuffer.readJunk(NonSyncDataInputBuffer,Random,long,int)",2,3,16
"org.apache.hadoop.hive.ql.io.TestHiveInputOutputBuffer.testBaseBuffers()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveInputOutputBuffer.testReadAndWrite()",1,1,1
"org.apache.hadoop.hive.ql.io.TestHiveInputOutputBuffer.writeJunk(DataOutput,Random,long,int)",2,3,16
"org.apache.hadoop.hive.ql.io.TestPerformTestRCFileAndSeqFile.testPerformTestRCFileAndSeqFileNoArgs()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.TestFSDataInputStream.TestFSDataInputStream(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.TestFSDataInputStream.close()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.TestFSDataInputStream.isClosed()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.cleanup()",3,2,4
"org.apache.hadoop.hive.ql.io.TestRCFile.createProperties()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.fullyReadTest(FileSystem,int,Path)",1,3,3
"org.apache.hadoop.hive.ql.io.TestRCFile.main(String[])",3,6,6
"org.apache.hadoop.hive.ql.io.TestRCFile.partialReadTest(FileSystem,int,Path)",1,3,3
"org.apache.hadoop.hive.ql.io.TestRCFile.setup()",1,1,2
"org.apache.hadoop.hive.ql.io.TestRCFile.splitAfterSync()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.splitBeforeSync()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.splitInMiddleOfSync()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.splitRightAfterSync()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.splitRightBeforeSync()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.teardown()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.testCloseForErroneousRCFile()",1,1,2
"org.apache.hadoop.hive.ql.io.TestRCFile.testExplicitRCFileHeader()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.testGetColumn()",1,6,7
"org.apache.hadoop.hive.ql.io.TestRCFile.testNonExplicitRCFileHeader()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.testRCFileHeader(char[],Configuration)",1,2,2
"org.apache.hadoop.hive.ql.io.TestRCFile.testReadCorruptFile()",3,5,6
"org.apache.hadoop.hive.ql.io.TestRCFile.testReadOldFileHeader()",1,2,2
"org.apache.hadoop.hive.ql.io.TestRCFile.testSimpleReadAndWrite()",1,6,6
"org.apache.hadoop.hive.ql.io.TestRCFile.testSynAndSplit()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.testSync()",1,5,5
"org.apache.hadoop.hive.ql.io.TestRCFile.testWriteAndFullyRead()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.testWriteAndPartialRead()",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.writeTest(FileSystem,int,Path,byte[][])",1,1,1
"org.apache.hadoop.hive.ql.io.TestRCFile.writeTest(FileSystem,int,Path,byte[][],Configuration)",1,3,3
"org.apache.hadoop.hive.ql.io.TestRCFile.writeThenReadByRecordReader(int,int,int,long,CompressionCodec)",1,5,6
"org.apache.hadoop.hive.ql.io.TestRecordIdentifier.TestOrdering()",1,1,1
"org.apache.hadoop.hive.ql.io.TestStorageFormatDescriptor.testNames()",1,1,1
"org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.setUp()",1,1,1
"org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.tearDown()",1,1,1
"org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.testAccuracy1()",1,3,3
"org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.testAccuracy2()",1,3,3
"org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.testCombine()",3,3,5
"org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.testFailure()",1,2,2
"org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.writeSymlinkFile(Path,Path...)",1,2,2
"org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.writeTextFile(Path,String)",1,1,1
"org.apache.hadoop.hive.ql.io.TextFileStorageFormatDescriptor.getInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.TextFileStorageFormatDescriptor.getNames()",1,1,1
"org.apache.hadoop.hive.ql.io.TextFileStorageFormatDescriptor.getOutputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.VectorizedRCFileInputFormat.VectorizedRCFileInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.VectorizedRCFileInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.VectorizedRCFileInputFormat.validateInput(FileSystem,HiveConf,ArrayList<FileStatus>)",3,3,5
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.RCFileSyncCache.RCFileSyncCache()",1,1,1
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.RCFileSyncCache.get(FileSplit)",2,1,2
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.RCFileSyncCache.put(FileSplit,long)",1,2,2
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.VectorizedRCFileRecordReader(Configuration,FileSplit)",1,4,5
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.close()",1,1,1
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.createValue()",1,1,2
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.getKeyBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.getKeyClass()",1,1,1
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.getProgress()",2,2,2
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.getStart()",1,1,1
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.getValueClass()",1,1,1
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.next(LongWritable)",3,3,4
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.next(NullWritable,VectorizedRowBatch)",3,4,6
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.nextBlock()",1,1,1
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.resetBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.seek(long)",1,1,1
"org.apache.hadoop.hive.ql.io.VectorizedRCFileRecordReader.sync(long)",1,1,1
"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.configure(JobConf)",1,1,1
"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.listStatus(JobConf)",1,2,2
"org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat.checkOutputSpecs(FileSystem,JobConf)",1,1,1
"org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat.getHiveRecordWriter(JobConf,Path,Class<? extends Writable>,boolean,Properties,Progressable)",1,3,4
"org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)",1,1,1
"org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.AvroGenericRecordReader(JobConf,FileSplit,Reporter)",1,2,3
"org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.close()",1,1,1
"org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.configure(JobConf)",1,1,1
"org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.getProgress()",1,2,2
"org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.getSchema(JobConf,FileSplit)",6,9,9
"org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.next(NullWritable,AvroGenericRecordWritable)",2,2,3
"org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.pathIsInPartition(Path,String)",2,2,2
"org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordWriter.AvroGenericRecordWriter(DataFileWriter<GenericRecord>)",1,1,1
"org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordWriter.close(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordWriter.write(Writable)",2,2,2
"org.apache.hadoop.hive.ql.io.merge.MergeMapper.checkPartitionsMatch(Path)",2,2,2
"org.apache.hadoop.hive.ql.io.merge.MergeMapper.close()",5,6,7
"org.apache.hadoop.hive.ql.io.merge.MergeMapper.configure(JobConf)",1,1,2
"org.apache.hadoop.hive.ql.io.merge.MergeMapper.fixTmpPath(Path)",1,3,4
"org.apache.hadoop.hive.ql.io.merge.MergeMapper.fixTmpPathAlterTable(Path)",1,5,6
"org.apache.hadoop.hive.ql.io.merge.MergeMapper.fixTmpPathConcatenate(Path)",1,3,4
"org.apache.hadoop.hive.ql.io.merge.MergeMapper.updatePaths(Path,Path)",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeOutputFormat.getMergeOutputPath(JobConf)",1,1,2
"org.apache.hadoop.hive.ql.io.merge.MergeOutputFormat.setMergeOutputPath(JobConf,Path)",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeTask.addInputPaths(JobConf,MergeWork)",1,2,2
"org.apache.hadoop.hive.ql.io.merge.MergeTask.backupOutputPath(FileSystem,Path,JobConf)",2,2,2
"org.apache.hadoop.hive.ql.io.merge.MergeTask.checkFatalErrors(Counters,StringBuilder)",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeTask.closeJob(Path,boolean,JobConf,LogHelper,DynamicPartitionCtx,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeTask.execute(DriverContext)",1,22,24
"org.apache.hadoop.hive.ql.io.merge.MergeTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeTask.initialize(HiveConf,QueryPlan,DriverContext)",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeTask.logPlanProgress(SessionState)",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeTask.main(String[])",1,20,23
"org.apache.hadoop.hive.ql.io.merge.MergeTask.printUsage()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeTask.requireLock()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.MergeWork()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.MergeWork(List<Path>,Path,Class<? extends InputFormat>)",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.MergeWork(List<Path>,Path,boolean,DynamicPartitionCtx,Class<? extends InputFormat>)",1,5,5
"org.apache.hadoop.hive.ql.io.merge.MergeWork.getDynPartCtx()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.getInputPaths()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.getInputformat()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.getListBucketingCtx()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.getMapperClass(Class<? extends InputFormat>)",3,2,3
"org.apache.hadoop.hive.ql.io.merge.MergeWork.getMergeLevel()",4,3,4
"org.apache.hadoop.hive.ql.io.merge.MergeWork.getMinSplitSize()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.getOutputDir()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.getSourceTableInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.getStringifiedInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.hasDynamicPartitions()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.isGatheringStats()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.isListBucketingAlterTableConcatenate()",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.resolveConcatenateMerge(HiveConf)",2,10,10
"org.apache.hadoop.hive.ql.io.merge.MergeWork.resolveDynamicPartitionStoredAsSubDirsMerge(HiveConf,Path,TableDesc,ArrayList<String>,PartitionDesc)",1,3,4
"org.apache.hadoop.hive.ql.io.merge.MergeWork.setDynPartCtx(DynamicPartitionCtx)",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.setHasDynamicPartitions(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.setInputPaths(List<Path>)",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.setListBucketingCtx(ListBucketingCtx)",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.setOutputDir(Path)",1,1,1
"org.apache.hadoop.hive.ql.io.merge.MergeWork.setSourceTableInputFormat(Class<? extends InputFormat>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.BitFieldReader.BitFieldReader(InStream,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.BitFieldReader.next()",1,2,3
"org.apache.hadoop.hive.ql.io.orc.BitFieldReader.nextVector(LongColumnVector,long)",1,3,7
"org.apache.hadoop.hive.ql.io.orc.BitFieldReader.readByte()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.BitFieldReader.seek(PositionProvider)",2,3,3
"org.apache.hadoop.hive.ql.io.orc.BitFieldReader.skip(long)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.BitFieldReader.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.BitFieldWriter.BitFieldWriter(PositionedOutputStream,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.BitFieldWriter.flush()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.BitFieldWriter.getPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.BitFieldWriter.write(int)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.BitFieldWriter.writeByte()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BinaryStatisticsImpl.BinaryStatisticsImpl()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BinaryStatisticsImpl.BinaryStatisticsImpl(ColumnStatistics)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BinaryStatisticsImpl.getSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BinaryStatisticsImpl.merge(ColumnStatisticsImpl)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BinaryStatisticsImpl.reset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BinaryStatisticsImpl.serialize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BinaryStatisticsImpl.toString()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BinaryStatisticsImpl.updateBinary(BytesWritable)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BooleanStatisticsImpl.BooleanStatisticsImpl()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BooleanStatisticsImpl.BooleanStatisticsImpl(ColumnStatistics)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BooleanStatisticsImpl.getFalseCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BooleanStatisticsImpl.getTrueCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BooleanStatisticsImpl.merge(ColumnStatisticsImpl)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BooleanStatisticsImpl.reset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BooleanStatisticsImpl.serialize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BooleanStatisticsImpl.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.BooleanStatisticsImpl.updateBoolean(boolean)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.ColumnStatisticsImpl()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.ColumnStatisticsImpl(ColumnStatistics)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DateStatisticsImpl.DateStatisticsImpl()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DateStatisticsImpl.DateStatisticsImpl(ColumnStatistics)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DateStatisticsImpl.getMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DateStatisticsImpl.getMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DateStatisticsImpl.merge(ColumnStatisticsImpl)",1,1,5
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DateStatisticsImpl.reset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DateStatisticsImpl.serialize()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DateStatisticsImpl.toString()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DateStatisticsImpl.updateDate(DateWritable)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DecimalStatisticsImpl.DecimalStatisticsImpl()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DecimalStatisticsImpl.DecimalStatisticsImpl(ColumnStatistics)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DecimalStatisticsImpl.getMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DecimalStatisticsImpl.getMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DecimalStatisticsImpl.getSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DecimalStatisticsImpl.merge(ColumnStatisticsImpl)",1,5,7
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DecimalStatisticsImpl.reset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DecimalStatisticsImpl.serialize()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DecimalStatisticsImpl.toString()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DecimalStatisticsImpl.updateDecimal(HiveDecimal)",1,4,5
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DoubleStatisticsImpl.DoubleStatisticsImpl()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DoubleStatisticsImpl.DoubleStatisticsImpl(ColumnStatistics)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DoubleStatisticsImpl.getMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DoubleStatisticsImpl.getMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DoubleStatisticsImpl.getSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DoubleStatisticsImpl.merge(ColumnStatisticsImpl)",1,1,5
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DoubleStatisticsImpl.reset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DoubleStatisticsImpl.serialize()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DoubleStatisticsImpl.toString()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.DoubleStatisticsImpl.updateDouble(double)",1,1,4
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.IntegerStatisticsImpl.IntegerStatisticsImpl()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.IntegerStatisticsImpl.IntegerStatisticsImpl(ColumnStatistics)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.IntegerStatisticsImpl.getMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.IntegerStatisticsImpl.getMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.IntegerStatisticsImpl.getSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.IntegerStatisticsImpl.isSumDefined()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.IntegerStatisticsImpl.merge(ColumnStatisticsImpl)",1,1,7
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.IntegerStatisticsImpl.reset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.IntegerStatisticsImpl.serialize()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.IntegerStatisticsImpl.toString()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.IntegerStatisticsImpl.updateInteger(long)",1,1,6
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.StringStatisticsImpl.StringStatisticsImpl()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.StringStatisticsImpl.StringStatisticsImpl(ColumnStatistics)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.StringStatisticsImpl.getMaximum()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.StringStatisticsImpl.getMinimum()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.StringStatisticsImpl.getSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.StringStatisticsImpl.merge(ColumnStatisticsImpl)",1,6,6
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.StringStatisticsImpl.reset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.StringStatisticsImpl.serialize()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.StringStatisticsImpl.toString()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.StringStatisticsImpl.updateString(Text)",1,3,4
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.TimestampStatisticsImpl.TimestampStatisticsImpl()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.TimestampStatisticsImpl.TimestampStatisticsImpl(ColumnStatistics)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.TimestampStatisticsImpl.getMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.TimestampStatisticsImpl.getMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.TimestampStatisticsImpl.merge(ColumnStatisticsImpl)",1,1,5
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.TimestampStatisticsImpl.reset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.TimestampStatisticsImpl.serialize()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.TimestampStatisticsImpl.toString()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.TimestampStatisticsImpl.updateTimestamp(Timestamp)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.create(ObjectInspector)",12,4,12
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.deserialize(ColumnStatistics)",9,8,9
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.getNumberOfValues()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.increment()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.merge(ColumnStatisticsImpl)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.reset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.serialize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.updateBinary(BytesWritable)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.updateBoolean(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.updateDate(DateWritable)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.updateDecimal(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.updateDouble(double)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.updateInteger(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.updateString(Text)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.updateTimestamp(Timestamp)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.DynamicByteArray()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.DynamicByteArray(int,int)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.add(byte)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.add(byte[],int,int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.clear()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.compare(byte[],int,int,int,int)",2,1,6
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.get()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.get(int)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.getSizeInBytes()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.grow(int)",1,3,4
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.readAll(InputStream)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.set(int,byte)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.setByteBuffer(ByteBuffer,int,int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.setText(Text,int,int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.size()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.toString()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.write(OutputStream,int,int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.DynamicIntArray.DynamicIntArray()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.DynamicIntArray.DynamicIntArray(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.DynamicIntArray.add(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.DynamicIntArray.clear()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.DynamicIntArray.get(int)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.DynamicIntArray.getSizeInBytes()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.DynamicIntArray.grow(int)",1,3,4
"org.apache.hadoop.hive.ql.io.orc.DynamicIntArray.increment(int,int)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.DynamicIntArray.set(int,int)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.DynamicIntArray.size()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.DynamicIntArray.toString()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.FileDump.FileDump()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.FileDump.getTotalPaddingSize(Reader)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.FileDump.main(String[])",8,23,24
"org.apache.hadoop.hive.ql.io.orc.InStream.CompressedStream.CompressedStream(String,ByteBuffer[],long[],long,CompressionCodec,int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.InStream.CompressedStream.allocateBuffer(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.InStream.CompressedStream.available()",3,3,4
"org.apache.hadoop.hive.ql.io.orc.InStream.CompressedStream.close()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.InStream.CompressedStream.rangeString()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.InStream.CompressedStream.read()",3,3,4
"org.apache.hadoop.hive.ql.io.orc.InStream.CompressedStream.read(byte[],int,int)",3,3,4
"org.apache.hadoop.hive.ql.io.orc.InStream.CompressedStream.readHeader()",3,7,8
"org.apache.hadoop.hive.ql.io.orc.InStream.CompressedStream.seek(PositionProvider)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.InStream.CompressedStream.seek(long)",4,6,6
"org.apache.hadoop.hive.ql.io.orc.InStream.CompressedStream.slice(int)",5,6,8
"org.apache.hadoop.hive.ql.io.orc.InStream.CompressedStream.toString()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.InStream.UncompressedStream.UncompressedStream(String,ByteBuffer[],long[],long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.InStream.UncompressedStream.available()",2,3,3
"org.apache.hadoop.hive.ql.io.orc.InStream.UncompressedStream.close()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.InStream.UncompressedStream.read()",3,3,4
"org.apache.hadoop.hive.ql.io.orc.InStream.UncompressedStream.read(byte[],int,int)",3,3,4
"org.apache.hadoop.hive.ql.io.orc.InStream.UncompressedStream.seek(PositionProvider)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.InStream.UncompressedStream.seek(long)",3,4,4
"org.apache.hadoop.hive.ql.io.orc.InStream.UncompressedStream.toString()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.InStream.create(String,ByteBuffer[],long[],long,CompressionCodec,int)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.MemoryManager.MemoryManager(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.MemoryManager.WriterInfo.WriterInfo(long,Callback)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.MemoryManager.addWriter(Path,long,Callback)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.MemoryManager.addedRow()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.MemoryManager.getAllocationScale()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.MemoryManager.getTotalMemoryPool()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.MemoryManager.notifyWriters()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.MemoryManager.removeWriter(Path)",1,2,4
"org.apache.hadoop.hive.ql.io.orc.MemoryManager.updateScale(boolean)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.Metadata.Metadata(Metadata)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.Metadata.getStripeStatistics()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcFile.OrcFile()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.OrcTableProperties.OrcTableProperties(String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.OrcTableProperties.getPropName()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.ReaderOptions.ReaderOptions(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.ReaderOptions.fileMetaInfo(FileMetaInfo)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.ReaderOptions.filesystem(FileSystem)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.ReaderOptions.getConfiguration()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.ReaderOptions.getFileMetaInfo()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.ReaderOptions.getFilesystem()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.ReaderOptions.getMaxLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.ReaderOptions.maxLength(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.Version.Version(String,int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.Version.byName(String)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcFile.Version.getMajor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.Version.getMinor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.Version.getName()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions.WriterOptions(Configuration)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions.blockPadding(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions.blockSize(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions.bufferSize(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions.callback(WriterCallback)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions.compress(CompressionKind)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions.encodingStrategy(EncodingStrategy)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions.fileSystem(FileSystem)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions.inspector(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions.memory(MemoryManager)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions.paddingTolerance(float)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions.rowIndexStride(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions.stripeSize(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions.version(Version)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(FileSystem,Path)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(Path,ReaderOptions)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.createWriter(FileSystem,Path,Configuration,ObjectInspector,long,CompressionKind,int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.createWriter(Path,WriterOptions)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcFile.getMemoryManager(Configuration)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcFile.readerOptions(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFile.writerOptions(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.compareTo(OrcFileKeyWrapper)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.getCompressBufferSize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.getCompression()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.getInputPath()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.getRowIndexStride()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.getTypes()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.getVersionList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.setCompressBufferSize(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.setCompression(CompressionKind)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.setInputPath(Path)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.setRowIndexStride(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.setTypes(List<Type>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.setVersionList(List<Integer>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileKeyWrapper.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileMergeMapper.checkCompatibility(OrcFileKeyWrapper,OrcFileValueWrapper)",6,6,6
"org.apache.hadoop.hive.ql.io.orc.OrcFileMergeMapper.close()",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcFileMergeMapper.configure(JobConf)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileMergeMapper.map(Object,OrcFileValueWrapper,OutputCollector<Object, Object>,Reporter)",2,8,8
"org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.OrcFileStripeMergeRecordReader(Configuration,FileSplit)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.close()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.getKeyClass()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.getProgress()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.getStart()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.getValueClass()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.next(OrcFileKeyWrapper,OrcFileValueWrapper)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.nextStripe(OrcFileKeyWrapper,OrcFileValueWrapper)",3,5,5
"org.apache.hadoop.hive.ql.io.orc.OrcFileStripeMergeRecordReader.seek(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileValueWrapper.compareTo(OrcFileValueWrapper)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcFileValueWrapper.getStripeInformation()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileValueWrapper.getStripeStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileValueWrapper.getUserMetadata()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileValueWrapper.isLastStripeInFile()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileValueWrapper.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileValueWrapper.setLastStripeInFile(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileValueWrapper.setStripeInformation(StripeInformation)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileValueWrapper.setStripeStatistics(StripeStatistics)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileValueWrapper.setUserMetadata(List<UserMetadataItem>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcFileValueWrapper.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.Context.Context(Configuration)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.Context.decrementSchedulers()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.Context.getErrors()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.Context.getResult(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.Context.getSchedulers()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.Context.notifyOnNonIOException(Throwable)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.Context.schedule(Runnable)",2,2,4
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.Context.waitForTasks()",3,3,4
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.FileGenerator.FileGenerator(Context,FileSystem,Path)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.FileGenerator.run()",1,11,13
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.FileGenerator.scheduleSplits(FileStatus,boolean,boolean,List<Long>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.FileGenerator.verifyCachedFileInfo(FileStatus)",3,7,7
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.FileInfo.FileInfo(long,long,List<StripeInformation>,Metadata,List<Type>,FileMetaInfo)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.NullKeyRecordReader.NullKeyRecordReader(RowReader<OrcStruct>,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.NullKeyRecordReader.close()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.NullKeyRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.NullKeyRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.NullKeyRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.NullKeyRecordReader.getProgress()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.NullKeyRecordReader.getRecordIdentifier()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.NullKeyRecordReader.next(NullWritable,OrcStruct)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.OrcRecordReader.OrcRecordReader(Reader,Configuration,FileSplit)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.OrcRecordReader.close()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.OrcRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.OrcRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.OrcRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.OrcRecordReader.getProgress()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.OrcRecordReader.getStats()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.OrcRecordReader.next(NullWritable,OrcStruct)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.SplitGenerator.SplitGenerator(Context,FileSystem,FileStatus,FileInfo,boolean,List<Long>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.SplitGenerator.createSplit(long,long,FileMetaInfo)",1,10,10
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.SplitGenerator.getOverlap(long,long,long,long)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.SplitGenerator.getPath()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.SplitGenerator.isStripeSatisfyPredicate(StripeStatistics,SearchArgument,int[])",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.SplitGenerator.populateAndCacheStripeDetails()",1,8,9
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.SplitGenerator.run()",3,21,21
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.SplitGenerator.schedule()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.SplitGenerator.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.createReaderFromFile(Reader,Configuration,long,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.createVectorizedReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.findOriginalBucket(FileSystem,Path,int)",3,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(Configuration)",4,6,6
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getInputPaths(Configuration)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(Configuration,boolean,int,ValidTxnList,Path,Path[])",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(InputSplit,Options)",1,7,7
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",6,6,7
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRootColumn(boolean)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(JobConf,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.includeColumnRecursive(List<Type>,boolean[],int,int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.setIncludedColumns(Options,List<Type>,Configuration,boolean)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.setSearchArgument(Options,List<Type>,Configuration,boolean)",1,3,8
"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.validateInput(FileSystem,HiveConf,ArrayList<FileStatus>)",4,3,5
"org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.OrcRecordReader.OrcRecordReader(Reader,Configuration,long,long)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.OrcRecordReader.close()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.OrcRecordReader.getCurrentKey()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.OrcRecordReader.getCurrentValue()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.OrcRecordReader.getProgress()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.OrcRecordReader.initialize(InputSplit,TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.OrcRecordReader.nextKeyValue()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.createRecordReader(InputSplit,TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.getSplits(JobContext)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcNewOutputFormat.OrcRecordWriter.OrcRecordWriter(Path,WriterOptions)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewOutputFormat.OrcRecordWriter.close(TaskAttemptContext)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcNewOutputFormat.OrcRecordWriter.write(NullWritable,OrcSerdeRow)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcNewOutputFormat.getRecordWriter(TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.OrcNewSplit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.OrcNewSplit(OrcSplit)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.getDeltas()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.getFileMetaInfo()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.hasBase()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.hasFooter()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.isOriginal()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.readFields(DataInput)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcNewSplit.write(DataOutput)",1,3,6
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.DummyOrcRecordUpdater.DummyOrcRecordUpdater(Path,Options)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.DummyOrcRecordUpdater.close(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.DummyOrcRecordUpdater.delete(long,long,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.DummyOrcRecordUpdater.flush()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.DummyOrcRecordUpdater.getStats()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.DummyOrcRecordUpdater.insert(long,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.DummyOrcRecordUpdater.stringifyObject(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.DummyOrcRecordUpdater.stringifyObject(StringBuilder,Object,ObjectInspector)",1,5,5
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.DummyOrcRecordUpdater.update(long,long,long,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.OrcRecordWriter.OrcRecordWriter(Path,WriterOptions)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.OrcRecordWriter.close(Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.OrcRecordWriter.close(boolean)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.OrcRecordWriter.getStats()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.OrcRecordWriter.write(NullWritable,OrcSerdeRow)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.OrcRecordWriter.write(Writable)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.getHiveRecordWriter(JobConf,Path,Class<? extends Writable>,boolean,Properties,Progressable)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.getOptions(JobConf,Properties)",1,9,9
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.getRawRecordWriter(Path,Options)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.getRecordUpdater(Path,Options)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.getSettingFromPropsFallingBackToConf(String,Properties,JobConf)",3,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.BinaryStatistics(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.BinaryStatistics(CodedInputStream,ExtensionRegistryLite)",2,6,8
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.BinaryStatistics(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.buildPartial()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.clearSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.getSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.hasSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.maybeForceBuilderInitialization()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.mergeFrom(BinaryStatistics)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.Builder.setSum(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.getSerializedSize()",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.getSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.hasSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.newBuilder(BinaryStatistics)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BinaryStatistics.writeTo(CodedOutputStream)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.BucketStatistics(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.BucketStatistics(CodedInputStream,ExtensionRegistryLite)",2,10,14
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.BucketStatistics(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.addAllCount(Iterable<? extends Long>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.addCount(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.buildPartial()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.clearCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.ensureCountIsMutable()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.getCount(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.getCountCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.getCountList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.maybeForceBuilderInitialization()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.mergeFrom(BucketStatistics)",2,3,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.Builder.setCount(int,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.getCount(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.getCountCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.getCountList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.getSerializedSize()",2,3,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.newBuilder(BucketStatistics)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.BucketStatistics.writeTo(CodedOutputStream)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.buildPartial()",1,1,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.clearDictionarySize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.clearKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.getDictionarySize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.getKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.hasDictionarySize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.hasKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.maybeForceBuilderInitialization()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.mergeFrom(ColumnEncoding)",2,3,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.setDictionarySize(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Builder.setKind(Kind)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.ColumnEncoding(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.ColumnEncoding(CodedInputStream,ExtensionRegistryLite)",2,8,10
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.ColumnEncoding(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind.Kind(int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind.getNumber()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind.getValueDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind.internalGetValueMap()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind.valueOf(EnumValueDescriptor)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.Kind.valueOf(int)",6,2,6
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.getDictionarySize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.getKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.getSerializedSize()",2,3,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.hasDictionarySize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.hasKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.isInitialized()",3,1,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.newBuilder(ColumnEncoding)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnEncoding.writeTo(CodedOutputStream)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.buildPartial()",1,9,18
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.clear()",1,9,9
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.clearBinaryStatistics()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.clearBucketStatistics()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.clearDateStatistics()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.clearDecimalStatistics()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.clearDoubleStatistics()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.clearIntStatistics()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.clearNumberOfValues()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.clearStringStatistics()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.clearTimestampStatistics()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getBinaryStatistics()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getBinaryStatisticsBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getBinaryStatisticsFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getBinaryStatisticsOrBuilder()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getBucketStatistics()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getBucketStatisticsBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getBucketStatisticsFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getBucketStatisticsOrBuilder()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDateStatistics()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDateStatisticsBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDateStatisticsFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDateStatisticsOrBuilder()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDecimalStatistics()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDecimalStatisticsBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDecimalStatisticsFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDecimalStatisticsOrBuilder()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDoubleStatistics()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDoubleStatisticsBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDoubleStatisticsFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getDoubleStatisticsOrBuilder()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getIntStatistics()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getIntStatisticsBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getIntStatisticsFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getIntStatisticsOrBuilder()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getNumberOfValues()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getStringStatistics()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getStringStatisticsBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getStringStatisticsFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getStringStatisticsOrBuilder()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getTimestampStatistics()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getTimestampStatisticsBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getTimestampStatisticsFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.getTimestampStatisticsOrBuilder()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.hasBinaryStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.hasBucketStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.hasDateStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.hasDecimalStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.hasDoubleStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.hasIntStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.hasNumberOfValues()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.hasStringStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.hasTimestampStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.maybeForceBuilderInitialization()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.mergeBinaryStatistics(BinaryStatistics)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.mergeBucketStatistics(BucketStatistics)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.mergeDateStatistics(DateStatistics)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.mergeDecimalStatistics(DecimalStatistics)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.mergeDoubleStatistics(DoubleStatistics)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.mergeFrom(ColumnStatistics)",2,10,11
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.mergeIntStatistics(IntegerStatistics)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.mergeStringStatistics(StringStatistics)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.mergeTimestampStatistics(TimestampStatistics)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setBinaryStatistics(BinaryStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setBinaryStatistics(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setBucketStatistics(BucketStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setBucketStatistics(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setDateStatistics(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setDateStatistics(DateStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setDecimalStatistics(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setDecimalStatistics(DecimalStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setDoubleStatistics(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setDoubleStatistics(DoubleStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setIntStatistics(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setIntStatistics(IntegerStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setNumberOfValues(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setStringStatistics(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setStringStatistics(StringStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setTimestampStatistics(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.Builder.setTimestampStatistics(TimestampStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.ColumnStatistics(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.ColumnStatistics(CodedInputStream,ExtensionRegistryLite)",2,30,32
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.ColumnStatistics(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getBinaryStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getBinaryStatisticsOrBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getBucketStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getBucketStatisticsOrBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDateStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDateStatisticsOrBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDecimalStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDecimalStatisticsOrBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDoubleStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getDoubleStatisticsOrBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getIntStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getIntStatisticsOrBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getNumberOfValues()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getSerializedSize()",2,10,11
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getStringStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getStringStatisticsOrBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getTimestampStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getTimestampStatisticsOrBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.hasBinaryStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.hasBucketStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.hasDateStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.hasDecimalStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.hasDoubleStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.hasIntStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.hasNumberOfValues()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.hasStringStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.hasTimestampStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.newBuilder(ColumnStatistics)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.ColumnStatistics.writeTo(CodedOutputStream)",1,10,10
"org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind.CompressionKind(int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind.getNumber()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind.getValueDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind.internalGetValueMap()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind.valueOf(EnumValueDescriptor)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.CompressionKind.valueOf(int)",6,2,6
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.buildPartial()",1,1,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.clearMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.clearMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.getMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.getMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.hasMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.hasMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.maybeForceBuilderInitialization()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.mergeFrom(DateStatistics)",2,3,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.setMaximum(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.Builder.setMinimum(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.DateStatistics(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.DateStatistics(CodedInputStream,ExtensionRegistryLite)",2,7,9
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.DateStatistics(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.getMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.getMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.getSerializedSize()",2,3,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.hasMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.hasMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.newBuilder(DateStatistics)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DateStatistics.writeTo(CodedOutputStream)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.buildPartial()",1,1,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.clearMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.clearMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.clearSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.getMaximum()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.getMaximumBytes()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.getMinimum()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.getMinimumBytes()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.getSum()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.getSumBytes()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.hasMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.hasMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.hasSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.maybeForceBuilderInitialization()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.mergeFrom(DecimalStatistics)",2,4,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.setMaximum(String)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.setMaximumBytes(ByteString)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.setMinimum(String)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.setMinimumBytes(ByteString)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.setSum(String)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.Builder.setSumBytes(ByteString)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.DecimalStatistics(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.DecimalStatistics(CodedInputStream,ExtensionRegistryLite)",2,8,10
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.DecimalStatistics(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getMaximum()",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getMaximumBytes()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getMinimum()",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getMinimumBytes()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getSerializedSize()",2,4,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getSum()",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getSumBytes()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.hasMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.hasMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.hasSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.newBuilder(DecimalStatistics)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DecimalStatistics.writeTo(CodedOutputStream)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.buildPartial()",1,1,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.clearMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.clearMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.clearSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.getMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.getMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.getSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.hasMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.hasMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.hasSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.maybeForceBuilderInitialization()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.mergeFrom(DoubleStatistics)",2,4,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.setMaximum(double)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.setMinimum(double)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.Builder.setSum(double)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.DoubleStatistics(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.DoubleStatistics(CodedInputStream,ExtensionRegistryLite)",2,8,10
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.DoubleStatistics(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.getMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.getMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.getSerializedSize()",2,4,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.getSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.hasMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.hasMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.hasSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.newBuilder(DoubleStatistics)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.DoubleStatistics.writeTo(CodedOutputStream)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addAllMetadata(Iterable<? extends UserMetadataItem>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addAllStatistics(Iterable<? extends ColumnStatistics>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addAllStripes(Iterable<? extends StripeInformation>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addAllTypes(Iterable<? extends Type>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addMetadata(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addMetadata(UserMetadataItem)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addMetadata(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addMetadata(int,UserMetadataItem)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addMetadataBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addMetadataBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addStatistics(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addStatistics(ColumnStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addStatistics(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addStatistics(int,ColumnStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addStatisticsBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addStatisticsBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addStripes(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addStripes(StripeInformation)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addStripes(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addStripes(int,StripeInformation)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addStripesBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addStripesBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addTypes(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addTypes(Type)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addTypes(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addTypes(int,Type)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addTypesBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.addTypesBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.buildPartial()",1,9,13
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.clear()",1,5,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.clearContentLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.clearHeaderLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.clearMetadata()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.clearNumberOfRows()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.clearRowIndexStride()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.clearStatistics()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.clearStripes()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.clearTypes()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.ensureMetadataIsMutable()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.ensureStatisticsIsMutable()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.ensureStripesIsMutable()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.ensureTypesIsMutable()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getContentLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getHeaderLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getMetadata(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getMetadataBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getMetadataBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getMetadataCount()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getMetadataFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getMetadataList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getMetadataOrBuilder(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getMetadataOrBuilderList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getNumberOfRows()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getRowIndexStride()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStatistics(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStatisticsBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStatisticsBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStatisticsCount()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStatisticsFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStatisticsList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStatisticsOrBuilder(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStatisticsOrBuilderList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStripes(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStripesBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStripesBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStripesCount()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStripesFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStripesList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStripesOrBuilder(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getStripesOrBuilderList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getTypes(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getTypesBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getTypesBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getTypesCount()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getTypesFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getTypesList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getTypesOrBuilder(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.getTypesOrBuilderList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.hasContentLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.hasHeaderLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.hasNumberOfRows()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.hasRowIndexStride()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.isInitialized()",5,3,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.maybeForceBuilderInitialization()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.mergeFrom(Footer)",2,29,30
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.removeMetadata(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.removeStatistics(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.removeStripes(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.removeTypes(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.setContentLength(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.setHeaderLength(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.setMetadata(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.setMetadata(int,UserMetadataItem)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.setNumberOfRows(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.setRowIndexStride(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.setStatistics(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.setStatistics(int,ColumnStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.setStripes(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.setStripes(int,StripeInformation)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.setTypes(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Builder.setTypes(int,Type)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Footer(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Footer(CodedInputStream,ExtensionRegistryLite)",2,17,23
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.Footer(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getContentLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getHeaderLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getMetadata(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getMetadataCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getMetadataList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getMetadataOrBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getMetadataOrBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getNumberOfRows()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getRowIndexStride()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getSerializedSize()",2,9,10
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getStatistics(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getStatisticsCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getStatisticsList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getStatisticsOrBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getStatisticsOrBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getStripes(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getStripesCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getStripesList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getStripesOrBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getStripesOrBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getTypes(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getTypesCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getTypesList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getTypesOrBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getTypesOrBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.hasContentLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.hasHeaderLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.hasNumberOfRows()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.hasRowIndexStride()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.isInitialized()",6,3,6
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.newBuilder(Footer)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Footer.writeTo(CodedOutputStream)",1,9,9
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.buildPartial()",1,1,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.clearMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.clearMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.clearSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.getMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.getMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.getSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.hasMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.hasMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.hasSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.maybeForceBuilderInitialization()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.mergeFrom(IntegerStatistics)",2,4,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.setMaximum(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.setMinimum(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.Builder.setSum(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.IntegerStatistics(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.IntegerStatistics(CodedInputStream,ExtensionRegistryLite)",2,8,10
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.IntegerStatistics(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getSerializedSize()",2,4,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.hasMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.hasMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.hasSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.newBuilder(IntegerStatistics)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.IntegerStatistics.writeTo(CodedOutputStream)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.addAllStripeStats(Iterable<? extends StripeStatistics>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.addStripeStats(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.addStripeStats(StripeStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.addStripeStats(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.addStripeStats(int,StripeStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.addStripeStatsBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.addStripeStatsBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.buildPartial()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.clear()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.clearStripeStats()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.ensureStripeStatsIsMutable()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.getStripeStats(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.getStripeStatsBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.getStripeStatsBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.getStripeStatsCount()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.getStripeStatsFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.getStripeStatsList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.getStripeStatsOrBuilder(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.getStripeStatsOrBuilderList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.maybeForceBuilderInitialization()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.mergeFrom(Metadata)",2,7,8
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.removeStripeStats(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.setStripeStats(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Builder.setStripeStats(int,StripeStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Metadata(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Metadata(CodedInputStream,ExtensionRegistryLite)",2,7,10
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.Metadata(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.getSerializedSize()",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.getStripeStats(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.getStripeStatsCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.getStripeStatsList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.getStripeStatsOrBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.getStripeStatsOrBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.newBuilder(Metadata)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Metadata.writeTo(CodedOutputStream)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.OrcProto()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.addAllVersion(Iterable<? extends Integer>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.addVersion(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.buildPartial()",1,2,7
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.clearCompression()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.clearCompressionBlockSize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.clearFooterLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.clearMagic()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.clearMetadataLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.clearVersion()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.ensureVersionIsMutable()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.getCompression()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.getCompressionBlockSize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.getFooterLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.getMagic()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.getMagicBytes()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.getMetadataLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.getVersion(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.getVersionCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.getVersionList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.hasCompression()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.hasCompressionBlockSize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.hasFooterLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.hasMagic()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.hasMetadataLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.maybeForceBuilderInitialization()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.mergeFrom(PostScript)",2,8,9
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.setCompression(CompressionKind)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.setCompressionBlockSize(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.setFooterLength(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.setMagic(String)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.setMagicBytes(ByteString)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.setMetadataLength(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.Builder.setVersion(int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.PostScript(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.PostScript(CodedInputStream,ExtensionRegistryLite)",2,16,20
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.PostScript(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getCompression()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getCompressionBlockSize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getFooterLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getMagic()",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getMagicBytes()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getMetadataLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getSerializedSize()",2,8,9
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getVersion(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getVersionCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.getVersionList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.hasCompression()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.hasCompressionBlockSize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.hasFooterLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.hasMagic()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.hasMetadataLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.newBuilder(PostScript)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.PostScript.writeTo(CodedOutputStream)",1,8,8
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.addAllEntry(Iterable<? extends RowIndexEntry>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.addEntry(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.addEntry(RowIndexEntry)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.addEntry(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.addEntry(int,RowIndexEntry)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.addEntryBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.addEntryBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.buildPartial()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.clear()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.clearEntry()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.ensureEntryIsMutable()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.getEntry(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.getEntryBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.getEntryBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.getEntryCount()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.getEntryFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.getEntryList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.getEntryOrBuilder(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.getEntryOrBuilderList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.maybeForceBuilderInitialization()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.mergeFrom(RowIndex)",2,7,8
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.removeEntry(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.setEntry(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.Builder.setEntry(int,RowIndexEntry)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.RowIndex(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.RowIndex(CodedInputStream,ExtensionRegistryLite)",2,7,10
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.RowIndex(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.getEntry(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.getEntryCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.getEntryList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.getEntryOrBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.getEntryOrBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.getSerializedSize()",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.newBuilder(RowIndex)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndex.writeTo(CodedOutputStream)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.addAllPositions(Iterable<? extends Long>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.addPositions(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.buildPartial()",1,3,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.clear()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.clearPositions()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.clearStatistics()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.ensurePositionsIsMutable()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.getPositions(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.getPositionsCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.getPositionsList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.getStatistics()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.getStatisticsBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.getStatisticsFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.getStatisticsOrBuilder()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.hasStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.maybeForceBuilderInitialization()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.mergeFrom(RowIndexEntry)",2,4,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.mergeStatistics(ColumnStatistics)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.setPositions(int,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.setStatistics(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.Builder.setStatistics(ColumnStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.RowIndexEntry(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.RowIndexEntry(CodedInputStream,ExtensionRegistryLite)",2,13,17
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.RowIndexEntry(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getPositions(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getPositionsCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getPositionsList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getSerializedSize()",2,4,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getStatisticsOrBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.hasStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.newBuilder(RowIndexEntry)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry.writeTo(CodedOutputStream)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.buildPartial()",1,1,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.clearColumn()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.clearKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.clearLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.getColumn()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.getKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.getLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.hasColumn()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.hasKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.hasLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.maybeForceBuilderInitialization()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.mergeFrom(Stream)",2,4,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.setColumn(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.setKind(Kind)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Builder.setLength(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind.Kind(int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind.getNumber()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind.getValueDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind.internalGetValueMap()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind.valueOf(EnumValueDescriptor)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Kind.valueOf(int)",9,2,9
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Stream(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Stream(CodedInputStream,ExtensionRegistryLite)",2,9,11
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.Stream(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getColumn()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getSerializedSize()",2,4,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.hasColumn()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.hasKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.hasLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.isInitialized()",3,1,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.newBuilder(Stream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Stream.writeTo(CodedOutputStream)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.buildPartial()",1,1,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.clearMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.clearMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.clearSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.getMaximum()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.getMaximumBytes()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.getMinimum()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.getMinimumBytes()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.getSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.hasMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.hasMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.hasSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.maybeForceBuilderInitialization()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.mergeFrom(StringStatistics)",2,4,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.setMaximum(String)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.setMaximumBytes(ByteString)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.setMinimum(String)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.setMinimumBytes(ByteString)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.Builder.setSum(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.StringStatistics(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.StringStatistics(CodedInputStream,ExtensionRegistryLite)",2,8,10
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.StringStatistics(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getMaximum()",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getMaximumBytes()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getMinimum()",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getMinimumBytes()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getSerializedSize()",2,4,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.hasMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.hasMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.hasSum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.newBuilder(StringStatistics)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StringStatistics.writeTo(CodedOutputStream)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.addAllColumns(Iterable<? extends ColumnEncoding>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.addAllStreams(Iterable<? extends Stream>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.addColumns(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.addColumns(ColumnEncoding)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.addColumns(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.addColumns(int,ColumnEncoding)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.addColumnsBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.addColumnsBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.addStreams(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.addStreams(Stream)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.addStreams(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.addStreams(int,Stream)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.addStreamsBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.addStreamsBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.buildPartial()",1,5,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.clear()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.clearColumns()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.clearStreams()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.ensureColumnsIsMutable()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.ensureStreamsIsMutable()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getColumns(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getColumnsBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getColumnsBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getColumnsCount()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getColumnsFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getColumnsList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getColumnsOrBuilder(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getColumnsOrBuilderList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getStreams(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getStreamsBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getStreamsBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getStreamsCount()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getStreamsFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getStreamsList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getStreamsOrBuilder(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.getStreamsOrBuilderList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.isInitialized()",5,3,5
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.maybeForceBuilderInitialization()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.mergeFrom(StripeFooter)",2,13,14
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.removeColumns(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.removeStreams(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.setColumns(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.setColumns(int,ColumnEncoding)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.setStreams(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.Builder.setStreams(int,Stream)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.StripeFooter(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.StripeFooter(CodedInputStream,ExtensionRegistryLite)",2,9,13
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.StripeFooter(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getColumns(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getColumnsCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getColumnsList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getColumnsOrBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getColumnsOrBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getSerializedSize()",2,3,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getStreams(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getStreamsCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getStreamsList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getStreamsOrBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getStreamsOrBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.isInitialized()",6,3,6
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.newBuilder(StripeFooter)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeFooter.writeTo(CodedOutputStream)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.buildPartial()",1,1,6
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.clearDataLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.clearFooterLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.clearIndexLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.clearNumberOfRows()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.clearOffset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.getDataLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.getFooterLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.getIndexLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.getNumberOfRows()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.getOffset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.hasDataLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.hasFooterLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.hasIndexLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.hasNumberOfRows()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.hasOffset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.maybeForceBuilderInitialization()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.mergeFrom(StripeInformation)",2,6,7
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.setDataLength(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.setFooterLength(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.setIndexLength(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.setNumberOfRows(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.Builder.setOffset(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.StripeInformation(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.StripeInformation(CodedInputStream,ExtensionRegistryLite)",2,10,12
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.StripeInformation(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getDataLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getFooterLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getIndexLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getNumberOfRows()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getOffset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getSerializedSize()",2,6,7
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.hasDataLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.hasFooterLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.hasIndexLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.hasNumberOfRows()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.hasOffset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.newBuilder(StripeInformation)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeInformation.writeTo(CodedOutputStream)",1,6,6
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.addAllColStats(Iterable<? extends ColumnStatistics>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.addColStats(Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.addColStats(ColumnStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.addColStats(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.addColStats(int,ColumnStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.addColStatsBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.addColStatsBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.buildPartial()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.clear()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.clearColStats()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.ensureColStatsIsMutable()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.getColStats(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.getColStatsBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.getColStatsBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.getColStatsCount()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.getColStatsFieldBuilder()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.getColStatsList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.getColStatsOrBuilder(int)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.getColStatsOrBuilderList()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.maybeForceBuilderInitialization()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.mergeFrom(StripeStatistics)",2,7,8
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.removeColStats(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.setColStats(int,Builder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.Builder.setColStats(int,ColumnStatistics)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.StripeStatistics(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.StripeStatistics(CodedInputStream,ExtensionRegistryLite)",2,7,10
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.StripeStatistics(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.getColStats(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.getColStatsCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.getColStatsList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.getColStatsOrBuilder(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.getColStatsOrBuilderList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.getSerializedSize()",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.newBuilder(StripeStatistics)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.StripeStatistics.writeTo(CodedOutputStream)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.buildPartial()",1,1,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.clearMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.clearMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.getMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.getMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.hasMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.hasMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.maybeForceBuilderInitialization()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.mergeFrom(TimestampStatistics)",2,3,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.setMaximum(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.Builder.setMinimum(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.TimestampStatistics(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.TimestampStatistics(CodedInputStream,ExtensionRegistryLite)",2,7,9
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.TimestampStatistics(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.getMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.getMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.getSerializedSize()",2,3,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.hasMaximum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.hasMinimum()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.newBuilder(TimestampStatistics)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.TimestampStatistics.writeTo(CodedOutputStream)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.addAllFieldNames(Iterable<String>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.addAllSubtypes(Iterable<? extends Integer>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.addFieldNames(String)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.addFieldNamesBytes(ByteString)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.addSubtypes(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.buildPartial()",1,2,7
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.clearFieldNames()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.clearKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.clearMaximumLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.clearPrecision()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.clearScale()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.clearSubtypes()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.ensureFieldNamesIsMutable()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.ensureSubtypesIsMutable()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.getFieldNames(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.getFieldNamesBytes(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.getFieldNamesCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.getFieldNamesList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.getKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.getMaximumLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.getPrecision()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.getScale()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.getSubtypes(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.getSubtypesCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.getSubtypesList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.hasKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.hasMaximumLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.hasPrecision()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.hasScale()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.isInitialized()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.maybeForceBuilderInitialization()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.mergeFrom(Type)",2,9,10
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.setFieldNames(int,String)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.setKind(Kind)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.setMaximumLength(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.setPrecision(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.setScale(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Builder.setSubtypes(int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.Kind(int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.getNumber()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.getValueDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.internalGetValueMap()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.valueOf(EnumValueDescriptor)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Kind.valueOf(int)",20,2,20
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Type(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Type(CodedInputStream,ExtensionRegistryLite)",2,16,22
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.Type(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getFieldNames(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getFieldNamesBytes(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getFieldNamesCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getFieldNamesList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getMaximumLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getPrecision()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getScale()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getSerializedSize()",2,8,9
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getSubtypes(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getSubtypesCount()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getSubtypesList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.hasKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.hasMaximumLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.hasPrecision()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.hasScale()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.isInitialized()",3,1,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.newBuilder(Type)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.Type.writeTo(CodedOutputStream)",1,8,8
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.Builder(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.build()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.buildPartial()",1,1,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.clearName()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.clearValue()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.getDescriptorForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.getName()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.getNameBytes()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.getValue()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.hasName()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.hasValue()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.isInitialized()",3,1,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.maybeForceBuilderInitialization()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.mergeFrom(CodedInputStream,ExtensionRegistryLite)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.mergeFrom(Message)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.mergeFrom(UserMetadataItem)",2,3,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.setName(String)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.setNameBytes(ByteString)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.Builder.setValue(ByteString)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.UserMetadataItem(Builder<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.UserMetadataItem(CodedInputStream,ExtensionRegistryLite)",2,7,9
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.UserMetadataItem(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.getDefaultInstance()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.getDefaultInstanceForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.getName()",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.getNameBytes()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.getParserForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.getSerializedSize()",2,3,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.getUnknownFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.getValue()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.hasName()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.hasValue()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.initFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.internalGetFieldAccessorTable()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.isInitialized()",4,1,4
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.newBuilder(UserMetadataItem)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.newBuilderForType()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.newBuilderForType(BuilderParent)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.parseDelimitedFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.parseDelimitedFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.parseFrom(ByteString)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.parseFrom(ByteString,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.parseFrom(CodedInputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.parseFrom(CodedInputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.parseFrom(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.parseFrom(InputStream,ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.parseFrom(byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.parseFrom(byte[],ExtensionRegistryLite)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.toBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.writeReplace()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem.writeTo(CodedOutputStream)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcProto.getDescriptor()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcProto.registerAllExtensions(ExtensionRegistry)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.OrcRawRecordMerger(Configuration,boolean,Reader,boolean,int,ValidTxnList,Options,Path[])",1,12,12
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.OriginalReaderPair.OriginalReaderPair(ReaderKey,Reader,int,RecordIdentifier,RecordIdentifier,Options)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.OriginalReaderPair.getColumns()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.OriginalReaderPair.next(OrcStruct)",1,6,6
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.ReaderKey.ReaderKey()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.ReaderKey.ReaderKey(long,int,long,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.ReaderKey.compareRow(RecordIdentifier)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.ReaderKey.compareTo(RecordIdentifier)",4,2,5
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.ReaderKey.equals(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.ReaderKey.getCurrentTransactionId()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.ReaderKey.set(RecordIdentifier)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.ReaderKey.setValues(long,int,long,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.ReaderKey.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.ReaderPair.ReaderPair(ReaderKey,Reader,int,RecordIdentifier,RecordIdentifier,Options)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.ReaderPair.getColumns()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.ReaderPair.next(OrcStruct)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.close()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.createEventOptions(Options)",1,3,5
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.createValue()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.discoverKeyBounds(Reader,Options)",4,3,6
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.discoverOriginalKeyBounds(Reader,int,Options)",4,4,6
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getColumns()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getCurrentReader()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getLastFlushLength(FileSystem,Path)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getMaxKey()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getMinKey()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getObjectInspector()",1,6,6
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getOtherReaders()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getProgress()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.next(RecordIdentifier,OrcStruct)",3,9,11
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.AcidStats.AcidStats()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.AcidStats.AcidStats(String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.AcidStats.serialize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.KeyIndexBuilder.addKey(int,long,int,long)",2,2,5
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.KeyIndexBuilder.preFooterWrite(WriterContext)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.KeyIndexBuilder.preStripeWrite(WriterContext)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.OrcOptions.OrcOptions(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.OrcOptions.getOrcOptions()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.OrcOptions.orcOptions(WriterOptions)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.OrcRecordUpdater(Path,Options)",1,9,9
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.addEvent(int,long,long,long,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.close(boolean)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.createEventSchema(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.delete(long,long,long)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.flush()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.getBucket(OrcStruct)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.getCurrentTransaction(OrcStruct)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.getOperation(OrcStruct)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.getOriginalTransaction(OrcStruct)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.getRow(OrcStruct)",2,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.getRowId(OrcStruct)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.getSideFile(Path)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.getStats()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.getWriter()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.insert(long,Object)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.parseKeyIndex(Reader)",1,3,4
"org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.update(long,long,long,Object)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OrcSerde.OrcSerdeRow.getInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSerde.OrcSerdeRow.getRow()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSerde.OrcSerdeRow.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSerde.OrcSerdeRow.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSerde.deserialize(Writable)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSerde.deserializeVector(Object,int,VectorizedRowBatch)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSerde.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSerde.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSerde.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSerde.initialize(Configuration,Properties)",1,7,7
"org.apache.hadoop.hive.ql.io.orc.OrcSerde.serialize(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSerde.serializeVector(VectorizedRowBatch,ObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcSplit.OrcSplit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSplit.OrcSplit(Path,long,long,String[],FileMetaInfo,boolean,boolean,List<Long>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSplit.getDeltas()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSplit.getFileMetaInfo()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSplit.hasBase()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSplit.hasFooter()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSplit.isOriginal()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcSplit.readFields(DataInput)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcSplit.write(DataOutput)",1,3,6
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.Field.Field(String,ObjectInspector,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.Field.getFieldComment()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.Field.getFieldID()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.Field.getFieldName()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.Field.getFieldObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcListObjectInspector.OrcListObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcListObjectInspector.OrcListObjectInspector(ListTypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcListObjectInspector.OrcListObjectInspector(int,List<Type>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcListObjectInspector.create(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcListObjectInspector.equals(Object)",3,4,4
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcListObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcListObjectInspector.getList(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcListObjectInspector.getListElement(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcListObjectInspector.getListElementObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcListObjectInspector.getListLength(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcListObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcListObjectInspector.resize(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcListObjectInspector.set(Object,int,Object)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.OrcMapObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.OrcMapObjectInspector(MapTypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.OrcMapObjectInspector(int,List<Type>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.clear(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.equals(Object)",3,5,5
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.getMap(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.getMapKeyObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.getMapSize(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.getMapValueElement(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.getMapValueObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.put(Object,Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcMapObjectInspector.remove(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcStruct(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcStructInspector.OrcStructInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcStructInspector.OrcStructInspector(List<StructField>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcStructInspector.OrcStructInspector(StructTypeInfo)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcStructInspector.OrcStructInspector(int,List<Type>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcStructInspector.create()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcStructInspector.equals(Object)",6,6,8
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcStructInspector.getAllStructFieldRefs()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcStructInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcStructInspector.getStructFieldData(Object,StructField)",3,1,3
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcStructInspector.getStructFieldRef(String)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcStructInspector.getStructFieldsDataAsList(Object)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcStructInspector.getTypeName()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.OrcStructInspector.setStructFieldData(Object,StructField,Object)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.createObjectInspector(TypeInfo)",22,7,22
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.createObjectInspector(int,List<Type>)",21,5,24
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.equals(Object)",7,5,8
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.getFieldValue(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.getNumFields()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.hashCode()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.linkFields(OrcStruct)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.setFieldValue(int,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.setNumFields(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.toString()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcStruct.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.OrcUnionObjectInspector.OrcUnionObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.OrcUnionObjectInspector.OrcUnionObjectInspector(UnionTypeInfo)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.OrcUnionObjectInspector.OrcUnionObjectInspector(int,List<Type>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.OrcUnionObjectInspector.equals(Object)",6,5,7
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.OrcUnionObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.OrcUnionObjectInspector.getField(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.OrcUnionObjectInspector.getObjectInspectors()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.OrcUnionObjectInspector.getTag(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.OrcUnionObjectInspector.getTypeName()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.equals(Object)",4,4,5
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.getObject()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.getTag()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.hashCode()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.set(byte,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OrcUnion.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OutStream.OutStream(String,int,CompressionCodec,OutputReceiver)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OutStream.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OutStream.flip()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.OutStream.flush()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OutStream.getBufferSize()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.OutStream.getNewInputBuffer()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OutStream.getNewOutputBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OutStream.getPosition(PositionRecorder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.OutStream.isSuppressed()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OutStream.spill()",2,11,13
"org.apache.hadoop.hive.ql.io.orc.OutStream.suppress()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OutStream.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.OutStream.write(byte[],int,int)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OutStream.write(int)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.OutStream.writeHeader(ByteBuffer,int,int,boolean)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.Reader.Options.clone()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.Reader.Options.getColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.Reader.Options.getInclude()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.Reader.Options.getLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.Reader.Options.getMaxOffset()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.Reader.Options.getOffset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.Reader.Options.getSearchArgument()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.Reader.Options.include(boolean[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.Reader.Options.range(long,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.Reader.Options.searchArgument(SearchArgument,String[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.Reader.Options.toString()",1,7,7
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.FileMetaInfo.FileMetaInfo(String,int,int,ByteBuffer)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.FileMetaInfo.FileMetaInfo(String,int,int,ByteBuffer,List<Integer>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.MetaInfoObjExtractor.MetaInfoObjExtractor(String,int,int,ByteBuffer)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.ReaderImpl(Path,ReaderOptions)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.StripeInformationImpl.StripeInformationImpl(StripeInformation)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.StripeInformationImpl.getDataLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.StripeInformationImpl.getFooterLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.StripeInformationImpl.getIndexLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.StripeInformationImpl.getLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.StripeInformationImpl.getNumberOfRows()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.StripeInformationImpl.getOffset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.StripeInformationImpl.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.checkOrcVersion(Log,Path,List<Integer>)",1,6,6
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.ensureOrcFooter(FSDataInputStream,Path,int,ByteBuffer)",4,2,4
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(FileSystem,Path,long)",2,4,8
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames(List<String>)",1,6,6
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getCompression()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getCompressionSize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getContentLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getFileMetaInfo()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getLastIdx()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getMetadata()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getMetadataKeys()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getMetadataValue(String)",3,4,4
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getNumberOfRows()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getOrcProtoStripeStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getOrcProtoUserMetadata()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSize()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeFromColIndices(List<Integer>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumn(int)",8,8,10
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumns(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRowIndexStride()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getStatistics()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getStripes()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getTypes()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.hasMetadataValue(String)",3,3,4
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rows()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rows(boolean[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rows(long,long,boolean[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rows(long,long,boolean[],SearchArgument,String[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(Options)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.ReaderImpl.versionString(List<Integer>)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BinaryTreeReader.BinaryTreeReader(Path,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BinaryTreeReader.checkEncoding(ColumnEncoding)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BinaryTreeReader.next(Object)",4,3,5
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BinaryTreeReader.nextVector(Object,long)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BinaryTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BinaryTreeReader.skipRows(long)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BinaryTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BooleanTreeReader.BooleanTreeReader(Path,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BooleanTreeReader.next(Object)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BooleanTreeReader.nextVector(Object,long)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BooleanTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BooleanTreeReader.skipRows(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BooleanTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BufferChunk.BufferChunk(ByteBuffer,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BufferChunk.toString()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteBufferAllocatorPool.Key.Key(int,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteBufferAllocatorPool.Key.compareTo(Key)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteBufferAllocatorPool.Key.equals(Object)",2,1,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteBufferAllocatorPool.Key.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteBufferAllocatorPool.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteBufferAllocatorPool.getBuffer(boolean,int)",2,3,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteBufferAllocatorPool.getBufferTree(boolean)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteBufferAllocatorPool.putBuffer(ByteBuffer)",3,3,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteTreeReader.ByteTreeReader(Path,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteTreeReader.next(Object)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteTreeReader.nextVector(Object,long)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteTreeReader.skipRows(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BytesColumnVectorUtil.setRefToOrcByteArrays(InStream,IntegerReader,LongColumnVector,BytesColumnVector,long)",3,7,12
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.CharTreeReader.CharTreeReader(Path,int,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.CharTreeReader.next(Object)",2,1,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DateTreeReader.DateTreeReader(Path,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DateTreeReader.checkEncoding(ColumnEncoding)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DateTreeReader.next(Object)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DateTreeReader.nextVector(Object,long)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DateTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DateTreeReader.skipRows(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DateTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DecimalTreeReader.DecimalTreeReader(Path,int,int,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DecimalTreeReader.checkEncoding(ColumnEncoding)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DecimalTreeReader.next(Object)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DecimalTreeReader.nextVector(Object,long)",1,7,8
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DecimalTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DecimalTreeReader.skipRows(long)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DecimalTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DiskRange.DiskRange(long,long)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DiskRange.equals(Object)",2,2,4
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DiskRange.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DoubleTreeReader.DoubleTreeReader(Path,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DoubleTreeReader.next(Object)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DoubleTreeReader.nextVector(Object,long)",1,3,7
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DoubleTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DoubleTreeReader.skipRows(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.DoubleTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.FloatTreeReader.FloatTreeReader(Path,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.FloatTreeReader.next(Object)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.FloatTreeReader.nextVector(Object,long)",1,3,7
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.FloatTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.FloatTreeReader.skipRows(long)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.FloatTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.IntTreeReader.IntTreeReader(Path,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.IntTreeReader.checkEncoding(ColumnEncoding)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.IntTreeReader.next(Object)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.IntTreeReader.nextVector(Object,long)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.IntTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.IntTreeReader.skipRows(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.IntTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ListTreeReader.ListTreeReader(Path,int,List<Type>,boolean[],Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ListTreeReader.checkEncoding(ColumnEncoding)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ListTreeReader.next(Object)",1,6,7
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ListTreeReader.nextVector(Object,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ListTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ListTreeReader.skipRows(long)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ListTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.LongTreeReader.LongTreeReader(Path,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.LongTreeReader.checkEncoding(ColumnEncoding)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.LongTreeReader.next(Object)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.LongTreeReader.nextVector(Object,long)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.LongTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.LongTreeReader.skipRows(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.LongTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.MapTreeReader.MapTreeReader(Path,int,List<Type>,boolean[],Configuration)",1,3,5
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.MapTreeReader.checkEncoding(ColumnEncoding)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.MapTreeReader.next(Object)",1,3,4
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.MapTreeReader.nextVector(Object,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.MapTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.MapTreeReader.skipRows(long)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.MapTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.PositionProviderImpl.PositionProviderImpl(RowIndexEntry)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.PositionProviderImpl.getNext()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.RecordReaderImpl(List<StripeInformation>,FileSystem,Path,Options,List<Type>,CompressionCodec,int,long,Configuration)",1,10,10
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ShortTreeReader.ShortTreeReader(Path,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ShortTreeReader.checkEncoding(ColumnEncoding)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ShortTreeReader.next(Object)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ShortTreeReader.nextVector(Object,long)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ShortTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ShortTreeReader.skipRows(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ShortTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDictionaryTreeReader.StringDictionaryTreeReader(Path,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDictionaryTreeReader.checkEncoding(ColumnEncoding)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDictionaryTreeReader.getDictionaryEntryLength(int,int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDictionaryTreeReader.next(Object)",1,3,4
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDictionaryTreeReader.nextVector(Object,long)",1,6,7
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDictionaryTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDictionaryTreeReader.skipRows(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDictionaryTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,3,5
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDirectTreeReader.StringDirectTreeReader(Path,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDirectTreeReader.checkEncoding(ColumnEncoding)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDirectTreeReader.next(Object)",4,3,5
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDirectTreeReader.nextVector(Object,long)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDirectTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDirectTreeReader.skipRows(long)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringDirectTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringTreeReader.StringTreeReader(Path,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringTreeReader.checkEncoding(ColumnEncoding)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringTreeReader.next(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringTreeReader.nextVector(Object,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringTreeReader.skipRows(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StringTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",2,2,4
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StructTreeReader.StructTreeReader(Path,int,List<Type>,boolean[],Configuration)",1,3,4
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StructTreeReader.next(Object)",1,6,6
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StructTreeReader.nextVector(Object,long)",1,4,5
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StructTreeReader.seek(PositionProvider[])",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StructTreeReader.skipRows(long)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.StructTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TimestampTreeReader.TimestampTreeReader(Path,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TimestampTreeReader.checkEncoding(ColumnEncoding)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TimestampTreeReader.next(Object)",1,2,4
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TimestampTreeReader.nextVector(Object,long)",1,3,12
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TimestampTreeReader.parseNanos(long)",1,1,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TimestampTreeReader.seek(PositionProvider[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TimestampTreeReader.skipRows(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TimestampTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TreeReader.TreeReader(Path,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TreeReader.checkEncoding(ColumnEncoding)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TreeReader.countNonNulls(long)",2,3,4
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TreeReader.createIntegerReader(Kind,InStream,boolean)",4,2,4
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TreeReader.next(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TreeReader.nextVector(Object,long)",1,3,6
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TreeReader.seek(PositionProvider[])",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.TreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.UnionTreeReader.UnionTreeReader(Path,int,List<Type>,boolean[],Configuration)",1,3,4
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.UnionTreeReader.next(Object)",1,2,4
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.UnionTreeReader.nextVector(Object,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.UnionTreeReader.seek(PositionProvider[])",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.UnionTreeReader.skipRows(long)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.UnionTreeReader.startStripe(Map<StreamName, InStream>,List<ColumnEncoding>)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.VarcharTreeReader.VarcharTreeReader(Path,int,int,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.VarcharTreeReader.next(Object)",2,1,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(long)",4,7,11
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.clearStreams()",1,5,5
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.close()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.compareToRange(Comparable<T>,T,T)",5,1,5
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.createStreams(List<Stream>,List<BufferChunk>,boolean[],CompressionCodec,int,Map<StreamName, InStream>)",1,7,10
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.createTreeReader(Path,int,List<Type>,boolean[],Configuration)",21,4,24
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.evaluatePredicate(ColumnStatistics,PredicateLeaf)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.evaluatePredicateRange(PredicateLeaf,Object,Object)",20,9,35
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.findColumns(String[],String,int)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.findStripe(long)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.getBaseObjectForComparison(Object,Object)",16,17,17
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.getConvertedStatsObj(Object,Object)",3,3,6
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.getIndexPosition(Kind,Kind,Kind,boolean,boolean)",13,2,16
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.getMax(ColumnStatistics)",7,7,7
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.getMin(ColumnStatistics)",7,7,7
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.getProgress()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.getRowNumber()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.hasNext()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.isDictionary(Kind,ColumnEncoding)",1,1,4
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.mapSargColumns(List<PredicateLeaf>,String[],int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.mergeDiskRanges(List<DiskRange>)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.next(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.nextBatch(VectorizedRowBatch)",1,6,14
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.overlap(long,long,long,long)",2,1,2
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.pickRowGroups()",4,6,11
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.planReadPartialDataStreams(List<Stream>,RowIndex[],boolean[],boolean[],boolean,List<ColumnEncoding>,List<Type>,int)",1,12,13
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readAllDataStreams(StripeInformation)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readDiskRanges(FSDataInputStream,long,List<DiskRange>)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readPartialDataStreams(StripeInformation)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readRowIndex(int)",1,5,7
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe()",1,7,10
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripeFooter(StripeInformation)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.seekToRow(long)",3,2,4
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.seekToRowEntry(int)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.stringifyDiskRanges(List<DiskRange>)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RedBlackTree.RedBlackTree(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RedBlackTree.add()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.RedBlackTree.add(int,boolean,int,int,int)",10,17,19
"org.apache.hadoop.hive.ql.io.orc.RedBlackTree.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RedBlackTree.getLeft(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RedBlackTree.getRight(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RedBlackTree.getSizeInBytes()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RedBlackTree.insert(int,int,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RedBlackTree.isRed(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RedBlackTree.setLeft(int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RedBlackTree.setLeft(int,int,boolean)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.RedBlackTree.setRed(int,boolean)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RedBlackTree.setRight(int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RedBlackTree.size()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthByteReader.RunLengthByteReader(InStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthByteReader.hasNext()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RunLengthByteReader.next()",1,2,3
"org.apache.hadoop.hive.ql.io.orc.RunLengthByteReader.nextVector(LongColumnVector,long)",1,3,7
"org.apache.hadoop.hive.ql.io.orc.RunLengthByteReader.readValues()",6,4,6
"org.apache.hadoop.hive.ql.io.orc.RunLengthByteReader.seek(PositionProvider)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RunLengthByteReader.skip(long)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RunLengthByteReader.toString()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.RunLengthByteWriter.RunLengthByteWriter(PositionedOutputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthByteWriter.flush()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthByteWriter.getPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthByteWriter.write(byte)",1,8,9
"org.apache.hadoop.hive.ql.io.orc.RunLengthByteWriter.writeValues()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReader.RunLengthIntegerReader(InStream,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReader.hasNext()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReader.next()",1,2,3
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReader.nextVector(LongColumnVector,long)",1,3,7
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReader.readValues()",4,6,7
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReader.seek(PositionProvider)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReader.skip(long)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.RunLengthIntegerReaderV2(InStream,boolean,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.hasNext()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.next()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.nextVector(LongColumnVector,long)",1,3,7
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readDeltaValues(int)",1,4,7
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readDirectValues(int)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readPatchedBaseValues(int)",2,2,11
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readShortRepeatValues(int)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readValues()",2,5,5
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.seek(PositionProvider)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.skip(long)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriter.RunLengthIntegerWriter(PositionedOutputStream,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriter.flush()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriter.getPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriter.write(long)",1,8,14
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriter.writeValues()",1,6,6
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.RunLengthIntegerWriterV2(PositionedOutputStream,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.RunLengthIntegerWriterV2(PositionedOutputStream,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.determineEncoding()",7,8,36
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.flush()",1,6,7
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.getOpcode()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.getPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.initializeLiterals(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.preparePatchedBlob()",1,2,11
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.write(long)",1,10,18
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.writeDeltaValues()",1,5,7
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.writeDirectValues()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.writePatchedBaseValues()",1,2,6
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.writeShortRepeatValues()",1,3,4
"org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.writeValues()",1,5,5
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.SerializationUtils()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.bytesToLongBE(InStream,int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.decodeBitWidth(int)",9,9,10
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.encodeBitWidth(int)",9,9,17
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.findClosestNumBits(long)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.getClosestAlignedFixedBits(int)",11,1,21
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.getClosestFixedBits(int)",10,1,18
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.getTotalBytesRequired(int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.percentileBits(long[],int,int,double)",4,4,6
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readBigInteger(InputStream)",3,6,7
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readDouble(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readFloat(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readInts(long[],int,int,int,InStream)",2,4,16
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readLongBE(InStream,long[],int,int,int)",2,3,11
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readLongBE2(InStream,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readLongBE3(InStream,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readLongBE4(InStream,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readLongBE5(InStream,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readLongBE6(InStream,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readLongBE7(InStream,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readLongBE8(InStream,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readLongLE(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readRemainingLongs(long[],int,InStream,int,int)",2,10,19
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readVslong(InputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readVulong(InputStream)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledBitPack1(long[],int,int,OutputStream)",1,3,4
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledBitPack16(long[],int,int,OutputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledBitPack2(long[],int,int,OutputStream)",1,3,4
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledBitPack24(long[],int,int,OutputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledBitPack32(long[],int,int,OutputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledBitPack4(long[],int,int,OutputStream)",1,3,4
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledBitPack40(long[],int,int,OutputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledBitPack48(long[],int,int,OutputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledBitPack56(long[],int,int,OutputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledBitPack64(long[],int,int,OutputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledBitPack8(long[],int,int,OutputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledBitPackBytes(long[],int,int,OutputStream,int)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledUnPack1(long[],int,int,InStream)",1,3,4
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledUnPack16(long[],int,int,InStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledUnPack2(long[],int,int,InStream)",1,3,4
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledUnPack24(long[],int,int,InStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledUnPack32(long[],int,int,InStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledUnPack4(long[],int,int,InStream)",1,3,4
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledUnPack40(long[],int,int,InStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledUnPack48(long[],int,int,InStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledUnPack56(long[],int,int,InStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledUnPack64(long[],int,int,InStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledUnPack8(long[],int,int,InStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.unrolledUnPackBytes(long[],int,int,InStream,int)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeBigInteger(OutputStream,BigInteger)",4,5,6
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeDouble(OutputStream,double)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeFloat(OutputStream,float)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeInts(long[],int,int,int,OutputStream)",3,6,22
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeLongBE(OutputStream,long[],int,int,int)",2,2,10
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeLongBE2(OutputStream,long,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeLongBE3(OutputStream,long,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeLongBE4(OutputStream,long,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeLongBE5(OutputStream,long,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeLongBE6(OutputStream,long,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeLongBE7(OutputStream,long,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeLongBE8(OutputStream,long,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeLongLE(OutputStream,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeRemainingLongs(OutputStream,int,long[],int,int)",2,9,18
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeVslong(OutputStream,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.writeVulong(OutputStream,long)",3,3,3
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.zigzagDecode(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SerializationUtils.zigzagEncode(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SnappyCodec.compress(ByteBuffer,ByteBuffer,ByteBuffer)",2,3,3
"org.apache.hadoop.hive.ql.io.orc.SnappyCodec.decompress(ByteBuffer,ByteBuffer)",2,3,3
"org.apache.hadoop.hive.ql.io.orc.SnappyCodec.directDecompress(ByteBuffer,ByteBuffer)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.SnappyCodec.isAvailable()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.StreamName.StreamName(int,Kind)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StreamName.compareTo(StreamName)",4,2,5
"org.apache.hadoop.hive.ql.io.orc.StreamName.equals(Object)",2,1,4
"org.apache.hadoop.hive.ql.io.orc.StreamName.getArea()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StreamName.getArea(Kind)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.StreamName.getColumn()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StreamName.getKind()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StreamName.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StreamName.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.StringRedBlackTree(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.VisitorContextImpl.getLength()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.VisitorContextImpl.getOriginalPosition()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.VisitorContextImpl.getText()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.VisitorContextImpl.setPosition(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.VisitorContextImpl.writeBytes(OutputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.add(String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.add(Text)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.addNewKey()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.compareValue(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.getCharacterSize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.getSizeInBytes()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.getText(Text,int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.recurse(int,Visitor,VisitorContextImpl)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.StringRedBlackTree.visit(Visitor)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StripeStatistics.StripeStatistics(List<ColumnStatistics>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.StripeStatistics.getColumnStatistics()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestBitFieldReader.runSeekTest(CompressionCodec)",1,7,7
"org.apache.hadoop.hive.ql.io.orc.TestBitFieldReader.testBiggerItems()",1,5,5
"org.apache.hadoop.hive.ql.io.orc.TestBitFieldReader.testCompressedSeek()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitFieldReader.testSkips()",1,6,6
"org.apache.hadoop.hive.ql.io.orc.TestBitFieldReader.testUncompressedSeek()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.deltaEncode(long[])",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.nextLong(Random,long)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.openFileSystem()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.runTest(int)",1,5,6
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test01BitPacking1Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test02BitPacking2Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test03BitPacking3Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test04BitPacking4Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test05BitPacking5Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test06BitPacking6Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test07BitPacking7Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test08BitPacking8Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test09BitPacking9Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test10BitPacking10Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test11BitPacking11Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test12BitPacking12Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test13BitPacking13Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test14BitPacking14Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test15BitPacking15Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test16BitPacking16Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test17BitPacking17Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test18BitPacking18Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test19BitPacking19Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test20BitPacking20Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test21BitPacking21Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test22BitPacking22Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test23BitPacking23Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test24BitPacking24Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test26BitPacking26Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test28BitPacking28Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test30BitPacking30Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test32BitPacking32Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test40BitPacking40Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test48BitPacking48Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test56BitPacking56Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.test64BitPacking64Bit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestBitPack.testBitPack64Large()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestDynamicArray.testByteArray()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestDynamicArray.testIntArray()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestFileDump.MyRecord.MyRecord(int,long,String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestFileDump.checkOutput(String,String)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestFileDump.openFileSystem()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestFileDump.testDictionaryThreshold()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestFileDump.testDump()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestInStream.OutputCollector.output(ByteBuffer)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInStream.PositionCollector.addPosition(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInStream.PositionCollector.getNext()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInStream.PositionCollector.reset()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInStream.PositionCollector.toString()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestInStream.testCompressed()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestInStream.testCorruptStream()",1,2,4
"org.apache.hadoop.hive.ql.io.orc.TestInStream.testDisjointBuffers()",1,8,8
"org.apache.hadoop.hive.ql.io.orc.TestInStream.testUncompressed()",1,5,5
"org.apache.hadoop.hive.ql.io.orc.TestInStream.testUncompressedDisjointBuffers()",1,8,8
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRow.BigRow(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRow.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRow.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRow.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowField.BigRowField(int,String,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowField.getFieldComment()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowField.getFieldID()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowField.getFieldName()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowField.getFieldObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowField.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowInspector.getAllStructFieldRefs()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowInspector.getStructFieldData(Object,StructField)",12,2,12
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowInspector.getStructFieldRef(String)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowInspector.getStructFieldsDataAsList(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockBlock.MockBlock(String...)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockBlock.toString()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFile.MockFile(String,int,byte[],MockBlock...)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFile.toString()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.MockFileSystem()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.MockFileSystem(Configuration,MockFile...)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.append(Path,int,Progressable)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.create(Path,FsPermission,boolean,int,short,long,Progressable)",3,3,4
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.createDirectory(Path)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.createStatus(MockFile)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.delete(Path)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.delete(Path,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.getFileBlockLocations(FileStatus,long,long)",3,5,5
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.getFileStatus(Path)",4,4,4
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.getUri()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.getWorkingDirectory()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.initialize(URI,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.listStatus(Path)",3,6,6
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.mkdirs(Path,FsPermission)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.open(Path,int)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.rename(Path,Path)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.setWorkingDirectory(Path)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockFileSystem.toString()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockInputStream.MockInputStream(MockFile)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockInputStream.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockInputStream.read()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockInputStream.seek(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockInputStream.seekToNewSource(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockOutputStream.MockOutputStream(MockFile)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockOutputStream.close()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockOutputStream.setBlocks(MockBlock...)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockPath.MockPath(FileSystem,String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MockPath.getFileSystem(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MyRow.MyRow(int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MyRow.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.MyRow.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.NestedRow.NestedRow(int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.NestedRow.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.NestedRow.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.StringRow.StringRow(String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.StringRow.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.StringRow.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.TestContext.TestContext(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.TestContext.schedule(Runnable)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.createMockExecutionEnvironment(Path,Path,String,ObjectInspector,boolean)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.createMockOrcFile(long...)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.fill(DataOutputBuffer,long)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.openFileSystem()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testAddSplit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testCombinationInputFormat()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testCombinationInputFormatWithAcid()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testDefaultTypes()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testEmptyFile()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testFileGenerator()",2,3,3
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testGetInputPaths()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testInOutFormat()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testMROutput()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testOverlap()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testSetSearchArgument()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testSplitGenerator()",3,6,6
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testVectorization()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testVectorizationWithAcid()",1,3,4
"org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testVectorizationWithBuckets()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestIntegerCompressionReader.runSeekTest(CompressionCodec)",1,11,11
"org.apache.hadoop.hive.ql.io.orc.TestIntegerCompressionReader.testCompressedSeek()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestIntegerCompressionReader.testSkips()",1,6,6
"org.apache.hadoop.hive.ql.io.orc.TestIntegerCompressionReader.testUncompressedSeek()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestMemoryManager.DoubleMatcher.DoubleMatcher(double,double)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestMemoryManager.DoubleMatcher.describeTo(Description)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestMemoryManager.DoubleMatcher.matches(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestMemoryManager.NullCallback.checkMemory(double)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestMemoryManager.closeTo(double,double)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestMemoryManager.testBasics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestMemoryManager.testCallback()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestMemoryManager.testConfig()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestNewInputOutputFormat.OrcTestMapper1.map(Object,Writable,Context)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestNewInputOutputFormat.OrcTestMapper2.map(Object,Text,Context)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestNewInputOutputFormat.OrcTestMapper3.map(Object,Text,Context)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestNewInputOutputFormat.OrcTestReducer3.reduce(IntWritable,Iterable<Text>,Context)",1,5,5
"org.apache.hadoop.hive.ql.io.orc.TestNewInputOutputFormat.setup()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestNewInputOutputFormat.testNewInputFormat()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.TestNewInputOutputFormat.testNewInputFormatPruning()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestNewInputOutputFormat.testNewOutputFormat()",1,2,3
"org.apache.hadoop.hive.ql.io.orc.TestNewInputOutputFormat.testNewOutputFormatComplex()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestNewInputOutputFormat.testNewOutputFormatWithCompression()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.Row.Row(int,long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.TSRow.TSRow(Timestamp)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.TestNewIntegerEncoding(EncodingStrategy)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.data()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.openFileSystem()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testBasicDelta1()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testBasicDelta2()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testBasicDelta3()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testBasicDelta4()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testBasicNew()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testBasicOld()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testBasicRow()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testDirectLargeNegatives()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testIntegerMax()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testIntegerMin()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testLongMax()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testLongMin()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBase510()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBase511()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBaseAt0()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBaseAt1()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBaseAt255()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBaseAt256()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBaseMax1()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBaseMax2()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBaseMax3()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBaseMax4()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBaseNegativeMin()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBaseNegativeMin2()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBaseNegativeMin3()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBaseNegativeMin4()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testPatchedBaseTimestamp()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testRandomInt()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testRandomLong()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestNewIntegerEncoding.testSeek()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.BigRow.BigRow(Boolean,Byte,Short,Integer,Long,Float,Double,BytesWritable,String,MiddleStruct,List<InnerStruct>,Map<Text, InnerStruct>)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.InnerStruct.InnerStruct(int,String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.MiddleStruct.MiddleStruct(InnerStruct...)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.MyMemoryManager.MyMemoryManager(Configuration,long,double)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.MyMemoryManager.addWriter(Path,long,Callback)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.MyMemoryManager.addedRow()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.MyMemoryManager.getAllocationScale()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.MyMemoryManager.getTotalMemoryPool()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.MyMemoryManager.removeWriter(Path)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.SimpleStruct.SimpleStruct(BytesWritable,String)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.TestOrcFile(Boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.byteBuf(int...)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.bytes(int...)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.columnProjection()",1,8,14
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.compareInner(InnerStruct,OrcStruct)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.compareList(List<InnerStruct>,List<OrcStruct>)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.createRandomRow(long[],double[],String[],BytesWritable[],String[],int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.data()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.emptyFile()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.inner(int,String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.list(InnerStruct...)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.map(InnerStruct...)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.metaData()",3,4,6
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.openFileSystem()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.test1()",4,4,4
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.testMemoryManagement()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.testPredicatePushdown()",1,5,5
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.testReadFormat_0_11()",4,6,6
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.testSeek()",1,11,11
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.testSnappy()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.testStringAndBinaryStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.testStripeLevelStats()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.testTimestamp()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.testUnionAndTimestamp()",1,9,10
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.testWithoutIndex()",1,5,5
"org.apache.hadoop.hive.ql.io.orc.TestOrcFile.testZeroCopySeek()",1,11,11
"org.apache.hadoop.hive.ql.io.orc.TestOrcNullOptimization.InnerStruct.InnerStruct(int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcNullOptimization.MyStruct.MyStruct(Integer,String,Boolean,List<InnerStruct>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcNullOptimization.openFileSystem()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcNullOptimization.testColumnsWithNullAndCompression()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestOrcNullOptimization.testMultiStripeWithNull()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestOrcNullOptimization.testMultiStripeWithoutNull()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.BigRow.BigRow(int,long,String,float,double)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.MyRow.MyRow(String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.createMaximalTxnList()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.createMockOriginalReader()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.createMockReader()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.createOriginalRow(String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.createStripes(long...)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.getValue(OrcStruct)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.setRow(OrcStruct,int,long,int,long,long,String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.testEmpty()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.testNewBase()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.testNewBaseAndDelta()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.testOrdering()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.testOriginalReaderPair()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.testOriginalReaderPairNoMin()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.testReaderPair()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.testReaderPairNoMin()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.testRecordReaderDelta()",1,4,4
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.testRecordReaderIncompleteDelta()",1,6,6
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.testRecordReaderNewBaseAndDelta()",1,9,9
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.testRecordReaderOldBaseAndDelta()",1,9,9
"org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.value(OrcStruct)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRecordUpdater.MyRow.MyRow(String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRecordUpdater.testAccessors()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRecordUpdater.testUpdates()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcRecordUpdater.testWriter()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.BigRow.BigRow(Boolean,Byte,Short,Integer,Long,Float,Double,BytesWritable,String,MiddleStruct,List<InnerStruct>,Map<Text, InnerStruct>,Timestamp,HiveDecimal)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.InnerStruct.InnerStruct(int,String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.ListStruct.ListStruct(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.MapStruct.MapStruct(Map<String, Double>)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.MiddleStruct.MiddleStruct(InnerStruct...)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.SimpleStruct.SimpleStruct(BytesWritable,String)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.bytes(int...)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.inner(int,String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.list(InnerStruct...)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.map(InnerStruct...)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.openFileSystem()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.testOrcSerDeStatsComplex()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.testOrcSerDeStatsComplexOldFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.testOrcSerDeStatsList()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.testOrcSerDeStatsMap()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.testOrcSerDeStatsSimpleWithNulls()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.testSerdeStatsOldFormat()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.testStringAndBinaryStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSplitElimination.AllTypesRow.AllTypesRow(Long,String,Double,HiveDecimal,Timestamp)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSplitElimination.openFileSystem()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSplitElimination.testSplitEliminationComplexExpr()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSplitElimination.testSplitEliminationLargeMaxSplit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSplitElimination.testSplitEliminationSmallMaxSplit()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcSplitElimination.writeData(Writer)",1,7,7
"org.apache.hadoop.hive.ql.io.orc.TestOrcStruct.testInspectorFromTypeInfo()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcStruct.testStruct()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcStruct.testUnion()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcWideTable.getRandomColumnNames(int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcWideTable.openFileSystem()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestOrcWideTable.testBufferSizeFor1000Col()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcWideTable.testBufferSizeFor1Col()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcWideTable.testBufferSizeFor2000Col()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcWideTable.testBufferSizeFor2000ColNoCompression()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcWideTable.testBufferSizeFor25000Col()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcWideTable.testBufferSizeFor4000Col()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcWideTable.testBufferSizeFor4000ColNoCompression()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcWideTable.testBufferSizeManualOverride1()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestOrcWideTable.testBufferSizeManualOverride2()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.BufferInStream.BufferInStream(byte[],int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.BufferInStream.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.BufferInStream.read()",2,1,2
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.BufferInStream.read(byte[],int,int)",2,1,3
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.BufferInStream.read(long,byte[],int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.BufferInStream.readFully(long,byte[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.BufferInStream.readFully(long,byte[],int,int)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.BufferInStream.seek(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.BufferInStream.seekToNewSource(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.createDateStats(int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.createDecimalStats(String,String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.createDoubleStats(double,double)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.createIntStats(Long,Long)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.createIntStats(int,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.createStringStats(String,String)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.diskRanges(Integer...)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testBetween()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testCompareToCharNeedConvert()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testCompareToRangeInt()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testCompareToRangeString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testEquals()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testGetIndexPosition()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testGetMax()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testGetMin()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testIn()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testIsNull()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testLessThan()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testLessThanEquals()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testMaxLengthToReader()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testMergeDiskRanges()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testNullSafeEquals()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testOverlap()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testPartialPlan()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testPartialPlanCompressed()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testPartialPlanString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testPredEvalWithDateStats()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testPredEvalWithDecimalStats()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testPredEvalWithDoubleStats()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testPredEvalWithIntStats()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRecordReaderImpl.testPredEvalWithStringStats()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRunLengthByteReader.testCompressedSeek()",1,7,7
"org.apache.hadoop.hive.ql.io.orc.TestRunLengthByteReader.testSkips()",1,6,6
"org.apache.hadoop.hive.ql.io.orc.TestRunLengthByteReader.testUncompressedSeek()",1,7,7
"org.apache.hadoop.hive.ql.io.orc.TestRunLengthIntegerReader.runSeekTest(CompressionCodec)",1,11,11
"org.apache.hadoop.hive.ql.io.orc.TestRunLengthIntegerReader.testCompressedSeek()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestRunLengthIntegerReader.testSkips()",1,6,6
"org.apache.hadoop.hive.ql.io.orc.TestRunLengthIntegerReader.testUncompressedSeek()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestSerializationUtils.fromBuffer(ByteArrayOutputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestSerializationUtils.main(String[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestSerializationUtils.testBigIntegers()",1,4,6
"org.apache.hadoop.hive.ql.io.orc.TestSerializationUtils.testDoubles()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestStreamName.test1()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestStringRedBlackTree.MyVisitor.MyVisitor(String[],int[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestStringRedBlackTree.MyVisitor.visit(VisitorContext)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestStringRedBlackTree.buildTree(String...)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.TestStringRedBlackTree.checkContents(StringRedBlackTree,int[],String...)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestStringRedBlackTree.checkSubtree(RedBlackTree,int,IntWritable)",7,5,7
"org.apache.hadoop.hive.ql.io.orc.TestStringRedBlackTree.checkTree(RedBlackTree)",3,3,3
"org.apache.hadoop.hive.ql.io.orc.TestStringRedBlackTree.main(String[])",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestStringRedBlackTree.printTree(RedBlackTree,String,int)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.TestStringRedBlackTree.test1()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestStringRedBlackTree.test2()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestStringRedBlackTree.test3()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestUnrolledBitPack.TestUnrolledBitPack(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestUnrolledBitPack.data()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestUnrolledBitPack.openFileSystem()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestUnrolledBitPack.testBitPacking()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestVectorizedORCReader.MyRecord.MyRecord(Boolean,Byte,Integer,Long,Short,Double,String,Timestamp,Date,HiveDecimal)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestVectorizedORCReader.checkVectorizedReader()",7,9,11
"org.apache.hadoop.hive.ql.io.orc.TestVectorizedORCReader.createFile()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.TestVectorizedORCReader.openFileSystem()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.TestZlib.testCorrupt()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.TestZlib.testNoOverflow()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowReader.VectorizedOrcAcidRowReader(RowReader<OrcStruct>,Configuration,FileSplit)",1,1,6
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowReader.close()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowReader.createValue()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowReader.getProgress()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowReader.next(NullWritable,VectorizedRowBatch)",3,4,7
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.VectorizedOrcInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.VectorizedOrcRecordReader.VectorizedOrcRecordReader(Reader,Configuration,FileSplit)",1,1,2
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.VectorizedOrcRecordReader.close()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.VectorizedOrcRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.VectorizedOrcRecordReader.createValue()",1,1,2
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.VectorizedOrcRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.VectorizedOrcRecordReader.getProgress()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.VectorizedOrcRecordReader.next(NullWritable,VectorizedRowBatch)",2,2,4
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.validateInput(FileSystem,HiveConf,ArrayList<FileStatus>)",3,2,4
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcSerde.VectorizedOrcSerde(ObjectInspector)",1,1,3
"org.apache.hadoop.hive.ql.io.orc.VectorizedOrcSerde.serialize(Object,ObjectInspector)",1,4,7
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BinaryTreeWriter.BinaryTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BinaryTreeWriter.getEncoding()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BinaryTreeWriter.recordPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BinaryTreeWriter.write(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BinaryTreeWriter.writeStripe(Builder,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BooleanTreeWriter.BooleanTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BooleanTreeWriter.recordPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BooleanTreeWriter.write(Object)",1,2,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BooleanTreeWriter.writeStripe(Builder,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BufferedStream.BufferedStream(String,int,CompressionCodec)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BufferedStream.clear()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BufferedStream.flush()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BufferedStream.getBufferSize()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BufferedStream.getOutputSize()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BufferedStream.isSuppressed()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BufferedStream.output(ByteBuffer)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BufferedStream.spillTo(OutputStream)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.BufferedStream.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.ByteTreeWriter.ByteTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.ByteTreeWriter.recordPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.ByteTreeWriter.write(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.ByteTreeWriter.writeStripe(Builder,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.CharTreeWriter.CharTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.CharTreeWriter.getTextValue(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DateTreeWriter.DateTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DateTreeWriter.getEncoding()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DateTreeWriter.recordPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DateTreeWriter.write(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DateTreeWriter.writeStripe(Builder,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DecimalTreeWriter.DecimalTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DecimalTreeWriter.getEncoding()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DecimalTreeWriter.recordPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DecimalTreeWriter.write(Object)",3,2,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DecimalTreeWriter.writeStripe(Builder,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DirectStream.DirectStream(FSDataOutputStream)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DirectStream.output(ByteBuffer)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DoubleTreeWriter.DoubleTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DoubleTreeWriter.recordPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DoubleTreeWriter.write(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.DoubleTreeWriter.writeStripe(Builder,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.FloatTreeWriter.FloatTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.FloatTreeWriter.recordPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.FloatTreeWriter.write(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.FloatTreeWriter.writeStripe(Builder,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.IntegerTreeWriter.IntegerTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,1,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.IntegerTreeWriter.getEncoding()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.IntegerTreeWriter.recordPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.IntegerTreeWriter.write(Object)",1,4,4
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.IntegerTreeWriter.writeStripe(Builder,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.ListTreeWriter.ListTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.ListTreeWriter.getEncoding()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.ListTreeWriter.recordPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.ListTreeWriter.write(Object)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.ListTreeWriter.writeStripe(Builder,int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.MapTreeWriter.MapTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.MapTreeWriter.getEncoding()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.MapTreeWriter.recordPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.MapTreeWriter.write(Object)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.MapTreeWriter.writeStripe(Builder,int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.RowIndexPositionRecorder.RowIndexPositionRecorder(Builder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.RowIndexPositionRecorder.addPosition(long)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StreamFactory.buildIndex()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StreamFactory.createStream(int,Kind)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StreamFactory.getConfiguration()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StreamFactory.getEncodingStrategy()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StreamFactory.getNextColumnId()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StreamFactory.getRowIndexStride()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StreamFactory.getVersion()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StreamFactory.isCompressed()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StringTreeWriter.StringTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StringTreeWriter.createRowIndexEntry()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StringTreeWriter.estimateMemory()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StringTreeWriter.getEncoding()",4,4,4
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StringTreeWriter.getTextValue(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StringTreeWriter.write(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StringTreeWriter.writeStripe(Builder,int)",1,11,11
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StructTreeWriter.StructTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StructTreeWriter.write(Object)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.StructTreeWriter.writeStripe(Builder,int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TimestampTreeWriter.TimestampTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TimestampTreeWriter.formatNanos(int)",3,1,5
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TimestampTreeWriter.getEncoding()",2,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TimestampTreeWriter.recordPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TimestampTreeWriter.write(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TimestampTreeWriter.writeStripe(Builder,int)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.TreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.createIntegerWriter(PositionedOutputStream,boolean,boolean,StreamFactory)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.createRowIndexEntry()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.estimateMemory()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.getChildrenWriters()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.getEncoding()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.getFileStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.getRowIndex()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.getRowIndexEntry()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.getStripeStatistics()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.isNewWriteFormat(StreamFactory)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.recordPosition(PositionRecorder)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.removeIsPresentPositions()",1,2,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.write(Object)",1,3,5
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.writeStripe(Builder,int)",3,5,6
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.TreeWriter.writeStripeStatistics(Builder,TreeWriter)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.UnionTreeWriter.UnionTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.UnionTreeWriter.recordPosition(PositionRecorder)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.UnionTreeWriter.write(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.UnionTreeWriter.writeStripe(Builder,int)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.VarcharTreeWriter.VarcharTreeWriter(int,ObjectInspector,StreamFactory,boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.VarcharTreeWriter.getTextValue(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.WriterImpl(FileSystem,Path,Configuration,ObjectInspector,long,CompressionKind,int,int,MemoryManager,boolean,Version,WriterCallback,EncodingStrategy,float,long)",2,1,4
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.addRow(Object)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.addUserMetadata(String,ByteBuffer)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.appendStripe(byte[],StripeInformation,StripeStatistics)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.appendStripe(byte[],int,int,StripeInformation,StripeStatistics)",1,3,5
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.appendUserMetadata(List<UserMetadataItem>)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.checkMemory(double)",2,3,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.close()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.computeRawDataSize()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.createCodec(CompressionKind)",6,3,9
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.createRowIndexEntry()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.createTreeWriter(ObjectInspector,StreamFactory,boolean)",20,20,20
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.estimateStripeSize()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.flushStripe()",1,10,22
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.getClosestBufferSize(int,int)",7,1,12
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.getEstimatedBufferSize(int)",3,3,5
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.getMemoryAvailableForORC()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.getNumberOfRows()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.getRawDataSize()",1,1,1
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.getRawDataSizeFromInspectors(TreeWriter,ObjectInspector)",2,3,5
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.getRawDataSizeFromPrimitives(TreeWriter,ObjectInspector)",7,7,10
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.getStream()",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.updateFileStatistics(StripeStatistics)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.writeCompressionKind(CompressionKind)",6,2,6
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.writeFileStatistics(Builder,TreeWriter)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.writeFooter(long)",1,3,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.writeIntermediateFooter()",1,3,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.writeMetadata(long)",1,2,2
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.writePostScript(int,int)",2,2,3
"org.apache.hadoop.hive.ql.io.orc.WriterImpl.writeTypes(Builder,TreeWriter)",3,7,26
"org.apache.hadoop.hive.ql.io.orc.ZlibCodec.compress(ByteBuffer,ByteBuffer,ByteBuffer)",4,5,5
"org.apache.hadoop.hive.ql.io.orc.ZlibCodec.decompress(ByteBuffer,ByteBuffer)",3,6,7
"org.apache.hadoop.hive.ql.io.orc.ZlibCodec.directDecompress(ByteBuffer,ByteBuffer)",1,1,1
"org.apache.hadoop.hive.ql.io.orc.ZlibCodec.isAvailable()",1,4,4
"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.MapredParquetInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.MapredParquetInputFormat(ParquetInputFormat<ArrayWritable>)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",2,4,5
"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.MapredParquetOutputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.MapredParquetOutputFormat(OutputFormat<Void, ArrayWritable>)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.checkOutputSpecs(FileSystem,JobConf)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.getHiveRecordWriter(JobConf,Path,Class<? extends Writable>,boolean,Properties,Progressable)",1,3,3
"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.getParquerRecordWriterWrapper(ParquetOutputFormat<ArrayWritable>,JobConf,String,Progressable)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.pushFilters(JobConf,TableScanOperator)",3,3,3
"org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.pushProjectionsAndFilters(JobConf,Path)",1,3,3
"org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.pushProjectionsAndFilters(JobConf,String,String)",3,9,11
"org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.updateMrWork(JobConf)",1,5,5
"org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.createHiveColumnsFrom(String)",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.createHiveTypeInfoFrom(String)",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testArray()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testArrayDecimal()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testCharType()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testConversion(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testDecimalType()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testMap()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testMapDecimal()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testMapOriginalType()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testSimpleType()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testStruct()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.testVarcharType()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestMapredParquetInputFormat.testConstructorWithParquetInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestMapredParquetInputFormat.testDefaultConstructor()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestMapredParquetOutputFormat.testConstructor()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestMapredParquetOutputFormat.testConstructorWithFormat()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestMapredParquetOutputFormat.testGetHiveRecordWriter()",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.TestMapredParquetOutputFormat.testGetRecordWriterThrowsException()",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.TestParquetSerDe.arrayWritableEquals(ArrayWritable,ArrayWritable)",7,3,7
"org.apache.hadoop.hive.ql.io.parquet.TestParquetSerDe.createProperties()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestParquetSerDe.deserializeAndSerializeLazySimple(ParquetHiveSerDe,ArrayWritable)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.TestParquetSerDe.testParquetHiveSerDe()",1,4,4
"org.apache.hadoop.hive.ql.io.parquet.VectorizedParquetInputFormat.VectorizedParquetInputFormat(ParquetInputFormat<ArrayWritable>)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.VectorizedParquetInputFormat.VectorizedParquetRecordReader.VectorizedParquetRecordReader(ParquetInputFormat<ArrayWritable>,FileSplit,JobConf,Reporter)",1,1,2
"org.apache.hadoop.hive.ql.io.parquet.VectorizedParquetInputFormat.VectorizedParquetRecordReader.close()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.VectorizedParquetInputFormat.VectorizedParquetRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.VectorizedParquetInputFormat.VectorizedParquetRecordReader.createValue()",1,1,2
"org.apache.hadoop.hive.ql.io.parquet.VectorizedParquetInputFormat.VectorizedParquetRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.VectorizedParquetInputFormat.VectorizedParquetRecordReader.getProgress()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.VectorizedParquetInputFormat.VectorizedParquetRecordReader.next(NullWritable,VectorizedRowBatch)",3,4,6
"org.apache.hadoop.hive.ql.io.parquet.VectorizedParquetInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,2
"org.apache.hadoop.hive.ql.io.parquet.convert.ArrayWritableGroupConverter.ArrayWritableGroupConverter(GroupType,HiveGroupConverter,int,List<TypeInfo>)",2,2,4
"org.apache.hadoop.hive.ql.io.parquet.convert.ArrayWritableGroupConverter.add(int,Writable)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.ArrayWritableGroupConverter.end()",1,1,2
"org.apache.hadoop.hive.ql.io.parquet.convert.ArrayWritableGroupConverter.getConverter(int)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.ArrayWritableGroupConverter.set(int,Writable)",2,1,5
"org.apache.hadoop.hive.ql.io.parquet.convert.ArrayWritableGroupConverter.start()",1,1,2
"org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.DataWritableGroupConverter(GroupType,GroupType,List<TypeInfo>)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.DataWritableGroupConverter(GroupType,HiveGroupConverter,int,GroupType,List<TypeInfo>)",3,3,3
"org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.DataWritableGroupConverter(GroupType,HiveGroupConverter,int,List<TypeInfo>)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.add(int,Writable)",3,3,3
"org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.end()",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.getConverter(int)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.getCurrentArray()",1,3,4
"org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.set(int,Writable)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.start()",1,1,2
"org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableRecordConverter.DataWritableRecordConverter(GroupType,GroupType,List<TypeInfo>)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableRecordConverter.getCurrentRecord()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableRecordConverter.getRootConverter()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.BinaryConverter.BinaryConverter(PrimitiveType,HiveGroupConverter,int)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.BinaryConverter.addBinary(Binary)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.BinaryConverter.addValueFromDictionary(int)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.BinaryConverter.hasDictionarySupport()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.BinaryConverter.setDictionary(Dictionary)",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.ETypeConverter(Class<?>)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.getNewConverter(PrimitiveType,int,HiveGroupConverter,List<TypeInfo>)",9,10,10
"org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.getType()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(Type,int,HiveGroupConverter,List<TypeInfo>)",4,3,4
"org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.convert(List<String>,List<TypeInfo>)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.convertArrayType(String,ListTypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.convertMapType(String,MapTypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.convertStructType(String,StructTypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.convertType(String,TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.convertType(String,TypeInfo,Repetition)",19,19,21
"org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.convertTypes(List<String>,List<TypeInfo>)",2,2,3
"org.apache.hadoop.hive.ql.io.parquet.convert.HiveSchemaConverter.listWrapper(String,OriginalType,GroupType)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.getColumnTypes(Configuration)",1,3,3
"org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.getColumns(String)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.init(Configuration,Map<String, String>,MessageType)",6,9,9
"org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.prepareForRead(Configuration,Map<String, String>,MessageType,ReadContext)",2,1,2
"org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.resolveSchemaAccess(MessageType,MessageType,Configuration)",1,3,3
"org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.ParquetRecordReaderWrapper(ParquetInputFormat<ArrayWritable>,InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.ParquetRecordReaderWrapper(ParquetInputFormat<ArrayWritable>,InputSplit,JobConf,Reporter,ProjectionPusher)",2,3,6
"org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.close()",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.createValue()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getProgress()",2,2,3
"org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getSplit(InputSplit,JobConf)",2,5,6
"org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(Void,ArrayWritable)",7,4,9
"org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.AbstractParquetMapInspector(ObjectInspector,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.clear(Object)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.create()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.equals(Object)",10,3,10
"org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.getMap(Object)",5,3,7
"org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.getMapKeyObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.getMapSize(Object)",5,4,6
"org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.getMapValueObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.hashCode()",1,3,3
"org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.put(Object,Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.AbstractParquetMapInspector.remove(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.ArrayWritableObjectInspector(StructTypeInfo)",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.StructFieldImpl.StructFieldImpl(String,ObjectInspector,int)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.StructFieldImpl.getFieldComment()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.StructFieldImpl.getFieldID()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.StructFieldImpl.getFieldName()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.StructFieldImpl.getFieldObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.StructFieldImpl.getIndex()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.create()",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.equals(Object)",4,3,6
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.getAllStructFieldRefs()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.getObjectInspector(TypeInfo)",18,20,20
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.getStructFieldData(Object,StructField)",4,3,4
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.getStructFieldRef(String)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.getStructFieldsDataAsList(Object)",3,2,3
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.hashCode()",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.setStructFieldData(Object,StructField,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.DeepParquetHiveMapInspector.DeepParquetHiveMapInspector(ObjectInspector,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.DeepParquetHiveMapInspector.getMapValueElement(Object,Object)",10,10,15
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.ParquetHiveArrayInspector(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.create(int)",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.equals(Object)",3,4,4
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.getList(Object)",5,3,7
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.getListElement(Object,int)",6,4,8
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.getListElementObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.getListLength(Object)",5,2,6
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.hashCode()",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.resize(Object,int)",1,3,3
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveArrayInspector.set(Object,int,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.createArray(Object,ListObjectInspector)",2,5,5
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.createMap(Object,MapObjectInspector)",2,5,5
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.createObject(Object,ObjectInspector)",6,6,6
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.createPrimitive(Object,PrimitiveObjectInspector)",13,13,21
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.createStruct(Object,StructObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.deserialize(Writable)",2,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.getSerDeStats()",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.initialize(Configuration,Properties)",2,3,4
"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.serialize(Object,ObjectInspector)",2,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.StandardParquetHiveMapInspector.StandardParquetHiveMapInspector(ObjectInspector,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.StandardParquetHiveMapInspector.getMapValueElement(Object,Object)",7,4,9
"org.apache.hadoop.hive.ql.io.parquet.serde.TestAbstractParquetMapInspector.TestableAbstractParquetMapInspector.TestableAbstractParquetMapInspector(ObjectInspector,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestAbstractParquetMapInspector.TestableAbstractParquetMapInspector.getMapValueElement(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestAbstractParquetMapInspector.setUp()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestAbstractParquetMapInspector.testEmptyContainer()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestAbstractParquetMapInspector.testHashMap()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestAbstractParquetMapInspector.testNullContainer()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestAbstractParquetMapInspector.testNullMap()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestAbstractParquetMapInspector.testRegularMap()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestDeepParquetHiveMapInspector.setUp()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestDeepParquetHiveMapInspector.testEmptyContainer()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestDeepParquetHiveMapInspector.testHashMap()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestDeepParquetHiveMapInspector.testNullContainer()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestDeepParquetHiveMapInspector.testNullMap()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestDeepParquetHiveMapInspector.testRegularMap()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetHiveArrayInspector.setUp()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetHiveArrayInspector.testEmptyContainer()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetHiveArrayInspector.testNullArray()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetHiveArrayInspector.testNullContainer()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetHiveArrayInspector.testRegularList()",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetTimestampUtils.testJulianDay()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetTimestampUtils.testNanos()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetTimestampUtils.testTimezone()",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetTimestampUtils.testValues()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestParquetTimestampUtils.verifyTsString(String)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestStandardParquetHiveMapInspector.setUp()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestStandardParquetHiveMapInspector.testEmptyContainer()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestStandardParquetHiveMapInspector.testHashMap()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestStandardParquetHiveMapInspector.testNullContainer()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestStandardParquetHiveMapInspector.testNullMap()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.TestStandardParquetHiveMapInspector.testRegularMap()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetByteInspector.ParquetByteInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetByteInspector.create(byte)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetByteInspector.get(Object)",2,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetByteInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetByteInspector.getPrimitiveWritableObject(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetByteInspector.set(Object,byte)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetPrimitiveInspectorFactory.ParquetPrimitiveInspectorFactory()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetShortInspector.ParquetShortInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetShortInspector.create(short)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetShortInspector.get(Object)",2,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetShortInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetShortInspector.getPrimitiveWritableObject(Object)",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetShortInspector.set(Object,short)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector.ParquetStringInspector()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector.create(String)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector.create(Text)",2,1,2
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector.getPrimitiveJavaObject(Object)",5,3,6
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector.getPrimitiveWritableObject(Object)",5,2,5
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector.set(Object,String)",1,2,3
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector.set(Object,Text)",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetByteInspector.setUp()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetByteInspector.testByteWritable()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetByteInspector.testCreate()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetByteInspector.testGet()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetByteInspector.testIntWritable()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetByteInspector.testNull()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetByteInspector.testSet()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetShortInspector.setUp()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetShortInspector.testCreate()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetShortInspector.testGet()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetShortInspector.testIntWritable()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetShortInspector.testNull()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetShortInspector.testSet()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetShortInspector.testShortWritable()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTime.NanoTime(int,long)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTime.fromBinary(Binary)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTime.getJulianDay()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTime.getTimeOfDayNanos()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTime.toBinary()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTime.toString()",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTime.writeValue(RecordConsumer)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTimeUtils.getCalendar()",1,2,2
"org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTimeUtils.getNanoTime(Timestamp)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTimeUtils.getTimestamp(NanoTime)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.getSchema(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.init(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.prepareForWrite(RecordConsumer)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.setSchema(MessageType,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(ArrayWritable)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.DataWritableWriter(RecordConsumer,GroupType)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.write(ArrayWritable)",2,1,2
"org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeArray(ArrayWritable,GroupType)",7,7,8
"org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeData(ArrayWritable,GroupType)",7,5,8
"org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writePrimitive(Writable)",12,11,12
"org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.ParquetRecordWriterWrapper(OutputFormat<Void, ArrayWritable>,JobConf,String,Progressable)",1,1,3
"org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.close(Reporter)",1,1,2
"org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.close(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(Void,ArrayWritable)",1,1,2
"org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(Writable)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeRecordReader.RCFileBlockMergeRecordReader(Configuration,FileSplit)",1,2,2
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeRecordReader.close()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeRecordReader.getKeyClass()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeRecordReader.getProgress()",2,2,2
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeRecordReader.getStart()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeRecordReader.getValueClass()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeRecordReader.next(RCFileKeyBufferWrapper,RCFileValueBufferWrapper)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeRecordReader.nextBlock(RCFileKeyBufferWrapper,RCFileValueBufferWrapper)",4,1,4
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeRecordReader.seek(long)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.RCFileKeyBufferWrapper()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.compareTo(RCFileKeyBufferWrapper)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.create(KeyBuffer)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.getCodec()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.getCompressedKeyLength()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.getInputPath()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.getKeyBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.getKeyLength()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.getRecordLength()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.setCodec(CompressionCodec)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.setCompressedKeyLength(int)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.setInputPath(Path)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.setKeyBuffer(KeyBuffer)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.setKeyLength(int)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.setRecordLength(int)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileMergeMapper.close()",2,1,2
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileMergeMapper.map(Object,RCFileValueBufferWrapper,OutputCollector<Object, Object>,Reporter)",2,6,7
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileValueBufferWrapper.RCFileValueBufferWrapper()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileValueBufferWrapper.compareTo(RCFileValueBufferWrapper)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileValueBufferWrapper.getValueBuffer()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileValueBufferWrapper.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileValueBufferWrapper.setValueBuffer(ValueBuffer)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileValueBufferWrapper.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanMapper.PartialScanMapper()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanMapper.close()",1,2,3
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanMapper.configure(JobConf)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanMapper.map(Object,RCFileValueBufferWrapper,OutputCollector<Object, Object>,Reporter)",1,4,6
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanMapper.publishStats()",5,6,6
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.addInputPaths(JobConf,PartialScanWork)",1,2,2
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.checkFatalErrors(Counters,StringBuilder)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.execute(DriverContext)",5,23,25
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.initialize(HiveConf,QueryPlan,DriverContext)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.logPlanProgress(SessionState)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.main(String[])",1,18,20
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.printUsage()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.requireLock()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork.PartialScanWork()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork.PartialScanWork(List<Path>)",1,3,3
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork.getAggKey()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork.getInputPaths()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork.getInputformat()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork.getMapperClass()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork.getMinSplitSize()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork.isGatheringStats()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork.setAggKey(String)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanWork.setInputPaths(List<Path>)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.ColumnTruncateMapper()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.backupOutputPath(FileSystem,Path,JobConf)",2,2,2
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.close()",4,3,5
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.configure(JobConf)",1,1,2
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.fixTmpPathConcatenate(Path,int)",1,3,4
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.jobClose(Path,boolean,JobConf,LogHelper,DynamicPartitionCtx,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.map(Object,RCFileValueBufferWrapper,OutputCollector<Object, Object>,Reporter)",1,7,8
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.updatePaths(Path,Path)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.addInputPaths(JobConf,ColumnTruncateWork)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.checkFatalErrors(Counters,StringBuilder)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.execute(DriverContext)",1,21,23
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.getName()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.getType()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.initialize(HiveConf,QueryPlan,DriverContext)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.logPlanProgress(SessionState)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.requireLock()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.ColumnTruncateWork()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.ColumnTruncateWork(List<Integer>,Path,Path)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.ColumnTruncateWork(List<Integer>,Path,Path,boolean,DynamicPartitionCtx)",1,2,2
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.getDroppedColumns()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.getDynPartCtx()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.getInputDir()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.getInputformat()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.getListBucketingCtx()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.getMapperClass()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.getMinSplitSize()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.getOutputDir()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.hasDynamicPartitions()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.isGatheringStats()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.isListBucketingAlterTableConcatenate()",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.setDroppedColumns(List<Integer>)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.setDynPartCtx(DynamicPartitionCtx)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.setHasDynamicPartitions(boolean)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.setInputPaths(Path)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.setListBucketingCtx(ListBucketingCtx)",1,1,1
"org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateWork.setOutputDir(Path)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgument.Factory.create(ExprNodeGenericFuncDesc)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgument.Factory.create(String)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgument.Factory.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue.and(TruthValue)",10,1,13
"org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue.isNeeded()",3,2,3
"org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue.not()",7,2,7
"org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue.or(TruthValue)",10,1,13
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.BuilderImpl.between(String,Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.BuilderImpl.boxLiteral(Object)",5,5,14
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.BuilderImpl.build()",2,2,2
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.BuilderImpl.end()",3,2,4
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.BuilderImpl.equals(String,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.BuilderImpl.getType(Object)",7,1,14
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.BuilderImpl.in(String,Object...)",2,2,3
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.BuilderImpl.isNull(String)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.BuilderImpl.lessThan(String,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.BuilderImpl.lessThanEquals(String,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.BuilderImpl.nullSafeEquals(String,Object)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.BuilderImpl.startAnd()",1,2,2
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.BuilderImpl.startNot()",1,2,2
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.BuilderImpl.startOr()",1,2,2
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.addChildren(ExpressionTree,ExprNodeGenericFuncDesc,List<PredicateLeaf>)",1,2,2
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.boxLiteral(ExprNodeConstantDesc)",6,5,6
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.buildLeafList(ExpressionTree,List<PredicateLeaf>,Map<PredicateLeaf, ExpressionTree>)",3,5,5
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.convertToCNF(ExpressionTree)",1,9,9
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.createLeaf(Operator,ExprNodeGenericFuncDesc,List<PredicateLeaf>)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.createLeaf(Operator,ExprNodeGenericFuncDesc,List<PredicateLeaf>,int)",6,3,12
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.expression(ExprNodeGenericFuncDesc)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.expression(ExpressionTree,List<PredicateLeaf>)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.findVariable(ExprNodeDesc)",4,2,4
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.flatten(ExpressionTree)",3,8,10
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.foldMaybe(ExpressionTree)",7,5,8
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.generateAllCombinations(List<ExpressionTree>,List<ExpressionTree>,List<ExpressionTree>)",1,7,7
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.getColumnName(ExprNodeGenericFuncDesc,int)",3,3,4
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.getLeaves()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.getLiteral(ExprNodeGenericFuncDesc)",5,3,5
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.getLiteralList(ExprNodeGenericFuncDesc,int)",3,3,3
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.getType(ExprNodeDesc)",9,3,9
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.negate(ExpressionTree)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.parse(ExprNodeDesc,List<PredicateLeaf>)",2,15,16
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionBuilder.pushDownNot(ExpressionTree)",5,9,11
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionTree.ExpressionTree()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionTree.ExpressionTree(ExpressionTree)",1,3,3
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionTree.ExpressionTree(Operator,ExpressionTree...)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionTree.ExpressionTree(TruthValue)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionTree.ExpressionTree(int)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionTree.evaluate(TruthValue[])",5,5,9
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionTree.getChildren()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionTree.getOperator()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.ExpressionTree.toString()",2,4,8
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.PredicateLeafImpl.PredicateLeafImpl()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.PredicateLeafImpl.PredicateLeafImpl(Operator,Type,String,Object,List<Object>)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.PredicateLeafImpl.equals(Object)",3,8,8
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.PredicateLeafImpl.getColumnName()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.PredicateLeafImpl.getLiteral()",2,2,2
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.PredicateLeafImpl.getLiteralList()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.PredicateLeafImpl.getOperator()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.PredicateLeafImpl.getType()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.PredicateLeafImpl.hashCode()",1,3,3
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.PredicateLeafImpl.isEqual(Object,Object)",3,3,4
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.PredicateLeafImpl.toString()",1,4,4
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.SearchArgumentImpl()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.SearchArgumentImpl(ExprNodeGenericFuncDesc)",1,2,2
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.SearchArgumentImpl(ExpressionTree,List<PredicateLeaf>)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.evaluate(TruthValue[])",1,2,2
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.fromKryo(String)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.getExpression()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.getLeaves()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.newBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.toKryo()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.toString()",1,2,2
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.and(ExpressionTree...)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.assertNoSharedNodes(ExpressionTree,Set<ExpressionTree>)",1,5,5
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.constant(TruthValue)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.createPredicateLeaf(Operator,Type,String,Object,List<Object>)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.getFuncDesc(String)",1,1,2
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.leaf(int)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.not(ExpressionTree)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.or(ExpressionTree...)",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testBuilder()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testBuilderComplexTypes()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testBuilderComplexTypes2()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testCNF()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testExpression1()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testExpression10()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testExpression2()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testExpression3()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testExpression4()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testExpression5()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testExpression7()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testExpression8()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testExpression9()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testFlatten()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testFoldMaybe()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.testNotPushdown()",1,1,1
"org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl.values(TruthValue...)",1,1,1
"org.apache.hadoop.hive.ql.io.udf.Rot13InputFormat.Rot13LineRecordReader.Rot13LineRecordReader(JobConf,FileSplit)",1,1,1
"org.apache.hadoop.hive.ql.io.udf.Rot13InputFormat.Rot13LineRecordReader.next(LongWritable,Text)",1,2,2
"org.apache.hadoop.hive.ql.io.udf.Rot13InputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.io.udf.Rot13InputFormat.rot13(byte[],int,int)",1,1,6
"org.apache.hadoop.hive.ql.io.udf.Rot13OutputFormat.getHiveRecordWriter(JobConf,Path,Class<? extends Writable>,boolean,Properties,Progressable)",3,3,3
"org.apache.hadoop.hive.ql.lib.CompositeProcessor.CompositeProcessor(NodeProcessor...)",1,1,1
"org.apache.hadoop.hive.ql.lib.CompositeProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,2,2
"org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.DefaultGraphWalker(Dispatcher)",1,1,1
"org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(Node,Stack<Node>)",1,1,1
"org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(Node,Stack<Node>)",1,3,3
"org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.getDispatchedList()",1,1,1
"org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.getToWalk()",1,1,1
"org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(Collection<Node>,HashMap<Node, Object>)",1,3,3
"org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(Node)",2,6,6
"org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.DefaultRuleDispatcher(NodeProcessor,Map<Rule, NodeProcessor>,NodeProcessorCtx)",1,1,1
"org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(Node,Stack<Node>,Object...)",2,4,6
"org.apache.hadoop.hive.ql.lib.ForwardWalker.ForwardWalker(Dispatcher)",1,1,1
"org.apache.hadoop.hive.ql.lib.ForwardWalker.addAllParents(Node)",2,1,2
"org.apache.hadoop.hive.ql.lib.ForwardWalker.allParentsDispatched(Node)",4,2,4
"org.apache.hadoop.hive.ql.lib.ForwardWalker.walk(Node)",2,5,5
"org.apache.hadoop.hive.ql.lib.PreOrderWalker.PreOrderWalker(Dispatcher)",1,1,1
"org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(Node)",1,3,3
"org.apache.hadoop.hive.ql.lib.RuleExactMatch.RuleExactMatch(String,String)",1,1,1
"org.apache.hadoop.hive.ql.lib.RuleExactMatch.cost(Stack<Node>)",2,3,4
"org.apache.hadoop.hive.ql.lib.RuleExactMatch.getName()",1,1,1
"org.apache.hadoop.hive.ql.lib.RuleRegExp.RuleRegExp(String,String)",1,1,1
"org.apache.hadoop.hive.ql.lib.RuleRegExp.cost(Stack<Node>)",3,4,4
"org.apache.hadoop.hive.ql.lib.RuleRegExp.getName()",1,1,1
"org.apache.hadoop.hive.ql.lib.TaskGraphWalker.TaskGraphWalker(Dispatcher)",1,1,1
"org.apache.hadoop.hive.ql.lib.TaskGraphWalker.TaskGraphWalkerContext.TaskGraphWalkerContext(HashMap<Node, Object>)",1,1,1
"org.apache.hadoop.hive.ql.lib.TaskGraphWalker.TaskGraphWalkerContext.addToDispatchList(Node)",1,2,2
"org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(Node,Stack<Node>)",1,3,3
"org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(Node,Stack<Node>,TaskGraphWalkerContext)",1,3,3
"org.apache.hadoop.hive.ql.lib.TaskGraphWalker.getDispatchedList()",1,1,1
"org.apache.hadoop.hive.ql.lib.TaskGraphWalker.getToWalk()",1,1,1
"org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(Collection<Node>,HashMap<Node, Object>)",1,3,3
"org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(Node)",4,10,12
"org.apache.hadoop.hive.ql.lib.Utils.findNode(Stack<Node>,Class<T>)",3,3,3
"org.apache.hadoop.hive.ql.lib.Utils.getNthAncestor(Stack<Node>,int)",1,3,3
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.DbHiveLock.DbHiveLock(long)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.DbHiveLock.equals(Object)",2,1,2
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.DbHiveLock.getHiveLockMode()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.DbHiveLock.getHiveLockObject()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.DbHiveLock.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.DbLockManager(HiveMetaStoreClient)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.backoff()",1,1,3
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.clearLocalLockRecords()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.close()",1,2,2
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.getLocks()",1,2,2
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.getLocks(HiveLockObject,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.getLocks(boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(HiveLockObject,HiveLockMode,boolean)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(List<HiveLockObj>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(LockRequest)",2,6,6
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.prepareRetry()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.refresh()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.releaseLocks(List<HiveLock>)",1,2,3
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.setContext(HiveLockManagerCtx)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbLockManager.unlock(HiveLock)",1,4,4
"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.DbTxnManager()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(QueryPlan,Context,String)",10,7,20
"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.commitTxn()",2,4,5
"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.destruct()",1,5,5
"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getLockManager()",1,1,2
"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getValidTxns()",1,2,2
"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.heartbeat()",4,8,8
"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.init()",3,3,4
"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.openTxn(String)",1,2,2
"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.rollbackTxn()",2,3,4
"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.supportsExplicitLock()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.useNewShowLocksFormat()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.acquireLocks(QueryPlan,Context,String)",4,12,16
"org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.commitTxn()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.dedupLockObjects(List<HiveLockObj>)",1,5,5
"org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.destruct()",1,3,3
"org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.getLockManager()",4,7,8
"org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.getLockObjects(QueryPlan,Database,Table,Partition,HiveLockMode)",5,8,9
"org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.getValidTxns()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.heartbeat()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.openTxn(String)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.rollbackTxn()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.supportsExplicitLock()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.useNewShowLocksFormat()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.EmbeddedLockManager()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.Node.Node()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.Node.getLockMode()",1,1,2
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.Node.getLocks(Stack<String>,boolean,boolean,List<HiveLock>,HiveConf)",1,4,4
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.Node.getLocks(String[],boolean,boolean,HiveConf)",2,2,2
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.Node.getLocks(String[],boolean,boolean,List<HiveLock>,HiveConf)",1,5,5
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.Node.getLocks(String[],int,boolean,boolean,List<HiveLock>,HiveConf)",2,3,3
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.Node.getLocks(boolean,boolean,HiveConf)",2,2,2
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.Node.hasChild()",1,2,2
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.Node.hasLock()",1,2,2
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.Node.lock(String[],HiveLockObjectData,boolean)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.Node.lock(String[],int,HiveLockObjectData,boolean)",4,6,8
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.Node.set(HiveLockObjectData,boolean)",1,1,2
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.Node.unlock(String[],HiveLockObjectData)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.Node.unlock(String[],int,HiveLockObjectData)",5,7,9
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.SimpleHiveLock.SimpleHiveLock(HiveLockObject,HiveLockMode)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.SimpleHiveLock.equals(Object)",2,2,3
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.SimpleHiveLock.getHiveLockMode()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.SimpleHiveLock.getHiveLockObject()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.SimpleHiveLock.toString()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.close()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.getLocks(HiveLockObject,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.getLocks(HiveLockObject,boolean,boolean,HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.getLocks(boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.getLocks(boolean,boolean,HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.lock(HiveLockObject,HiveLockMode,boolean)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.lock(HiveLockObject,HiveLockMode,int,int)",3,3,4
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.lock(List<HiveLockObj>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.lock(List<HiveLockObj>,int,int)",3,3,4
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.lockPrimitive(HiveLockObject,HiveLockMode)",2,1,2
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.lockPrimitive(List<HiveLockObj>,int,int)",3,3,3
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.prepareRetry()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.refresh()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.releaseLocks(List<HiveLock>)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.releaseLocks(List<HiveLock>,int,int)",1,3,3
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.setContext(HiveLockManagerCtx)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.sleep(int)",1,1,2
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.sortLocks(List<HiveLockObj>)",4,2,4
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.unlock(HiveLock)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.unlock(HiveLock,int,int)",3,3,4
"org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager.verify(boolean,String[],HiveLockObjectData,HiveConf)",5,2,8
"org.apache.hadoop.hive.ql.lockmgr.HiveLockManagerCtx.HiveLockManagerCtx()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockManagerCtx.HiveLockManagerCtx(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockManagerCtx.getConf()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockManagerCtx.setConf(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObj.HiveLockObj(HiveLockObject,HiveLockMode)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObj.getMode()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObj.getName()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObj.getObj()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObj.setMode(HiveLockMode)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObj.setObj(HiveLockObject)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObject()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObject(DummyPartition,HiveLockObjectData)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObject(Partition,HiveLockObjectData)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObject(String,HiveLockObjectData)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObject(String[],HiveLockObjectData)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObject(Table,HiveLockObjectData)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData.HiveLockObjectData(String)",2,1,3
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData.HiveLockObjectData(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData.equals(Object)",2,15,16
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData.getClientIp()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData.getLockMode()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData.getLockTime()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData.getQueryId()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData.getQueryStr()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData.hashCode()",1,6,6
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData.setClientIp(String)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.HiveLockObjectData.toString()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.equals(Object)",2,4,5
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.getData()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.getDisplayName()",4,1,6
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.getName()",2,3,4
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.getPaths()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.hashCode()",1,2,2
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.removeDelimiter(String)",2,1,2
"org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.setData(HiveLockObjectData)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.closeTxnManager()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.finalize()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.setHiveConf(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.LockException.LockException()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.LockException.LockException(String)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.LockException.LockException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.LockException.LockException(Throwable)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.MockQueryPlan.MockQueryPlan(TestDbTxnManager)",1,1,3
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.MockQueryPlan.getInputs()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.MockQueryPlan.getOutputs()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.TestDbTxnManager()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.addDynamicPartitionedOutput(Table,WriteType)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.addPartitionInput(Table)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.addPartitionOutput(Table,WriteType)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.addTableInput()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.addTableOutput(WriteType)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.newTable(boolean)",1,2,2
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.setUp()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.tearDown()",1,2,2
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.testDDLExclusive()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.testDDLNoLock()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.testDDLShared()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.testDelete()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.testJoin()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.testReadWrite()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.testRollback()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.testSingleReadMultiPartition()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.testSingleReadPartition()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.testSingleReadTable()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.testSingleWritePartition()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.testSingleWriteTable()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.testUpdate()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.testWriteDynamicPartition()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestDummyTxnManager.testDedupLockObjects()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestEmbeddedLockManager.lockObj(String,String)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestEmbeddedLockManager.testLocking()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TestHiveLockObject.testEqualsAndHashCode()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TxnManagerFactory.TxnManagerFactory()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.TxnManagerFactory.getTxnManager(HiveConf)",2,4,4
"org.apache.hadoop.hive.ql.lockmgr.TxnManagerFactory.getTxnManagerFactory()",1,1,2
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.setup()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.testDeleteNoChildren()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.testDeleteNoChildrenNodeDoesNotExist()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.testDeleteWithChildren()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.testGetQuorumServers()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLock.ZooKeeperHiveLock(String,HiveLockObject,HiveLockMode)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLock.equals(Object)",2,3,4
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLock.getHiveLockMode()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLock.getHiveLockObject()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLock.getPath()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLock.setHiveLockMode(HiveLockMode)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLock.setHiveLockObject(HiveLockObject)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLock.setPath(String)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.DummyWatcher.process(WatchedEvent)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.ZooKeeperHiveLockManager()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.checkRedundantNode(String)",2,5,6
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.close()",1,4,4
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.createChild(String,byte[],CreateMode)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.getLastObjectName(String,HiveLockObject)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.getLockMode(HiveConf,String)",3,1,3
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.getLockName(String,HiveLockMode)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.getLockObject(HiveConf,String,HiveLockMode,HiveLockObjectData,String,boolean)",6,3,9
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.getLocks(HiveConf,ZooKeeper,HiveLockObject,String,boolean,boolean)",5,12,17
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.getLocks(HiveLockObject,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.getLocks(boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.getObjectNames(HiveLockObject)",1,2,2
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.getQuorumServers(HiveConf)",1,4,4
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.getSequenceNumber(String,String)",1,1,2
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.lock(HiveLockObject,HiveLockMode,boolean)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.lock(HiveLockObject,HiveLockMode,boolean,boolean)",6,8,10
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.lock(List<HiveLockObj>,boolean)",7,6,9
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.lockPrimitive(HiveLockObject,HiveLockMode,boolean,boolean,Set<String>)",6,11,15
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.prepareRetry()",1,2,4
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.printConflictingLocks(HiveLockObject,HiveLockMode,Set<String>)",1,3,3
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.refresh()",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.releaseAllLocks(HiveConf)",2,5,7
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.releaseLocks(List<HiveLock>)",1,4,4
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.removeAllRedundantNodes()",1,3,3
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.renewZookeeperInstance(int,String)",2,1,2
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.setContext(HiveLockManagerCtx)",1,4,4
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlock(HiveLock)",1,1,1
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlockPrimitive(HiveConf,ZooKeeper,HiveLock,String)",1,6,6
"org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlockWithRetry(HiveConf,ZooKeeper,HiveLock,String)",3,5,5
"org.apache.hadoop.hive.ql.log.PerfLogger.PerfLogBegin(String,String)",1,1,1
"org.apache.hadoop.hive.ql.log.PerfLogger.PerfLogEnd(String,String)",1,3,3
"org.apache.hadoop.hive.ql.log.PerfLogger.PerfLogger()",1,1,1
"org.apache.hadoop.hive.ql.log.PerfLogger.close(Log,QueryPlan)",1,1,1
"org.apache.hadoop.hive.ql.log.PerfLogger.getEndTime(String)",1,1,1
"org.apache.hadoop.hive.ql.log.PerfLogger.getPerfLogger()",1,1,1
"org.apache.hadoop.hive.ql.log.PerfLogger.getPerfLogger(boolean)",2,4,4
"org.apache.hadoop.hive.ql.log.PerfLogger.getStartTime(String)",1,1,1
"org.apache.hadoop.hive.ql.log.PidDailyRollingFileAppender.setFile(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.AuthorizationException.AuthorizationException()",1,1,1
"org.apache.hadoop.hive.ql.metadata.AuthorizationException.AuthorizationException(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.AuthorizationException.AuthorizationException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.ql.metadata.AuthorizationException.AuthorizationException(Throwable)",1,1,1
"org.apache.hadoop.hive.ql.metadata.CheckResult.PartitionResult.compareTo(PartitionResult)",1,2,2
"org.apache.hadoop.hive.ql.metadata.CheckResult.PartitionResult.getPartitionName()",1,1,1
"org.apache.hadoop.hive.ql.metadata.CheckResult.PartitionResult.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.metadata.CheckResult.PartitionResult.setPartitionName(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.CheckResult.PartitionResult.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.CheckResult.PartitionResult.toString()",1,1,1
"org.apache.hadoop.hive.ql.metadata.CheckResult.getPartitionsNotInMs()",1,1,1
"org.apache.hadoop.hive.ql.metadata.CheckResult.getPartitionsNotOnFs()",1,1,1
"org.apache.hadoop.hive.ql.metadata.CheckResult.getTablesNotInMs()",1,1,1
"org.apache.hadoop.hive.ql.metadata.CheckResult.getTablesNotOnFs()",1,1,1
"org.apache.hadoop.hive.ql.metadata.CheckResult.setPartitionsNotInMs(List<PartitionResult>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.CheckResult.setPartitionsNotOnFs(List<PartitionResult>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.CheckResult.setTablesNotInMs(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.CheckResult.setTablesNotOnFs(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.configureInputJobProperties(TableDesc,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.configureJobConf(TableDesc,JobConf)",1,1,1
"org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.configureOutputJobProperties(TableDesc,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.configureTableJobProperties(TableDesc,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.getAuthorizationProvider()",1,1,1
"org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.getConf()",1,1,1
"org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.getInputFormatClass()",1,1,1
"org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.getMetaHook()",1,1,1
"org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.getOutputFormatClass()",1,1,1
"org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.getSerDeClass()",1,1,1
"org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.toString()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Dimension.Dimension(Class<?>,String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Dimension.equals(Object)",4,3,5
"org.apache.hadoop.hive.ql.metadata.Dimension.getDimensionId()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Dimension.getDimensionType()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Dimension.hashCode()",1,3,3
"org.apache.hadoop.hive.ql.metadata.Dimension.hashCode(Object)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Dimension.toString()",1,1,1
"org.apache.hadoop.hive.ql.metadata.DummyCreateTableHook.postAnalyze(HiveSemanticAnalyzerHookContext,List<Task<? extends Serializable>>)",1,1,2
"org.apache.hadoop.hive.ql.metadata.DummyCreateTableHook.preAnalyze(HiveSemanticAnalyzerHookContext,ASTNode)",3,3,3
"org.apache.hadoop.hive.ql.metadata.DummyPartition.DummyPartition()",1,1,1
"org.apache.hadoop.hive.ql.metadata.DummyPartition.DummyPartition(Table,String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.DummyPartition.DummyPartition(Table,String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.DummyPartition.getCompleteName()",1,1,1
"org.apache.hadoop.hive.ql.metadata.DummyPartition.getName()",1,1,1
"org.apache.hadoop.hive.ql.metadata.DummyPartition.getSpec()",1,1,1
"org.apache.hadoop.hive.ql.metadata.DummyPartition.getValues()",1,2,2
"org.apache.hadoop.hive.ql.metadata.DummyPartition.setName(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook.DummySemanticAnalyzerHook()",1,1,1
"org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook.postAnalyze(HiveSemanticAnalyzerHookContext,List<Task<? extends Serializable>>)",1,2,2
"org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook.preAnalyze(HiveSemanticAnalyzerHookContext,ASTNode)",3,2,4
"org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook1.DummySemanticAnalyzerHook1()",1,1,1
"org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook1.postAnalyze(HiveSemanticAnalyzerHookContext,List<Task<? extends Serializable>>)",2,1,3
"org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook1.preAnalyze(HiveSemanticAnalyzerHookContext,ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.Hive(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.alterDatabase(String,Database)",1,1,4
"org.apache.hadoop.hive.ql.metadata.Hive.alterFunction(String,String,Function)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.alterIndex(String,String,Index)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.alterIndex(String,String,String,Index)",1,1,3
"org.apache.hadoop.hive.ql.metadata.Hive.alterPartition(String,Partition)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.alterPartition(String,String,Partition)",1,2,4
"org.apache.hadoop.hive.ql.metadata.Hive.alterPartitions(String,List<Partition>)",1,3,5
"org.apache.hadoop.hive.ql.metadata.Hive.alterTable(String,Table)",1,2,4
"org.apache.hadoop.hive.ql.metadata.Hive.cancelDelegationToken(String)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.checkPaths(HiveConf,FileSystem,FileStatus[],FileSystem,Path,boolean)",6,14,17
"org.apache.hadoop.hive.ql.metadata.Hive.clearPartitionStats(Partition)",2,2,3
"org.apache.hadoop.hive.ql.metadata.Hive.close()",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.closeCurrent()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.compact(String,String,String,String)",3,3,4
"org.apache.hadoop.hive.ql.metadata.Hive.constructListBucketingLocationMap(Path,SkewedInfo)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.constructOneLBLocationMap(FileStatus,Map<List<String>, String>,Path,SkewedInfo)",1,9,9
"org.apache.hadoop.hive.ql.metadata.Hive.convertAddSpecToMetaPartition(Table,OnePartitionDesc)",1,13,13
"org.apache.hadoop.hive.ql.metadata.Hive.convertFromMetastore(Table,List<Partition>,List<Partition>)",2,3,4
"org.apache.hadoop.hive.ql.metadata.Hive.copyFiles(HiveConf,Path,Path,FileSystem,boolean)",5,6,9
"org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Database)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Database,boolean)",2,1,4
"org.apache.hadoop.hive.ql.metadata.Hive.createFunction(Function)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.createIndex(String,String,String,List<String>,String,boolean,String,String,String,String,String,Map<String, String>,Map<String, String>,Map<String, String>,String,String,String,String,String,String)",7,20,29
"org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient()",3,2,4
"org.apache.hadoop.hive.ql.metadata.Hive.createPartition(Table,Map<String, String>)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.createPartitions(AddPartitionDesc)",1,4,4
"org.apache.hadoop.hive.ql.metadata.Hive.createRole(String,String)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.createTable(String,List<String>,List<String>,Class<? extends InputFormat>,Class<?>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.createTable(String,List<String>,List<String>,Class<? extends InputFormat>,Class<?>,int,List<String>)",2,4,5
"org.apache.hadoop.hive.ql.metadata.Hive.createTable(Table)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.createTable(Table,boolean)",2,7,10
"org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.deletePartitionColumnStatistics(String,String,String,String)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.deleteTableColumnStatistics(String,String,String)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.destExists(List<List<Path[]>>,Path)",4,3,4
"org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(String,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(String,boolean,boolean,boolean)",1,1,3
"org.apache.hadoop.hive.ql.metadata.Hive.dropFunction(String,String)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.dropIndex(String,String,String,boolean)",1,1,3
"org.apache.hadoop.hive.ql.metadata.Hive.dropIndex(String,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.dropPartition(String,List<String>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.dropPartition(String,String,List<String>,boolean)",1,1,3
"org.apache.hadoop.hive.ql.metadata.Hive.dropPartitions(String,List<PartSpec>,boolean,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.dropPartitions(String,String,List<PartSpec>,boolean,boolean,boolean)",1,2,4
"org.apache.hadoop.hive.ql.metadata.Hive.dropRole(String)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.dropTable(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.dropTable(String,String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.dropTable(String,String,boolean,boolean)",2,1,4
"org.apache.hadoop.hive.ql.metadata.Hive.exchangeTablePartitions(Map<String, String>,String,String,String,String)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.get()",1,3,3
"org.apache.hadoop.hive.ql.metadata.Hive.get(Configuration,Class<?>)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.get(HiveConf)",2,4,4
"org.apache.hadoop.hive.ql.metadata.Hive.get(HiveConf,boolean)",2,2,3
"org.apache.hadoop.hive.ql.metadata.Hive.getAggrColStatsFor(String,String,List<String>,List<String>)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases()",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.getAllPartitionsOf(Table)",2,4,4
"org.apache.hadoop.hive.ql.metadata.Hive.getAllRoleNames()",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.getAllTables()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.getAllTables(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.getConf()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(String)",1,1,3
"org.apache.hadoop.hive.ql.metadata.Hive.getDatabaseCurrent()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.getDatabasesByPattern(String)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.getDelegationToken(String,String)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.getFieldsFromDeserializer(String,Deserializer)",1,3,3
"org.apache.hadoop.hive.ql.metadata.Hive.getFunction(String,String)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.getFunctions(String,String)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.getGroupNames()",2,3,3
"org.apache.hadoop.hive.ql.metadata.Hive.getIndex(String,String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.getIndex(String,String,String)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.getIndexes(String,String,short)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.getMSC()",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.getMetaConf(String)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.getPartition(Table,Map<String, String>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.getPartition(Table,Map<String, String>,boolean,String,boolean)",7,14,17
"org.apache.hadoop.hive.ql.metadata.Hive.getPartitionColumnStatistics(String,String,List<String>,List<String>)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.getPartitionNames(String,String,Map<String, String>,short)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.getPartitionNames(String,String,short)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.getPartitionNames(String,short)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.getPartitions(Table)",2,4,4
"org.apache.hadoop.hive.ql.metadata.Hive.getPartitions(Table,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.getPartitions(Table,Map<String, String>,short)",2,3,4
"org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByExpr(Table,ExprNodeGenericFuncDesc,HiveConf,List<Partition>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByFilter(Table,String)",2,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByNames(Table,List<String>)",2,8,9
"org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByNames(Table,Map<String, String>)",2,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.getQualifiedNames(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.getRoleGrantInfoForPrincipal(String,PrincipalType)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.getTable(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.getTable(String,String)",2,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.getTable(String,String,boolean)",3,10,13
"org.apache.hadoop.hive.ql.metadata.Hive.getTable(String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.getTableColumnStatistics(String,String,List<String>)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.getTablesByPattern(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.getTablesByPattern(String,String)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.getTablesForDb(String,String)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.getUserName()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.get_privilege_set(HiveObjectType,String,String,List<String>,String,String,List<String>)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.grantPrivileges(PrivilegeBag)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.grantRole(String,String,PrincipalType,String,PrincipalType,boolean)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.isHadoop1()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.listRoles(String,PrincipalType)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.loadDynamicPartitions(Path,String,Map<String, String>,boolean,int,boolean,boolean)",4,7,9
"org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Path,String,Map<String, String>,boolean,boolean,boolean,boolean,boolean)",1,10,11
"org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Path,String,boolean,boolean,boolean,boolean)",2,5,6
"org.apache.hadoop.hive.ql.metadata.Hive.newTable(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.renameFile(HiveConf,Path,Path,FileSystem,boolean,boolean)",1,9,13
"org.apache.hadoop.hive.ql.metadata.Hive.renamePartition(Table,Map<String, String>,Partition)",5,6,11
"org.apache.hadoop.hive.ql.metadata.Hive.replaceFiles(Path,Path,Path,HiveConf,boolean)",7,18,21
"org.apache.hadoop.hive.ql.metadata.Hive.revokePrivileges(PrivilegeBag,boolean)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.revokeRole(String,String,PrincipalType,boolean)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.set(Hive)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Hive.setMetaConf(String,String)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.setPartitionColumnStatistics(SetPartitionsStatsRequest)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.showCompactions()",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.showPrivilegeGrant(HiveObjectType,String,PrincipalType,String,String,List<String>,String)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Hive.showTransactions()",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.updatePartitionColumnStatistics(ColumnStatistics)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.updateTableColumnStatistics(ColumnStatistics)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.validatePartitionNameCharacters(List<String>)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Hive.walkDirTree(FileStatus,FileSystem,Map<List<String>, String>,Path,SkewedInfo)",2,4,4
"org.apache.hadoop.hive.ql.metadata.HiveException.HiveException()",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveException.HiveException(ErrorMsg,String...)",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveException.HiveException(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveException.HiveException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveException.HiveException(Throwable)",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveException.HiveException(Throwable,ErrorMsg,String...)",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveException.getCanonicalErrorMsg()",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveFatalException.HiveFatalException()",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveFatalException.HiveFatalException(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveFatalException.HiveFatalException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveFatalException.HiveFatalException(Throwable)",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.HiveMetaStoreChecker(Hive)",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkMetastore(String,String,List<? extends Map<String, String>>,CheckResult)",1,7,10
"org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkTable(String,String,List<? extends Map<String, String>>,CheckResult)",1,7,7
"org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkTable(Table,List<Partition>,boolean,CheckResult)",4,6,7
"org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.findUnknownPartitions(Table,Set<Path>,CheckResult)",1,3,3
"org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.findUnknownTables(String,List<String>,CheckResult)",1,8,8
"org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.getAllLeafDirs(Path,Set<Path>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.getAllLeafDirs(Path,Set<Path>,FileSystem)",1,4,4
"org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.getPartitionName(Path,Path)",1,4,4
"org.apache.hadoop.hive.ql.metadata.HiveUtils.HiveUtils()",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveUtils.escapeString(String)",2,5,11
"org.apache.hadoop.hive.ql.metadata.HiveUtils.escapeText(Text)",2,3,8
"org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthenticator(Configuration,ConfVars)",1,4,5
"org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthorizeProviderManager(Configuration,String,HiveAuthenticationProvider,boolean)",3,5,7
"org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthorizerFactory(Configuration,ConfVars)",2,1,2
"org.apache.hadoop.hive.ql.metadata.HiveUtils.getIndexHandler(HiveConf,String)",2,2,3
"org.apache.hadoop.hive.ql.metadata.HiveUtils.getMetaStoreAuthorizeProviderManagers(Configuration,ConfVars,HiveAuthenticationProvider)",2,2,3
"org.apache.hadoop.hive.ql.metadata.HiveUtils.getStorageHandler(Configuration,String)",2,2,3
"org.apache.hadoop.hive.ql.metadata.HiveUtils.getUnparsedColumnNamesFromFieldSchema(List<FieldSchema>)",1,3,3
"org.apache.hadoop.hive.ql.metadata.HiveUtils.lightEscapeString(String)",2,3,6
"org.apache.hadoop.hive.ql.metadata.HiveUtils.unescapeText(Text)",2,9,14
"org.apache.hadoop.hive.ql.metadata.HiveUtils.unparseIdentifier(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.HiveUtils.unparseIdentifier(String,Configuration)",1,4,4
"org.apache.hadoop.hive.ql.metadata.InputEstimator.Estimation.Estimation(int,long)",1,1,1
"org.apache.hadoop.hive.ql.metadata.InputEstimator.Estimation.getRowCount()",1,1,1
"org.apache.hadoop.hive.ql.metadata.InputEstimator.Estimation.getTotalLength()",1,1,1
"org.apache.hadoop.hive.ql.metadata.InvalidTableException.InvalidTableException(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.InvalidTableException.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.Partition()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.Partition(Table)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Partition.Partition(Table,Map<String, String>,Path)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.Partition(Table,Partition)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.canDrop()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.canWrite()",1,1,2
"org.apache.hadoop.hive.ql.metadata.Partition.checkValidity()",1,2,2
"org.apache.hadoop.hive.ql.metadata.Partition.cloneSd(Table)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Partition.createMetaPartitionObject(Table,Map<String, String>,Path)",3,6,6
"org.apache.hadoop.hive.ql.metadata.Partition.getBucketCols()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getBucketCount()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getBucketPath(int)",2,1,2
"org.apache.hadoop.hive.ql.metadata.Partition.getCols()",2,3,3
"org.apache.hadoop.hive.ql.metadata.Partition.getCompleteName()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getDataLocation()",2,2,2
"org.apache.hadoop.hive.ql.metadata.Partition.getDeserializer()",2,2,4
"org.apache.hadoop.hive.ql.metadata.Partition.getInputFormatClass()",2,5,6
"org.apache.hadoop.hive.ql.metadata.Partition.getLastAccessTime()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getLocation()",2,2,2
"org.apache.hadoop.hive.ql.metadata.Partition.getMetadataFromPartitionSchema()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getName()",1,1,2
"org.apache.hadoop.hive.ql.metadata.Partition.getOutputFormatClass()",2,6,7
"org.apache.hadoop.hive.ql.metadata.Partition.getParameters()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getPartitionPath()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getPath()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getPath(Sample)",9,10,10
"org.apache.hadoop.hive.ql.metadata.Partition.getProtectMode()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getSchemaFromTableSchema(Properties)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getSkewedColNames()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getSkewedColValueLocationMaps()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getSkewedColValues()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getSortColNames()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getSortCols()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getSortedPaths()",2,2,5
"org.apache.hadoop.hive.ql.metadata.Partition.getSpec()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getTPartition()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getTable()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.getValues()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.initialize(Table,Partition)",3,7,8
"org.apache.hadoop.hive.ql.metadata.Partition.isOffline()",2,1,2
"org.apache.hadoop.hive.ql.metadata.Partition.isStoredAsSubDirectories()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.setBucketCount(int)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.setInputFormatClass(Class<? extends InputFormat>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.setLastAccessTime(int)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.setLocation(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.setOutputFormatClass(Class<? extends HiveOutputFormat>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.setProtectMode(ProtectMode)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Partition.setSkewedValueLocationMap(List<String>,String)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Partition.setTPartition(Partition)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.setTable(Table)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Partition.setValues(Map<String, String>)",3,2,3
"org.apache.hadoop.hive.ql.metadata.Partition.toString()",1,1,2
"org.apache.hadoop.hive.ql.metadata.RandomDimension.RandomDimension(Class,String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.RandomDimension.hashCode(Object)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Sample.Sample(int,int,Dimension)",2,1,3
"org.apache.hadoop.hive.ql.metadata.Sample.equals(Object)",4,4,6
"org.apache.hadoop.hive.ql.metadata.Sample.getSampleDimension()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Sample.getSampleFraction()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Sample.getSampleNum()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Sample.hashCode()",1,2,2
"org.apache.hadoop.hive.ql.metadata.Sample.inSample(Object)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Sample.toString()",1,1,1
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.SessionHiveMetaStoreClient(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.SessionHiveMetaStoreClient(HiveConf,HiveMetaHookLoader)",1,1,1
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alterTempTable(String,String,Table,Table,EnvironmentContext)",5,7,11
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table(String,String,Table,EnvironmentContext)",2,2,2
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.copyColumnStatisticsObjList(Map<String, ColumnStatisticsObj>)",1,2,2
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.createTempTable(Table,EnvironmentContext)",6,6,8
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(Table,EnvironmentContext)",2,2,2
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.deepCopyAndLowerCaseTable(Table)",1,1,1
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.deleteTableColumnStatistics(String,String,String)",2,2,2
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.deleteTempTableColumnStats(String,String,String)",2,2,3
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.deleteTempTableColumnStatsForTable(String,String)",2,2,2
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.dropTempTable(Table,boolean,EnvironmentContext)",4,8,9
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.drop_table_with_environment_context(String,String,boolean,EnvironmentContext)",2,3,3
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.fieldSchemaEqualsIgnoreComment(FieldSchema,FieldSchema)",3,1,3
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getAllTables(String)",2,2,3
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(String,String)",2,2,2
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTableColumnStatistics(String,String,List<String>)",2,2,2
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTableObjectsByName(String,List<String>)",2,4,5
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTables(String,String)",2,5,6
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTempTable(String,String)",3,3,3
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTempTableColumnStats(String,String,List<String>)",1,4,4
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTempTableColumnStatsForTable(String,String)",2,2,2
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTempTablesForDatabase(String)",2,2,2
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getWh()",1,1,2
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.get_privilege_set(HiveObjectRef,String,List<String>)",3,3,3
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.haveTableColumnsChanged(Table,Table)",4,2,4
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.mergeColumnStats(Map<String, ColumnStatisticsObj>,ColumnStatistics)",1,3,3
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.tableExists(String,String)",2,1,2
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.updateTableColumnStatistics(ColumnStatistics)",2,2,2
"org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.updateTempTableColumnStats(String,String,ColumnStatistics)",2,2,3
"org.apache.hadoop.hive.ql.metadata.Table.Table()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.Table(String,String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.Table(Table)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.canDrop()",1,1,4
"org.apache.hadoop.hive.ql.metadata.Table.canWrite()",1,1,2
"org.apache.hadoop.hive.ql.metadata.Table.checkValidity()",7,5,10
"org.apache.hadoop.hive.ql.metadata.Table.clearSerDeInfo()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.copy()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.copyFiles(Path,boolean)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Table.createSpec(Partition)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Table.equals(Object)",7,2,7
"org.apache.hadoop.hive.ql.metadata.Table.getAllCols()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getAllIndexes(short)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getBucketCols()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getBucketingDimensionId()",2,3,4
"org.apache.hadoop.hive.ql.metadata.Table.getCols()",2,3,3
"org.apache.hadoop.hive.ql.metadata.Table.getCompleteName()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getCompleteName(String,String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getDataLocation()",1,2,2
"org.apache.hadoop.hive.ql.metadata.Table.getDbName()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getDeserializer()",1,2,2
"org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore()",1,1,3
"org.apache.hadoop.hive.ql.metadata.Table.getEmptyTable(String,String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getField(String)",1,1,2
"org.apache.hadoop.hive.ql.metadata.Table.getFields()",1,2,3
"org.apache.hadoop.hive.ql.metadata.Table.getInputFormatClass()",4,3,5
"org.apache.hadoop.hive.ql.metadata.Table.getLastAccessTime()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getMetadata()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getNumBuckets()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getOutputFormatClass()",6,6,9
"org.apache.hadoop.hive.ql.metadata.Table.getOwner()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getParameters()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getPartCols()",1,2,2
"org.apache.hadoop.hive.ql.metadata.Table.getPartitionKeys()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getPath()",2,1,2
"org.apache.hadoop.hive.ql.metadata.Table.getProperty(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getProtectMode()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getRetention()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getSd()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getSerdeInfo()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getSerdeParam(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getSerializationLib()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getSkewedColNames()",1,2,2
"org.apache.hadoop.hive.ql.metadata.Table.getSkewedColValueLocationMaps()",1,2,2
"org.apache.hadoop.hive.ql.metadata.Table.getSkewedColValues()",1,2,2
"org.apache.hadoop.hive.ql.metadata.Table.getSkewedInfo()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getSortCols()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getSortedPaths()",2,2,5
"org.apache.hadoop.hive.ql.metadata.Table.getStorageHandler()",2,1,3
"org.apache.hadoop.hive.ql.metadata.Table.getTTable()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getTableType()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getViewExpandedText()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.getViewOriginalText()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.hasMetastoreBasedSchema(HiveConf,StorageDescriptor)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.hasMetastoreBasedSchema(HiveConf,String)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Table.hashCode()",1,2,2
"org.apache.hadoop.hive.ql.metadata.Table.initialize(Table)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.isDummyTable()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.isField(String)",3,2,3
"org.apache.hadoop.hive.ql.metadata.Table.isImmutable()",1,2,2
"org.apache.hadoop.hive.ql.metadata.Table.isIndexTable()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.isNonNative()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.isOffline()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.isPartitionKey(String)",3,2,3
"org.apache.hadoop.hive.ql.metadata.Table.isPartitioned()",2,1,2
"org.apache.hadoop.hive.ql.metadata.Table.isStoredAsSubDirectories()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.isTemporary()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.isView()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.normalize(String)",2,1,2
"org.apache.hadoop.hive.ql.metadata.Table.replaceFiles(Path,boolean)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setBucketCols(List<String>)",4,3,4
"org.apache.hadoop.hive.ql.metadata.Table.setCreateTime(int)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setDataLocation(Path)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setDbName(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setFields(List<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setInputFormatClass(Class<? extends InputFormat>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setInputFormatClass(String)",2,2,3
"org.apache.hadoop.hive.ql.metadata.Table.setLastAccessTime(int)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setNumBuckets(int)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setOutputFormatClass(Class<? extends HiveOutputFormat>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setOutputFormatClass(String)",2,2,3
"org.apache.hadoop.hive.ql.metadata.Table.setOwner(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setPartCols(List<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setProperty(String,String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setProtectMode(ProtectMode)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Table.setRetention(int)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setSerdeParam(String,String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setSerializationLib(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setSkewedColNames(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setSkewedColValues(List<List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setSkewedInfo(SkewedInfo)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setSkewedValueLocationMap(List<String>,String)",1,2,2
"org.apache.hadoop.hive.ql.metadata.Table.setSortCols(List<Order>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setStoredAsSubDirectories(boolean)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setTTable(Table)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setTableType(TableType)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setViewExpandedText(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.setViewOriginalText(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.toString()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.unsetDataLocation()",1,1,1
"org.apache.hadoop.hive.ql.metadata.Table.validateColumns(List<FieldSchema>,List<FieldSchema>)",6,4,6
"org.apache.hadoop.hive.ql.metadata.Table.validatePartColumnNames(Map<String, String>,boolean)",9,5,12
"org.apache.hadoop.hive.ql.metadata.TestHive.createTestTable(String,String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.TestHive.setNullCreateTableGrants()",1,1,1
"org.apache.hadoop.hive.ql.metadata.TestHive.setUp()",1,2,2
"org.apache.hadoop.hive.ql.metadata.TestHive.tearDown()",1,2,2
"org.apache.hadoop.hive.ql.metadata.TestHive.testGetAndDropTables()",1,3,3
"org.apache.hadoop.hive.ql.metadata.TestHive.testHiveCloseCurrent()",1,1,1
"org.apache.hadoop.hive.ql.metadata.TestHive.testHiveRefreshOnConfChange()",1,1,1
"org.apache.hadoop.hive.ql.metadata.TestHive.testIndex()",1,7,8
"org.apache.hadoop.hive.ql.metadata.TestHive.testPartition()",1,6,6
"org.apache.hadoop.hive.ql.metadata.TestHive.testTable()",1,6,6
"org.apache.hadoop.hive.ql.metadata.TestHive.testThriftTable()",1,4,4
"org.apache.hadoop.hive.ql.metadata.TestHive.validateTable(Table,String)",1,2,2
"org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.dropDbTable()",1,1,3
"org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.setUp()",1,2,2
"org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.tearDown()",1,1,1
"org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.testDataDeletion()",1,1,1
"org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.testPartitionsCheck()",1,2,2
"org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.testTableCheck()",1,1,1
"org.apache.hadoop.hive.ql.metadata.TestHiveRemote.RunMS.RunMS(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.TestHiveRemote.RunMS.run()",1,2,2
"org.apache.hadoop.hive.ql.metadata.TestHiveRemote.findFreePort()",1,1,1
"org.apache.hadoop.hive.ql.metadata.TestHiveRemote.setUp()",1,2,2
"org.apache.hadoop.hive.ql.metadata.TestSemanticAnalyzerHookLoading.testHookLoading()",1,1,1
"org.apache.hadoop.hive.ql.metadata.VirtualColumn.VirtualColumn(String,PrimitiveTypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.metadata.VirtualColumn.VirtualColumn(String,TypeInfo,boolean,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.metadata.VirtualColumn.equals(Object)",3,2,4
"org.apache.hadoop.hive.ql.metadata.VirtualColumn.getIsHidden()",1,1,1
"org.apache.hadoop.hive.ql.metadata.VirtualColumn.getName()",1,1,1
"org.apache.hadoop.hive.ql.metadata.VirtualColumn.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.metadata.VirtualColumn.getRegistry(Configuration)",1,2,2
"org.apache.hadoop.hive.ql.metadata.VirtualColumn.getStatsRegistry(Configuration)",1,2,2
"org.apache.hadoop.hive.ql.metadata.VirtualColumn.getTypeInfo()",1,1,1
"org.apache.hadoop.hive.ql.metadata.VirtualColumn.getVCSObjectInspector(List<VirtualColumn>)",1,2,2
"org.apache.hadoop.hive.ql.metadata.VirtualColumn.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.metadata.VirtualColumn.isHidden()",1,1,1
"org.apache.hadoop.hive.ql.metadata.VirtualColumn.removeVirtualColumnTypes(List<String>,List<TypeInfo>)",2,4,4
"org.apache.hadoop.hive.ql.metadata.VirtualColumn.removeVirtualColumns(Collection<String>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.asJson(OutputStream,Map<String, Object>)",1,1,2
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.describeTable(DataOutputStream,String,String,Table,Partition,List<FieldSchema>,boolean,boolean,boolean,boolean,List<ColumnStatisticsObj>)",1,3,3
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.error(OutputStream,String,int,String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.error(OutputStream,String,int,String,String)",1,3,3
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.makeAllTableStatus(Hive,HiveConf,List<Table>,Map<String, String>,Partition)",1,2,3
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.makeColsUnformatted(List<FieldSchema>)",1,2,2
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.makeOneColUnformatted(FieldSchema)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.makeOneTablePartition(String)",1,5,5
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.makeOneTableStatus(Table,Hive,HiveConf,Map<String, String>,Partition)",1,7,7
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.makeTablePartions(List<String>)",1,2,3
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.makeTableStatusLocations(Table,Hive,Partition)",1,7,7
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.putFileSystemsStats(MapBuilder,List<Path>,HiveConf,Path)",6,5,16
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.showDatabaseDescription(DataOutputStream,String,String,String,String,String,Map<String, String>)",1,5,5
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.showDatabases(DataOutputStream,List<String>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.showTablePartitons(DataOutputStream,List<String>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.showTableStatus(DataOutputStream,Hive,HiveConf,List<Table>,Map<String, String>,Partition)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.showTables(DataOutputStream,Set<String>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder.MapBuilder()",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder.build()",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder.create()",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder.put(String,Object)",1,2,2
"org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder.put(String,T,boolean)",1,2,2
"org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder.put(String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder.put(String,int)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder.put(String,long)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.MetaDataFormatUtils()",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.appendColumnStats(StringBuilder,Object,Object,Object,Object,Object,Object,Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.appendColumnStatsNoFormatting(StringBuilder,Object,Object,Object,Object,Object,Object,Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.convertToString(Decimal)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.displayAllParameters(Map<String, String>,StringBuilder)",1,2,2
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.formatAllFields(StringBuilder,List<FieldSchema>,boolean,List<ColumnStatisticsObj>)",1,3,3
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.formatColumnsHeader(StringBuilder,List<ColumnStatisticsObj>)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.formatDate(long)",2,2,2
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.formatOutput(String,String,StringBuilder)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.formatOutput(String[],StringBuilder)",3,3,3
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.formatWithIndentation(String,String,String,StringBuilder,List<ColumnStatisticsObj>)",1,12,12
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.formatWithoutIndentation(String,String,String,StringBuilder,List<ColumnStatisticsObj>)",1,9,10
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.getAllColumnsInformation(Index)",1,3,3
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.getAllColumnsInformation(List<FieldSchema>,List<FieldSchema>,boolean,boolean,boolean)",1,5,5
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.getAllColumnsInformation(List<FieldSchema>,boolean,boolean,List<ColumnStatisticsObj>)",1,2,2
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.getColumnStatisticsObject(String,String,List<ColumnStatisticsObj>)",4,5,6
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.getColumnsHeader(List<ColumnStatisticsObj>)",1,1,2
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.getComment(FieldSchema)",1,2,2
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.getFormatter(HiveConf)",2,2,2
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.getIndexColumnsHeader()",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.getPartitionInformation(Partition)",1,2,2
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.getPartitionMetaDataInformation(StringBuilder,Partition)",1,2,3
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.getStorageDescriptorInfo(StringBuilder,StorageDescriptor)",1,11,12
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.getTableInformation(Table)",1,2,2
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.getTableMetaDataInformation(StringBuilder,Table)",1,3,4
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.getViewInfo(StringBuilder,Table)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataPrettyFormatUtils.MetaDataPrettyFormatUtils()",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataPrettyFormatUtils.appendFormattedColumn(StringBuilder,String,int)",1,2,2
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataPrettyFormatUtils.breakCommentIntoMultipleLines(String,int,int)",3,10,12
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataPrettyFormatUtils.findMaxColumnNameLen(List<FieldSchema>)",1,2,3
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataPrettyFormatUtils.formatAllFieldsPretty(StringBuilder,List<FieldSchema>,int,int)",1,2,2
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataPrettyFormatUtils.formatColumnsHeaderPretty(StringBuilder,int,int)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataPrettyFormatUtils.formatOutputPretty(String,String,String,StringBuilder,int,int)",1,2,2
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataPrettyFormatUtils.getAllColumnsInformation(List<FieldSchema>,List<FieldSchema>,int)",1,3,3
"org.apache.hadoop.hive.ql.metadata.formatting.MetaDataPrettyFormatUtils.trimTrailingWS(String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.TextMetaDataFormatter(int,boolean)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.describeTable(DataOutputStream,String,String,Table,Partition,List<FieldSchema>,boolean,boolean,boolean,boolean,List<ColumnStatisticsObj>)",1,9,10
"org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.error(OutputStream,String,int,String)",1,1,1
"org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.error(OutputStream,String,int,String,String)",1,3,4
"org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.showDatabaseDescription(DataOutputStream,String,String,String,String,String,Map<String, String>)",1,7,8
"org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.showDatabases(DataOutputStream,List<String>)",1,2,3
"org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.showTablePartitons(DataOutputStream,List<String>)",1,5,6
"org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.showTableStatus(DataOutputStream,Hive,HiveConf,List<Table>,Map<String, String>,Partition)",1,14,15
"org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.showTables(DataOutputStream,Set<String>)",1,2,3
"org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.writeFileSystemStats(DataOutputStream,HiveConf,List<Path>,Path,boolean,int)",6,13,30
"org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.AbstractBucketJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.AbstractBucketJoinProc(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.canConvertMapJoinToBucketMapJoin(MapJoinOperator,ParseContext,BucketJoinProcCtx)",2,7,10
"org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.checkBucketColumns(List<String>,List<String>,Integer[])",4,4,7
"org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.checkConvertBucketMapJoin(ParseContext,BucketJoinProcCtx,QBJoinTree,Map<Byte, List<ExprNodeDesc>>,String,List<String>)",18,19,27
"org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.checkNumberOfBucketsAgainstBigTable(Map<String, List<Integer>>,int)",4,1,5
"org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.convert(Map<Partition, List<String>>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.convertMapJoinToBucketMapJoin(MapJoinOperator,BucketJoinProcCtx)",3,6,7
"org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.fillMappingBigTableBucketFileNameToSmallTableBucketFileNames(List<Integer>,List<List<String>>,Map<String, List<String>>,int,List<String>,Map<String, Integer>)",1,5,5
"org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.getBucketFilePathsOfPartition(Path,ParseContext)",1,3,4
"org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.toColumns(List<ExprNodeDesc>)",5,5,5
"org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.AbstractSMBJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.AbstractSMBJoinProc(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.canConvertBucketMapJoinToSMBJoin(MapJoinOperator,Stack<Node>,SortBucketJoinProcCtx,Object...)",5,7,9
"org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.canConvertJoinToBucketMapJoin(JoinOperator,ParseContext,SortBucketJoinProcCtx)",5,5,9
"org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.canConvertJoinToSMBJoin(JoinOperator,SortBucketJoinProcCtx,ParseContext)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.checkConvertJoinToSMBJoin(JoinOperator,SortBucketJoinProcCtx,ParseContext)",4,2,4
"org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.checkSortColsAndJoinCols(List<Order>,List<String>,List<Order>)",4,3,5
"org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.convertBucketMapJoinToSMBJoin(MapJoinOperator,SortBucketJoinProcCtx,ParseContext)",1,6,6
"org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.convertJoinToBucketMapJoin(JoinOperator,SortBucketJoinProcCtx,ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.convertJoinToSMBJoin(JoinOperator,SortBucketJoinProcCtx,ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.AbstractSMBJoinProc.isEligibleForBucketSortMergeJoin(SortBucketJoinProcCtx,ParseContext,List<ExprNodeDesc>,QBJoinTree,String[],int,List<Order>)",10,11,15
"org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ.getBigTablePosition(ParseContext,JoinOperator,Set<Integer>)",4,5,11
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.BucketJoinProcCtx(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.getBaseBigAlias()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.getBigTblPartsToBucketFileNames()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.getBigTblPartsToBucketNumber()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.getConf()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.getConvertedJoinOps()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.getJoinAliases()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.getRejectedJoinOps()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.getTblAliasToBucketedFilePathsInEachPartition()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.getTblAliasToNumberOfBucketsInEachPartition()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.isBigTablePartitioned()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.setBaseBigAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.setBigTablePartitioned(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.setBigTblPartsToBucketFileNames(Map<Partition, List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.setBigTblPartsToBucketNumber(Map<Partition, Integer>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.setConvertedJoinOps(Set<JoinOperator>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.setJoinAliases(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.setRejectedJoinOps(Set<JoinOperator>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.setTblAliasToBucketedFilePathsInEachPartition(Map<String, List<List<String>>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx.setTblAliasToNumberOfBucketsInEachPartition(Map<String, List<Integer>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.BucketMapJoinOptimizer()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.getBucketMapjoinProc(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.transform(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketMapjoinProc.BucketMapjoinProc(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketMapjoinProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,4,4
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.BucketSortReduceSinkProcessor.BucketSortReduceSinkProcessor(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.BucketSortReduceSinkProcessor.checkPartition(Partition,List<Integer>,List<Integer>,List<Integer>,int)",2,3,4
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.BucketSortReduceSinkProcessor.checkTable(Table,List<Integer>,List<Integer>,List<Integer>,int)",2,3,4
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.BucketSortReduceSinkProcessor.findColumnPosition(List<FieldSchema>,String)",3,2,3
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.BucketSortReduceSinkProcessor.getBucketPositions(List<String>,List<FieldSchema>)",4,4,4
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.BucketSortReduceSinkProcessor.getSortPositionsOrder(List<Order>,List<FieldSchema>)",4,4,4
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.BucketSortReduceSinkProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",34,18,40
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.BucketSortReduceSinkProcessor.removeReduceSink(ReduceSinkOperator,TableScanOperator,FileSinkOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.BucketSortReduceSinkProcessor.removeReduceSink(ReduceSinkOperator,TableScanOperator,FileSinkOperator,FileStatus[])",2,1,2
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.BucketSortReduceSinkProcessor.storeBucketPathMapping(TableScanOperator,FileStatus[])",1,2,2
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.BucketSortReduceSinkProcessor.validateSMBJoinKeys(SMBJoinDesc,List<ExprNodeColumnDesc>,List<ExprNodeColumnDesc>,List<Integer>)",9,5,12
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.BucketingSortingReduceSinkOptimizer()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.getBucketSortReduceSinkProc(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.transform(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPruner.ColumnPruner()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPruner.ColumnPrunerWalker.ColumnPrunerWalker(Dispatcher)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPruner.ColumnPrunerWalker.walk(Node)",2,5,8
"org.apache.hadoop.hive.ql.optimizer.ColumnPruner.transform(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.ColumnPrunerProcCtx(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.genColLists(Operator<? extends OperatorDesc>)",4,4,6
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.getColsFromSelectExpr(SelectOperator)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.getJoinPrunedColLists()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.getOpToParseCtxMap()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.getParseContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.getPrunedColList(Operator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.getPrunedColLists()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.getSelectColsFromChildren(SelectOperator,List<String>)",2,6,6
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.getSelectColsFromLVJoin(RowResolver,List<String>)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerDefaultProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerFilterProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerGroupByProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerJoinProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerLateralViewForwardProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerLateralViewJoinProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,3,5
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerMapJoinProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerPTFProc.buildPrunedRR(List<String>,RowResolver,ArrayList<ColumnInfo>)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerPTFProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,2,2
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerPTFProc.prunedColumnsList(List<String>,WindowTableFunctionDef)",4,8,9
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerPTFProc.prunedInputList(List<String>,WindowTableFunctionDef)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerReduceSinkProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",4,6,7
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerScriptProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,5,5
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerSelectProc.handleChildren(SelectOperator,List<String>,ColumnPrunerProcCtx)",1,6,6
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerSelectProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",6,10,12
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.ColumnPrunerTableScanProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,3,4
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.getFilterProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.getGroupByProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.getJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.getLateralViewForwardProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.getLateralViewJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.getMapJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.getPTFProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.getPruneReduceSinkOpRetainFlags(List<String>,ReduceSinkOperator)",5,5,6
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.getReduceSinkProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.getScriptProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.getSelectProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.getTableScanProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.preserveColumnOrder(Operator<? extends OperatorDesc>,List<String>)",2,4,4
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.pruneJoinOperator(NodeProcessorCtx,CommonJoinOperator,JoinDesc,Map<String, ExprNodeDesc>,Map<Byte, List<Integer>>,boolean)",5,16,18
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.pruneOperator(NodeProcessorCtx,Operator<? extends OperatorDesc>,List<String>)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.pruneReduceSinkOperator(boolean[],ReduceSinkOperator,ColumnPrunerProcCtx)",4,5,6
"org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.setupNeededColumns(TableScanOperator,RowResolver,List<String>)",4,7,8
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagate.ConstantPropagate()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagate.ConstantPropagateWalker.ConstantPropagateWalker(Dispatcher)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagate.ConstantPropagateWalker.walk(Node)",2,4,4
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagate.transform(ParseContext)",3,3,4
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.ConstantPropagateProcCtx(Map<Operator<? extends OperatorDesc>, OpParseContext>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.addOpToDelete(Operator<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.getOpToConstantExprs()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.getOpToDelete()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.getOpToParseCtxMap()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.getPropagatedConstants(Operator<? extends Serializable>)",10,13,17
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.getRowResolver(Operator<? extends Serializable>)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.resolve(ColumnInfo,RowResolver,RowResolver)",2,5,6
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.ConstantPropagateDefaultProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.ConstantPropagateFileSinkProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",5,4,6
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.ConstantPropagateFileSinkProc.pruneDP(FileSinkDesc)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.ConstantPropagateFilterProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.ConstantPropagateGroupByProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,2,3
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.ConstantPropagateJoinProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",7,8,11
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.ConstantPropagateProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.ConstantPropagateReduceSinkProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",7,10,15
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.ConstantPropagateReduceSinkProc.skipFolding(JoinDesc,int)",10,5,10
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.ConstantPropagateSelectProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.ConstantPropagateStopProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.ConstantPropagateTableScanProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,2,3
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.evaluateColumn(ExprNodeColumnDesc,ConstantPropagateProcCtx,Operator<? extends Serializable>)",8,9,10
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.evaluateFunction(GenericUDF,List<ExprNodeDesc>,List<ExprNodeDesc>)",10,9,13
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.foldExpr(ExprNodeDesc,Map<ColumnInfo, ExprNodeDesc>,ConstantPropagateProcCtx,Operator<? extends Serializable>,int,boolean)",8,11,12
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.foldOperator(Operator<? extends Serializable>,ConstantPropagateProcCtx)",1,10,10
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.getFileSinkProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.getFilterProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.getGroupByProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.getJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.getReduceSinkProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.getSelectProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.getStopProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.getTableScanProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.isDeterministicUdf(GenericUDF)",4,4,7
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.propagate(GenericUDF,List<ExprNodeDesc>,RowResolver,Map<ColumnInfo, ExprNodeDesc>)",4,8,12
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.resolveColumn(RowResolver,ExprNodeColumnDesc)",4,2,5
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.shortcutFunction(GenericUDF,List<ExprNodeDesc>)",9,9,9
"org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.typeCast(ExprNodeDesc,TypeInfo)",5,6,11
"org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.checkConvertJoinBucketMapJoin(JoinOperator,OptimizeTezProcContext,int,TezBucketJoinProcCtx)",9,9,11
"org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.convertJoinBucketMapJoin(JoinOperator,OptimizeTezProcContext,int)",2,2,2
"org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.convertJoinMapJoin(JoinOperator,OptimizeTezProcContext,int)",3,5,7
"org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.mapJoinConversionPos(JoinOperator,OptimizeTezProcContext,int)",9,8,13
"org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",6,9,15
"org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.setAllChildrenTraitsToNull(Operator<? extends OperatorDesc>)",4,2,5
"org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.setNumberOfBucketsOnChildren(Operator<? extends OperatorDesc>)",1,3,4
"org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.GenMRFileSink1()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,13,13
"org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.processFS(FileSinkOperator,Stack<Node>,NodeProcessorCtx,boolean)",6,8,9
"org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.processLinkedFileDesc(GenMRProcContext,Task<? extends Serializable>)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.GenMROperator.GenMROperator()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMROperator.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRProcContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRProcContext(HiveConf,HashMap<Operator<? extends OperatorDesc>, Task<? extends Serializable>>,ParseContext,List<Task<MoveWork>>,List<Task<? extends Serializable>>,LinkedHashMap<Operator<? extends OperatorDesc>, GenMapRedCtx>,Set<ReadEntity>,Set<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRUnionCtx.GenMRUnionCtx(Task<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRUnionCtx.addListTopOperators(Operator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRUnionCtx.addTTDesc(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRUnionCtx.addTaskTmpDir(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRUnionCtx.getListTopOperators()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRUnionCtx.getTTDesc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRUnionCtx.getTaskTmpDir()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRUnionCtx.getUTask()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRUnionCtx.setListTopOperators(List<Operator<? extends OperatorDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx.GenMapRedCtx()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx.GenMapRedCtx(Task<? extends Serializable>,String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx.getCurrAliasId()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx.getCurrTask()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.addRootIfPossible(Task<? extends Serializable>)",3,4,4
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.addSeenOp(Task,Operator)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getConf()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getCurrAliasId()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getCurrTask()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getCurrTopOp()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getCurrUnionOp()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getDependencyTaskForMultiInsert()",1,3,3
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getInputs()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getLinkedFileDescTasks()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getMapCurrCtx()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getMvTask()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getOpTaskMap()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getOutputs()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getParseCtx()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getRootTasks()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getSeenFileSinkOps()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.getUnionTask(UnionOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.isSeenOp(Task,Operator)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.setConf(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.setCurrAliasId(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.setCurrTask(Task<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.setCurrTopOp(Operator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.setCurrUnionOp(UnionOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.setLinkedFileDescTasks(Map<FileSinkDesc, Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.setMapCurrCtx(LinkedHashMap<Operator<? extends OperatorDesc>, GenMapRedCtx>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.setMvTask(List<Task<MoveWork>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.setOpTaskMap(HashMap<Operator<? extends OperatorDesc>, Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.setParseCtx(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.setRootTasks(List<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.setSeenFileSinkOps(List<FileSinkOperator>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.setUnionTask(UnionOperator,GenMRUnionCtx)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRRedSink1.GenMRRedSink1()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRRedSink1.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,5,5
"org.apache.hadoop.hive.ql.optimizer.GenMRRedSink2.GenMRRedSink2()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRRedSink2.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,3,3
"org.apache.hadoop.hive.ql.optimizer.GenMRRedSink3.GenMRRedSink3()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRRedSink3.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,6,6
"org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.GenMRTableScan1()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.handlePartialScanCommand(TableScanOperator,GenMRProcContext,ParseContext,Task<? extends Serializable>,QBParseInfo,StatsWork,Task<StatsWork>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,11,12
"org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.GenMRUnion1()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,11,11
"org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.processMapOnlyUnion(UnionOperator,Stack<Node>,GenMRProcContext,UnionProcContext)",1,9,9
"org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.processSubQueryUnionCreateIntermediate(Operator<? extends OperatorDesc>,Operator<? extends OperatorDesc>,Task<? extends Serializable>,GenMRProcContext,GenMRUnionCtx)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.processSubQueryUnionMerge(GenMRProcContext,GenMRUnionCtx,UnionOperator,Stack<Node>)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.shouldBeRootTask(Task<? extends Serializable>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.GenMapRedUtils()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.addDependentMoveTasks(Task<MoveWork>,HiveConf,Task<? extends Serializable>,DependencyCollectionTask)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.addStatsTask(FileSinkOperator,MoveTask,Task<? extends Serializable>,HiveConf)",1,6,6
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.createCondTask(HiveConf,Task<? extends Serializable>,MoveWork,Serializable,String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.createMRWorkForMergingFiles(FileSinkOperator,Path,DependencyCollectionTask,List<Task<MoveWork>>,HiveConf,Task<? extends Serializable>)",2,12,13
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.createMRWorkForMergingFiles(HiveConf,Operator<? extends OperatorDesc>,FileSinkDesc)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.createMergeTask(FileSinkDesc,Path,boolean)",2,7,7
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.createMoveTask(Task<? extends Serializable>,boolean,FileSinkOperator,ParseContext,List<Task<MoveWork>>,HiveConf,DependencyCollectionTask)",1,6,6
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.createTemporaryFile(Operator<? extends OperatorDesc>,Operator<? extends OperatorDesc>,Path,TableDesc,ParseContext)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.createTemporaryTableScanOperator(RowSchema)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.findAlias(MapWork,Operator<?>)",3,3,3
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.findAliases(MapWork,Operator<?>)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.findMoveTask(List<Task<MoveWork>>,FileSinkOperator)",3,5,6
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.findTopOps(Operator<?>,Class<?>)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.getConfirmedPartitionsForScan(QBParseInfo)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.getInputPathsForPartialScan(QBParseInfo,StringBuffer)",2,3,5
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.getMapRedWork(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.getMapRedWorkFromConf(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.getPartitionColumns(QBParseInfo)",2,2,2
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.hasBranchFinished(Object...)",3,1,3
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.initPlan(ReduceSinkOperator,GenMRProcContext)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.initUnionPlan(GenMRProcContext,UnionOperator,Task<? extends Serializable>,boolean)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.initUnionPlan(ReduceSinkOperator,UnionOperator,GenMRProcContext,Task<? extends Serializable>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.isInsertInto(ParseContext,FileSinkOperator)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.isMergeRequired(List<Task<MoveWork>>,HiveConf,FileSinkOperator,Task<? extends Serializable>,boolean)",8,15,18
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.isSkewedStoredAsDirs(FileSinkDesc)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.joinPlan(Task<? extends Serializable>,Task<? extends Serializable>,GenMRProcContext)",1,8,10
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.joinUnionPlan(GenMRProcContext,UnionOperator,Task<? extends Serializable>,Task<? extends Serializable>,boolean)",1,12,13
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.linkMoveTask(FileSinkOperator,ConditionalTask,List<Task<MoveWork>>,HiveConf,DependencyCollectionTask)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.linkMoveTask(Task<MoveWork>,Task<? extends Serializable>,HiveConf,DependencyCollectionTask)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.mergeInput(Operator<? extends OperatorDesc>,GenMRProcContext,Task<? extends Serializable>,boolean)",2,2,2
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.needsTagging(ReduceWork)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.putOpInsertMap(Operator<? extends OperatorDesc>,RowResolver,ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.replaceMapWork(String,String,MapWork,MapWork)",3,9,11
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.setKeyAndValueDesc(ReduceWork,Operator<? extends OperatorDesc>)",2,4,5
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.setKeyAndValueDesc(ReduceWork,ReduceSinkOperator)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.setKeyAndValueDescForTaskTree(Task<? extends Serializable>)",2,12,13
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.setMapWork(MapWork,ParseContext,Set<ReadEntity>,PrunedPartitionList,Operator<? extends OperatorDesc>,String,HiveConf,boolean)",10,37,43
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.setTaskPlan(String,Operator<? extends OperatorDesc>,Task<?>,boolean,GenMRProcContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.setTaskPlan(String,Operator<? extends OperatorDesc>,Task<?>,boolean,GenMRProcContext,PrunedPartitionList)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.setTaskPlan(String,String,Operator<? extends OperatorDesc>,MapWork,boolean,TableDesc)",2,3,6
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.setUnionPlan(GenMRProcContext,boolean,Task<? extends Serializable>,GenMRUnionCtx,boolean)",1,9,9
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.splitPlan(ReduceSinkOperator,GenMRProcContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.splitPlan(ReduceSinkOperator,Task<? extends Serializable>,Task<? extends Serializable>,GenMRProcContext)",1,1,2
"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.splitTasks(ReduceSinkOperator,Task<? extends Serializable>,Task<? extends Serializable>,GenMRProcContext)",2,10,10
"org.apache.hadoop.hive.ql.optimizer.GlobalLimitOptimizer.checkQbpForGlobalLimit(QB)",6,16,20
"org.apache.hadoop.hive.ql.optimizer.GlobalLimitOptimizer.transform(ParseContext)",5,12,13
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.GroupByOptimizer()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.GroupByOptimizerContext.GroupByOptimizerContext(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.GroupByOptimizerContext.getConf()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.GroupByOptimizerContext.getListGroupByOperatorsProcessed()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.GroupByOptimizerContext.setConf(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.GroupByOptimizerContext.setListGroupByOperatorsProcessed(List<GroupByOperator>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.SortGroupByProcessor.SortGroupByProcessor(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.SortGroupByProcessor.checkGroupByOperatorProcessed(GroupByOptimizerContext,GroupByOperator)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.SortGroupByProcessor.checkSortGroupBy(Stack<Node>,GroupByOperator)",18,18,28
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.SortGroupByProcessor.convertGroupByMapSideSortedGroupBy(HiveConf,GroupByOperator,int)",2,2,3
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.SortGroupByProcessor.matchBucketSortCols(List<String>,List<String>,List<String>)",5,6,10
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.SortGroupByProcessor.matchColumnOrder(List<String>,List<String>)",4,4,9
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.SortGroupByProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.SortGroupByProcessor.processGroupBy(GroupByOptimizerContext,Stack<Node>,GroupByOperator,int)",4,11,16
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.SortGroupBySkewProcessor.SortGroupBySkewProcessor(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.SortGroupBySkewProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.getMapSortedGroupbyProc(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.getMapSortedGroupbySkewProc(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.transform(ParseContext)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.IndexUtils.IndexUtils()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.IndexUtils.checkPartitionsCoveredByIndex(TableScanOperator,ParseContext,List<Index>)",4,3,5
"org.apache.hadoop.hive.ql.optimizer.IndexUtils.containsPartition(Hive,Partition,List<Index>)",5,4,5
"org.apache.hadoop.hive.ql.optimizer.IndexUtils.createRootTask(HiveConf,Set<ReadEntity>,Set<WriteEntity>,StringBuilder,LinkedHashMap<String, String>,String,String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.IndexUtils.getIndexes(Table,List<String>)",1,3,4
"org.apache.hadoop.hive.ql.optimizer.IndexUtils.isIndexPartitionFresh(Hive,Index,Partition)",3,3,4
"org.apache.hadoop.hive.ql.optimizer.IndexUtils.isIndexTableFresh(Hive,List<Index>,Table)",5,5,7
"org.apache.hadoop.hive.ql.optimizer.JoinReorder.getBigTables(ParseContext)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.JoinReorder.getOutputSize(Operator<? extends OperatorDesc>,Set<String>)",6,6,9
"org.apache.hadoop.hive.ql.optimizer.JoinReorder.reorder(JoinOperator,Set<String>)",1,3,4
"org.apache.hadoop.hive.ql.optimizer.JoinReorder.transform(ParseContext)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ.getBigTablePosition(ParseContext,JoinOperator,Set<Integer>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.LimitPushdownOptimizer.LimitPushdownContext.LimitPushdownContext(HiveConf)",2,1,3
"org.apache.hadoop.hive.ql.optimizer.LimitPushdownOptimizer.TopNReducer.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",5,5,8
"org.apache.hadoop.hive.ql.optimizer.LimitPushdownOptimizer.transform(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinFactory.MapJoinFactory()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinFactory.TableScanMapJoinProcessor.initMapJoinPlan(AbstractMapJoinOperator<? extends MapJoinDesc>,Task<? extends Serializable>,GenMRProcContext,boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinFactory.TableScanMapJoinProcessor.joinMapJoinPlan(AbstractMapJoinOperator<? extends MapJoinDesc>,Task<? extends Serializable>,GenMRProcContext,boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinFactory.TableScanMapJoinProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.MapJoinFactory.TableScanMapJoinProcessor.setupBucketMapJoinInfo(MapWork,AbstractMapJoinOperator<? extends MapJoinDesc>)",4,10,11
"org.apache.hadoop.hive.ql.optimizer.MapJoinFactory.getPositionParent(AbstractMapJoinOperator<? extends MapJoinDesc>,Stack<Node>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.MapJoinFactory.getTableScanMapJoin()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.CurrentMapJoin.findGrandChildSubqueryMapjoin(MapJoinWalkerCtx,MapJoinOperator)",7,5,13
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.CurrentMapJoin.nonSubqueryMapJoin(ParseContext,MapJoinOperator,MapJoinOperator)",2,2,3
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.CurrentMapJoin.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,5,5
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.Default.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.MapJoinDefault.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.MapJoinFS.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,2,3
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.MapJoinProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.MapJoinWalkerCtx.MapJoinWalkerCtx(List<AbstractMapJoinOperator<? extends MapJoinDesc>>,ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.MapJoinWalkerCtx.getCurrMapJoinOp()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.MapJoinWalkerCtx.getListMapJoinsNoRed()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.MapJoinWalkerCtx.getListRejectedMapJoins()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.MapJoinWalkerCtx.getpGraphContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.MapJoinWalkerCtx.setCurrMapJoinOp(AbstractMapJoinOperator<? extends MapJoinDesc>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.MapJoinWalkerCtx.setListMapJoins(List<AbstractMapJoinOperator<? extends MapJoinDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.MapJoinWalkerCtx.setListRejectedMapJoins(List<AbstractMapJoinOperator<? extends MapJoinDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.MapJoinWalkerCtx.setpGraphContext(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.addNoReducerMapJoinToCtx(MapJoinWalkerCtx,AbstractMapJoinOperator<? extends MapJoinDesc>)",2,3,5
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.addRejectMapJoinToCtx(MapJoinWalkerCtx,AbstractMapJoinOperator<? extends MapJoinDesc>)",2,4,6
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.checkChildOperatorType(Operator<? extends OperatorDesc>)",2,4,4
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.checkMapJoin(int,JoinCondDesc[])",2,1,2
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.checkParentOperatorType(Operator<? extends OperatorDesc>)",2,4,4
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.convertJoinOpMapJoinOp(HiveConf,LinkedHashMap<Operator<? extends OperatorDesc>, OpParseContext>,JoinOperator,QBJoinTree,int,boolean)",6,19,23
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.convertMapJoin(HiveConf,LinkedHashMap<Operator<? extends OperatorDesc>, OpParseContext>,JoinOperator,QBJoinTree,int,boolean,boolean)",3,8,8
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.convertSMBJoinToMapJoin(HiveConf,Map<Operator<? extends OperatorDesc>, OpParseContext>,SMBMapJoinOperator,QBJoinTree,int,boolean)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genLocalWorkForMapJoin(MapredWork,MapJoinOperator,int)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genMapJoinLocalWork(MapredWork,MapJoinOperator,int)",6,13,15
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genMapJoinOpAndLocalWork(HiveConf,MapredWork,JoinOperator,int)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.genSelectPlan(ParseContext,MapJoinOperator)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.generateMapJoinOperator(ParseContext,JoinOperator,QBJoinTree,int)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.getBigTableCandidates(JoinCondDesc[])",3,10,12
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.getCurrentMapJoin()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.getDefault()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.getMapJoinDefault()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.getMapJoinFS()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.mapSideJoin(JoinOperator,QBJoinTree)",7,5,8
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.needValueIndex(int[])",3,1,3
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.putOpInsertMap(Operator<? extends OperatorDesc>,RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.transform(ParseContext)",1,5,5
"org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.validateMapJoinTypes(Operator<? extends OperatorDesc>)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.NonBlockingOpDeDupProc.FilterDedup.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,2,3
"org.apache.hadoop.hive.ql.optimizer.NonBlockingOpDeDupProc.SelectDedup.checkReferences(ExprNodeDesc,Set<String>,Set<String>)",6,5,7
"org.apache.hadoop.hive.ql.optimizer.NonBlockingOpDeDupProc.SelectDedup.checkReferences(List<ExprNodeDesc>,Set<String>)",3,2,3
"org.apache.hadoop.hive.ql.optimizer.NonBlockingOpDeDupProc.SelectDedup.getFunctionOutputs(List<String>,List<ExprNodeDesc>)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.NonBlockingOpDeDupProc.SelectDedup.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",5,5,8
"org.apache.hadoop.hive.ql.optimizer.NonBlockingOpDeDupProc.transform(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.Optimizer.getPctx()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.Optimizer.initialize(HiveConf)",1,26,26
"org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize()",1,2,2
"org.apache.hadoop.hive.ql.optimizer.Optimizer.setPctx(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.PrunerExpressionOperatorFactory.ColumnExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.PrunerExpressionOperatorFactory.DefaultExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,3,3
"org.apache.hadoop.hive.ql.optimizer.PrunerExpressionOperatorFactory.FieldExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,4,5
"org.apache.hadoop.hive.ql.optimizer.PrunerExpressionOperatorFactory.GenericFuncExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,9,11
"org.apache.hadoop.hive.ql.optimizer.PrunerExpressionOperatorFactory.getDefaultExprProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.PrunerExpressionOperatorFactory.getFieldProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.PrunerExpressionOperatorFactory.getGenericFuncProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.PrunerOperatorFactory.DefaultPruner.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.PrunerOperatorFactory.FilterPruner.addPruningPred(Map<TableScanOperator, ExprNodeDesc>,TableScanOperator,ExprNodeDesc)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.PrunerOperatorFactory.FilterPruner.addPruningPred(Map<TableScanOperator, Map<String, ExprNodeDesc>>,TableScanOperator,ExprNodeDesc,Partition)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.PrunerOperatorFactory.FilterPruner.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,3,5
"org.apache.hadoop.hive.ql.optimizer.PrunerOperatorFactory.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.PrunerUtils.PrunerUtils()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.PrunerUtils.walkExprTree(ExprNodeDesc,NodeProcessorCtx,NodeProcessor,NodeProcessor,NodeProcessor,NodeProcessor)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.PrunerUtils.walkOperatorTree(ParseContext,NodeProcessorCtx,NodeProcessor,NodeProcessor)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",8,23,30
"org.apache.hadoop.hive.ql.optimizer.SamplePruner.AddPathReturnStatus.AddPathReturnStatus(boolean,boolean,long)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SamplePruner.DefaultPPR.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SamplePruner.FilterPPR.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,4,5
"org.apache.hadoop.hive.ql.optimizer.SamplePruner.SamplePrunerCtx.SamplePrunerCtx(HashMap<TableScanOperator, sampleDesc>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SamplePruner.SamplePrunerCtx.getOpToSamplePruner()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SamplePruner.SamplePrunerCtx.setOpToSamplePruner(HashMap<TableScanOperator, sampleDesc>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SamplePruner.addPath(FileSystem,String,long,int,Collection<Path>)",6,4,7
"org.apache.hadoop.hive.ql.optimizer.SamplePruner.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SamplePruner.getFilterProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SamplePruner.limitPrune(Partition,long,int,Collection<Path>)",5,1,6
"org.apache.hadoop.hive.ql.optimizer.SamplePruner.prune(Partition,sampleDesc)",5,6,8
"org.apache.hadoop.hive.ql.optimizer.SamplePruner.transform(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SetReducerParallelism.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,6,6
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchAggregation.SingleGBYProcessor.SingleGBYProcessor(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchAggregation.SingleGBYProcessor.createIntermediateFS(Operator<?>,Path)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchAggregation.SingleGBYProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,3,4
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchAggregation.transform(ParseContext)",2,6,7
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.FetchData.FetchData(ReadEntity,Table,PrunedPartitionList,SplitSample,boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.FetchData.FetchData(ReadEntity,Table,SplitSample)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.FetchData.calculateLength(ParseContext,long)",4,4,5
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.FetchData.completed(ParseContext,FetchWork)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.FetchData.convertToWork()",2,4,4
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.FetchData.getFileLength(JobConf,Path,Class<? extends InputFormat>)",2,2,3
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.FetchData.getInputLength(ParseContext,long)",3,4,4
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.FetchData.hasOnlyPruningFilter()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.checkOperators(FetchData,TableScanOperator,boolean,boolean)",8,7,15
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.checkThreshold(FetchData,int,ParseContext)",4,3,5
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.checkTree(boolean,ParseContext,String,TableScanOperator)",7,7,12
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.optimize(ParseContext,String,TableScanOperator)",2,3,3
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.replaceFSwithLS(Operator<?>,String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.transform(ParseContext)",4,6,7
"org.apache.hadoop.hive.ql.optimizer.SizeBasedBigTableSelectorForAutoSMJ.getListTopOps(Operator<? extends OperatorDesc>,List<TableScanOperator>)",2,5,6
"org.apache.hadoop.hive.ql.optimizer.SizeBasedBigTableSelectorForAutoSMJ.getSize(HiveConf,Partition)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SizeBasedBigTableSelectorForAutoSMJ.getSize(HiveConf,String,Path)",2,2,4
"org.apache.hadoop.hive.ql.optimizer.SizeBasedBigTableSelectorForAutoSMJ.getSize(HiveConf,Table)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinOptProcCtx.SkewJoinOptProcCtx(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinOptProcCtx.getCloneTSOpMap()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinOptProcCtx.getDoneJoins()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinOptProcCtx.getpGraphContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinOptProcCtx.setCloneTSOpMap(Map<TableScanOperator, TableScanOperator>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinOptProcCtx.setDoneJoins(Set<JoinOperator>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinOptProcCtx.setPGraphContext(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinProc.SkewJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinProc.constructFilterExpr(Map<List<ExprNodeDesc>, List<List<String>>>,boolean)",1,7,8
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinProc.createConstDesc(String,ExprNodeColumnDesc)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinProc.getSkewedJoinValues(List<List<String>>,List<Integer>)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinProc.getSkewedValues(Operator<? extends OperatorDesc>,List<TableScanOperator>)",7,19,21
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinProc.getTable(ParseContext,Operator<? extends OperatorDesc>,List<TableScanOperator>)",5,6,7
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinProc.getTableScanOps(Operator<? extends OperatorDesc>,List<TableScanOperator>)",5,3,5
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinProc.getTableScanOpsForJoin(JoinOperator,List<TableScanOperator>)",3,2,3
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinProc.getTopOps(Operator<? extends OperatorDesc>)",1,5,5
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinProc.insertFilterOnTop(TableScanOperator,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinProc.insertRowResolvers(Operator<? extends OperatorDesc>,Operator<? extends OperatorDesc>,SkewJoinOptProcCtx)",1,7,7
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinProc.insertSkewFilter(List<TableScanOperator>,Map<List<ExprNodeDesc>, List<List<String>>>,boolean)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.SkewJoinProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",5,9,14
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.getSkewJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.transform(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortBucketJoinProcCtx.SortBucketJoinProcCtx(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortBucketJoinProcCtx.getBigTablePosition()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortBucketJoinProcCtx.getKeyExprMap()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortBucketJoinProcCtx.getSrcs()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortBucketJoinProcCtx.setBigTablePosition(int)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortBucketJoinProcCtx.setKeyExprMap(Map<Byte, List<ExprNodeDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortBucketJoinProcCtx.setSrcs(String[])",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.SortedDynamicPartitionProc.SortedDynamicPartitionProc(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.SortedDynamicPartitionProc.copyRowResolver(RowResolver)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.SortedDynamicPartitionProc.getBucketPositions(List<String>,List<FieldSchema>)",4,4,4
"org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.SortedDynamicPartitionProc.getPartitionPositions(DynamicPartitionCtx,RowSchema)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.SortedDynamicPartitionProc.getPositionsToExprNodes(List<Integer>,List<ColumnInfo>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.SortedDynamicPartitionProc.getReduceSinkDesc(List<Integer>,List<Integer>,List<Integer>,ArrayList<ExprNodeDesc>,ArrayList<ExprNodeDesc>,int,Operator<? extends OperatorDesc>)",1,14,16
"org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.SortedDynamicPartitionProc.getSortPositionsOrder(List<Order>,List<FieldSchema>)",4,4,4
"org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.SortedDynamicPartitionProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",4,8,8
"org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.SortedDynamicPartitionProc.putOpInsertMap(Operator<?>,RowResolver,ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.SortedDynamicPartitionProc.removeRSInsertedByEnforceBucketing(FileSinkOperator)",6,6,9
"org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.getSortDynPartProc(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.transform(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.SortedMergeBucketMapJoinOptimizer()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.getCheckCandidateJoin()",4,5,5
"org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.getListOfRejectedJoins(ParseContext,SortBucketJoinProcCtx)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.getSortedMergeBucketMapjoinProc(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.getSortedMergeJoinProc(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapJoinOptimizer.transform(ParseContext)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapjoinProc.SortedMergeBucketMapjoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapjoinProc.SortedMergeBucketMapjoinProc(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortedMergeBucketMapjoinProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,4,5
"org.apache.hadoop.hive.ql.optimizer.SortedMergeJoinProc.SortedMergeJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortedMergeJoinProc.SortedMergeJoinProc(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.SortedMergeJoinProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.MetaDataProcessor.MetaDataProcessor(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.MetaDataProcessor.getNullcountFor(StatType,ColumnStatisticsData)",7,7,7
"org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.MetaDataProcessor.getRowCnt(ParseContext,TableScanOperator,Table)",4,5,5
"org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.MetaDataProcessor.getType(String)",6,6,7
"org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.MetaDataProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",59,48,76
"org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.MetaDataProcessor.validateSingleColStat(List<ColumnStatisticsObj>)",3,3,3
"org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.MetaDataProcessor.verifyAndGetPartStats(Hive,Table,String,Set<Partition>)",4,4,4
"org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.transform(ParseContext)",2,6,7
"org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ.getBigTablePosition(ParseContext,JoinOperator,Set<Integer>)",4,5,8
"org.apache.hadoop.hive.ql.optimizer.TezBucketJoinProcCtx.TezBucketJoinProcCtx(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.TezBucketJoinProcCtx.getNumBuckets()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.TezBucketJoinProcCtx.isSubQuery()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.TezBucketJoinProcCtx.setIsSubQuery(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.TezBucketJoinProcCtx.setNumBuckets(int)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.AbstractCorrelationProcCtx.AbstractCorrelationProcCtx(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.AbstractCorrelationProcCtx.addRemovedOperator(Operator<?>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.AbstractCorrelationProcCtx.getPctx()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.AbstractCorrelationProcCtx.hasBeenRemoved(Operator<?>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.AbstractCorrelationProcCtx.minReducer()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.AbstractCorrelationProcCtx.setPctx(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.AbstractCorrelationProcCtx.trustScript()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProc.analyzeReduceSinkOperatorsOfJoinOperator(JoinCondDesc[],List<Operator<? extends OperatorDesc>>,Operator<? extends OperatorDesc>,Set<ReduceSinkOperator>)",2,6,10
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProc.exploitJobFlowCorrelation(ReduceSinkOperator,CorrelationNodeProcCtx,IntraQueryCorrelation)",1,8,13
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProc.findCorrelatedReduceSinkOperators(Operator<? extends OperatorDesc>,List<ExprNodeDesc>,List<ExprNodeDesc>,String,Operator<? extends OperatorDesc>,IntraQueryCorrelation)",15,30,36
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",5,8,10
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProc.sameKeys(List<ExprNodeDesc>,List<ExprNodeDesc>)",6,3,6
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProc.sameOrder(String,String)",5,5,8
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProcCtx.CorrelationNodeProcCtx(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProcCtx.addCorrelation(IntraQueryCorrelation)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProcCtx.addWalked(ReduceSinkOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProcCtx.addWalkedAll(Collection<ReduceSinkOperator>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProcCtx.getAbortReasons()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProcCtx.getCorrelations()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProcCtx.incrementFileSinkOperatorCount()",1,2,2
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProcCtx.isAbort()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProcCtx.isWalked(ReduceSinkOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProcCtx.removeWalked(ReduceSinkOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProcCtx.removeWalkedAll(Collection<ReduceSinkOperator>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationNodeProcCtx.setAbort(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.CorrelationOptimizer()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.findPossibleAutoConvertedJoinOperators()",11,9,15
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.getDefaultProc()",1,3,3
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationOptimizer.transform(ParseContext)",1,5,5
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.findParents(JoinOperator,Class<T>)",5,4,5
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.findPossibleParent(Operator<?>,Class<T>,boolean)",1,1,3
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.findPossibleParents(Operator<?>,Class<T>,boolean)",6,4,12
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.findSiblingOperators(Operator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.findSiblingReduceSinkOperators(ReduceSinkOperator)",3,3,3
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.findTableScanOperators(Operator<?>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.findTableScanOperators(Operator<?>,Set<TableScanOperator>)",3,4,5
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.getColumnName(Map<String, ExprNodeDesc>,ExprNodeDesc)",3,4,4
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.getSingleChild(Operator<?>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.getSingleChild(Operator<?>,Class<T>)",1,1,2
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.getSingleChild(Operator<?>,boolean)",5,6,6
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.getSingleParent(Operator<?>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.getSingleParent(Operator<?>,Class<T>)",1,1,2
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.getSingleParent(Operator<?>,boolean)",5,6,6
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.getSortedTags(JoinOperator)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.getStartForGroupBy(ReduceSinkOperator)",1,1,2
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.hasGroupingSet(ReduceSinkOperator)",2,2,3
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.indexOf(ExprNodeDesc,ExprNodeDesc[],Operator,Operator[],boolean[])",3,3,4
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.insertOperatorBetween(Operator<?>,Operator<?>,Operator<?>)",3,3,3
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.isExisted(ExprNodeDesc,List<ExprNodeDesc>)",3,3,4
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.isNullOperator(Operator<?>)",2,2,2
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.isSortedTag(JoinOperator,int)",8,4,9
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.putOpInsertMap(Operator<?>,RowResolver,ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.removeOperator(Operator<?>,Operator<?>,Operator<?>,ParseContext)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.removeOperator(Operator<?>,ParseContext)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.removeReduceSinkForGroupBy(ReduceSinkOperator,GroupByOperator,ParseContext,AbstractCorrelationProcCtx)",1,7,7
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.replaceOperatorWithSelect(Operator<?>,ParseContext,AbstractCorrelationProcCtx)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.replaceReduceSinkWithSelectOperator(ReduceSinkOperator,ParseContext,AbstractCorrelationProcCtx)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.IntraQueryCorrelation.IntraQueryCorrelation(int)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.IntraQueryCorrelation.addToAllReduceSinkOperators(ReduceSinkOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.IntraQueryCorrelation.adjustNumReducers(int)",5,1,5
"org.apache.hadoop.hive.ql.optimizer.correlation.IntraQueryCorrelation.getAllReduceSinkOperators()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.IntraQueryCorrelation.getBottomReduceSinkOperators()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.IntraQueryCorrelation.getNewTagToChildIndex()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.IntraQueryCorrelation.getNewTagToOldTag()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.IntraQueryCorrelation.getNumReducers()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.IntraQueryCorrelation.hasJobFlowCorrelation()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.IntraQueryCorrelation.setJobFlowCorrelation(boolean,List<ReduceSinkOperator>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.IntraQueryCorrelation.setNewTag(Integer,Integer,Integer)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.QueryPlanTreeTransformation.applyCorrelation(ParseContext,CorrelationNodeProcCtx,IntraQueryCorrelation)",8,21,23
"org.apache.hadoop.hive.ql.optimizer.correlation.QueryPlanTreeTransformation.setNewTag(IntraQueryCorrelation,List<Operator<? extends OperatorDesc>>,ReduceSinkOperator,Map<ReduceSinkOperator, Integer>)",1,2,3
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.AbsctractReducerReducerProc.checkExprs(List<ExprNodeDesc>,List<ExprNodeDesc>,ReduceSinkOperator,ReduceSinkOperator)",5,7,9
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.AbsctractReducerReducerProc.checkNumDistributionKey(int,int)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.AbsctractReducerReducerProc.checkNumReducer(int,int)",5,1,5
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.AbsctractReducerReducerProc.checkOrder(String,String)",5,5,8
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.AbsctractReducerReducerProc.checkStatus(ReduceSinkOperator,ReduceSinkOperator,int)",5,3,7
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.AbsctractReducerReducerProc.merge(ReduceSinkOperator,JoinOperator,int)",9,9,15
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.AbsctractReducerReducerProc.merge(ReduceSinkOperator,ReduceSinkOperator,int)",4,12,14
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.AbsctractReducerReducerProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",6,5,8
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.AbsctractReducerReducerProc.sameKeys(List<ExprNodeDesc>,List<ExprNodeDesc>,Operator<?>,Operator<?>)",6,5,7
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.DefaultProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.GroupbyReducerProc.process(ReduceSinkOperator,GroupByOperator,ReduceSinkDeduplicateProcCtx)",3,3,4
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.GroupbyReducerProc.process(ReduceSinkOperator,ReduceSinkDeduplicateProcCtx)",3,3,4
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.JoinReducerProc.process(ReduceSinkOperator,GroupByOperator,ReduceSinkDeduplicateProcCtx)",2,3,3
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.JoinReducerProc.process(ReduceSinkOperator,ReduceSinkDeduplicateProcCtx)",2,3,3
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.ReduceSinkDeduplicateProcCtx.ReduceSinkDeduplicateProcCtx(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.ReduceSinkDeduplicateProcFactory.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.ReduceSinkDeduplicateProcFactory.getGroupbyReducerProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.ReduceSinkDeduplicateProcFactory.getJoinReducerProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.ReduceSinkDeduplicateProcFactory.getReducerReducerProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.ReducerReducerProc.process(ReduceSinkOperator,GroupByOperator,ReduceSinkDeduplicateProcCtx)",2,3,3
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.ReducerReducerProc.process(ReduceSinkOperator,ReduceSinkDeduplicateProcCtx)",2,3,3
"org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.transform(ParseContext)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.RewriteCanApplyCtx(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.addTable(String,String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.findBaseTable(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.getAggFuncCnt()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.getAggFuncColList()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.getAggFunction()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.getAllColumns()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.getBaseTableName()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.getBaseToIdxTableMap()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.getGbKeyNameList()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.getIndexTableName()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.getInstance(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.getParseContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.getPredicateColumnsList()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.getSelectColumnsList()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.isAggFuncColsFetchException()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.isAggFuncIsNotCount()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.isCountOfOne()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.isCountOnAllCols()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.isGbyKeysFetchException()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.isIndexUsableForQueryBranchRewrite(Index,Set<String>)",7,7,9
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.isQueryHasGroupBy()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.isQueryHasMultipleTables()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.isSelClauseColsFetchException()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.isWhrClauseColsFetchException()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.populateRewriteVars(TableScanOperator)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.resetCanApplyCtx()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setAggFuncCnt(int)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setAggFuncColList(Set<String>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setAggFuncColsFetchException(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setAggFuncIsNotCount(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setAggFunction(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setBaseTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setCountOfOne(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setCountOnAllCols(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setGbKeyNameList(Set<String>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setGbyKeysFetchException(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setIndexTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setPredicateColumnsList(Set<String>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setQueryHasGroupBy(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setQueryHasMultipleTables(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setSelClauseColsFetchException(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setSelectColumnsList(Set<String>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyCtx.setWhrClauseColsFetchException(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyProcFactory.CheckFilterProc.CheckFilterProc(TableScanOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyProcFactory.CheckFilterProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,4,4
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyProcFactory.CheckGroupByProc.CheckGroupByProc(TableScanOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyProcFactory.CheckGroupByProc.checkExpression(RewriteCanApplyCtx,ExprNodeDesc)",1,6,6
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyProcFactory.CheckGroupByProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",9,15,16
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyProcFactory.canApplyOnFilterOperator(TableScanOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteCanApplyProcFactory.canApplyOnGroupByOperator(TableScanOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.checkIfAllRewriteCriteriaIsMet(RewriteCanApplyCtx)",9,9,9
"org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.checkIfIndexBuiltOnAllTablePartitions(TableScanOperator,List<Index>)",3,2,4
"org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.checkIfRewriteCanBeApplied(String,TableScanOperator,Table,List<Index>)",3,5,5
"org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.getIndexToKeysMap(List<Index>)",2,5,5
"org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.getIndexesForRewrite()",1,3,3
"org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.getName()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.ifQueryHasMultipleTables()",2,3,3
"org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.rewriteOriginalQuery()",1,2,2
"org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.shouldApplyOptimization()",7,5,7
"org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex.transform(ParseContext)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.RewriteParseContextGenerator()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.doSemanticAnalysis(SemanticAnalyzer,ASTNode,Context)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.generateOperatorTree(HiveConf,String)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndex.NewQueryGroupbySchemaProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,11,12
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndex.NewQuerySelectSchemaProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndex.ReplaceTableScanOpProc.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,5,5
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndex.RewriteQueryUsingAggregateIndex()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndex.getNewQueryGroupbySchemaProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndex.getNewQuerySelectSchemaProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndex.getReplaceTableScanProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.RewriteQueryUsingAggregateIndexCtx(ParseContext,Hive,String,String,Set<String>,String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.getAggrExprNode()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.getAggregateFunction()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.getColumns()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.getEval()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.getHiveDb()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.getIndexName()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.getInstance(ParseContext,Hive,String,String,Set<String>,String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.getOpc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.getParseContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.invokeRewriteQueryProc(Operator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.setAggrExprNode(ExprNodeColumnDesc)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.setEval(GenericUDAFEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcCtx.ExprProcCtx(LineageCtx,Operator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcCtx.getInputOperator()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcCtx.getLineageCtx()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcCtx.getResolver()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcFactory.ColumnExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,3,4
"org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcFactory.DefaultExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,2
"org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcFactory.GenericExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,2,4
"org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcFactory.getColumnProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcFactory.getDefaultExprProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcFactory.getExprDependency(LineageCtx,Operator<? extends OperatorDesc>,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcFactory.getFieldProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.ExprProcFactory.getGenericFuncProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.Generator.transform(ParseContext)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.Index.Index()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.Index.clear()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.Index.getDependency(Operator<? extends OperatorDesc>,ColumnInfo)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.Index.mergeDependency(Operator<? extends OperatorDesc>,ColumnInfo,Dependency)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.Index.putDependency(Operator<? extends OperatorDesc>,ColumnInfo,Dependency)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.LineageCtx(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.getIndex()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.getNewDependencyType(DependencyType,DependencyType)",3,1,5
"org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.getParseCtx()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.DefaultLineage.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.GroupByLineage.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,12,12
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.JoinLineage.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,2,3
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.LateralViewJoinLineage.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,4,5
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.ReduceSinkLineage.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",5,10,11
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.SelectLineage.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,3,3
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.TableScanLineage.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.TransformLineage.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.UnionLineage.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.getGroupByProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.getJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.getLateralViewJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.getParent(Stack<Node>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.getReduceSinkProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.getSelProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.getTSProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.getTransformProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory.getUnionProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBExprProcCtx.LBExprProcCtx(String,Partition)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBExprProcCtx.getPart()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBExprProcCtx.getTabAlias()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBExprProcCtx.setTabAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBExprProcFactory.LBExprProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBExprProcFactory.LBPRColumnExprProcessor.isPruneForListBucketing(Partition,String)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBExprProcFactory.LBPRColumnExprProcessor.processColumnDesc(NodeProcessorCtx,ExprNodeColumnDesc)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBExprProcFactory.genPruner(String,ExprNodeDesc,Partition)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBExprProcFactory.getColumnProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBOpPartitionWalkerCtx.LBOpPartitionWalkerCtx(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBOpPartitionWalkerCtx.getParseContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBOpPartitionWalkerCtx.getPartitions()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBOpPartitionWalkerCtx.setPartitions(PrunedPartitionList)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBOpWalkerCtx.LBOpWalkerCtx(Map<TableScanOperator, Map<String, ExprNodeDesc>>,Partition)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBOpWalkerCtx.getOpToPartToLBPruner()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBOpWalkerCtx.getPart()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBPartitionProcFactory.LBPRPartitionFilterPruner.generatePredicate(NodeProcessorCtx,FilterOperator,TableScanOperator)",2,4,4
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBPartitionProcFactory.LBPartitionProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBPartitionProcFactory.getFilterProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBProcFactory.LBPRFilterPruner.generatePredicate(NodeProcessorCtx,FilterOperator,TableScanOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBProcFactory.LBProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBProcFactory.getFilterProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.DynamicMultiDimensionalCollection.flat(List<List<String>>)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.DynamicMultiDimensionalCollection.generateCollection(List<List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.DynamicMultiDimensionalCollection.uniqueElementsList(List<List<String>>,String)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.DynamicMultiDimensionalCollection.uniqueSkewedValueList(List<List<String>>)",2,6,7
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.DynamicMultiDimensionalCollection.walker(List<List<String>>,List<List<String>>,List<String>,int)",2,4,4
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.decideDefaultDirSelection(Partition,List<Path>,List<Boolean>)",3,2,5
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.decideSkewedValueDirSelection(Partition,ExprNodeDesc,List<Path>,List<List<String>>,List<List<String>>)",1,5,6
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.execute(ParseContext,Partition,ExprNodeDesc)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.generateFinalPath(Partition,List<Path>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.prune(ParseContext,Partition,ExprNodeDesc)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner.transform(ParseContext)",1,6,6
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.andBoolOperand(Boolean,Boolean)",4,1,7
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.coreComparisonInEqualNode(String,String,List<String>)",1,4,5
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.evaluateAndNode(ExprNodeDesc,List<String>,List<String>,List<List<String>>)",2,3,3
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.evaluateEqualNd(ExprNodeDesc,List<String>,List<String>,List<List<String>>)",1,2,3
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.evaluateExprOnCell(List<String>,List<String>,ExprNodeDesc,List<List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.evaluateNotNode(ExprNodeDesc,List<String>,List<String>,List<List<String>>)",2,3,3
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.evaluateOrNode(ExprNodeDesc,List<String>,List<String>,List<List<String>>)",2,3,3
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.isListBucketingPart(Partition)",1,6,6
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.isUnknownState(ExprNodeDesc)",1,3,4
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.notBoolOperand(Boolean)",2,1,3
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.orBoolOperand(Boolean,Boolean)",4,1,6
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.recursiveExpr(ExprNodeDesc,List<String>,List<String>,List<List<String>>)",7,6,7
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.skipSkewedDirectory(Boolean)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPrunerUtils.startComparisonInEqualNode(List<String>,List<String>,List<List<String>>,Boolean,ExprNodeDesc,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestDynamicMultiDimeCollection.testFlat1()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestDynamicMultiDimeCollection.testFlat2()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestDynamicMultiDimeCollection.testFlat3()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestDynamicMultiDimeCollection.testUniqueElementsList1()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestDynamicMultiDimeCollection.testUniqueElementsList2()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestDynamicMultiDimeCollection.testUniqueElementsList3()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestListBucketingPrunner.testAndBoolOperand()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestListBucketingPrunner.testNotBoolOperand()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestListBucketingPrunner.testOrBoolOperand()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestListBucketingPrunner.testSkipSkewedDirectory1()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestListBucketingPrunner.testSkipSkewedDirectory2()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.TestListBucketingPrunner.testSkipSkewedDirectory3()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.AnnotateOpTraitsProcCtx.AnnotateOpTraitsProcCtx(ParseContext)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.AnnotateOpTraitsProcCtx.getConf()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.AnnotateOpTraitsProcCtx.getParseContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.AnnotateOpTraitsProcCtx.setConf(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.AnnotateOpTraitsProcCtx.setParseContext(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.AnnotateWithOpTraits.transform(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.DefaultRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.GroupByRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.JoinRule.getOutputColNames(JoinOperator,ReduceSinkOperator,byte)",8,8,8
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.JoinRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,3,4
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.MultiParentRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.ReduceSinkRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,5,5
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.SelectRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,9,9
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.TableScanRule.checkBucketedTable(Table,ParseContext,PrunedPartitionList)",6,6,8
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.TableScanRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,2,3
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.getDefaultRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.getGroupByRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.getJoinRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.getMultiParentRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.getReduceSinkRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.getSelectRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.getTableScanRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PartitionConditionRemover.transform(ParseContext)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcCtx.PcrExprProcCtx(String,List<Partition>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcCtx.PcrExprProcCtx(String,List<Partition>,List<VirtualColumn>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcCtx.getPartList()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcCtx.getTabAlias()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcCtx.getVirtualColumns()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.ColumnExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,2,3
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.DefaultExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,1,3
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.FieldExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",4,1,4
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.GenericFuncExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",25,30,35
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.NodeInfoWrapper.NodeInfoWrapper(WalkState,Boolean[],ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.PcrExprProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.evalExprWithPart(ExprNodeDesc,Partition,List<VirtualColumn>)",1,1,3
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.getColumnProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.getDefaultExprProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.getFieldProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.getGenericFuncProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.getOutExpr(ExprNodeGenericFuncDesc,Object[])",1,3,3
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.getResultWrapFromResults(Boolean[],ExprNodeGenericFuncDesc,Object[])",3,3,3
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.ifResultsAgree(Boolean[])",5,4,5
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.ifResultsAgree(Object[])",5,4,5
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.opAnd(Boolean,Boolean)",3,4,7
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.opNot(Boolean)",4,2,4
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.opOr(Boolean,Boolean)",3,4,7
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.walkExprTree(String,ArrayList<Partition>,List<VirtualColumn>,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.DefaultPCR.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.FilterPCR.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",7,7,12
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.PcrOpProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.getFilterProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpWalkerCtx.OpToDeleteInfo.OpToDeleteInfo(Operator<? extends OperatorDesc>,FilterOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpWalkerCtx.OpToDeleteInfo.getOperator()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpWalkerCtx.OpToDeleteInfo.getParent()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpWalkerCtx.PcrOpWalkerCtx(ParseContext,List<OpToDeleteInfo>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpWalkerCtx.getOpToRemove()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpWalkerCtx.getParseContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.AbstractJoinTaskDispatcher(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.dispatch(Node,Stack<Node>,Object...)",2,5,7
"org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.getTotalKnownInputSize(Context,MapWork,Map<String, ArrayList<String>>,HashMap<String, Long>)",1,5,6
"org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.replaceTask(Task<? extends Serializable>,Task<? extends Serializable>,PhysicalContext)",1,5,5
"org.apache.hadoop.hive.ql.optimizer.physical.AbstractJoinTaskDispatcher.replaceTaskWithConditionalTask(Task<? extends Serializable>,ConditionalTask,PhysicalContext)",4,6,7
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.BucketCol.BucketCol()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.BucketCol.BucketCol(String,int)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.BucketCol.addAlias(String,Integer)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.BucketCol.getIndexes()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.BucketCol.getNames()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.BucketCol.toString()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.BucketingSortingCtx(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.SortCol.SortCol()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.SortCol.SortCol(String,int,char)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.SortCol.SortCol(char)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.SortCol.addAlias(String,Integer)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.SortCol.getIndexes()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.SortCol.getNames()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.SortCol.getSortOrder()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.SortCol.toString()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.getBucketedCols(Operator<? extends OperatorDesc>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.getBucketedColsByDirectory()",1,1,2
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.getSortedCols(Operator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.getSortedColsByDirectory()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.setBucketedCols(Operator<? extends OperatorDesc>,List<BucketCol>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.setBucketedColsByDirectory(Map<String, List<BucketCol>>)",1,1,2
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.setSortedCols(Operator<? extends OperatorDesc>,List<SortCol>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.setSortedColsByDirectory(Map<String, List<SortCol>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingInferenceOptimizer.inferBucketingSorting(List<ExecDriver>)",4,2,4
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingInferenceOptimizer.resolve(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.DefaultInferrer.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.ExtractInferrer.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,2,3
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.FileSinkInferrer.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.ForwardingInferrer.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.GroupByInferrer.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.GroupByInferrer.processGroupBy(Operator<? extends OperatorDesc>,GroupByOperator,BucketingSortingCtx)",3,2,3
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.GroupByInferrer.processGroupByReduceSink(ReduceSinkOperator,GroupByOperator,BucketingSortingCtx)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.JoinInferrer.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,10,10
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.MultiGroupByInferrer.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,1,3
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.SelectInferrer.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",4,6,8
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.extractBucketCols(ReduceSinkOperator,List<ExprNodeDesc>)",4,4,4
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.extractSortCols(ReduceSinkOperator,List<ExprNodeDesc>)",4,2,4
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.extractTraits(BucketingSortingCtx,ReduceSinkOperator,Operator<?>)",2,5,6
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.findBucketingSortingColumns(List<ExprNodeDesc>,List<ColumnInfo>,List<BucketCol>,List<SortCol>,BucketCol[],SortCol[])",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.findBucketingSortingColumns(List<ExprNodeDesc>,List<ColumnInfo>,List<BucketCol>,List<SortCol>,BucketCol[],SortCol[],int)",3,7,9
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getExtractProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getFileSinkProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getFilterProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getForwardProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getGroupByProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getLateralViewForwardProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getLateralViewJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getLimitProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getMultiGroupByProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getNewBucketCols(List<BucketCol>,List<ColumnInfo>)",4,4,4
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getNewSortCols(List<SortCol>,List<ColumnInfo>)",4,4,4
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getParent(Stack<Node>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.getSelProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.indexOfColName(List<? extends BucketSortCol>,String)",3,2,3
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.processForward(Operator<? extends OperatorDesc>,BucketingSortingCtx,Operator<? extends OperatorDesc>)",2,3,5
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.setBucketingColsIfComplete(BucketingSortingCtx,Operator<? extends OperatorDesc>,BucketCol[])",1,3,3
"org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.setSortingColsIfComplete(BucketingSortingCtx,Operator<? extends OperatorDesc>,SortCol[])",1,3,3
"org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.resolve(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.CommonJoinTaskDispatcher(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.calculateLocalTableTotalSize(MapredLocalWork)",4,2,4
"org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.cannotConvert(long,long,long)",2,1,3
"org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.checkOperatorOKMapJoinConversion(Operator<? extends OperatorDesc>)",5,2,5
"org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.convertTaskToMapJoinTask(MapredWork,int)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.getJoinOp(MapRedTask)",5,3,5
"org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.isLocalTableTotalSizeUnderLimitAfterMerge(Configuration,MapredLocalWork...)",4,2,4
"org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.mergeMapJoinTaskIntoItsChildMapRedTask(MapRedTask,Configuration)",14,21,30
"org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.multiInsertBigTableCheck(JoinOperator,Set<Integer>)",5,8,8
"org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.processCurrentTask(MapRedTask,ConditionalTask,Context)",11,9,23
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.ExtractReduceSinkInfo.ExtractReduceSinkInfo(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.ExtractReduceSinkInfo.Info.Info(List<ExprNodeDesc>,List<String>)",1,1,2
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.ExtractReduceSinkInfo.Info.Info(List<ExprNodeDesc>,String[])",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.ExtractReduceSinkInfo.analyze(BaseWork)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.ExtractReduceSinkInfo.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,3,4
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.MapJoinCheck.MapJoinCheck(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.MapJoinCheck.analyze(BaseWork)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.MapJoinCheck.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,4,7
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.NoopProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.checkForCrossProduct(String,Operator<? extends OperatorDesc>,Map<Integer, Info>)",2,3,4
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.checkMRReducer(String,MapredWork)",2,2,3
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.checkMapJoins(MapRedTask)",1,6,6
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.checkMapJoins(TezWork)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.checkTezReducer(TezWork)",3,4,5
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.dispatch(Node,Stack<Node>,Object...)",1,5,5
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.getReducerInfo(TezWork,String,String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.resolve(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.CrossProductCheck.warn(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.GenMRSkewJoinProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.getBigKeysDir(Path,Byte)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.getBigKeysSkewJoinResultDir(Path,Byte)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.getSmallKeysDir(Path,Byte,Byte)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.processSkewJoin(JoinOperator,Task<? extends Serializable>,ParseContext)",6,20,26
"org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.skewJoinEnabled(HiveConf,JoinOperator)",5,2,6
"org.apache.hadoop.hive.ql.optimizer.physical.IndexWhereResolver.resolve(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.LocalMapJoinProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.LocalMapJoinProcessor.hasAnyDirectFetch(List<Operator<?>>)",3,1,3
"org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.LocalMapJoinProcessor.hasGroupBy(Operator<? extends OperatorDesc>,LocalMapJoinProcCtx)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.LocalMapJoinProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",6,16,20
"org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.MapJoinFollowedByGroupByProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.getGroupByProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.getJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinProcCtx.LocalMapJoinProcCtx(Task<? extends Serializable>,ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinProcCtx.addDirectWorks(MapJoinOperator,List<Operator<? extends OperatorDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinProcCtx.addDummyParentOp(Operator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinProcCtx.getCurrentTask()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinProcCtx.getDirectWorks()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinProcCtx.getDummyParentOp()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinProcCtx.getParseCtx()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinProcCtx.isFollowedByGroupBy()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinProcCtx.setCurrentTask(Task<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinProcCtx.setDirectWorks(Map<MapJoinOperator, List<Operator<? extends OperatorDesc>>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinProcCtx.setDummyParentOp(List<Operator<? extends OperatorDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinProcCtx.setFollowedByGroupBy(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinProcCtx.setParseCtx(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinTaskDispatcher.LocalMapJoinTaskDispatcher(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinTaskDispatcher.adjustLocalTask(MapredLocalTask)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinTaskDispatcher.dispatch(Node,Stack<Node>,Object...)",1,5,5
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinTaskDispatcher.getPhysicalContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinTaskDispatcher.processCurrentTask(Task<? extends Serializable>,ConditionalTask)",3,13,13
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinTaskDispatcher.setPhysicalContext(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.resolve(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.FileSinkProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",5,5,6
"org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.TableScanProcessor.TableScanProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.TableScanProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,5,6
"org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.WalkerCtx.convertMetadataOnly()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.WalkerCtx.convertNotMetadataOnly()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.WalkerCtx.getMayBeMetadataOnlyTableScans()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.WalkerCtx.getMetadataOnlyTableScans()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.WalkerCtx.setMayBeMetadataOnly(TableScanOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.resolve(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.NullScanOptimizer.Limit0Processor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.physical.NullScanOptimizer.TSMarker.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.NullScanOptimizer.WhereFalseProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,3,5
"org.apache.hadoop.hive.ql.optimizer.physical.NullScanOptimizer.resolve(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.NullScanTaskDispatcher.NullScanTaskDispatcher(PhysicalContext,Map<Rule, NodeProcessor>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.NullScanTaskDispatcher.changePartitionToMetadataOnly(PartitionDesc)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.NullScanTaskDispatcher.dispatch(Node,Stack<Node>,Object...)",3,7,7
"org.apache.hadoop.hive.ql.optimizer.physical.NullScanTaskDispatcher.encode(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.NullScanTaskDispatcher.getAliasForTableScanOperator(MapWork,TableScanOperator)",3,3,3
"org.apache.hadoop.hive.ql.optimizer.physical.NullScanTaskDispatcher.getPathsForAlias(MapWork,String)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.physical.NullScanTaskDispatcher.processAlias(MapWork,String)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.PhysicalContext(HiveConf,ParseContext,Context,List<Task<? extends Serializable>>,Task<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.addToRootTask(Task<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.getConf()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.getContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.getFetchTask()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.getParseContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.getRootTasks()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.removeFromRootTask(Task<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.setConf(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.setContext(Context)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.setFetchTask(Task<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.setParseContext(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.setRootTasks(List<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.PhysicalOptimizer(PhysicalContext,HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.initialize(HiveConf)",1,12,12
"org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.optimize()",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.SamplingOptimizer.resolve(PhysicalContext)",6,9,13
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinProcFactory.SkewJoinDefaultProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinProcFactory.SkewJoinJoinProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinProcFactory.SkewJoinProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinProcFactory.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinProcFactory.getJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.SkewJoinProcCtx.SkewJoinProcCtx(Task<? extends Serializable>,ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.SkewJoinProcCtx.getCurrentTask()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.SkewJoinProcCtx.getParseCtx()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.SkewJoinProcCtx.setCurrentTask(Task<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.SkewJoinProcCtx.setParseCtx(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.SkewJoinTaskDispatcher.SkewJoinTaskDispatcher(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.SkewJoinTaskDispatcher.dispatch(Node,Stack<Node>,Object...)",2,4,5
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.SkewJoinTaskDispatcher.getPhysicalContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.SkewJoinTaskDispatcher.setPhysicalContext(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.resolve(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinResolver.resolve(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.SortMergeJoinTaskDispatcher(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.convertSMBTaskToMapJoinTask(MapredWork,int,SMBMapJoinOperator,QBJoinTree)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.convertSMBWorkToJoinWork(MapredWork,SMBMapJoinOperator)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.genSMBJoinWork(MapWork,SMBMapJoinOperator)",4,11,12
"org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.getMapJoinOperator(MapRedTask,MapredWork,SMBMapJoinOperator,QBJoinTree,int)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.getSMBMapJoinOp(MapredWork)",4,4,5
"org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.getSMBMapJoinOp(Operator<? extends OperatorDesc>,Operator<? extends OperatorDesc>)",7,4,9
"org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.isEligibleForOptimization(SMBMapJoinOperator)",8,5,9
"org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.processCurrentTask(MapRedTask,ConditionalTask,Context)",6,3,7
"org.apache.hadoop.hive.ql.optimizer.physical.SortMergeJoinTaskDispatcher.reducerAllowedSMBJoinOp(Operator<? extends OperatorDesc>)",5,3,6
"org.apache.hadoop.hive.ql.optimizer.physical.StageIDsRearranger.TaskTraverse.accepted(Task<?>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.StageIDsRearranger.TaskTraverse.getChildTasks(Task<?>)",2,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.StageIDsRearranger.TaskTraverse.isReady(Task<?>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.StageIDsRearranger.TaskTraverse.next(Task<?>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.StageIDsRearranger.TaskTraverse.rejected(Task<?>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.StageIDsRearranger.TaskTraverse.traverse(Task<?>)",1,6,6
"org.apache.hadoop.hive.ql.optimizer.physical.StageIDsRearranger.executionOrder(List<Task<?>>)",1,5,5
"org.apache.hadoop.hive.ql.optimizer.physical.StageIDsRearranger.getExplainOrder(HiveConf,List<Task<?>>)",2,3,3
"org.apache.hadoop.hive.ql.optimizer.physical.StageIDsRearranger.getExplainOrder(PhysicalContext)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.StageIDsRearranger.getFetchSources(List<Task<?>>)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.physical.StageIDsRearranger.resolve(PhysicalContext)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.StageIDsRearranger.traverseOrder(ArrangeType,List<Task<?>>)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.FakeGenericUDF.evaluate(DeferredObject[])",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.FakeGenericUDF.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.FakeGenericUDF.initialize(ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.prepareAbstractMapJoin(AbstractMapJoinOperator<? extends MapJoinDesc>,MapJoinDesc)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.setUp()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.testAggregateOnUDF()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.testValidateMapJoinOperator()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.testValidateNestedExpressions()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.TestVectorizer.testValidateSMBJoinOperator()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.MapWorkValidationNodeProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",4,5,6
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.MapWorkVectorizationNodeProcessor.MapWorkVectorizationNodeProcessor(MapWork)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.MapWorkVectorizationNodeProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",6,5,6
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.ReduceWorkValidationNodeProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,3,3
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.ReduceWorkVectorizationNodeProcessor.ReduceWorkVectorizationNodeProcessor(ReduceWork,int,int)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.ReduceWorkVectorizationNodeProcessor.getRootVectorOp()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.ReduceWorkVectorizationNodeProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,2,5
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.ValidatorVectorizationContext.ValidatorVectorizationContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.ValidatorVectorizationContext.getInputColumnIndex(ExprNodeColumnDesc)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.ValidatorVectorizationContext.getInputColumnIndex(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationDispatcher.VectorizationDispatcher(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationDispatcher.addMapWorkRules(Map<Rule, NodeProcessor>,NodeProcessor)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationDispatcher.addReduceWorkRules(Map<Rule, NodeProcessor>,NodeProcessor)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationDispatcher.convertMapWork(MapWork)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationDispatcher.convertReduceWork(ReduceWork)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationDispatcher.dispatch(Node,Stack<Node>,Object...)",1,6,6
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationDispatcher.getOnlyStructObjectInspectors(ReduceWork)",4,1,7
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationDispatcher.validateMapWork(MapWork)",6,5,6
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationDispatcher.validateReduceWork(ReduceWork)",5,3,5
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationDispatcher.vectorizeMapWork(MapWork)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationDispatcher.vectorizeReduceWork(ReduceWork)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationNodeProcessor.doVectorize(Operator<? extends OperatorDesc>,VectorizationContext)",1,4,5
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationNodeProcessor.getScratchColumnMap()",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationNodeProcessor.getScratchColumnVectorTypes()",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationNodeProcessor.nonVectorizableChildOfGroupBy(Operator<? extends OperatorDesc>)",3,4,4
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationNodeProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.VectorizationNodeProcessor.walkStackToFindVectorizationContext(Stack<Node>,Operator<? extends OperatorDesc>)",4,4,4
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.Vectorizer()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.fixupParentChildOperators(Operator<? extends OperatorDesc>,Operator<? extends OperatorDesc>)",1,5,5
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.getReduceVectorizationContext(Map<String, Integer>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.getVectorizationContext(Operator,PhysicalContext)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.isVirtualColumn(ColumnInfo)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.resolve(PhysicalContext)",2,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateAggregationDesc(AggregationDesc)",3,2,3
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateAggregationDesc(List<AggregationDesc>)",3,2,3
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateDataType(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(ExprNodeDesc,Mode)",3,3,5
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(List<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(List<ExprNodeDesc>,Mode)",3,2,3
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDescRecursive(ExprNodeDesc)",9,8,10
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExtractOperator(ExtractOperator)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateFileSinkOperator(FileSinkOperator)",2,1,2
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateFilterOperator(FilterOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateGenericUdf(ExprNodeGenericFuncDesc)",3,2,3
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateGroupByOperator(GroupByOperator)",3,2,3
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateMapJoinDesc(MapJoinDesc)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateMapJoinOperator(MapJoinOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateMapWorkOperator(Operator<? extends OperatorDesc>)",2,4,11
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateReduceSinkOperator(ReduceSinkOperator)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateReduceWorkOperator(Operator<? extends OperatorDesc>)",2,2,8
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateSMBMapJoinOperator(SMBMapJoinOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateSelectOperator(SelectOperator)",3,2,3
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateTableScanOperator(TableScanOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.vectorizeOperator(Operator<? extends OperatorDesc>,VectorizationContext)",2,3,4
"org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcCtx.IndexWhereProcCtx(Task<? extends Serializable>,ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcCtx.getCurrentTask()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcCtx.getParseContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcessor.IndexWhereProcessor(Map<TableScanOperator, List<Index>>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcessor.findLeaves(List<Task<?>>,Set<Task<?>>)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcessor.insertIndexQuery(ParseContext,IndexWhereProcCtx,List<Task<?>>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",5,10,14
"org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereProcessor.rewriteForIndexes(ExprNodeDesc,List<Index>,ParseContext,Task<MapredWork>,HiveIndexQueryContext)",2,3,4
"org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereTaskDispatcher.IndexWhereTaskDispatcher(PhysicalContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereTaskDispatcher.createOperatorRules(ParseContext)",2,3,4
"org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereTaskDispatcher.dispatch(Node,Stack<Node>,Object...)",3,2,3
"org.apache.hadoop.hive.ql.optimizer.physical.index.IndexWhereTaskDispatcher.getDefaultProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcCtx.ExprProcCtx(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcCtx.getTabAlias()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcCtx.setTabAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcFactory.ExprProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcFactory.PPRColumnExprProcessor.processColumnDesc(NodeProcessorCtx,ExprNodeColumnDesc)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcFactory.genPruner(String,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcFactory.getColumnProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.ExprPrunerInfo.getTabAlias()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.ExprPrunerInfo.setTabAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.OpProcFactory.FilterPPR.generatePredicate(NodeProcessorCtx,FilterOperator,TableScanOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.OpProcFactory.OpProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.OpProcFactory.getFilterProc()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.OpWalkerCtx.OpWalkerCtx(HashMap<TableScanOperator, ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.OpWalkerCtx.getOpToPartPruner()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.evalExprWithPart(ExprNodeDesc,Partition,List<VirtualColumn>,StructObjectInspector)",2,5,6
"org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.evaluateExprOnPart(ObjectPair<PrimitiveObjectInspector, ExprNodeEvaluator>,Object)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.prepareExpr(ExprNodeGenericFuncDesc,List<String>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore.convertExprToFilter(byte[])",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore.deserializeExpr(byte[])",2,2,3
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore.filterPartitionsByExpr(List<String>,byte[],String,List<String>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.compactExpr(ExprNodeDesc)",8,11,12
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.extractPartColNames(Table)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.getAllPartitions(Table)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.getPartitionsFromServer(Table,ExprNodeDesc,HiveConf,String)",5,9,12
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.hasColumnExpr(ExprNodeDesc)",6,3,6
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.hasUserFunctions(ExprNodeDesc)",5,2,5
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.onlyContainsPartnCols(Table,ExprNodeDesc)",7,6,9
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.prune(Table,ExprNodeDesc,HiveConf,String,Map<String, PrunedPartitionList>)",2,2,3
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.prune(TableScanOperator,ParseContext,String)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.pruneBySequentialScan(Table,List<Partition>,ExprNodeGenericFuncDesc,HiveConf)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.prunePartitionNames(List<String>,ExprNodeGenericFuncDesc,String,List<String>)",4,7,10
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.removeNonPartCols(ExprNodeDesc,List<String>,Set<String>)",3,5,5
"org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.transform(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateStatsProcCtx.AnnotateStatsProcCtx(ParseContext)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateStatsProcCtx.getAndExprStats()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateStatsProcCtx.getConf()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateStatsProcCtx.getParseContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateStatsProcCtx.setAndExprStats(Statistics)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateStatsProcCtx.setConf(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateStatsProcCtx.setParseContext(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.DefaultStatsRule.isAllParentsContainStatistics(Operator<? extends OperatorDesc>)",3,2,3
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.DefaultStatsRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,8,8
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.FilterStatsRule.evaluateChildExpr(Statistics,ExprNodeDesc,AnnotateStatsProcCtx,List<String>)",19,19,30
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.FilterStatsRule.evaluateColEqualsNullExpr(Statistics,ExprNodeDesc,AnnotateStatsProcCtx)",5,5,5
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.FilterStatsRule.evaluateExpression(Statistics,ExprNodeDesc,AnnotateStatsProcCtx,List<String>)",7,13,13
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.FilterStatsRule.evaluateNotExpr(Statistics,ExprNodeDesc,AnnotateStatsProcCtx,List<String>)",9,9,10
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.FilterStatsRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,9,9
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.GroupByStatsRule.applyGBYRule(long,long)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.GroupByStatsRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",5,20,22
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.JoinStatsRule.computeNewRowCount(List<Long>,long)",1,5,5
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.JoinStatsRule.getDenominator(List<Long>)",3,6,7
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.JoinStatsRule.getEasedOutDenominator(List<Long>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.JoinStatsRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,26,30
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.JoinStatsRule.updateJoinColumnsNDV(Map<Integer, List<String>>,Map<String, ColStatistics>,int)",1,8,8
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.JoinStatsRule.updateStatsForJoinType(Statistics,long,JoinDesc,Map<String, Long>,Map<String, String>)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.LimitStatsRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,8,8
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.ReduceSinkStatsRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,11,11
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.SelectStatsRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,9,9
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.TableScanStatsRule.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.getDefaultRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.getFilterRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.getGroupByRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.getJoinRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.getLimitRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.getReduceSinkRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.getSelectRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.getTableScanRule()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.satisfyPrecondition(Statistics)",1,3,3
"org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.updateStats(Statistics,long,boolean)",1,4,4
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.UnionParseContext.UnionParseContext(int)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.UnionParseContext.allMapOnlySubQ()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.UnionParseContext.allMapOnlySubQSet()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.UnionParseContext.allRootTasks()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.UnionParseContext.getMapOnlySubq(int)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.UnionParseContext.getNumInputs()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.UnionParseContext.getRootTask(int)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.UnionParseContext.isAllTrue(boolean[])",3,1,3
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.UnionParseContext.setMapOnlySubq(int,boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.UnionParseContext.setRootTask(int,boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.UnionProcContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.getParseContext()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.getUnionParseContext(UnionOperator)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.isMapOnlySubq()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.setMapOnlySubq(boolean)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.setParseContext(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.setUnionParseContext(UnionOperator,UnionParseContext)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.MapRedUnion.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.MapUnion.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.NoUnion.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.UnionNoProcessFile.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",7,4,8
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.UnionNoProcessFile.pushOperatorsAboveUnion(UnionOperator,Stack<Node>,int)",1,7,9
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.UnionProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.UnknownUnion.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,5,6
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.getMapRedUnion()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.getMapUnion()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.getNoUnion()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.getPositionParent(UnionOperator,Stack<Node>)",1,2,2
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.getUnionNoProcessFile()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.getUnknownUnion()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcessor.UnionProcessor()",1,1,1
"org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcessor.transform(ParseContext)",3,3,3
"org.apache.hadoop.hive.ql.parse.ASTErrorNode.ASTErrorNode(TokenStream,Token,Token,RecognitionException)",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTErrorNode.getText()",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTErrorNode.getType()",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTErrorNode.isNil()",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTErrorNode.toString()",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTNode.ASTNode()",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTNode.ASTNode(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTNode.ASTNode(Token)",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTNode.dump()",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTNode.dump(StringBuilder,String)",1,4,4
"org.apache.hadoop.hive.ql.parse.ASTNode.dupNode()",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTNode.getChildren()",2,2,3
"org.apache.hadoop.hive.ql.parse.ASTNode.getName()",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTNode.getOrigin()",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTNode.setOrigin(ASTNodeOrigin)",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTNodeOrigin.ASTNodeOrigin(String,String,String,String,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTNodeOrigin.getObjectDefinition()",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTNodeOrigin.getObjectName()",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTNodeOrigin.getObjectType()",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTNodeOrigin.getUsageAlias()",1,1,1
"org.apache.hadoop.hive.ql.parse.ASTNodeOrigin.getUsageNode()",1,1,1
"org.apache.hadoop.hive.ql.parse.AbstractSemanticAnalyzerHook.postAnalyze(HiveSemanticAnalyzerHookContext,List<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.AbstractSemanticAnalyzerHook.preAnalyze(HiveSemanticAnalyzerHookContext,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.AlterTablePartMergeFilesDesc(String,HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.getInputDir()",1,1,1
"org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.getInputFormatClass()",1,1,1
"org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.getLbCtx()",1,1,1
"org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.getOutputDir()",1,1,1
"org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.setInputDir(List<Path>)",1,1,1
"org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.setInputFormatClass(Class<? extends InputFormat>)",1,1,1
"org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.setLbCtx(ListBucketingCtx)",1,1,1
"org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.setOutputDir(Path)",1,1,1
"org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.setPartSpec(HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.BaseSemanticAnalyzer(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.BaseSemanticAnalyzer(HiveConf,Hive)",1,1,2
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.ErrorPartSpec(Map<String, String>,List<FieldSchema>)",1,3,3
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.RowFormatParams.analyzeRowFormat(ASTNode)",4,6,11
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(ASTNode,Context)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyzeDDLSkewedValues(List<List<String>>,ASTNode)",5,6,8
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyzeSkewedTablDDLColNames(List<String>,ASTNode)",3,3,3
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyzeStoredAdDirs(ASTNode)",1,2,3
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.charSetString(String,String)",2,3,5
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.constructListBucketingCtx(List<String>,List<List<String>>,Map<List<String>, String>,boolean,HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.createHiveDB(HiveConf)",1,1,2
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.extractPartitionSpecs(Tree)",1,2,2
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getColumnAccessInfo()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getColumnNames(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getColumnNamesOrder(ASTNode)",1,3,3
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getColumns(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getColumns(ASTNode,boolean)",1,5,5
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDatabase(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDatabase(String,boolean)",2,3,4
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDb()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDotName(String[])",2,1,2
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getFetchTask()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getIdToTableNameMap()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getInputs()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getLineageInfo()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getOutputs()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getPartExprNodeDesc(ASTNode,Map<ASTNode, ExprNodeDesc>)",3,8,9
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getPartition(Table,Map<String, String>,boolean)",2,3,4
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getPartitions(Table,Map<String, String>,boolean)",2,5,5
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getQualifiedTableName(ASTNode)",3,5,5
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getQueryProperties()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getResultSchema()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getRootTasks()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getSkewedValueFromASTNode(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getSkewedValuesFromASTNode(Node)",3,3,3
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getStructTypeStringFromAST(ASTNode)",2,3,4
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(String,String,boolean)",2,4,5
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(String[])",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(String[],boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTableAccessInfo()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTypeStringFromAST(ASTNode)",6,6,6
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getUnescapedName(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getUnescapedName(ASTNode,String)",4,3,4
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getUnescapedUnqualifiedTableName(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getUnionTypeStringFromAST(ASTNode)",2,3,4
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.init()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.initCtx(Context)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.isValidPrefixSpec(Table,Map<String, String>)",6,5,9
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.normalizeColSpec(Map<String, String>,String,String,String,Object)",2,3,4
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.normalizeDateCol(Object,String)",3,3,3
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.readProps(ASTNode,Map<String, String>)",1,3,3
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.reset()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.setColumnAccessInfo(ColumnAccessInfo)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.setFetchTask(FetchTask)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.setLineageInfo(LineageInfo)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.setTableAccessInfo(TableAccessInfo)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.skipAuthorization()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.stripIdentifierQuotes(String)",1,3,3
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.stripQuotes(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.tableSpec.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.tableSpec.setPartSpec(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.tableSpec.tableSpec(Hive,HiveConf,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.tableSpec.tableSpec(Hive,HiveConf,ASTNode,boolean,boolean)",10,26,26
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.tableSpec.tableSpec(Hive,HiveConf,String,Map<String, String>)",1,2,2
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.tableSpec.toString()",2,2,2
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.toMessage(ErrorMsg,Object)",1,2,2
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.unescapeIdentifier(String)",2,3,4
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.unescapeSQLString(String)",8,14,32
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.validate()",1,1,1
"org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.validatePartSpec(Table,Map<String, String>,ASTNode,HiveConf,boolean)",5,6,9
"org.apache.hadoop.hive.ql.parse.ColumnAccessAnalyzer.ColumnAccessAnalyzer()",1,1,1
"org.apache.hadoop.hive.ql.parse.ColumnAccessAnalyzer.ColumnAccessAnalyzer(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.parse.ColumnAccessAnalyzer.analyzeColumnAccess()",1,6,6
"org.apache.hadoop.hive.ql.parse.ColumnAccessInfo.ColumnAccessInfo()",1,1,1
"org.apache.hadoop.hive.ql.parse.ColumnAccessInfo.add(String,String)",1,2,2
"org.apache.hadoop.hive.ql.parse.ColumnAccessInfo.getTableToColumnAccessMap()",1,2,2
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.ColumnStatsSemanticAnalyzer(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.ColumnStatsSemanticAnalyzer(HiveConf,ASTNode)",1,4,5
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.analyze(ASTNode,Context)",1,2,2
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.checkForPartitionColumns(List<String>,List<String>)",4,4,4
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.genPartitionClause(Map<String, String>)",1,9,9
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.genRewrittenQuery(List<String>,int,Map<String, String>,boolean)",1,6,6
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.genRewrittenTree(String)",1,3,3
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.getColTypeOf(String)",3,3,3
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.getColumnName(ASTNode)",3,4,5
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.getColumnTypes(List<String>)",1,4,4
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.getNumBitVectorsForNDVEstimation(HiveConf)",2,12,12
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.getPartKeyValuePairsFromAST(ASTNode)",2,3,4
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.getTable(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.handlePartialPartitionSpec(Map<String, String>)",4,9,10
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.isPartitionLevelStats(ASTNode)",1,2,3
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.shouldRewrite(ASTNode)",1,4,5
"org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.validateSpecifiedColumnNames(List<String>)",3,3,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.DDLSemanticAnalyzer(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.DDLSemanticAnalyzer(HiveConf,Hive)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil.getAttemptTableName(Hive,String,boolean)",3,2,5
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil.getColPath(Hive,ASTNode,ASTNode,String,Map<String, String>)",4,4,5
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil.getDBName(Hive,ASTNode)",3,3,5
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil.getFullyQualifiedName(ASTNode)",4,4,4
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil.getPartitionSpec(Hive,ASTNode,String)",4,5,7
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.QualifiedNameUtil.getTableName(Hive,ASTNode)",5,4,5
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.TablePartition.TablePartition()",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.TablePartition.TablePartition(ASTNode)",1,3,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addAlterDbDesc(AlterDatabaseDesc)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(String,Map<String, String>,AlterTableDesc)",5,12,12
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addLocationToOutputs(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTableDropPartsOutputs(Table,Collection<List<ExprNodeGenericFuncDesc>>,boolean,boolean)",8,10,10
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(Table,List<Map<String, String>>,WriteType)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(Table,List<Map<String, String>>,boolean,WriteType)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(Table,List<Map<String, String>>,boolean,boolean,ASTNode,WriteType)",5,9,9
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterDatabaseOwner(ASTNode)",3,1,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterDatabaseProperties(ASTNode)",3,3,4
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterIndexProps(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterIndexRebuild(ASTNode)",1,3,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableAddParts(String[],CommonTree,boolean)",7,13,18
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableArchive(String[],CommonTree,boolean)",4,6,7
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableBucketNum(ASTNode,String,HashMap<String, String>)",2,3,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableClusterSort(ASTNode,String,HashMap<String, String>)",3,4,6
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableCompact(ASTNode,String,HashMap<String, String>)",2,3,4
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableDropParts(String[],ASTNode,boolean)",2,2,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableFileFormat(ASTNode,String,HashMap<String, String>)",2,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableLocation(ASTNode,String,HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableModifyCols(String[],ASTNode,AlterTableTypes)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTablePartColType(String[],ASTNode)",2,3,4
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTablePartMergeFiles(ASTNode,String,HashMap<String, String>)",9,8,16
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableProps(String[],ASTNode,boolean,boolean)",1,3,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableProtectMode(ASTNode,String,HashMap<String, String>)",4,4,9
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableRename(String[],ASTNode,boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableRenameCol(String[],ASTNode)",2,9,9
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableRenamePart(ASTNode,String,HashMap<String, String>)",2,1,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableSerde(ASTNode,String,HashMap<String, String>)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableSerdeProps(ASTNode,String,HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableSkewedColNames(List<String>,ASTNode)",3,3,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableSkewedLocation(ASTNode,String,HashMap<String, String>)",10,11,11
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableTouch(String[],CommonTree)",1,3,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableUpdateStats(ASTNode,String,Map<String, String>)",5,5,6
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAltertableSkewedby(String[],ASTNode)",2,3,5
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeCreateDatabase(ASTNode)",3,4,8
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeCreateIndex(ASTNode)",5,6,15
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeCreateRole(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeDescDatabase(ASTNode)",3,3,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeDescFunction(ASTNode)",3,3,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeDescribeTable(ASTNode)",1,5,6
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeDropDatabase(ASTNode)",3,4,8
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeDropIndex(ASTNode)",2,4,4
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeDropRole(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeDropTable(ASTNode,boolean)",1,3,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeExchangePartition(String[],ASTNode)",4,4,6
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeGrant(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeGrantRevokeRole(boolean,ASTNode)",1,3,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(ASTNode)",2,37,82
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeLockDatabase(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeLockTable(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeMetastoreCheck(CommonTree)",1,4,4
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeRevoke(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeSetShowRole(ASTNode)",2,2,4
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowColumns(ASTNode)",3,2,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowCompactions(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowConf(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowCreateTable(ASTNode)",2,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowDatabases(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowDbLocks(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowFunctions(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowGrant(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowIndexes(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowLocks(ASTNode)",1,6,7
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowPartitions(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowRoleGrant(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowRolePrincipals(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowRoles(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowTableProperties(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowTableStatus(ASTNode)",6,7,7
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowTables(ASTNode)",3,3,6
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeShowTxns(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeSwitchDatabase(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeTruncateTable(ASTNode)",17,26,29
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeUnlockDatabase(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeUnlockTable(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.calculatePartPrefix(Table,HashSet<String>)",3,2,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.createFetchTask(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.getColumnValues(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.getFullPartitionSpecs(CommonTree,Table,boolean)",6,7,10
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.getIndexBuilderMapRed(String[],String,HashMap<String, String>)",1,3,4
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.getPartSpec(ASTNode)",2,3,4
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.getPartitionForOutput(Table,Map<String, String>)",1,3,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.getPartitionSpecs(CommonTree)",1,3,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.getProps(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.getTypeName(ASTNode)",3,3,6
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.handleAlterTableDisableStoredAsDirs(String,Table)",2,5,5
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.handleAlterTableSkewedBy(ASTNode,String,Table)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.isConstant(ASTNode)",2,2,10
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.isFullSpec(Table,Map<String, String>)",3,2,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.isPartitionValueContinuous(List<FieldSchema>,Map<String, String>)",3,2,4
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.makeBinaryPredicate(String,ExprNodeDesc,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.preparePartitions(Table,HashMap<String, String>,Table,Hive,List<Partition>)",3,7,7
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.validateAlterTableType(Table,AlterTableTypes)",1,1,1
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.validateAlterTableType(Table,AlterTableTypes,boolean)",6,6,7
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.validateDatabase(String)",2,3,3
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.validatePartitionValues(Map<String, String>)",4,4,4
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.validateSkewedLocationString(String)",2,3,5
"org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.validateTable(String,Map<String, String>)",1,2,2
"org.apache.hadoop.hive.ql.parse.EximUtil.EximUtil()",1,1,1
"org.apache.hadoop.hive.ql.parse.EximUtil.checkCompatibility(String,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.EximUtil.createExportDump(FileSystem,Path,Table,List<Partition>)",1,6,6
"org.apache.hadoop.hive.ql.parse.EximUtil.doCheckCompatibility(String,String,String)",6,7,9
"org.apache.hadoop.hive.ql.parse.EximUtil.getValidatedURI(HiveConf,String)",2,7,9
"org.apache.hadoop.hive.ql.parse.EximUtil.makePartSpec(List<FieldSchema>,List<String>)",1,2,2
"org.apache.hadoop.hive.ql.parse.EximUtil.readMetaData(FileSystem,Path)",1,6,7
"org.apache.hadoop.hive.ql.parse.EximUtil.relativeToAbsolutePath(HiveConf,String)",2,4,5
"org.apache.hadoop.hive.ql.parse.EximUtil.schemaCompare(List<FieldSchema>,List<FieldSchema>)",5,4,6
"org.apache.hadoop.hive.ql.parse.EximUtil.validateTable(Table)",4,4,4
"org.apache.hadoop.hive.ql.parse.ExplainSQRewriteSemanticAnalyzer.ExplainSQRewriteSemanticAnalyzer(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.parse.ExplainSQRewriteSemanticAnalyzer.analyzeInternal(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.ExplainSQRewriteSemanticAnalyzer.getResultSchema()",1,1,1
"org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.ExplainSemanticAnalyzer(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ASTNode)",1,5,10
"org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.getResultSchema()",1,1,1
"org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.skipAuthorization()",1,3,3
"org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.ExportSemanticAnalyzer(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.analyzeInternal(ASTNode)",3,9,10
"org.apache.hadoop.hive.ql.parse.FileSinkProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.FunctionSemanticAnalyzer(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.addEntities(String,boolean)",2,4,4
"org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.analyzeCreateFunction(ASTNode)",2,2,3
"org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.analyzeDropFunction(ASTNode)",3,4,4
"org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.analyzeInternal(ASTNode)",1,3,3
"org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.getResourceList(ASTNode)",5,5,5
"org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.getResourceType(ASTNode)",5,2,5
"org.apache.hadoop.hive.ql.parse.GenMapRedWalker.GenMapRedWalker(Dispatcher)",1,1,1
"org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(Node)",2,3,4
"org.apache.hadoop.hive.ql.parse.GenTezProcContext.GenTezProcContext(HiveConf,ParseContext,List<Task<MoveWork>>,List<Task<? extends Serializable>>,Set<ReadEntity>,Set<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.parse.GenTezUtils.GenTezUtils()",1,1,1
"org.apache.hadoop.hive.ql.parse.GenTezUtils.createMapWork(GenTezProcContext,Operator<?>,TezWork,PrunedPartitionList)",1,1,1
"org.apache.hadoop.hive.ql.parse.GenTezUtils.createReduceWork(GenTezProcContext,Operator<?>,TezWork)",1,4,6
"org.apache.hadoop.hive.ql.parse.GenTezUtils.createUnionWork(GenTezProcContext,Operator<?>,TezWork)",1,1,1
"org.apache.hadoop.hive.ql.parse.GenTezUtils.getUtils()",1,1,2
"org.apache.hadoop.hive.ql.parse.GenTezUtils.processFileSink(GenTezProcContext,FileSinkOperator)",1,5,5
"org.apache.hadoop.hive.ql.parse.GenTezUtils.removeUnionOperators(Configuration,GenTezProcContext,BaseWork)",1,11,13
"org.apache.hadoop.hive.ql.parse.GenTezUtils.resetSequenceNumber()",1,1,1
"org.apache.hadoop.hive.ql.parse.GenTezUtils.setupMapWork(MapWork,GenTezProcContext,PrunedPartitionList,Operator<? extends OperatorDesc>,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.GenTezUtils.setupReduceSink(GenTezProcContext,ReduceWork,ReduceSinkOperator)",1,1,1
"org.apache.hadoop.hive.ql.parse.GenTezWork.GenTezWork(GenTezUtils)",1,1,1
"org.apache.hadoop.hive.ql.parse.GenTezWork.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,21,25
"org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.GenTezWorkWalker(Dispatcher,GenTezProcContext)",1,1,1
"org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.setRoot(Node)",1,1,1
"org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.startWalking(Collection<Node>,HashMap<Node, Object>)",1,3,3
"org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(Node)",1,3,4
"org.apache.hadoop.hive.ql.parse.GlobalLimitCtx.disableOpt()",1,1,1
"org.apache.hadoop.hive.ql.parse.GlobalLimitCtx.enableOpt(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.GlobalLimitCtx.getGlobalLimit()",1,1,1
"org.apache.hadoop.hive.ql.parse.GlobalLimitCtx.getLastReduceLimitDesc()",1,1,1
"org.apache.hadoop.hive.ql.parse.GlobalLimitCtx.ifHasTransformOrUDTF()",1,1,1
"org.apache.hadoop.hive.ql.parse.GlobalLimitCtx.isEnable()",1,1,1
"org.apache.hadoop.hive.ql.parse.GlobalLimitCtx.setHasTransformOrUDTF(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.GlobalLimitCtx.setLastReduceLimitDesc(LimitDesc)",1,1,1
"org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContextImpl.getConf()",1,1,1
"org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContextImpl.getHive()",1,1,1
"org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContextImpl.getInputs()",1,1,1
"org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContextImpl.getOutputs()",1,1,1
"org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContextImpl.getUserName()",1,1,1
"org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContextImpl.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContextImpl.setUserName(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContextImpl.update(BaseSemanticAnalyzer)",1,1,1
"org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.ImportSemanticAnalyzer(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.addSinglePartition(URI,FileSystem,CreateTableDesc,Table,Warehouse,AddPartitionDesc)",2,5,5
"org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.analyzeInternal(ASTNode)",11,28,31
"org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.checkParams(Map<String, String>,Map<String, String>,String[])",10,7,10
"org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.checkTable(Table,CreateTableDesc)",22,23,25
"org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.checkTargetLocationEmpty(FileSystem,Path)",3,3,3
"org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.existsTable()",1,1,1
"org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.loadTable(URI,Table)",1,1,1
"org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.partSpecToString(Map<String, String>)",1,3,3
"org.apache.hadoop.hive.ql.parse.IndexUpdater.IndexUpdater(List<LoadTableDesc>,Set<ReadEntity>,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.parse.IndexUpdater.IndexUpdater(LoadTableDesc,Set<ReadEntity>,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.parse.IndexUpdater.containsPartition(Index,Map<String, String>)",1,2,2
"org.apache.hadoop.hive.ql.parse.IndexUpdater.doIndexUpdate(Index,Map<String, String>)",1,3,3
"org.apache.hadoop.hive.ql.parse.IndexUpdater.doIndexUpdate(List<Index>)",1,2,2
"org.apache.hadoop.hive.ql.parse.IndexUpdater.doIndexUpdate(List<Index>,Map<String, String>)",1,3,3
"org.apache.hadoop.hive.ql.parse.IndexUpdater.generateUpdateTasks()",1,4,4
"org.apache.hadoop.hive.ql.parse.InputSignature.InputSignature(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.InputSignature.InputSignature(String,Class<?>...)",1,3,3
"org.apache.hadoop.hive.ql.parse.InputSignature.InputSignature(String,TypeInfo...)",1,3,3
"org.apache.hadoop.hive.ql.parse.InputSignature.add(TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.parse.InputSignature.equals(Object)",2,2,4
"org.apache.hadoop.hive.ql.parse.InputSignature.getName()",1,1,1
"org.apache.hadoop.hive.ql.parse.InputSignature.getTypeArray()",1,1,1
"org.apache.hadoop.hive.ql.parse.InputSignature.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.parse.InputSignature.toString()",1,3,3
"org.apache.hadoop.hive.ql.parse.JoinCond.JoinCond()",1,1,1
"org.apache.hadoop.hive.ql.parse.JoinCond.JoinCond(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.JoinCond.JoinCond(int,int,JoinType)",1,1,1
"org.apache.hadoop.hive.ql.parse.JoinCond.getJoinType()",1,1,1
"org.apache.hadoop.hive.ql.parse.JoinCond.getLeft()",1,1,1
"org.apache.hadoop.hive.ql.parse.JoinCond.getPreserved()",1,1,1
"org.apache.hadoop.hive.ql.parse.JoinCond.getRight()",1,1,1
"org.apache.hadoop.hive.ql.parse.JoinCond.setJoinType(JoinType)",1,1,1
"org.apache.hadoop.hive.ql.parse.JoinCond.setLeft(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.JoinCond.setRight(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.LeadLagInfo.addLLFuncExprForTopExpr(ExprNodeDesc,ExprNodeGenericFuncDesc)",1,2,3
"org.apache.hadoop.hive.ql.parse.LeadLagInfo.addLeadLagExpr(ExprNodeGenericFuncDesc)",1,1,2
"org.apache.hadoop.hive.ql.parse.LeadLagInfo.getLLFuncExprsInTopExpr(ExprNodeDesc)",2,1,2
"org.apache.hadoop.hive.ql.parse.LeadLagInfo.getLeadLagExprs()",1,1,1
"org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.LoadSemanticAnalyzer(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(ASTNode)",9,23,29
"org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.applyConstraints(URI,URI,Tree,boolean)",6,10,11
"org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.initializeFromURI(String)",1,7,7
"org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.matchFilesOrDir(FileSystem,Path)",1,6,7
"org.apache.hadoop.hive.ql.parse.MacroSemanticAnalyzer.MacroSemanticAnalyzer(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.parse.MacroSemanticAnalyzer.addEntities()",1,1,1
"org.apache.hadoop.hive.ql.parse.MacroSemanticAnalyzer.analyzeCreateMacro(ASTNode)",4,6,9
"org.apache.hadoop.hive.ql.parse.MacroSemanticAnalyzer.analyzeDropMacro(ASTNode)",3,4,5
"org.apache.hadoop.hive.ql.parse.MacroSemanticAnalyzer.analyzeInternal(ASTNode)",1,3,3
"org.apache.hadoop.hive.ql.parse.MapReduceCompiler.MapReduceCompiler()",1,1,1
"org.apache.hadoop.hive.ql.parse.MapReduceCompiler.breakOperatorTree(Operator<? extends OperatorDesc>)",2,3,4
"org.apache.hadoop.hive.ql.parse.MapReduceCompiler.breakTaskTree(Task<? extends Serializable>)",2,7,8
"org.apache.hadoop.hive.ql.parse.MapReduceCompiler.decideExecMode(List<Task<? extends Serializable>>,Context,GlobalLimitCtx)",5,8,11
"org.apache.hadoop.hive.ql.parse.MapReduceCompiler.generateTaskTree(List<Task<? extends Serializable>>,ParseContext,List<Task<MoveWork>>,Set<ReadEntity>,Set<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.parse.MapReduceCompiler.getNumberOfReducers(MapredWork,HiveConf)",3,2,3
"org.apache.hadoop.hive.ql.parse.MapReduceCompiler.optimizeTaskPlan(List<Task<? extends Serializable>>,ParseContext,Context)",1,2,2
"org.apache.hadoop.hive.ql.parse.MapReduceCompiler.setInputFormat(MapWork,Operator<? extends OperatorDesc>)",2,4,4
"org.apache.hadoop.hive.ql.parse.MapReduceCompiler.setInputFormat(Task<? extends Serializable>)",1,8,8
"org.apache.hadoop.hive.ql.parse.MetaDataExportListener.MetaDataExportListener(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.parse.MetaDataExportListener.export_meta_data(PreDropTableEvent)",1,7,7
"org.apache.hadoop.hive.ql.parse.MetaDataExportListener.onEvent(PreEventContext)",1,2,2
"org.apache.hadoop.hive.ql.parse.OpParseContext.OpParseContext()",1,1,1
"org.apache.hadoop.hive.ql.parse.OpParseContext.OpParseContext(RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.parse.OpParseContext.getRowResolver()",1,1,1
"org.apache.hadoop.hive.ql.parse.OpParseContext.setRowResolver(RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext.OptimizeTezProcContext(HiveConf,ParseContext,Set<ReadEntity>,Set<WriteEntity>,Deque<Operator<?>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderExpression.OrderExpression()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderExpression.OrderExpression(PartitionExpression)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderExpression.equals(Object)",5,1,5
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderExpression.getOrder()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderExpression.hashCode()",1,2,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderExpression.setOrder(Order)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderExpression.toString()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderSpec.OrderSpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderSpec.OrderSpec(PartitionSpec)",1,2,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderSpec.addExpression(OrderExpression)",1,1,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderSpec.equals(Object)",7,2,7
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderSpec.getExpressions()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderSpec.hashCode()",1,2,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderSpec.isPrefixedBy(PartitionSpec)",5,4,7
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderSpec.prefixBy(PartitionSpec)",2,3,5
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderSpec.setExpressions(ArrayList<OrderExpression>)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.OrderSpec.toString()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PTFInputSpec.getAstNode()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PTFInputSpec.setAstNode(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PTFQueryInputSpec.getInput()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PTFQueryInputSpec.getQueryInput()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PTFQueryInputSpec.getQueryInputName()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PTFQueryInputSpec.getSource()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PTFQueryInputSpec.getType()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PTFQueryInputSpec.setSource(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PTFQueryInputSpec.setType(PTFQueryInputType)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionExpression.PartitionExpression()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionExpression.PartitionExpression(PartitionExpression)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionExpression.equals(Object)",7,2,7
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionExpression.getExpression()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionExpression.hashCode()",1,2,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionExpression.setExpression(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionExpression.toString()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionSpec.addExpression(PartitionExpression)",1,1,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionSpec.equals(Object)",7,2,7
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionSpec.getExpressions()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionSpec.hashCode()",1,2,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionSpec.setExpressions(ArrayList<PartitionExpression>)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionSpec.toString()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.addArg(ASTNode)",1,1,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.getArgs()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.getInput()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.getName()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.getOrder()",1,2,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.getPartition()",1,2,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.getPartitioning()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.getQueryInput()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.getQueryInputName()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.getStartOfChain()",2,2,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.setAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.setArgs(List<ASTNode>)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.setInput(PTFInputSpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.setName(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.setOrder(OrderSpec)",1,1,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.setPartition(PartitionSpec)",1,1,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitionedTableFunctionSpec.setPartitioning(PartitioningSpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitioningSpec.equals(Object)",10,3,10
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitioningSpec.getOrderSpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitioningSpec.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitioningSpec.hashCode()",1,3,3
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitioningSpec.setOrderSpec(OrderSpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.PartitioningSpec.setPartSpec(PartitionSpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.getFunction()",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.getQueryInput()",1,2,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.getQueryInputName()",1,2,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.getStartOfChain()",1,2,2
"org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.setFunction(PartitionedTableFunctionSpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFTranslator.ValidateNoLeadLag.ValidateNoLeadLag(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFTranslator.ValidateNoLeadLag.checkValid()",2,2,2
"org.apache.hadoop.hive.ql.parse.PTFTranslator.ValidateNoLeadLag.visit(Object,Object,int,Map)",1,2,3
"org.apache.hadoop.hive.ql.parse.PTFTranslator._visit(Object,Object,int,ContextVisitor)",2,2,3
"org.apache.hadoop.hive.ql.parse.PTFTranslator.addInputColumnsToList(ShapeDetails,ArrayList<String>,ArrayList<ObjectInspector>)",1,2,2
"org.apache.hadoop.hive.ql.parse.PTFTranslator.addPartitionExpressionsToOrderList(ArrayList<PartitionExpression>,ArrayList<OrderExpression>)",4,10,11
"org.apache.hadoop.hive.ql.parse.PTFTranslator.applyConstantPartition(PartitionedTableFunctionSpec)",2,2,3
"org.apache.hadoop.hive.ql.parse.PTFTranslator.buildExpressionDef(ShapeDetails,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFTranslator.buildRowResolverForNoop(String,StructObjectInspector,RowResolver)",1,9,11
"org.apache.hadoop.hive.ql.parse.PTFTranslator.buildRowResolverForPTF(String,String,StructObjectInspector,List<String>,RowResolver)",2,3,3
"org.apache.hadoop.hive.ql.parse.PTFTranslator.buildRowResolverForWindowing(WindowTableFunctionDef)",1,6,7
"org.apache.hadoop.hive.ql.parse.PTFTranslator.componentize(PTFInvocationSpec)",4,11,12
"org.apache.hadoop.hive.ql.parse.PTFTranslator.copyShape(ShapeDetails)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFTranslator.createLazyBinarySerDe(Configuration,StructObjectInspector,Map<String, String>)",1,1,2
"org.apache.hadoop.hive.ql.parse.PTFTranslator.getASTNode(ColumnInfo,RowResolver)",3,2,3
"org.apache.hadoop.hive.ql.parse.PTFTranslator.getStandardStructOI(RowResolver)",1,2,2
"org.apache.hadoop.hive.ql.parse.PTFTranslator.getTypeMap(StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFTranslator.init(SemanticAnalyzer,HiveConf,RowResolver,UnparseTranslator)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFTranslator.initExprNodeEvaluator(ExprNodeEvaluator,ExprNodeDesc,ShapeDetails)",1,3,3
"org.apache.hadoop.hive.ql.parse.PTFTranslator.setupRankingArgs(WindowTableFunctionDef,WindowFunctionDef,WindowFunctionSpec)",2,2,3
"org.apache.hadoop.hive.ql.parse.PTFTranslator.setupShape(StructObjectInspector,List<String>,RowResolver)",1,1,2
"org.apache.hadoop.hive.ql.parse.PTFTranslator.setupShapeForNoop(ShapeDetails,StructObjectInspector,List<String>,RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFTranslator.setupTableFnShape(String,ShapeDetails,StructObjectInspector,List<String>,RowResolver)",2,2,2
"org.apache.hadoop.hive.ql.parse.PTFTranslator.setupWdwFnEvaluator(WindowFunctionDef)",1,3,3
"org.apache.hadoop.hive.ql.parse.PTFTranslator.translate(PTFInvocationSpec,SemanticAnalyzer,HiveConf,RowResolver,UnparseTranslator)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFTranslator.translate(PTFQueryInputSpec,int)",1,2,2
"org.apache.hadoop.hive.ql.parse.PTFTranslator.translate(PartitionedTableFunctionSpec,PTFInputDef,int)",4,6,7
"org.apache.hadoop.hive.ql.parse.PTFTranslator.translate(ShapeDetails,BoundarySpec)",4,3,5
"org.apache.hadoop.hive.ql.parse.PTFTranslator.translate(ShapeDetails,OrderExpression)",1,1,2
"org.apache.hadoop.hive.ql.parse.PTFTranslator.translate(ShapeDetails,OrderSpec,PartitionDef)",2,2,3
"org.apache.hadoop.hive.ql.parse.PTFTranslator.translate(ShapeDetails,PartitionExpression)",1,1,2
"org.apache.hadoop.hive.ql.parse.PTFTranslator.translate(ShapeDetails,PartitionSpec)",2,4,5
"org.apache.hadoop.hive.ql.parse.PTFTranslator.translate(ShapeDetails,WindowFrameSpec)",3,2,3
"org.apache.hadoop.hive.ql.parse.PTFTranslator.translate(String,ShapeDetails,WindowSpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFTranslator.translate(WindowTableFunctionDef,WindowFunctionSpec)",8,9,11
"org.apache.hadoop.hive.ql.parse.PTFTranslator.translate(WindowingSpec,SemanticAnalyzer,HiveConf,RowResolver,UnparseTranslator)",3,7,8
"org.apache.hadoop.hive.ql.parse.PTFTranslator.translatePTFChain()",1,4,4
"org.apache.hadoop.hive.ql.parse.PTFTranslator.translatePartitioning(PartitionedTableFunctionDef,PartitionedTableFunctionSpec)",2,1,2
"org.apache.hadoop.hive.ql.parse.PTFTranslator.validateComparable(ObjectInspector,String)",2,1,2
"org.apache.hadoop.hive.ql.parse.PTFTranslator.validateNoLeadLagInValueBoundarySpec(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.PTFTranslator.validateValueBoundaryExprType(ObjectInspector)",3,3,4
"org.apache.hadoop.hive.ql.parse.PTFTranslator.visit(Object,ContextVisitor)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.ParseContext()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.ParseContext(HiveConf,QB,ASTNode,HashMap<TableScanOperator, ExprNodeDesc>,HashMap<TableScanOperator, PrunedPartitionList>,HashMap<String, Operator<? extends OperatorDesc>>,HashMap<String, Operator<? extends OperatorDesc>>,LinkedHashMap<Operator<? extends OperatorDesc>, OpParseContext>,Map<JoinOperator, QBJoinTree>,Map<SMBMapJoinOperator, QBJoinTree>,HashMap<TableScanOperator, Table>,HashMap<TableScanOperator, Map<String, String>>,Map<FileSinkOperator, Table>,List<LoadTableDesc>,List<LoadFileDesc>,Context,HashMap<String, String>,int,UnionProcContext,List<AbstractMapJoinOperator<? extends MapJoinDesc>>,Map<GroupByOperator, Set<String>>,Map<String, PrunedPartitionList>,HashMap<TableScanOperator, sampleDesc>,GlobalLimitCtx,HashMap<String, SplitSample>,HashSet<ReadEntity>,List<Task<? extends Serializable>>,Map<TableScanOperator, Map<String, ExprNodeDesc>>,Map<String, ReadEntity>,List<ReduceSinkOperator>,QueryProperties)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getConf()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getContext()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getDestTableId()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getFetchSink()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getFetchSource()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getFetchTabledesc()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getFetchTask()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getFsopToTable()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getGlobalLimitCtx()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getGroupOpToInputTables()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getIdToTableNameMap()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getJoinContext()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getLineageInfo()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getListMapJoinOpsNoReducer()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getLoadFileWork()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getLoadTableWork()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getMapJoinContext()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getNameToSplitSample()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getOpParseCtx()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getOpToPartList()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getOpToPartPruner()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getOpToPartToSkewedPruner()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getOpToSamplePruner()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getParseTree()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getPrunedPartitions()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getPrunedPartitions(String,TableScanOperator)",1,2,2
"org.apache.hadoop.hive.ql.parse.ParseContext.getQB()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getQueryProperties()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getReduceSinkOperatorsAddedByEnforceBucketingSorting()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getSemanticInputs()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getSmbMapJoinContext()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getTopOps()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getTopSelOps()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getTopToProps()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getTopToTable()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getUCtx()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.getViewAliasToInput()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.removeOpParseCtx(Operator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.replaceRootTask(Task<? extends Serializable>,List<? extends Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setConf(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setContext(Context)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setDestTableId(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setFetchSink(ListSinkOperator)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setFetchSource(Operator<?>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setFetchTabledesc(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setFetchTask(FetchTask)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setFsopToTable(Map<FileSinkOperator, Table>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setGlobalLimitCtx(GlobalLimitCtx)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setGroupOpToInputTables(Map<GroupByOperator, Set<String>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setIdToTableNameMap(HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setJoinContext(Map<JoinOperator, QBJoinTree>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setLineageInfo(LineageInfo)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setListMapJoinOpsNoReducer(List<AbstractMapJoinOperator<? extends MapJoinDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setLoadFileWork(List<LoadFileDesc>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setLoadTableWork(List<LoadTableDesc>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setMapJoinContext(Map<MapJoinOperator, QBJoinTree>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setNameToSplitSample(HashMap<String, SplitSample>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setOpParseCtx(LinkedHashMap<Operator<? extends OperatorDesc>, OpParseContext>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setOpPartToSkewedPruner(HashMap<TableScanOperator, Map<String, ExprNodeDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setOpToPartPruner(HashMap<TableScanOperator, ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setOpToSamplePruner(HashMap<TableScanOperator, sampleDesc>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setParseTree(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setPrunedPartitions(Map<String, PrunedPartitionList>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setQB(QB)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setQueryProperties(QueryProperties)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setReduceSinkOperatorsAddedByEnforceBucketingSorting(List<ReduceSinkOperator>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setSmbMapJoinContext(Map<SMBMapJoinOperator, QBJoinTree>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setTopOps(HashMap<String, Operator<? extends OperatorDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setTopSelOps(HashMap<String, Operator<? extends OperatorDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setTopToProps(HashMap<TableScanOperator, Map<String, String>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setTopToTable(HashMap<TableScanOperator, Table>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.setUCtx(UnionProcContext)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseContext.updateOpParseCtx(Operator<? extends OperatorDesc>,OpParseContext)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseDriver.ANTLRNoCaseStringStream.ANTLRNoCaseStringStream(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseDriver.ANTLRNoCaseStringStream.LA(int)",3,1,3
"org.apache.hadoop.hive.ql.parse.ParseDriver.HiveLexerX.HiveLexerX()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseDriver.HiveLexerX.HiveLexerX(CharStream)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseDriver.HiveLexerX.displayRecognitionError(String[],RecognitionException)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseDriver.HiveLexerX.getErrorMessage(RecognitionException,String[])",1,2,2
"org.apache.hadoop.hive.ql.parse.ParseDriver.HiveLexerX.getErrors()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseDriver.parse(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseDriver.parse(String,Context)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseDriver.parse(String,Context,boolean)",3,7,7
"org.apache.hadoop.hive.ql.parse.ParseDriver.parseSelect(String,Context)",3,6,6
"org.apache.hadoop.hive.ql.parse.ParseError.ParseError(BaseRecognizer,RecognitionException,String[])",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseError.getBaseRecognizer()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseError.getMessage()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseError.getRecognitionException()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseError.getTokenNames()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseException.ParseException(ArrayList<ParseError>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseException.getMessage()",1,3,3
"org.apache.hadoop.hive.ql.parse.ParseUtils.ParseUtils()",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseUtils.checkJoinFilterRefersOneAlias(String[],ASTNode)",6,4,10
"org.apache.hadoop.hive.ql.parse.ParseUtils.createConversionCast(ExprNodeDesc,PrimitiveTypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.parse.ParseUtils.ensureClassExists(String)",2,1,3
"org.apache.hadoop.hive.ql.parse.ParseUtils.findRootNonNullToken(ASTNode)",1,3,3
"org.apache.hadoop.hive.ql.parse.ParseUtils.getCharTypeInfo(ASTNode)",2,1,2
"org.apache.hadoop.hive.ql.parse.ParseUtils.getDecimalTypeTypeInfo(ASTNode)",2,3,4
"org.apache.hadoop.hive.ql.parse.ParseUtils.getIndex(String[],String)",3,2,3
"org.apache.hadoop.hive.ql.parse.ParseUtils.getVarcharTypeInfo(ASTNode)",2,1,2
"org.apache.hadoop.hive.ql.parse.ParseUtils.isJoinToken(ASTNode)",3,2,3
"org.apache.hadoop.hive.ql.parse.ParseUtils.validateColumnNameUniqueness(List<FieldSchema>)",4,4,4
"org.apache.hadoop.hive.ql.parse.PrintOpTreeProcessor.PrintOpTreeProcessor()",1,1,1
"org.apache.hadoop.hive.ql.parse.PrintOpTreeProcessor.PrintOpTreeProcessor(PrintStream)",1,1,1
"org.apache.hadoop.hive.ql.parse.PrintOpTreeProcessor.getChildren(Operator<? extends OperatorDesc>)",1,4,4
"org.apache.hadoop.hive.ql.parse.PrintOpTreeProcessor.getParents(Operator<? extends OperatorDesc>)",1,4,4
"org.apache.hadoop.hive.ql.parse.PrintOpTreeProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,2,3
"org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.ProcessAnalyzeTable(GenTezUtils)",1,1,1
"org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.handlePartialScanCommand(TableScanOperator,ParseContext,QBParseInfo,StatsWork,GenTezProcContext,Task<StatsWork>)",1,1,1
"org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,9,11
"org.apache.hadoop.hive.ql.parse.PrunedPartitionList.PrunedPartitionList(Table,Set<Partition>,List<String>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.PrunedPartitionList.getNotDeniedPartns()",1,1,1
"org.apache.hadoop.hive.ql.parse.PrunedPartitionList.getPartitions()",1,1,1
"org.apache.hadoop.hive.ql.parse.PrunedPartitionList.getReferredPartCols()",1,1,1
"org.apache.hadoop.hive.ql.parse.PrunedPartitionList.getSourceTable()",1,1,1
"org.apache.hadoop.hive.ql.parse.PrunedPartitionList.hasUnknownPartitions()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.QB()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.QB(String,String,boolean)",1,2,2
"org.apache.hadoop.hive.ql.parse.QB.addAlias(String)",1,2,2
"org.apache.hadoop.hive.ql.parse.QB.addDestToWindowingSpec(String,WindowingSpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.addPTFNodeToSpec(ASTNode,PTFInvocationSpec)",1,1,2
"org.apache.hadoop.hive.ql.parse.QB.countSel()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.countSelDi()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.exists(String)",2,2,3
"org.apache.hadoop.hive.ql.parse.QB.getAliases()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getAllWindowingSpecs()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getAppendedAliasFromId(String,String)",1,1,2
"org.apache.hadoop.hive.ql.parse.QB.getHavingClauseSubQueryPredicate()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getId()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getIsQuery()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getLLocalDirectoryDesc()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getMetaData()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getNumGbys()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getNumJoins()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getNumSelDi()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getNumSels()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getNumSubQueryPredicates()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getPTFInvocationSpec(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.QB.getPTFNodeToSpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getParseInfo()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getQbJoinTree()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getSkewedColumnNames(String)",1,4,4
"org.apache.hadoop.hive.ql.parse.QB.getSubQueryPredicateDef()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getSubqAliases()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getSubqForAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getTabAliases()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getTabNameForAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getTabPropsForAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getWhereClauseSubQueryPredicate()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.getWindowingSpec(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.hasTableSample(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.hasWindowingSpec(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.incrNumSubQueryPredicates()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.isAnalyzeRewrite()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.isCTAS()",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.isSimpleSelectQuery()",1,4,4
"org.apache.hadoop.hive.ql.parse.QB.print(String)",1,2,2
"org.apache.hadoop.hive.ql.parse.QB.rewriteCTEToSubq(String,String,QBExpr)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.rewriteViewToSubq(String,String,QBExpr)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.setAnalyzeRewrite(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.setHavingClauseSubQueryPredicate(QBSubQuery)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.setIsQuery(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.setLocalDirectoryDesc(CreateTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.setQBParseInfo(QBParseInfo)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.setQbJoinTree(QBJoinTree)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.setSubQueryDef(QBSubQuery)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.setSubqAlias(String,QBExpr)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.setTabAlias(String,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.setTabProps(String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.setTableDesc(CreateTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.parse.QB.setWhereClauseSubQueryPredicate(QBSubQuery)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBExpr.QBExpr(Opcode,QBExpr,QBExpr)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBExpr.QBExpr(QB)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBExpr.QBExpr(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBExpr.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBExpr.getOpcode()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBExpr.getQB()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBExpr.getQBExpr1()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBExpr.getQBExpr2()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBExpr.print(String)",1,2,2
"org.apache.hadoop.hive.ql.parse.QBExpr.setAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBExpr.setOpcode(Opcode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBExpr.setQB(QB)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBExpr.setQBExpr1(QBExpr)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBExpr.setQBExpr2(QBExpr)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.QBJoinTree()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.addFilterMapping(int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.addPostJoinFilter(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.addRHSSemijoin(String)",1,2,2
"org.apache.hadoop.hive.ql.parse.QBJoinTree.addRHSSemijoinColumns(String,ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.QBJoinTree.addRHSSemijoinColumns(String,ArrayList<ASTNode>)",1,2,2
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getAliasToOpInfo()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getBaseSrc()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getExpressions()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getFilterMap()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getFilters()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getFiltersForPushing()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getId()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getJoinCond()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getJoinSrc()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getLeftAlias()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getLeftAliases()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getMapAliases()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getNextTag()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getNoOuterJoin()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getNoSemiJoin()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getNullSafes()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getPostJoinFilters()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getRHSSemijoinColumns(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getRightAliases()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.getStreamAliases()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.isMapSideJoin()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.mergeRHSSemijoin(QBJoinTree)",1,3,3
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setAliasToOpInfo(Map<String, Operator<? extends OperatorDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setBaseSrc(String[])",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setExpressions(ArrayList<ArrayList<ASTNode>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setFilterMap(int[][])",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setFilters(ArrayList<ArrayList<ASTNode>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setFiltersForPushing(ArrayList<ArrayList<ASTNode>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setId(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setJoinCond(JoinCond[])",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setJoinSrc(QBJoinTree)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setLeftAlias(String)",1,2,3
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setLeftAliases(String[])",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setMapAliases(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setMapSideJoin(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setNoOuterJoin(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setNoSemiJoin(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setNullSafes(ArrayList<Boolean>)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setRightAliases(String[])",1,1,1
"org.apache.hadoop.hive.ql.parse.QBJoinTree.setStreamAliases(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.QBMetaData()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.getAliasToTable()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.getDPCtx(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.getDestFileForAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.getDestPartitionForAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.getDestTableForAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.getDestTypeForAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.getPartSpecForAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.getSrcForAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.getTableForAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.setDPCtx(String,DynamicPartitionCtx)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.setDestForAlias(String,Partition)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.setDestForAlias(String,String,boolean)",1,2,2
"org.apache.hadoop.hive.ql.parse.QBMetaData.setDestForAlias(String,Table)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.setPartSpecForAlias(String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBMetaData.setSrcForAlias(String,Table)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.QBParseInfo(String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.addAggregationExprsForClause(String,LinkedHashMap<String, ASTNode>)",1,2,2
"org.apache.hadoop.hive.ql.parse.QBParseInfo.addInsertIntoTable(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.addLateralViewForAlias(String,ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.QBParseInfo.addTableSpec(String,tableSpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.addWindowingExprToClause(String,ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.QBParseInfo.clearAggregationExprsForClause(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.clearDistinctFuncExprsForClause(String)",1,2,2
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getAggregationExprsForClause(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getAliasToLateralViews()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getAllExprToColumnAlias()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getClauseNames()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getClauseNamesForDest()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getClusterByForClause(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getColName()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getColType()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestCubes()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestForClause(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestGroupingSets()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestLimit(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestRollups()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestToAggregationExprs()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestToClusterBy()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestToDistinctFuncExprs()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestToDistributeBy()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestToGroupBy()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestToHaving()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestToLateralView()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestToLimit()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestToOrderBy()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestToSortBy()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDestToWhereExpr()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDistinctFuncExprsForClause(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getDistributeByForClause(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getExprToColumnAlias(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getGroupByForClause(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getHavingForClause(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getHints()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getIsSubQ()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getJoinExpr()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getLateralViewsForAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getNameToSample()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getOrderByForClause(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getOuterQueryLimit()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getSelForClause(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getSortByForClause(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getSrcForAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getTabSample(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getTableSpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getTableSpec(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getWhrForClause(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.getWindowingExprsForClause(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.hasExprToColumnAlias(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.isAnalyzeCommand()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.isInsertIntoTable(String,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.isInsertToTable()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.isNoScanAnalyzeCommand()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.isPartialScanAnalyzeCommand()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.isSimpleSelectQuery()",9,10,14
"org.apache.hadoop.hive.ql.parse.QBParseInfo.isTblLvl()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setAggregationExprsForClause(String,LinkedHashMap<String, ASTNode>)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setClusterByExprForClause(String,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setColName(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setColType(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setDestForClause(String,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setDestLimit(String,Integer)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setDistinctFuncExprsForClause(String,List<ASTNode>)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setDistributeByExprForClause(String,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setExprToColumnAlias(ASTNode,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setGroupByExprForClause(String,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setHavingExprForClause(String,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setHints(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setIsAnalyzeCommand(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setIsInsertToTable(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setJoinExpr(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setNoScanAnalyzeCommand(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setOrderByExprForClause(String,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setOuterQueryLimit(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setPartialScanAnalyzeCommand(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setSelExprForClause(String,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setSortByExprForClause(String,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setSrcForAlias(String,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setTabSample(String,TableSample)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setTblLvl(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBParseInfo.setWhrExprForClause(String,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.Conjunct.Conjunct(ASTNode,ASTNode,ExprType,ExprType,ColumnInfo,ColumnInfo)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.Conjunct.eitherSideRefersBoth()",3,1,3
"org.apache.hadoop.hive.ql.parse.QBSubQuery.Conjunct.getLeftExpr()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.Conjunct.getLeftExprType()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.Conjunct.getLeftOuterColInfo()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.Conjunct.getRightExpr()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.Conjunct.getRightExprType()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.Conjunct.getRightOuterColInfo()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.Conjunct.isCorrelated()",2,2,2
"org.apache.hadoop.hive.ql.parse.QBSubQuery.Conjunct.refersOuterOnly()",2,1,2
"org.apache.hadoop.hive.ql.parse.QBSubQuery.ConjunctAnalyzer.ConjunctAnalyzer(RowResolver,boolean,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.ConjunctAnalyzer.analyzeConjunct(ASTNode)",2,2,2
"org.apache.hadoop.hive.ql.parse.QBSubQuery.ConjunctAnalyzer.analyzeExpr(ASTNode)",6,7,8
"org.apache.hadoop.hive.ql.parse.QBSubQuery.ConjunctAnalyzer.firstDot(ASTNode)",2,3,3
"org.apache.hadoop.hive.ql.parse.QBSubQuery.ConjunctAnalyzer.resolveDot(ASTNode)",2,2,3
"org.apache.hadoop.hive.ql.parse.QBSubQuery.ExprType.ExprType(boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.ExprType.refersParent()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.ExprType.refersSubQuery()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.NotInCheck.NotInCheck()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.NotInCheck.addCorrExpr(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.NotInCheck.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.NotInCheck.getJoinConditionAST()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.NotInCheck.getJoinType()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.NotInCheck.getOuterQueryId()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.NotInCheck.getSubQuery()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.NotInCheck.getSubQueryAST()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.NotInCheck.setSQRR(RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.QBSubQuery(String,int,ASTNode,ASTNode,SubQueryTypeDef,ASTNode,Context)",1,1,2
"org.apache.hadoop.hive.ql.parse.QBSubQuery.SubQueryType.get(ASTNode)",6,2,6
"org.apache.hadoop.hive.ql.parse.QBSubQuery.SubQueryTypeDef.SubQueryTypeDef(ASTNode,SubQueryType)",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.SubQueryTypeDef.getAst()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.SubQueryTypeDef.getType()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.buildJoinCondition(RowResolver,RowResolver,boolean,String)",1,7,8
"org.apache.hadoop.hive.ql.parse.QBSubQuery.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.getDiagnostic()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.getJoinConditionAST()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.getJoinType()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.getNextCorrExprAlias()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.getNotInCheck()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.getNumOfCorrelationExprsAddedToSQSelect()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.getOperator()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.getOriginalSubQueryASTForRewrite()",1,3,3
"org.apache.hadoop.hive.ql.parse.QBSubQuery.getOuterQueryExpression()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.getOuterQueryId()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.getSubQuery()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.getSubQueryAST()",1,1,1
"org.apache.hadoop.hive.ql.parse.QBSubQuery.getSubQueryGroupByAST()",2,6,7
"org.apache.hadoop.hive.ql.parse.QBSubQuery.rewrite(RowResolver,boolean,String)",5,16,17
"org.apache.hadoop.hive.ql.parse.QBSubQuery.rewriteCorrConjunctForHaving(ASTNode,boolean,String,RowResolver,ColumnInfo)",1,2,2
"org.apache.hadoop.hive.ql.parse.QBSubQuery.setJoinType()",1,2,3
"org.apache.hadoop.hive.ql.parse.QBSubQuery.updateOuterQueryFilter(ASTNode)",3,1,3
"org.apache.hadoop.hive.ql.parse.QBSubQuery.validateAndRewriteAST(RowResolver,boolean,String,Set<String>)",11,21,25
"org.apache.hadoop.hive.ql.parse.RowResolver.RowResolver()",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.addMappingOnly(String,String,ColumnInfo)",1,4,4
"org.apache.hadoop.hive.ql.parse.RowResolver.checkColumn(String,String)",2,4,4
"org.apache.hadoop.hive.ql.parse.RowResolver.get(String,String)",7,5,10
"org.apache.hadoop.hive.ql.parse.RowResolver.getAlternateMappings(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.getColumnInfos()",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.getExpression(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.getExpressionMap()",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.getExpressionSource(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.getFieldMap(String)",2,2,2
"org.apache.hadoop.hive.ql.parse.RowResolver.getInvRslvMap()",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.getIsExprResolver()",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.getPosition(String)",3,2,3
"org.apache.hadoop.hive.ql.parse.RowResolver.getReferenceableColumnAliases(String,int)",6,8,14
"org.apache.hadoop.hive.ql.parse.RowResolver.getRowSchema()",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.getRslvMap()",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.getTableNames()",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.hasTableAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.put(String,String,ColumnInfo)",1,2,2
"org.apache.hadoop.hive.ql.parse.RowResolver.putExpression(ASTNode,ColumnInfo)",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.reverseLookup(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.setExprResolver(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.setExpressionMap(Map<String, ASTNode>)",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.setInvRslvMap(HashMap<String, String[]>)",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.setIsExprResolver(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.setRowSchema(RowSchema)",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.setRslvMap(HashMap<String, LinkedHashMap<String, ColumnInfo>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.RowResolver.toString()",1,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.AggregationExprCheck.AggregationExprCheck(HashMap<String, ASTNode>)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.AggregationExprCheck.isAggr()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.AggregationExprCheck.reset()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.AggregationExprCheck.visit(Object,Object,int,Map)",2,1,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.ConstantExprCheck.isConstant()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.ConstantExprCheck.reset()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.ConstantExprCheck.visit(Object,Object,int,Map)",2,1,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.LVmergeRowResolvers(RowResolver,RowResolver,Map<String, ExprNodeDesc>,ArrayList<String>)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.SemanticAnalyzer(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.SortBucketRSCtx.SortBucketRSCtx()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.SortBucketRSCtx.getNumFiles()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.SortBucketRSCtx.getPartnCols()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.SortBucketRSCtx.getTotalFiles()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.SortBucketRSCtx.isMultiFileSpray()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.SortBucketRSCtx.setMultiFileSpray(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.SortBucketRSCtx.setNumFiles(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.SortBucketRSCtx.setPartnCols(ArrayList<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.SortBucketRSCtx.setTotalFiles(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.addAlternateGByKeyMappings(ASTNode,ColumnInfo,Operator<? extends OperatorDesc>,RowResolver)",1,6,8
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.addCTEAsSubQuery(QB,String,String)",1,1,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.addDbAndTabToOutputs(String[])",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.addDefaultProperties(Map<String, String>)",4,5,7
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.addGroupingSetKey(List<ExprNodeDesc>,RowResolver,RowResolver,List<String>,Map<String, ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(ASTNode,QB)",23,25,48
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateView(ASTNode,QB)",3,4,14
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(ASTNode)",7,16,20
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.applyEqualityPredicateToQBJoinTree(QBJoinTree,JoinType,List<String>,ASTNode,ASTNode,ASTNode,List<String>,List<String>,List<String>,List<String>)",11,38,42
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.assertCombineInputFormat(Tree,String)",2,3,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.buildPTFReduceSinkDetails(PartitionedTableFunctionDef,RowResolver,ArrayList<ExprNodeDesc>,ArrayList<ExprNodeDesc>,ArrayList<ExprNodeDesc>,Map<String, ExprNodeDesc>,List<String>,StringBuilder,RowResolver,RowResolver)",3,9,10
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.checkExpressionsForGroupingSet(List<ASTNode>,List<ASTNode>,Map<String, ASTNode>,RowResolver)",6,9,9
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.checkForNoAggr(List<Integer>)",1,1,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.checkHoldDDLTime(QB)",4,2,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.checkNoScan(ASTNode)",1,4,5
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.checkPartialScan(ASTNode)",1,4,5
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.combineExprNodeLists(List<ExprNodeDesc>,List<ExprNodeDesc>,List<ExprNodeDesc>)",1,3,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.containsLeadLagUDF(ASTNode)",6,5,7
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.convertRowSchemaToResultSetSchema(RowResolver,boolean)",3,4,6
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.convertRowSchemaToViewSchema(RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.createCommonReduceSink(QB,Operator)",1,9,9
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.createDummyFile()",2,1,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.createInputForDests(QB,Operator<? extends OperatorDesc>,Set<String>)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.createNewGroupingKey(List<ExprNodeDesc>,List<String>,RowResolver,Map<String, ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.determineWriteType(LoadTableDesc,boolean)",2,2,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.distinctExprsExists(QB)",3,2,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.doPhase1(ASTNode,QB,Phase1Ctx)",22,39,61
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.doPhase1GetAggregationsFromSelect(ASTNode,QB,String)",3,8,9
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.doPhase1GetAllAggregations(ASTNode,HashMap<String, ASTNode>,List<ASTNode>)",7,9,11
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.doPhase1GetColumnAliasesFromSelect(ASTNode,QBParseInfo)",1,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.doPhase1GetDistinctFuncExprs(HashMap<String, ASTNode>)",1,3,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.doPhase1QBExpr(ASTNode,QBExpr,String,String)",2,2,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.enforceScanLimits(ParseContext,FetchTask)",8,9,10
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.extractColumns(Set<String>,ExprNodeDesc)",2,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.extractJoinAlias(ASTNode,String)",5,4,5
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.extractJoinCondsFromWhereClause(QBJoinTree,QB,String,ASTNode,Map<String, Operator>)",4,9,15
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.failIfColAliasExists(Set<String>,String)",2,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.fetchFilesNotInLocalFilesystem(String)",2,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.findAlias(ASTNode,Map<String, Operator>)",6,6,6
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.findCTEFromName(QB,String)",3,8,9
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.findMergePos(QBJoinTree,QBJoinTree)",10,6,12
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(ASTNode,RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(ASTNode,RowResolver,TypeCheckCtx)",7,4,9
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(QB,Operator,Map<String, Operator>)",9,25,26
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(String,Operator,QB,TableDesc,Table,SortBucketRSCtx)",1,11,12
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genColListRegex(String,String,ASTNode,ArrayList<ExprNodeDesc>,RowResolver,Integer,RowResolver,List<String>,boolean)",9,15,19
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genCommonGroupByPlanReduceSinkOperator(QB,List<String>,Operator)",6,4,7
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genConversionOps(String,QB,Operator)",2,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genConversionSelectOperator(String,QB,Operator,TableDesc,DynamicPartitionCtx)",7,18,21
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genConvertCol(String,QB,Table,TableDesc,Operator,List<Integer>,boolean)",4,6,7
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(ASTNode,RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(ASTNode,RowResolver,TypeCheckCtx)",2,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(String,QB,Operator)",25,68,74
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFilterPlan(ASTNode,QB,Operator,Map<String, Operator>,boolean)",6,12,12
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFilterPlan(QB,ASTNode,Operator)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlan1MR(String,QB,Operator)",2,2,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlan1ReduceMultiGBY(List<String>,QB,Operator,Map<String, Operator>)",5,7,8
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlan2MR(String,QB,Operator)",2,2,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlan2MRMultiGroupBy(String,QB,Operator)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanGroupByOperator(QBParseInfo,String,Operator,ReduceSinkOperator,Mode,Map<String, GenericUDAFEvaluator>)",6,9,12
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanGroupByOperator1(QBParseInfo,String,Operator,Mode,Map<String, GenericUDAFEvaluator>,boolean,List<Integer>,boolean,boolean)",8,14,21
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanGroupByOperator2MR(QBParseInfo,String,Operator,Mode,Map<String, GenericUDAFEvaluator>,boolean)",5,7,8
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapAggr2MR(String,QB,Operator)",4,4,6
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapAggrNoSkew(String,QB,Operator)",3,5,11
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapGroupByOperator(QB,String,List<ASTNode>,Operator,Mode,Map<String, GenericUDAFEvaluator>,List<Integer>,boolean)",1,11,12
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanReduceSinkOperator(QB,String,Operator,List<ASTNode>,int,boolean,int,boolean,boolean)",1,4,6
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanReduceSinkOperator2MR(QBParseInfo,String,Operator,int,int,boolean)",1,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genHavingPlan(String,QB,Operator,Map<String, Operator>)",1,3,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genInputSelectForUnion(Operator<? extends OperatorDesc>,Map<String, ColumnInfo>,String,RowResolver,String)",2,4,5
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinKeys(QBJoinTree,Operator[])",1,3,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinOperator(QB,QBJoinTree,Map<String, Operator>,Operator)",1,10,13
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinOperatorChildren(QBJoinTree,Operator,Operator[],HashSet<Integer>)",6,11,15
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinOperatorTypeCheck(ExprNodeDesc[][])",4,6,8
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinPlan(QB,Map<String, Operator>)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinReduceSinkChild(QB,ExprNodeDesc[],Operator<?>,String[],int)",6,12,13
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinTree(QB,ASTNode,Map<String, Operator>)",2,16,25
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genLateralViewPlan(QB,Operator,ASTNode)",1,3,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genLateralViewPlanForDest(String,QB,Operator)",2,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genLateralViewPlans(Map<String, Operator>,QB)",1,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genLimitMapRedPlan(String,QB,Operator,int,boolean)",2,1,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genLimitPlan(String,QB,Operator,int)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genMapGroupByForSemijoin(QB,ArrayList<ASTNode>,Operator,Mode)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genNotNullFilterForJoinSourcePlan(QB,Operator,QBJoinTree,ExprNodeDesc[])",8,7,14
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPTFPlan(PTFInvocationSpec,Operator)",1,3,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPTFPlanForComponentQuery(PTFInvocationSpec,Operator)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(QB)",5,14,15
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(QBExpr)",3,3,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlanForSubQueryPredicate(QB,ISubQueryJoinInfo)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(Operator,String,QB,Map<String, Operator>)",3,16,24
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genReduceSinkPlan(String,QB,Operator<?>,int)",7,24,25
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genReduceSinkPlanForSortingBucketing(Table,Operator,ArrayList<ExprNodeDesc>,List<Integer>,ArrayList<ExprNodeDesc>,int)",1,5,6
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genReduceSinkPlanForWindowing(WindowingSpec,RowResolver,Operator)",3,12,13
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSQJoinTree(QB,ISubQueryJoinInfo,Operator,Map<String, Operator>)",2,5,11
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSamplePredicate(TableSample,List<String>,boolean,String,RowResolver,QBMetaData,ExprNodeDesc)",1,5,5
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genScriptPlan(ASTNode,QB,Operator)",1,13,17
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(ASTNode,QB,Operator<?>,boolean)",6,50,60
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(String,QB,Operator<?>)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genTablePlan(String,QB)",8,28,38
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genUDTFPlan(GenericUDTF,String,ArrayList<String>,QB,Operator,boolean)",7,13,13
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genUnionPlan(String,String,Operator,String,Operator)",9,12,14
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genUniqueJoinTree(QB,ASTNode,Map<String, Operator>)",6,10,15
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genWindowingPlan(WindowingSpec,Operator)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.generateErrorMessage(ASTNode,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getAliasId(String,QB)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getColAlias(ASTNode,String,RowResolver,boolean,int)",3,11,13
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getColumnInternalName(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getCommonDistinctExprs(QB,Operator)",10,8,15
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getCommonGroupByDestGroups(QB,Map<String, Operator<? extends OperatorDesc>>)",11,11,15
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getDefaultRecordReader()",1,1,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getDistinctColIndicesForReduceSink(QBParseInfo,String,List<ExprNodeDesc>,RowResolver,RowResolver,List<String>,Map<String, ExprNodeDesc>)",6,6,7
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getDistinctExprs(QBParseInfo,String,RowResolver)",1,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getDummyTable()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getExprNodeDescCached(ASTNode,RowResolver)",2,3,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGenericUDAFEvaluator(String,ArrayList<ExprNodeDesc>,ASTNode,boolean,boolean)",2,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGenericUDAFInfo(GenericUDAFEvaluator,Mode,ArrayList<ExprNodeDesc>)",1,2,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGroupByForClause(QBParseInfo,String)",6,10,12
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGroupByGroupingSetsForClause(QBParseInfo,String)",1,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGroupingSets(List<ASTNode>,QBParseInfo,String)",7,8,9
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGroupingSetsForCube(int)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGroupingSetsForRollup(int)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMapSideJoinTables(QB)",1,7,7
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(QB)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(QB,ReadEntity)",29,46,50
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(QBExpr,ReadEntity)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getModifiedAlias(QB,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getParseContext()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getPartitionColsFromBucketCols(String,QB,Table,TableDesc,Operator,boolean)",4,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getPositionFromInternalName(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getQB()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getRecordReader(ASTNode)",1,2,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getRecordWriter(ASTNode)",1,2,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getReduceKeysForReduceSink(List<ASTNode>,String,RowResolver,RowResolver,List<String>,Map<String, ExprNodeDesc>)",3,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getReduceValuesForReduceSinkNoMapAgg(QBParseInfo,String,RowResolver,RowResolver,List<String>,ArrayList<ExprNodeDesc>,Map<String, ExprNodeDesc>)",1,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getReducersBucketing(int,int)",3,1,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getResultSchema()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getRowResolver(Operator)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getScriptArgs(String)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getScriptProgName(String)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getSortCols(String,QB,Table,TableDesc,Operator,boolean)",4,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getSortOrders(String,QB,Table,Operator)",4,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getStandardObjectInspector(ArrayList<TypeInfo>)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getTableDescFromSerDe(ASTNode,String,String,boolean)",6,10,16
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getType(JoinCond[])",3,2,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getTypeInfo(ArrayList<ExprNodeDesc>)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getUDAFEvaluators(ArrayList<AggregationDesc>)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getWritableObjectInspector(ArrayList<ExprNodeDesc>)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.groupByDescModeToUDAFMode(Mode,boolean)",9,2,11
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.handleQueryWindowClauses(QB,Phase1Ctx,ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.hasCommonElement(Set<String>,Set<String>)",3,2,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.init()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.initParseCtx(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.initPhase1Ctx()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.insertSelectAllPlanForGroupBy(Operator)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.insertSelectForSemijoin(ArrayList<ASTNode>,Operator)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isConstantParameterInAggregationParameters(String,List<ExprNodeDesc>)",6,5,8
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isDeterministic(ExprNodeDesc)",1,1,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isJoinToken(ASTNode)",2,7,8
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isPresent(String[],String)",3,2,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isRegex(String,HiveConf)",4,3,5
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isSkewedCol(String,QB,String)",1,2,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isValidGroupBySelectList(QB,String)",6,5,8
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.matchExprLists(List<ExprNodeDesc>,List<ExprNodeDesc>)",4,2,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.mergeJoinTree(QB)",12,10,17
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.mergeJoins(QB,QBJoinTree,QBJoinTree,int,int[])",1,21,31
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.optimizeMapAggrGroupBy(String,QB)",3,2,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.parseJoinCondPopulateAlias(QBJoinTree,ASTNode,ArrayList<String>,ArrayList<String>,ArrayList<String>,Map<String, Operator>)",4,18,22
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.parseJoinCondition(QBJoinTree,ASTNode,List<String>,JoinType,Map<String, Operator>)",10,19,26
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.parseJoinCondition(QBJoinTree,ASTNode,List<String>,Map<String, Operator>)",2,3,6
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.parseSelect(String)",4,7,8
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.parseStreamTables(QBJoinTree,QB)",1,4,5
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.populateAliases(List<String>,List<String>,ASTNode,QBJoinTree,List<String>)",4,7,7
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processBoundary(int,ASTNode)",5,4,8
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processCTE(QB,ASTNode)",3,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processGroupingSetReduceSinkOperator(RowResolver,RowResolver,List<ExprNodeDesc>,List<String>,Map<String, ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processJoin(QB,ASTNode)",8,13,13
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processLateralView(QB,ASTNode)",2,2,5
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processNoScanCommand(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processOrderSpec(ASTNode)",1,3,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processPTF(QB,ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processPTFChain(QB,ASTNode)",2,7,7
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processPTFPartitionSpec(ASTNode)",1,5,7
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processPTFSource(QB,ASTNode)",2,2,5
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processPartialScanCommand(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processPartitionSpec(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processPositionAlias(ASTNode)",13,14,22
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processQueryWindowClause(WindowingSpec,ASTNode)",2,3,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processSubQuery(QB,ASTNode)",3,3,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processTable(QB,ASTNode)",7,18,26
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processWindowFrame(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processWindowFunction(ASTNode,ASTNode)",3,5,6
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.processWindowSpec(ASTNode)",2,6,8
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.pushJoinFilters(QB,QBJoinTree,Map<String, Operator>)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.pushJoinFilters(QB,QBJoinTree,Map<String, Operator>,boolean)",1,6,6
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.putAccessedColumnsToReadEntity(HashSet<ReadEntity>,ColumnAccessInfo)",2,5,7
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.putOpInsertMap(Operator<T>,RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.recommendName(ExprNodeDesc,String)",3,2,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.removeMappingForKeys(ASTNode,Map<ASTNode, ExprNodeDesc>,List<ExprNodeDesc>)",1,4,4
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.removeRecursively(ASTNode,Map<ASTNode, ExprNodeDesc>)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.replaceViewReferenceWithDefinition(QB,Table,String,String)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.reset()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.saveViewDefinition()",8,11,11
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.setBit(int,int)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.setQB(QB)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.setupStats(TableScanDesc,QBParseInfo,Table,String,RowResolver)",4,9,9
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.translatePTFInvocationSpec(PTFInvocationSpec,RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validate()",12,17,23
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validate(Task<? extends Serializable>,boolean)",2,2,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validateAnalyzeNoscan(ASTNode)",2,3,3
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validateAnalyzePartialscan(ASTNode)",4,5,5
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.validateCreateView(CreateViewDesc)",7,14,16
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.SemanticAnalyzerFactory()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(HiveConf,ASTNode)",16,7,17
"org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.setSessionCommandType(HiveOperation)",1,2,2
"org.apache.hadoop.hive.ql.parse.SemanticException.SemanticException()",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticException.SemanticException(ErrorMsg,String...)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticException.SemanticException(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticException.SemanticException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.ql.parse.SemanticException.SemanticException(Throwable)",1,1,1
"org.apache.hadoop.hive.ql.parse.SplitSample.SplitSample()",1,1,1
"org.apache.hadoop.hive.ql.parse.SplitSample.SplitSample(double,int)",1,1,1
"org.apache.hadoop.hive.ql.parse.SplitSample.SplitSample(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.SplitSample.SplitSample(long,int)",1,1,1
"org.apache.hadoop.hive.ql.parse.SplitSample.getPercent()",1,1,1
"org.apache.hadoop.hive.ql.parse.SplitSample.getRowCount()",1,1,1
"org.apache.hadoop.hive.ql.parse.SplitSample.getSeedNum()",1,1,1
"org.apache.hadoop.hive.ql.parse.SplitSample.getTargetSize(long)",1,1,2
"org.apache.hadoop.hive.ql.parse.SplitSample.getTotalLength()",1,1,1
"org.apache.hadoop.hive.ql.parse.SplitSample.setPercent(Double)",1,1,1
"org.apache.hadoop.hive.ql.parse.SplitSample.setRowCount(Integer)",1,1,1
"org.apache.hadoop.hive.ql.parse.SplitSample.setSeedNum(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.SplitSample.setTotalLength(Long)",1,1,1
"org.apache.hadoop.hive.ql.parse.StorageFormat.StorageFormat(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.parse.StorageFormat.fillDefaultStorageFormat()",1,4,5
"org.apache.hadoop.hive.ql.parse.StorageFormat.fillStorageFormat(ASTNode)",3,5,9
"org.apache.hadoop.hive.ql.parse.StorageFormat.getInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.parse.StorageFormat.getOutputFormat()",1,1,1
"org.apache.hadoop.hive.ql.parse.StorageFormat.getSerde()",1,1,1
"org.apache.hadoop.hive.ql.parse.StorageFormat.getSerdeProps()",1,1,1
"org.apache.hadoop.hive.ql.parse.StorageFormat.getStorageHandler()",1,1,1
"org.apache.hadoop.hive.ql.parse.StorageFormat.processStorageFormat(String)",3,4,6
"org.apache.hadoop.hive.ql.parse.StorageFormat.setSerde(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite.QBSubQueryRewrite(QBSubQuery,TokenRewriteStream)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite.addCondition(StringBuilder,ASTNode,boolean)",1,2,2
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite.addGByClauseRewrite(ASTNode)",1,2,4
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite.addJoinCondition(ASTNode,boolean,boolean)",1,2,2
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite.addPostJoinCondition(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite.addReference(StringBuilder,ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite.addSelectClauseRewrite(ASTNode,String)",1,1,2
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite.addWhereClauseRewrite(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite.addWhereClauseRewrite(String)",1,1,2
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite.getJoiningCondition()",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite.getOuterQueryPostJoinCond()",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite.getRewrittenQuery()",1,5,5
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite.setAddGroupByClause()",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewriteNoop.QBSubQueryRewriteNoop(QBSubQuery,TokenRewriteStream)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewriteNoop.addGByClauseRewrite(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewriteNoop.addJoinCondition(ASTNode,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewriteNoop.addPostJoinCondition(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewriteNoop.addSelectClauseRewrite(ASTNode,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewriteNoop.addWhereClauseRewrite(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewriteNoop.addWhereClauseRewrite(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewriteNoop.getJoiningCondition()",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewriteNoop.getOuterQueryPostJoinCond()",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewriteNoop.getRewrittenQuery()",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewriteNoop.setAddGroupByClause()",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.getRewrite(QBSubQuery,TokenRewriteStream,Context)",2,1,2
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.ParentQueryWhereClauseRewrite.ParentQueryWhereClauseRewrite(ASTNode,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.ParentQueryWhereClauseRewrite.getParentInWhereClause(ASTNode)",2,1,3
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.ParentQueryWhereClauseRewrite.remove()",2,1,2
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.ParentQueryWhereClauseRewrite.removeSubQuery(ASTNode)",5,12,12
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.addGroupExpressionToFront(ASTNode,ASTNode)",1,3,3
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.alterCorrelatedPredicate(ASTNode,ASTNode,boolean)",1,2,2
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.andAST(ASTNode,ASTNode)",3,3,3
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.buildGroupBy()",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.buildNotInNullCheckFrom(ASTNode,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.buildNotInNullCheckInsert()",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.buildNotInNullCheckQuery(ASTNode,String,String,List<ASTNode>,RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.buildNotInNullCheckSelect(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.buildNotInNullCheckWhere(ASTNode,String,List<ASTNode>,RowResolver)",1,4,4
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.buildNotInNullJoinCond(String,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.buildOuterJoinPostCond(String,RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.buildOuterQryToSQJoinCond(ASTNode,String,RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.buildPostJoinNullCheck(List<ASTNode>)",1,2,2
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.buildSQJoinExpr(String,RowResolver,boolean)",1,2,2
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.buildSQOperator(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.buildSelectExpr(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.buildSubQuery(String,int,ASTNode,ASTNode,Context)",2,3,3
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.checkAggOrWindowing(ASTNode)",5,4,8
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.constructTrueCond()",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.createAliasAST(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.createColRefAST(String,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.createSelectItem(ASTNode,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.createTabRefAST(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.extractConjuncts(ASTNode,List<ASTNode>)",2,2,2
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.findSubQueries(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.findSubQueries(ASTNode,List<ASTNode>)",2,3,4
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.getAlias(Operator,Map<String, Operator>)",3,3,3
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.getTableAliasesInSubQuery(ASTNode,List<String>)",1,6,6
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.getTableAliasesInSubQuery(QBSubQuery)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.hasUnQualifiedColumnReferences(ASTNode)",5,2,5
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.isNull(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.orAST(ASTNode,ASTNode)",3,3,3
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.rewriteParentQueryWhere(ASTNode,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.setOriginDeep(ASTNode,ASTNodeOrigin)",2,2,3
"org.apache.hadoop.hive.ql.parse.SubQueryUtils.subQueryWhere(ASTNode)",2,3,3
"org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.GroupByProcessor.GroupByProcessor(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.GroupByProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,1,3
"org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.JoinProcessor.JoinProcessor(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.JoinProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",5,3,5
"org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.TableAccessAnalyzer()",1,1,1
"org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.TableAccessAnalyzer(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.analyzeTableAccess()",1,1,1
"org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.genColNameMap(Operator<? extends OperatorDesc>,List<String>)",7,7,9
"org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.genRootTableScan(Operator<? extends OperatorDesc>,List<String>)",5,5,7
"org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.parse.TableAccessAnalyzer.getKeyColNames(List<ExprNodeDesc>)",4,3,5
"org.apache.hadoop.hive.ql.parse.TableAccessCtx.TableAccessCtx()",1,1,1
"org.apache.hadoop.hive.ql.parse.TableAccessCtx.addOperatorTableAccess(Operator<? extends OperatorDesc>,Map<String, List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.TableAccessCtx.getTableAccessInfo()",1,1,1
"org.apache.hadoop.hive.ql.parse.TableAccessInfo.TableAccessInfo()",1,1,1
"org.apache.hadoop.hive.ql.parse.TableAccessInfo.add(Operator<? extends OperatorDesc>,Map<String, List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.TableAccessInfo.getOperatorToTableAccessMap()",1,1,1
"org.apache.hadoop.hive.ql.parse.TableSample.TableSample(String,String,ArrayList<ASTNode>)",1,1,1
"org.apache.hadoop.hive.ql.parse.TableSample.TableSample(int,int)",1,1,1
"org.apache.hadoop.hive.ql.parse.TableSample.getDenominator()",1,1,1
"org.apache.hadoop.hive.ql.parse.TableSample.getExprs()",1,1,1
"org.apache.hadoop.hive.ql.parse.TableSample.getInputPruning()",1,1,1
"org.apache.hadoop.hive.ql.parse.TableSample.getNumerator()",1,1,1
"org.apache.hadoop.hive.ql.parse.TableSample.setDenominator(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.TableSample.setExprs(ArrayList<ASTNode>)",1,1,1
"org.apache.hadoop.hive.ql.parse.TableSample.setInputPruning(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.TableSample.setNumerator(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(ParseContext,List<Task<? extends Serializable>>,HashSet<ReadEntity>,HashSet<WriteEntity>)",11,32,37
"org.apache.hadoop.hive.ql.parse.TaskCompiler.genColumnStatsTask(QB,List<LoadTableDesc>,List<LoadFileDesc>,List<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.TaskCompiler.getLeafTasks(List<Task<? extends Serializable>>,HashSet<Task<? extends Serializable>>)",1,2,2
"org.apache.hadoop.hive.ql.parse.TaskCompiler.getLeafTasks(Task<? extends Serializable>,HashSet<Task<? extends Serializable>>)",1,3,3
"org.apache.hadoop.hive.ql.parse.TaskCompiler.getParseContext(ParseContext,List<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.parse.TaskCompiler.init(HiveConf,LogHelper,Hive)",1,1,1
"org.apache.hadoop.hive.ql.parse.TaskCompiler.optimizeOperatorPlan(ParseContext,Set<ReadEntity>,Set<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.parse.TaskCompilerFactory.TaskCompilerFactory()",1,1,1
"org.apache.hadoop.hive.ql.parse.TaskCompilerFactory.getCompiler(HiveConf,ParseContext)",2,1,2
"org.apache.hadoop.hive.ql.parse.TestEximUtil.setUp()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestEximUtil.tearDown()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestEximUtil.testCheckCompatibility()",1,1,4
"org.apache.hadoop.hive.ql.parse.TestGenTezWork.setUp()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestGenTezWork.tearDown()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestGenTezWork.testCreateMap()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestGenTezWork.testCreateReduce()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestHiveDecimalParse.createDriver()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestHiveDecimalParse.getColumnType(String)",2,1,2
"org.apache.hadoop.hive.ql.parse.TestHiveDecimalParse.testDecimalType()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestHiveDecimalParse.testDecimalType1()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestHiveDecimalParse.testDecimalType2()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestHiveDecimalParse.testDecimalType3()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestHiveDecimalParse.testDecimalType4()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestHiveDecimalParse.testDecimalType5()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestHiveDecimalParse.testDecimalType6()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestHiveDecimalParse.testDecimalType7()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestHiveDecimalParse.testDecimalType8()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestHiveDecimalParse.testDecimalType9()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.initialize()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.parse(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.setup()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.testDeleteNoWhere()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.testDeleteWithWhere()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.testInsertIntoTableAsSelectFromNamedVirtTable()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.testInsertIntoTableFromAnonymousTable()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.testInsertIntoTableFromAnonymousTable1Row()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.testSelectStarFromAnonymousVirtTable1Row()",1,2,2
"org.apache.hadoop.hive.ql.parse.TestIUD.testSelectStarFromVirtTable1Row()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.testSelectStarFromVirtTable2Row()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.testSelectStarFromVirtTable2RowNamedProjections()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.testStandardInsertIntoTable()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.testUpdateNoWhereMultiSet()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.testUpdateNoWhereSingleSet()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.testUpdateWithWhereMultiSet()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestIUD.testUpdateWithWhereSingleSet()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.analyze(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.parse(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.setup()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testCannotUseReservedWordAsName()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testDropMacro()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testDropMacroDoesNotExist()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testDropMacroExistsDoNotIgnoreErrors()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testDropMacroNonExistent()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testDropMacroNonExistentWithIfExists()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testDropMacroNonExistentWithIfExistsDoNotIgnoreNonExistent()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testNoBody()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testOneInputParamters()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testOneUnusedParameterName()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testThreeDuplicateParameters()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testThreeInputParamters()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testTwoDuplicateParameterNames()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testTwoInputParamters()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testTwoUnusedParameterNames()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testUnknownInputParameter()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestMacroSemanticAnalyzer.testZeroInputParamters()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBCompact.init()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBCompact.parseAndAnalyze(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBCompact.showCompactions()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBCompact.showTxns()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBCompact.testBogusLevel()",1,2,2
"org.apache.hadoop.hive.ql.parse.TestQBCompact.testMajor()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBCompact.testMinor()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBCompact.testNonPartitionedTable()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.applyEqPredicate(QBJoinTree,String,String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.constructColRef(String,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.constructEqualityCond(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.constructIdentifier(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.constructTabRef(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.createJoinTree(JoinType,String,QBJoinTree,String)",1,2,3
"org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.initialize()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.setup()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.test3WayJoin()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.test3WayJoinSwitched()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.test4WayJoin()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.test4WayJoinSwitched()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.testSimpleCondn()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBSubQuery.initialize()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBSubQuery.parse(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBSubQuery.select(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBSubQuery.setup()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBSubQuery.testCheckAggOrWindowing()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBSubQuery.testExtractConjuncts()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBSubQuery.testExtractSubQueries()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBSubQuery.testRewriteOuterQueryWhere()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBSubQuery.testRewriteOuterQueryWhere2()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestQBSubQuery.where(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.TestSemanticAnalyzer.checkNormalization(String,String,String,Object)",1,2,2
"org.apache.hadoop.hive.ql.parse.TestSemanticAnalyzer.testNormalizeColSpec()",1,1,3
"org.apache.hadoop.hive.ql.parse.TestSemanticAnalyzerFactory.setup()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestSemanticAnalyzerFactory.testCreate()",1,1,1
"org.apache.hadoop.hive.ql.parse.TestSemanticAnalyzerFactory.testDrop()",1,1,1
"org.apache.hadoop.hive.ql.parse.TezCompiler.TezCompiler()",1,1,1
"org.apache.hadoop.hive.ql.parse.TezCompiler.decideExecMode(List<Task<? extends Serializable>>,Context,GlobalLimitCtx)",1,1,1
"org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(List<Task<? extends Serializable>>,ParseContext,List<Task<MoveWork>>,Set<ReadEntity>,Set<WriteEntity>)",1,3,3
"org.apache.hadoop.hive.ql.parse.TezCompiler.init(HiveConf,LogHelper,Hive)",1,1,1
"org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(ParseContext,Set<ReadEntity>,Set<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeTaskPlan(List<Task<? extends Serializable>>,ParseContext,Context)",1,6,6
"org.apache.hadoop.hive.ql.parse.TezCompiler.setInputFormat(MapWork,Operator<? extends OperatorDesc>)",2,4,4
"org.apache.hadoop.hive.ql.parse.TezCompiler.setInputFormat(Task<? extends Serializable>)",1,10,10
"org.apache.hadoop.hive.ql.parse.TezWalker.TezWalker(Dispatcher)",1,1,1
"org.apache.hadoop.hive.ql.parse.TezWalker.walk(Node)",1,3,4
"org.apache.hadoop.hive.ql.parse.TypeCheckCtx.TypeCheckCtx(RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckCtx.getAllowStatefulFunctions()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckCtx.getError()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckCtx.getErrorSrcNode()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckCtx.getInputRR()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckCtx.getUnparseTranslator()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckCtx.isAllowDistinctFunctions()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckCtx.setAllowDistinctFunctions(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckCtx.setAllowStatefulFunctions(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckCtx.setError(String,ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckCtx.setInputRR(RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckCtx.setUnparseTranslator(UnparseTranslator)",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.BoolExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",4,2,6
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.ColumnExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",9,10,13
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.DateExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,1,4
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.DefaultExprProcessor.getFuncExprNodeDesc(String,ExprNodeDesc...)",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.DefaultExprProcessor.getFuncExprNodeDescWithUdfData(String,TypeInfo,ExprNodeDesc...)",3,3,5
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.DefaultExprProcessor.getFunctionText(ASTNode,boolean)",1,6,6
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.DefaultExprProcessor.getXpathOrFuncExprNodeDesc(ASTNode,boolean,ArrayList<ExprNodeDesc>,TypeCheckCtx)",21,47,55
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.DefaultExprProcessor.isDescendant(Node,Node)",5,2,5
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.DefaultExprProcessor.isRedundantConversionFunction(ASTNode,boolean,ArrayList<ExprNodeDesc>)",4,1,4
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.DefaultExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",13,30,35
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.NullExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,1,3
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.NumExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",8,7,10
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.StrExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",4,3,8
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.SubQueryExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",3,1,3
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.TypeCheckProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.convert(Map<Node, Object>)",1,5,5
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(ASTNode,TypeCheckCtx)",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.getBoolExprProcessor()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.getColumnExprProcessor()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.getDateExprProcessor()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.getDefaultExprProcessor()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.getNullExprProcessor()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.getNumExprProcessor()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.getStrExprProcessor()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.getSubQueryExprProcessor()",1,1,1
"org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.processGByExpr(Node,Object)",3,3,5
"org.apache.hadoop.hive.ql.parse.UnparseTranslator.Translation.toString()",1,1,1
"org.apache.hadoop.hive.ql.parse.UnparseTranslator.UnparseTranslator(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.parse.UnparseTranslator.addCopyTranslation(ASTNode,ASTNode)",3,1,3
"org.apache.hadoop.hive.ql.parse.UnparseTranslator.addIdentifierTranslation(ASTNode)",2,1,2
"org.apache.hadoop.hive.ql.parse.UnparseTranslator.addTableNameTranslation(ASTNode,String)",3,3,4
"org.apache.hadoop.hive.ql.parse.UnparseTranslator.addTranslation(ASTNode,String)",6,7,9
"org.apache.hadoop.hive.ql.parse.UnparseTranslator.applyTranslations(TokenRewriteStream)",3,4,5
"org.apache.hadoop.hive.ql.parse.UnparseTranslator.enable()",1,1,1
"org.apache.hadoop.hive.ql.parse.UnparseTranslator.isEnabled()",1,1,1
"org.apache.hadoop.hive.ql.parse.VariableSubstitution.getSubstitute(Configuration,String)",1,4,4
"org.apache.hadoop.hive.ql.parse.VariableSubstitution.substitute(HiveConf,String)",3,2,3
"org.apache.hadoop.hive.ql.parse.WindowingComponentizer.WindowingComponentizer(WindowingSpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingComponentizer.groupFunctions()",1,3,3
"org.apache.hadoop.hive.ql.parse.WindowingComponentizer.hasNext()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingComponentizer.next(HiveConf,SemanticAnalyzer,UnparseTranslator,RowResolver)",2,2,3
"org.apache.hadoop.hive.ql.parse.WindowingExprNodeEvaluatorFactory.ExprNodeWalker.ExprNodeWalker(FindLeadLagFuncExprs)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingExprNodeEvaluatorFactory.ExprNodeWalker.walk(ExprNodeDesc)",2,4,5
"org.apache.hadoop.hive.ql.parse.WindowingExprNodeEvaluatorFactory.FindLeadLagFuncExprs.FindLeadLagFuncExprs(LeadLagInfo,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingExprNodeEvaluatorFactory.FindLeadLagFuncExprs.visit(ExprNodeGenericFuncDesc)",1,2,3
"org.apache.hadoop.hive.ql.parse.WindowingExprNodeEvaluatorFactory.get(LeadLagInfo,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.CurrentRowSpec.CurrentRowSpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.CurrentRowSpec.compareTo(BoundarySpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.CurrentRowSpec.getAmt()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.CurrentRowSpec.getDirection()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.CurrentRowSpec.setAmt(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.CurrentRowSpec.setDirection(Direction)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.CurrentRowSpec.toString()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.RangeBoundarySpec.RangeBoundarySpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.RangeBoundarySpec.RangeBoundarySpec(Direction,int)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.RangeBoundarySpec.compareTo(BoundarySpec)",2,1,2
"org.apache.hadoop.hive.ql.parse.WindowingSpec.RangeBoundarySpec.getAmt()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.RangeBoundarySpec.getDirection()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.RangeBoundarySpec.setAmt(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.RangeBoundarySpec.setDirection(Direction)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.RangeBoundarySpec.toString()",1,1,2
"org.apache.hadoop.hive.ql.parse.WindowingSpec.ValueBoundarySpec.ValueBoundarySpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.ValueBoundarySpec.ValueBoundarySpec(Direction,int)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.ValueBoundarySpec.compareTo(BoundarySpec)",2,1,2
"org.apache.hadoop.hive.ql.parse.WindowingSpec.ValueBoundarySpec.getAmt()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.ValueBoundarySpec.getDirection()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.ValueBoundarySpec.getExpression()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.ValueBoundarySpec.setAmt(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.ValueBoundarySpec.setDirection(Direction)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.ValueBoundarySpec.setExpression(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.ValueBoundarySpec.toString()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowExpressionSpec.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowExpressionSpec.getExpression()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowExpressionSpec.setAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowExpressionSpec.setExpression(ASTNode)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFrameSpec.WindowFrameSpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFrameSpec.WindowFrameSpec(BoundarySpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFrameSpec.WindowFrameSpec(BoundarySpec,BoundarySpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFrameSpec.getEnd()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFrameSpec.getStart()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFrameSpec.setEnd(BoundarySpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFrameSpec.setStart(BoundarySpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFrameSpec.toString()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFunctionSpec.addArg(ASTNode)",1,1,2
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFunctionSpec.getArgs()",1,1,2
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFunctionSpec.getName()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFunctionSpec.getWindowSpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFunctionSpec.isDistinct()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFunctionSpec.isStar()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFunctionSpec.setArgs(ArrayList<ASTNode>)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFunctionSpec.setDistinct(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFunctionSpec.setName(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFunctionSpec.setStar(boolean)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFunctionSpec.setWindowSpec(WindowSpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowFunctionSpec.toString()",1,8,8
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowSpec.ensureOrderSpec()",1,2,2
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowSpec.getOrder()",1,2,2
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowSpec.getPartition()",1,2,2
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowSpec.getPartitioning()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowSpec.getSourceId()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowSpec.getWindowFrame()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowSpec.setOrder(OrderSpec)",1,1,2
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowSpec.setPartition(PartitionSpec)",1,1,2
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowSpec.setPartitioning(PartitioningSpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowSpec.setSourceId(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.WindowSpec.setWindowFrame(WindowFrameSpec)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.addWindowFunction(WindowFunctionSpec)",1,1,3
"org.apache.hadoop.hive.ql.parse.WindowingSpec.addWindowSpec(String,WindowSpec)",1,1,2
"org.apache.hadoop.hive.ql.parse.WindowingSpec.applyContantPartition(WindowSpec)",1,2,2
"org.apache.hadoop.hive.ql.parse.WindowingSpec.effectiveWindowFrame(WindowFunctionSpec,WindowSpec)",1,6,8
"org.apache.hadoop.hive.ql.parse.WindowingSpec.fillInWindowSpec(String,WindowSpec,ArrayList<String>)",4,8,8
"org.apache.hadoop.hive.ql.parse.WindowingSpec.getAliasToWdwExpr()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.getQueryOrderSpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.getQueryPartitionSpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.getQueryPartitioningSpec()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.getWindowExpressions()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.getWindowSpecs()",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.setAliasToWdwExpr(HashMap<String, WindowExpressionSpec>)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.setWindowExpressions(ArrayList<WindowExpressionSpec>)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.setWindowSpecs(HashMap<String, WindowSpec>)",1,1,1
"org.apache.hadoop.hive.ql.parse.WindowingSpec.validateAndMakeEffective()",1,4,4
"org.apache.hadoop.hive.ql.parse.WindowingSpec.validateValueBoundary(BoundarySpec,OrderSpec)",4,3,4
"org.apache.hadoop.hive.ql.parse.WindowingSpec.validateWindowFrame(WindowSpec)",3,3,5
"org.apache.hadoop.hive.ql.parse.authorization.AuthorizationParseUtils.analyzePrincipalListDef(ASTNode)",1,2,2
"org.apache.hadoop.hive.ql.parse.authorization.AuthorizationParseUtils.getPrincipalDesc(ASTNode)",2,2,2
"org.apache.hadoop.hive.ql.parse.authorization.AuthorizationParseUtils.getPrincipalType(ASTNode)",5,2,5
"org.apache.hadoop.hive.ql.parse.authorization.AuthorizationTestUtil.ListSizeMatcher.ListSizeMatcher(List<E>)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.AuthorizationTestUtil.ListSizeMatcher.ofSize(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.AuthorizationTestUtil.analyze(ASTNode,HiveConf,Hive)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.AuthorizationTestUtil.analyze(String,HiveConf,Hive)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.AuthorizationTestUtil.inList(List<E>)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.AuthorizationTestUtil.parse(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.HiveAuthorizationTaskFactoryImpl(HiveConf,Hive)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.analyzeGrantRevokeRole(boolean,ASTNode,HashSet<ReadEntity>,HashSet<WriteEntity>)",1,5,6
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.analyzePrivilegeListDef(ASTNode)",3,4,4
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.analyzePrivilegeObject(ASTNode,HashSet<WriteEntity>)",1,3,3
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.createCreateRoleTask(ASTNode,HashSet<ReadEntity>,HashSet<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.createDropRoleTask(ASTNode,HashSet<ReadEntity>,HashSet<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.createGrantRoleTask(ASTNode,HashSet<ReadEntity>,HashSet<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.createGrantTask(ASTNode,HashSet<ReadEntity>,HashSet<WriteEntity>)",1,5,5
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.createRevokeRoleTask(ASTNode,HashSet<ReadEntity>,HashSet<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.createRevokeTask(ASTNode,HashSet<ReadEntity>,HashSet<WriteEntity>)",1,2,3
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.createSetRoleTask(String,HashSet<ReadEntity>,HashSet<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.createShowCurrentRoleTask(HashSet<ReadEntity>,HashSet<WriteEntity>,Path)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.createShowGrantTask(ASTNode,Path,HashSet<ReadEntity>,HashSet<WriteEntity>)",1,6,6
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.createShowRoleGrantTask(ASTNode,Path,HashSet<ReadEntity>,HashSet<WriteEntity>)",2,2,4
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.createShowRolePrincipalsTask(ASTNode,Path,HashSet<ReadEntity>,HashSet<WriteEntity>)",2,2,2
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.createShowRolesTask(ASTNode,Path,HashSet<ReadEntity>,HashSet<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.getPartition(Table,Map<String, String>)",3,3,4
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.getTable(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.getTable(String,String)",3,4,5
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.parsePrivObject(ASTNode)",1,5,5
"org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.toMessage(ErrorMsg,Object)",1,2,2
"org.apache.hadoop.hive.ql.parse.authorization.ListSizeMatcher.ListSizeMatcher(List<E>)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.ListSizeMatcher.inList(List<E>)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.ListSizeMatcher.ofSize(int)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.PrivilegesTestBase.grantUserTable(String,PrivilegeType,HiveConf,Hive)",1,3,3
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.analyze(String)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.setup()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testCreateRole()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testDropRole()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testGrantGroupTable()",1,3,3
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testGrantRoleGroup()",1,3,3
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testGrantRoleRole()",1,3,3
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testGrantRoleTable()",1,3,3
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testGrantRoleUser()",1,3,3
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testGrantUserTable()",1,3,3
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testRevokeGroupTable()",1,3,3
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testRevokeRoleGroup()",1,3,3
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testRevokeRoleRole()",1,3,3
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testRevokeRoleTable()",1,3,3
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testRevokeRoleUser()",1,3,3
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testRevokeUserTable()",1,3,3
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testShowGrantGroupOnTable()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testShowGrantRoleOnTable()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testShowGrantUserOnTable()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testShowRoleGrantGroup()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testShowRoleGrantRole()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestHiveAuthorizationTaskFactory.testShowRoleGrantUser()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.grantUserTable(String,PrivilegeType)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.grantUserTableFail(String)",1,1,2
"org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.setup()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.testPrivInGrant()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV1.testPrivInGrantNotAccepted()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV2.grantUserTable(String,PrivilegeType)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV2.setup()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestPrivilegesV2.testPrivInGrant()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.HiveAuthorizerStoringUserNameFactory.createHiveAuthorizer(HiveMetastoreClientFactory,HiveConf,HiveAuthenticationProvider,HiveAuthzSessionContext)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.getAuthV2HiveConf()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.setup()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.setupDataNucleusFreeHive(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.testSessionConstructorUser()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.testSessionDefaultUser()",1,1,1
"org.apache.hadoop.hive.ql.parse.authorization.plugin.sqlstd.TestOperation2Privilege.checkHiveOperationTypeMatch()",1,3,3
"org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.clone()",1,1,1
"org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.getOpTraits()",1,1,1
"org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.getStatistics()",1,1,1
"org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.setOpTraits(OpTraits)",1,1,1
"org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.setStatistics(Statistics)",1,1,1
"org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.setVectorMode(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.AddPartitionDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.AddPartitionDesc(String,String,Map<String, String>,String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.AddPartitionDesc(String,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.OnePartitionDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.OnePartitionDesc(Map<String, String>,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.OnePartitionDesc(Map<String, String>,String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.getBucketCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.getCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.getInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.getLocation()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.getNumBuckets()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.getOutputFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.getPartParams()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.getSerdeParams()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.getSerializationLib()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.getSortCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.setBucketCols(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.setCols(List<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.setInputFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.setLocation(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.setNumBuckets(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.setOutputFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.setPartParams(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.setSerdeParams(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.setSerializationLib(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.OnePartitionDesc.setSortCols(List<Order>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.addPartition(Map<String, String>,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.addPartition(Map<String, String>,String,Map<String, String>)",1,1,2
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.getDbName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.getLocationForExplain()",2,4,5
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.getPartSpecStringForExplain()",2,4,5
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.getPartition(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.getPartitionCount()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.isIfNotExists()",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.setDbName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.setIfNotExists(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.AddPartitionDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AggregationDesc.AggregationDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.AggregationDesc.AggregationDesc(String,GenericUDAFEvaluator,ArrayList<ExprNodeDesc>,boolean,Mode)",1,1,1
"org.apache.hadoop.hive.ql.plan.AggregationDesc.getDistinct()",1,1,1
"org.apache.hadoop.hive.ql.plan.AggregationDesc.getExprString()",1,4,4
"org.apache.hadoop.hive.ql.plan.AggregationDesc.getGenericUDAFEvaluator()",3,1,4
"org.apache.hadoop.hive.ql.plan.AggregationDesc.getGenericUDAFEvaluatorClassName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AggregationDesc.getGenericUDAFName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AggregationDesc.getGenericUDAFWritableEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.plan.AggregationDesc.getMode()",1,1,1
"org.apache.hadoop.hive.ql.plan.AggregationDesc.getParameters()",1,1,1
"org.apache.hadoop.hive.ql.plan.AggregationDesc.setDistinct(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.AggregationDesc.setGenericUDAFEvaluator(GenericUDAFEvaluator)",1,2,3
"org.apache.hadoop.hive.ql.plan.AggregationDesc.setGenericUDAFEvaluatorClassName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AggregationDesc.setGenericUDAFName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AggregationDesc.setGenericUDAFWritableEvaluator(GenericUDAFEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.plan.AggregationDesc.setMode(Mode)",1,1,1
"org.apache.hadoop.hive.ql.plan.AggregationDesc.setParameters(ArrayList<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.AlterDatabaseDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.AlterDatabaseDesc(String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.AlterDatabaseDesc(String,PrincipalDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.getAlterType()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.getDatabaseName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.getDatabaseProperties()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.getOwnerPrincipal()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.setAlterType(ALTER_DB_TYPES)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.setDatabaseName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.setDatabaseProperties(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.setOwnerPrincipal(PrincipalDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterIndexDesc.AlterIndexDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterIndexDesc.AlterIndexDesc(AlterIndexTypes)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterIndexDesc.getBaseTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterIndexDesc.getIndexName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterIndexDesc.getOp()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterIndexDesc.getProps()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterIndexDesc.getSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterIndexDesc.setBaseTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterIndexDesc.setIndexName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterIndexDesc.setOp(AlterIndexTypes)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterIndexDesc.setProps(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterIndexDesc.setSpec(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableAlterPartDesc.AlterTableAlterPartDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableAlterPartDesc.AlterTableAlterPartDesc(String,FieldSchema)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableAlterPartDesc.getPartKeySpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableAlterPartDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableAlterPartDesc.setPartKeySpec(FieldSchema)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableAlterPartDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableDesc(AlterTableTypes)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableDesc(AlterTableTypes,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableDesc(String,HashMap<String, String>,int)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableDesc(String,List<FieldSchema>,AlterTableTypes)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableDesc(String,Map<List<String>, String>,HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableDesc(String,String,HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableDesc(String,String,String,String,String,HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableDesc(String,String,String,String,String,boolean,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableDesc(String,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableDesc(String,boolean,HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableDesc(String,boolean,List<String>,List<List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableDesc(String,int,List<String>,List<Order>,HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableTypes.AlterTableTypes(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.AlterTableTypes.getName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getAfterCol()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getAlterTableTypeString()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getBucketColumns()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getExpectView()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getFirst()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getIsDropIfExists()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getNewColComment()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getNewColName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getNewColType()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getNewCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getNewColsString()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getNewLocation()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getNewName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getNumberBuckets()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getOldColName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getOldName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getOp()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getOutputFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getProps()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getProtectModeType()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getSerdeName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getSkewedColNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getSkewedColValues()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getSkewedLocations()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getSortColumns()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getStorageHandler()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.getTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.isProtectModeEnable()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.isStoredAsSubDirectories()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.isTurnOffSkewed()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.isTurnOffSorting()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setAfterCol(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setBucketColumns(ArrayList<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setDropIfExists(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setExpectView(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setFirst(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setInputFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setNewColComment(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setNewColName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setNewColType(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setNewCols(ArrayList<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setNewLocation(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setNewName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setNumberBuckets(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setOldColName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setOldName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setOp(AlterTableTypes)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setOutputFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setPartSpec(HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setProps(HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setProtectModeEnable(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setProtectModeType(ProtectModeType)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setSerdeName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setSkewedColNames(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setSkewedColValues(List<List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setSkewedLocations(Map<List<String>, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setSortColumns(ArrayList<Order>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setStorageHandler(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setStoredAsSubDirectories(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setTable(Table)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.setTurnOffSkewed(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableDesc.validate()",1,2,2
"org.apache.hadoop.hive.ql.plan.AlterTableExchangePartition.AlterTableExchangePartition(Table,Table,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableExchangePartition.getDestinationTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableExchangePartition.getPartitionSpecs()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableExchangePartition.getSourceTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableExchangePartition.setDestinationTable(Table)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableExchangePartition.setPartitionSpecs(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableExchangePartition.setSourceTable(Table)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.AlterTableSimpleDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.AlterTableSimpleDesc(String,LinkedHashMap<String, String>,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.AlterTableSimpleDesc(String,Map<String, String>,AlterTableTypes)",1,1,2
"org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.getCompactionType()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.getType()",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.setPartSpec(LinkedHashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.AlterTableSimpleDesc.setType(AlterTableTypes)",1,1,1
"org.apache.hadoop.hive.ql.plan.ArchiveWork.ArchiveWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.ArchiveWork.getType()",1,1,1
"org.apache.hadoop.hive.ql.plan.ArchiveWork.setType(ArchiveActionType)",1,1,1
"org.apache.hadoop.hive.ql.plan.BaseWork.BaseWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.BaseWork.BaseWork(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.BaseWork.addDummyOp(HashTableDummyOperator)",1,1,2
"org.apache.hadoop.hive.ql.plan.BaseWork.getAllOperators()",1,3,3
"org.apache.hadoop.hive.ql.plan.BaseWork.getDummyOps()",1,1,1
"org.apache.hadoop.hive.ql.plan.BaseWork.getName()",1,1,1
"org.apache.hadoop.hive.ql.plan.BaseWork.getScratchColumnMap()",1,1,1
"org.apache.hadoop.hive.ql.plan.BaseWork.getScratchColumnVectorTypes()",1,1,1
"org.apache.hadoop.hive.ql.plan.BaseWork.getVectorMode()",1,1,1
"org.apache.hadoop.hive.ql.plan.BaseWork.isGatheringStats()",1,1,1
"org.apache.hadoop.hive.ql.plan.BaseWork.setDummyOps(List<HashTableDummyOperator>)",1,1,1
"org.apache.hadoop.hive.ql.plan.BaseWork.setGatheringStats(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.BaseWork.setName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.BaseWork.setScratchColumnMap(Map<String, Map<String, Integer>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.BaseWork.setScratchColumnVectorTypes(Map<String, Map<Integer, String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.BaseWork.setVectorMode(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.BucketMapJoinContext()",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.BucketMapJoinContext(MapJoinDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.createFileId(String)",2,2,2
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.createFileName(String,String)",2,2,2
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.deriveBucketMapJoinMapping()",1,5,5
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.getAliasBucketBaseFileNameMapping()",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.getAliasBucketFileNameMapping()",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.getBaseFileName(String)",1,2,3
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.getBigTablePartSpecToFileMapping()",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.getBucketFileNameMapping()",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.getBucketMatcherClass()",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.getMapJoinBigTableAlias()",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.prependPartSpec(String,String)",1,4,4
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.revert(Map<String, List<String>>)",1,3,3
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.setAliasBucketBaseFileNameMapping(Map<String, Map<String, List<String>>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.setAliasBucketFileNameMapping(Map<String, Map<String, List<String>>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.setBigTablePartSpecToFileMapping(Map<String, List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.setBucketFileNameMapping(Map<String, Integer>)",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.setBucketMatcherClass(Class<? extends BucketMatcher>)",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.setMapJoinBigTableAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.toString()",2,2,2
"org.apache.hadoop.hive.ql.plan.ColStatistics.ColStatistics()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.ColStatistics(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.clone()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.getAvgColLen()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.getColumnName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.getColumnType()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.getCountDistint()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.getFullyQualifiedColName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.getNumFalses()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.getNumNulls()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.getNumTrues()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.getTableAlias()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.setAvgColLen(double)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.setColumnName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.setColumnType(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.setCountDistint(long)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.setFullyQualifiedColName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.setNumFalses(long)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.setNumNulls(long)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.setNumTrues(long)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.setTableAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColStatistics.toString()",1,1,1
"org.apache.hadoop.hive.ql.plan.CollectDesc.CollectDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.CollectDesc.CollectDesc(Integer)",1,1,1
"org.apache.hadoop.hive.ql.plan.CollectDesc.getBufferSize()",1,1,1
"org.apache.hadoop.hive.ql.plan.CollectDesc.setBufferSize(Integer)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.ColumnStatsDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.ColumnStatsDesc(String,List<String>,List<String>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.getColName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.getColType()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.isTblLevel()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.setColName(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.setColType(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsDesc.setTblLevel(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsUpdateWork.ColumnStatsUpdateWork(ColumnStatsDesc,String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsUpdateWork.getColStats()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsUpdateWork.getMapProp()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsUpdateWork.getPartName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsUpdateWork.toString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsWork.ColumnStatsWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsWork.ColumnStatsWork(FetchWork,ColumnStatsDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsWork.getColStats()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsWork.getLeastNumRows()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsWork.getLimit()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsWork.getSink()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsWork.getfWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsWork.initializeForFetch()",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsWork.setColStats(ColumnStatsDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsWork.setfWork(FetchWork)",1,1,1
"org.apache.hadoop.hive.ql.plan.ColumnStatsWork.toString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoin()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoinCtx.ConditionalResolverCommonJoinCtx()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoinCtx.clone()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoinCtx.getAliasToKnownSize()",1,1,2
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoinCtx.getCommonJoinTask()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoinCtx.getHdfsTmpDir()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoinCtx.getLocalTmpDir()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoinCtx.getPathToAliases()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoinCtx.getTaskToAliases()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoinCtx.setAliasToKnownSize(HashMap<String, Long>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoinCtx.setCommonJoinTask(Task<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoinCtx.setHdfsTmpDir(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoinCtx.setLocalTmpDir(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoinCtx.setPathToAliases(HashMap<String, ArrayList<String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.ConditionalResolverCommonJoinCtx.setTaskToAliases(HashMap<Task<? extends Serializable>, Set<String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.getParticipants(ConditionalResolverCommonJoinCtx)",1,2,2
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.getTasks(HiveConf,Object)",1,3,3
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.resolveDriverAlias(ConditionalResolverCommonJoinCtx,HiveConf)",1,2,2
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.resolveMapJoinTask(ConditionalResolverCommonJoinCtx,HiveConf)",4,3,7
"org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.resolveUnknownSizes(ConditionalResolverCommonJoinCtx,HiveConf)",4,10,10
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.AverageSize.AverageSize(long,int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.AverageSize.getNumFiles()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.AverageSize.getTotalSize()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.ConditionalResolverMergeFiles()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.ConditionalResolverMergeFilesCtx.ConditionalResolverMergeFilesCtx()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.ConditionalResolverMergeFilesCtx.ConditionalResolverMergeFilesCtx(List<Task<? extends Serializable>>,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.ConditionalResolverMergeFilesCtx.getDPCtx()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.ConditionalResolverMergeFilesCtx.getDir()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.ConditionalResolverMergeFilesCtx.getLbCtx()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.ConditionalResolverMergeFilesCtx.getListTasks()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.ConditionalResolverMergeFilesCtx.setDPCtx(DynamicPartitionCtx)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.ConditionalResolverMergeFilesCtx.setDir(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.ConditionalResolverMergeFilesCtx.setLbCtx(ListBucketingCtx)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.ConditionalResolverMergeFilesCtx.setListTasks(List<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.generateActualTasks(HiveConf,List<Task<? extends Serializable>>,long,long,Task<? extends Serializable>,Task<? extends Serializable>,Task<? extends Serializable>,Path,FileSystem,ConditionalResolverMergeFilesCtx,MapWork,int)",1,7,8
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.generateDPFullPartSpec(DynamicPartitionCtx,FileStatus[],TableDesc,int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.getAverageSize(FileSystem,Path)",4,3,5
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.getMergeSize(FileSystem,Path,long)",4,2,4
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.getTasks(HiveConf,Object)",1,10,10
"org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.setupMapRedWork(HiveConf,MapWork,long,long)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.ConditionalResolverSkewJoin()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.ConditionalResolverSkewJoinCtx.ConditionalResolverSkewJoinCtx()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.ConditionalResolverSkewJoinCtx.ConditionalResolverSkewJoinCtx(HashMap<Path, Task<? extends Serializable>>,Task<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.ConditionalResolverSkewJoinCtx.getDirToTaskMap()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.ConditionalResolverSkewJoinCtx.getNoSkewTask()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.ConditionalResolverSkewJoinCtx.setDirToTaskMap(HashMap<Path, Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.ConditionalResolverSkewJoinCtx.setNoSkewTask(Task<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.getTasks(HiveConf,Object)",1,8,9
"org.apache.hadoop.hive.ql.plan.ConditionalWork.ConditionalWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalWork.ConditionalWork(List<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalWork.getListWorks()",1,1,1
"org.apache.hadoop.hive.ql.plan.ConditionalWork.setListWorks(List<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CopyWork.CopyWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.CopyWork.CopyWork(Path,Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.CopyWork.CopyWork(Path,Path,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CopyWork.getFromPath()",1,1,1
"org.apache.hadoop.hive.ql.plan.CopyWork.getToPath()",1,1,1
"org.apache.hadoop.hive.ql.plan.CopyWork.isErrorOnSrcEmpty()",1,1,1
"org.apache.hadoop.hive.ql.plan.CopyWork.setErrorOnSrcEmpty(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.CreateDatabaseDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.CreateDatabaseDesc(String,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.CreateDatabaseDesc(String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.getComment()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.getDatabaseProperties()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.getIfNotExists()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.getLocationUri()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.getName()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.setComment(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.setDatabaseProperties(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.setIfNotExists(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.setLocationUri(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateDatabaseDesc.setName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateFunctionDesc.CreateFunctionDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateFunctionDesc.CreateFunctionDesc(String,boolean,String,List<ResourceUri>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateFunctionDesc.getClassName()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateFunctionDesc.getFunctionName()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateFunctionDesc.getResources()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateFunctionDesc.isTemp()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateFunctionDesc.setClassName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateFunctionDesc.setFunctionName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateFunctionDesc.setResources(List<ResourceUri>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateFunctionDesc.setTemp(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.CreateIndexDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.CreateIndexDesc(String,String,List<String>,String,boolean,String,String,String,String,String,Map<String, String>,Map<String, String>,String,Map<String, String>,String,String,String,String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getCollItemDelim()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getDeferredRebuild()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getFieldDelim()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getFieldEscape()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getIdxProps()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getIndexComment()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getIndexName()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getIndexTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getIndexTypeHandlerClass()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getIndexedCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getLineDelim()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getLocation()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getMapKeyDelim()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getOutputFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getSerde()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getSerdeProps()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getStorageHandler()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.getTblProps()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.isDeferredRebuild()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setCollItemDelim(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setDeferredRebuild(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setFieldDelim(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setFieldEscape(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setIdxProps(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setIndexComment(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setIndexName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setIndexTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setIndexTypeHandlerClass(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setIndexedCols(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setInputFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setLineDelim(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setLocation(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setMapKeyDelim(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setOutputFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setSerde(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setSerdeProps(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setStorageHandler(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateIndexDesc.setTblProps(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateMacroDesc.CreateMacroDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateMacroDesc.CreateMacroDesc(String,List<String>,List<TypeInfo>,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateMacroDesc.getBody()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateMacroDesc.getColNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateMacroDesc.getColTypes()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateMacroDesc.getMacroName()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.CreateTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.CreateTableDesc(String,String,boolean,boolean,List<FieldSchema>,List<FieldSchema>,List<String>,List<Order>,int,String,String,String,String,String,String,String,String,String,String,String,Map<String, String>,Map<String, String>,boolean,List<String>,List<List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.CreateTableDesc(String,boolean,boolean,List<FieldSchema>,List<FieldSchema>,List<String>,List<Order>,int,String,String,String,String,String,String,String,String,String,String,String,Map<String, String>,Map<String, String>,boolean,List<String>,List<List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.copyList(List<T>)",1,1,2
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getBucketCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getCollItemDelim()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getColsString()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getComment()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getDatabaseName()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getFieldDelim()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getFieldEscape()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getIfNotExists()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getLineDelim()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getLocation()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getMapKeyDelim()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getNullFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getNumBuckets()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getNumBucketsExplain()",2,1,2
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getOutputFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getPartCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getPartColsString()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getSerName()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getSerdeProps()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getSkewedColNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getSkewedColValues()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getSortCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getStorageHandler()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.getTblProps()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.isExternal()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.isStoredAsSubDirectories()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.isTemporary()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setBucketCols(ArrayList<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setCollItemDelim(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setCols(ArrayList<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setComment(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setExternal(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setFieldDelim(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setFieldEscape(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setIfNotExists(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setInputFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setLineDelim(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setLocation(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setMapKeyDelim(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setNullFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setNumBuckets(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setOutputFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setPartCols(ArrayList<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setSerName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setSerdeProps(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setSkewedColNames(ArrayList<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setSkewedColValues(ArrayList<List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setSortCols(ArrayList<Order>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setStorageHandler(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setStoredAsSubDirectories(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setTblProps(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.setTemporary(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableDesc.validate(HiveConf)",20,21,23
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.CreateTableLikeDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.CreateTableLikeDesc(String,boolean,boolean,String,String,String,String,Map<String, String>,Map<String, String>,boolean,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.getDefaultInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.getDefaultOutputFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.getDefaultSerName()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.getDefaultSerdeProps()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.getIfNotExists()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.getLikeTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.getLocation()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.getTblProps()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.isExternal()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.isTemporary()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.setDefaultSerName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.setDefaultSerdeProps(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.setExternal(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.setIfNotExists(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.setInputFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.setLikeTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.setLocation(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.setOutputFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.setTblProps(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateTableLikeDesc.setTemporary(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.CreateViewDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.CreateViewDesc(String,List<FieldSchema>,String,Map<String, String>,List<String>,boolean,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.getComment()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.getIfNotExists()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.getIsAlterViewAs()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.getOrReplace()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.getPartColNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.getPartCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.getPartColsString()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.getSchemaString()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.getTblProps()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.getViewExpandedText()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.getViewName()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.getViewOriginalText()",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.setComment(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.setIfNotExists(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.setIsAlterViewAs(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.setOrReplace(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.setPartColNames(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.setPartCols(List<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.setSchema(List<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.setTblProps(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.setViewExpandedText(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.setViewName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.CreateViewDesc.setViewOriginalText(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(AlterIndexDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,AddPartitionDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,AlterDatabaseDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,AlterIndexDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,AlterTableAlterPartDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,AlterTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,AlterTableExchangePartition)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,AlterTablePartMergeFilesDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,AlterTableSimpleDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,CreateDatabaseDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,CreateIndexDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,CreateTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,CreateTableLikeDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,CreateViewDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,DescDatabaseDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,DescFunctionDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,DescTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,DropDatabaseDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,DropIndexDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,DropTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,GrantDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,GrantRevokeRoleDDL)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,LockDatabaseDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,LockTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,MsckDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,RenamePartitionDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,RevokeDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,RoleDDLDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,ShowColumnsDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,ShowCompactionsDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,ShowConfDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,ShowCreateTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,ShowDatabasesDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,ShowFunctionsDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,ShowGrantDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,ShowIndexesDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,ShowLocksDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,ShowPartitionsDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,ShowTableStatusDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,ShowTablesDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,ShowTblPropertiesDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,ShowTxnsDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,SwitchDatabaseDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,TruncateTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,UnlockDatabaseDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.DDLWork(HashSet<ReadEntity>,HashSet<WriteEntity>,UnlockTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getAddPartitionDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getAlterDatabaseDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getAlterIndexDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getAlterTableAlterPartDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getAlterTableExchangePartition()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getAlterTblDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getAlterTblSimpleDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getCreateDatabaseDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getCreateIndexDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getCreateTblDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getCreateTblLikeDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getCreateViewDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getCreateVwDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getDescDatabaseDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getDescFunctionDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getDescTblDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getDropDatabaseDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getDropIdxDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getDropTblDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getGrantDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getGrantRevokeRoleDDL()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getInputs()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getLockDatabaseDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getLockTblDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getMergeFilesDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getMsckDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getNeedLock()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getOutputs()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getRenamePartitionDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getRevokeDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getRoleDDLDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getShowColumnsDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getShowCompactionsDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getShowConfDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getShowCreateTblDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getShowDatabasesDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getShowFuncsDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getShowGrantDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getShowIndexesDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getShowLocksDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getShowPartsDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getShowTblPropertiesDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getShowTblStatusDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getShowTblsDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getShowTxnsDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getSwitchDatabaseDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getTruncateTblDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getUnlockDatabaseDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.getUnlockTblDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setAddPartitionDesc(AddPartitionDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setAlterDatabaseDesc(AlterDatabaseDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setAlterIndexDesc(AlterIndexDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setAlterTableAlterPartDesc(AlterTableAlterPartDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setAlterTableExchangePartition(AlterTableExchangePartition)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setAlterTblDesc(AlterTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setAlterTblSimpleDesc(AlterTableSimpleDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setCreateDatabaseDesc(CreateDatabaseDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setCreateIndexDesc(CreateIndexDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setCreateTblDesc(CreateTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setCreateTblLikeDesc(CreateTableLikeDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setCreateViewDesc(CreateViewDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setCreateVwDesc(CreateViewDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setDescFuncDesc(DescFunctionDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setDescFunctionDesc(DescFunctionDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setDescTblDesc(DescTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setDropDatabaseDesc(DropDatabaseDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setDropIdxDesc(DropIndexDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setDropTblDesc(DropTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setGrantDesc(GrantDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setGrantRevokeRoleDDL(GrantRevokeRoleDDL)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setInputs(HashSet<ReadEntity>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setLockDatabaseDesc(LockDatabaseDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setLockTblDesc(LockTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setMergeFilesDesc(AlterTablePartMergeFilesDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setMsckDesc(MsckDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setNeedLock(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setOutputs(HashSet<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setRenamePartitionDesc(RenamePartitionDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setRevokeDesc(RevokeDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setRoleDDLDesc(RoleDDLDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setShowColumnsDesc(ShowColumnsDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setShowCompactionsDesc(ShowCompactionsDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setShowConfDesc(ShowConfDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setShowCreateTblDesc(ShowCreateTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setShowDatabasesDesc(ShowDatabasesDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setShowFuncsDesc(ShowFunctionsDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setShowGrantDesc(ShowGrantDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setShowIndexesDesc(ShowIndexesDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setShowLocksDesc(ShowLocksDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setShowPartsDesc(ShowPartitionsDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setShowTblPropertiesDesc(ShowTblPropertiesDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setShowTblStatusDesc(ShowTableStatusDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setShowTblsDesc(ShowTablesDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setShowTxnsDesc(ShowTxnsDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setSwitchDatabaseDesc(SwitchDatabaseDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setTruncateTblDesc(TruncateTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setUnlockDatabaseDesc(UnlockDatabaseDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DDLWork.setUnlockTblDesc(UnlockTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.DemuxDesc.DemuxDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DemuxDesc.DemuxDesc(Map<Integer, Integer>,Map<Integer, Integer>,Map<Integer, Integer>,List<TableDesc>,List<TableDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DemuxDesc.getChildIndexToOriginalNumParents()",1,1,1
"org.apache.hadoop.hive.ql.plan.DemuxDesc.getKeysSerializeInfos()",1,1,1
"org.apache.hadoop.hive.ql.plan.DemuxDesc.getNewTagToChildIndex()",1,1,1
"org.apache.hadoop.hive.ql.plan.DemuxDesc.getNewTagToOldTag()",1,1,1
"org.apache.hadoop.hive.ql.plan.DemuxDesc.getValuesSerializeInfos()",1,1,1
"org.apache.hadoop.hive.ql.plan.DemuxDesc.setChildIndexToOriginalNumParents(Map<Integer, Integer>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DemuxDesc.setKeysSerializeInfos(List<TableDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DemuxDesc.setNewTagToChildIndex(Map<Integer, Integer>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DemuxDesc.setNewTagToOldTag(Map<Integer, Integer>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DemuxDesc.setValuesSerializeInfos(List<TableDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DependencyCollectionWork.DependencyCollectionWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescDatabaseDesc.DescDatabaseDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescDatabaseDesc.DescDatabaseDesc(Path,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescDatabaseDesc.getDatabaseName()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescDatabaseDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescDatabaseDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescDatabaseDesc.isExt()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescDatabaseDesc.setDatabaseName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescDatabaseDesc.setExt(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescDatabaseDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescFunctionDesc.DescFunctionDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescFunctionDesc.DescFunctionDesc(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescFunctionDesc.DescFunctionDesc(Path,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescFunctionDesc.getName()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescFunctionDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescFunctionDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescFunctionDesc.getTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescFunctionDesc.isExtended()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescFunctionDesc.setExtended(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescFunctionDesc.setName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescFunctionDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.DescTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.DescTableDesc(Path,String,Map<String, String>,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.getColumnPath()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.getSchema(boolean)",2,1,2
"org.apache.hadoop.hive.ql.plan.DescTableDesc.getTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.isExt()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.isFormatted()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.isPretty()",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.setColPath(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.setExt(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.setFormatted(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.setPartSpec(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.setPartSpecs(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.setPretty(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DescTableDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropDatabaseDesc.DropDatabaseDesc(String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropDatabaseDesc.DropDatabaseDesc(String,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropDatabaseDesc.getDatabaseName()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropDatabaseDesc.getIfExists()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropDatabaseDesc.isCasdade()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropDatabaseDesc.setDatabaseName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropDatabaseDesc.setIfExists(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropDatabaseDesc.setIsCascade(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropFunctionDesc.DropFunctionDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropFunctionDesc.DropFunctionDesc(String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropFunctionDesc.getFunctionName()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropFunctionDesc.isTemp()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropFunctionDesc.setFunctionName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropFunctionDesc.setTemp(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropIndexDesc.DropIndexDesc(String,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropIndexDesc.getIndexName()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropIndexDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropIndexDesc.setIndexName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropIndexDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropMacroDesc.DropMacroDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropMacroDesc.DropMacroDesc(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropMacroDesc.getMacroName()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropTableDesc.DropTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropTableDesc.DropTableDesc(String,Map<Integer, List<ExprNodeGenericFuncDesc>>,boolean,boolean)",1,3,3
"org.apache.hadoop.hive.ql.plan.DropTableDesc.DropTableDesc(String,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropTableDesc.PartSpec.PartSpec(ExprNodeGenericFuncDesc,int)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropTableDesc.PartSpec.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropTableDesc.PartSpec.getPrefixLength()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropTableDesc.getExpectView()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropTableDesc.getIfExists()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropTableDesc.getIgnoreProtection()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropTableDesc.getPartSpecs()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropTableDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.DropTableDesc.setExpectView(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropTableDesc.setIfExists(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropTableDesc.setIgnoreProtection(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.DropTableDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DummyStoreDesc.DummyStoreDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.DummyStoreDesc.clone()",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.DynamicPartitionCtx()",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.DynamicPartitionCtx(DynamicPartitionCtx)",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.DynamicPartitionCtx(Table,Map<String, String>,String,int)",1,4,4
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.getDPColNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.getDefaultPartitionName()",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.getInputToDPCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.getMaxPartitionsPerNode()",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.getNumBuckets()",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.getNumDPCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.getNumSPCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.getRootPath()",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.getSPColNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.getSPPath()",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.mapInputToDP(List<ColumnInfo>)",1,3,3
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.setDPColNames(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.setDefaultPartitionName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.setInputToDPCols(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.setMaxPartitionsPerNode(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.setNumBuckets(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.setNumDPCols(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.setNumSPCols(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.setPartSpec(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.setRootPath(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.setSPColNames(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx.setSPPath(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainSQRewriteWork.ExplainSQRewriteWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainSQRewriteWork.ExplainSQRewriteWork(String,QB,ASTNode,Context)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainSQRewriteWork.getAst()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainSQRewriteWork.getCtx()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainSQRewriteWork.getQb()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainSQRewriteWork.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.ExplainWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.ExplainWork(Path,ParseContext,List<Task<? extends Serializable>>,Task<? extends Serializable>,String,BaseSemanticAnalyzer,boolean,boolean,boolean,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.getAnalyzer()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.getAstStringTree()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.getDependency()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.getExtended()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.getFetchTask()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.getInputs()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.getParseContext()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.getRootTasks()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.isAppendTaskType()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.isAuthorize()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.isFormatted()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.isLogical()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.setAppendTaskType(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.setAstStringTree(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.setAuthorize(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.setDependency(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.setExtended(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.setFetchTask(Task<? extends Serializable>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.setFormatted(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.setInputs(HashSet<ReadEntity>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.setLogical(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.setParseContext(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.setResFile(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplainWork.setRootTasks(ArrayList<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplosionDesc.ExplosionDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplosionDesc.ExplosionDesc(String,int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplosionDesc.getFieldName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplosionDesc.getPosition()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplosionDesc.setFieldName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExplosionDesc.setPosition(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.ExprNodeColumnDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.ExprNodeColumnDesc(Class<?>,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.ExprNodeColumnDesc(TypeInfo,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.ExprNodeColumnDesc(TypeInfo,String,String,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.clone()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.getCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.getColumn()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.getExprString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.getIsPartitionColOrVirtualCol()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.getTabAlias()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.isSame(Object)",6,2,7
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.isSkewedCol()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.setColumn(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.setIsPartitionColOrVirtualCol(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.setSkewedCol(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.setTabAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.toString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnListDesc.addColumn(ExprNodeColumnDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnListDesc.clone()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnListDesc.getChildren()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnListDesc.getCols()",1,2,2
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnListDesc.getTypeInfo()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnListDesc.getTypeString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnListDesc.getWritableObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnListDesc.isSame(Object)",2,2,2
"org.apache.hadoop.hive.ql.plan.ExprNodeColumnListDesc.setTypeInfo(TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.ExprNodeConstantDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.ExprNodeConstantDesc(Object)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.ExprNodeConstantDesc(TypeInfo,Object)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.clone()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.getExprString()",4,3,5
"org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.getFoldedFromCol()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.getValue()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.getWritableObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.isSame(Object)",6,2,6
"org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.setFoldedFromCol(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.setValue(Object)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.toString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.ExprNodeDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.ExprNodeDesc(TypeInfo)",2,1,2
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.ExprNodeDescEqualityWrapper.ExprNodeDescEqualityWrapper(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.ExprNodeDescEqualityWrapper.equals(Object)",2,1,3
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.ExprNodeDescEqualityWrapper.getExprNodeDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.ExprNodeDescEqualityWrapper.hashCode()",1,2,2
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.ExprNodeDescEqualityWrapper.setExprNodeDesc(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.getChildren()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.getCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.getExprString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.getName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.getTypeInfo()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.getTypeString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.getWritableObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDesc.setTypeInfo(TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.backtrack(ExprNodeColumnDesc,Operator<?>,Operator<?>)",2,3,3
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.backtrack(ExprNodeDesc,Operator<?>,Operator<?>)",5,4,5
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.backtrack(List<ExprNodeDesc>,Operator<?>,Operator<?>)",1,2,2
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.clone(List<ExprNodeDesc>)",1,2,2
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.containsPredicate(ExprNodeDesc,ExprNodeDesc)",4,3,5
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.extractColumn(ExprNodeDesc)",3,2,3
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.extractComparePair(ExprNodeDesc,ExprNodeDesc)",5,3,11
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.extractConstant(ExprNodeDesc)",2,1,3
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.extractFields(ExprNodeDesc,List<String>)",3,2,3
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.extractFields(ExprNodeFieldDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.foldConstant(ExprNodeGenericFuncDesc)",10,8,14
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.getSingleParent(Operator<?>,Operator<?>)",6,4,8
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.indexOf(ExprNodeDesc,List<ExprNodeDesc>)",3,2,3
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.isDeterministic(ExprNodeDesc)",6,4,6
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.mergePredicates(ExprNodeDesc,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.mergePredicates(List<ExprNodeDesc>)",3,2,3
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.recommendInputName(ExprNodeDesc)",3,5,5
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.replace(ExprNodeDesc,List<ExprNodeDesc>,List<ExprNodeDesc>)",7,5,9
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.split(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.split(ExprNodeDesc,List<ExprNodeDesc>)",2,4,4
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.ExprNodeFieldDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.ExprNodeFieldDesc(TypeInfo,ExprNodeDesc,String,Boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.clone()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.getChildren()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.getCols()",1,2,2
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.getDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.getExprString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.getFieldName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.getIsList()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.isSame(Object)",4,3,6
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.setDesc(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.setFieldName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.setIsList(Boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.toString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.ExprNodeGenericFuncDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.ExprNodeGenericFuncDesc(ObjectInspector,GenericUDF,List<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.ExprNodeGenericFuncDesc(ObjectInspector,GenericUDF,String,List<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.ExprNodeGenericFuncDesc(TypeInfo,GenericUDF,List<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.ExprNodeGenericFuncDesc(TypeInfo,GenericUDF,String,List<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.clone()",1,2,2
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.getChildren()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.getCols()",1,3,3
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.getExprString()",1,2,2
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.getFuncText()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.getGenericUDF()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.getWritableObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.hashCode()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.isSame(Object)",8,6,11
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.isSortedExpr()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(GenericUDF,List<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(GenericUDF,String,List<ExprNodeDesc>)",8,17,19
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.setChildren(List<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.setGenericUDF(GenericUDF)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.setSortedExpr(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.toString()",1,3,3
"org.apache.hadoop.hive.ql.plan.ExprNodeNullDesc.ExprNodeNullDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeNullDesc.clone()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeNullDesc.getExprString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeNullDesc.getValue()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExprNodeNullDesc.isSame(Object)",3,1,3
"org.apache.hadoop.hive.ql.plan.ExprNodeNullDesc.toString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExtractDesc.ExtractDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExtractDesc.ExtractDesc(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ExtractDesc.getCol()",1,1,1
"org.apache.hadoop.hive.ql.plan.ExtractDesc.setCol(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.FetchWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.FetchWork(List<List<Object>>,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.FetchWork(List<Path>,List<PartitionDesc>,TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.FetchWork(List<Path>,List<PartitionDesc>,TableDesc,int)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.FetchWork(Path,TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.FetchWork(Path,TableDesc,int)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.getLeastNumRows()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.getLimit()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.getPartDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.getPartDescOrderedByPartDir()",3,5,6
"org.apache.hadoop.hive.ql.plan.FetchWork.getPartDescs(List<Path>)",1,2,2
"org.apache.hadoop.hive.ql.plan.FetchWork.getPartDir()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.getRowsComputedUsingStats()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.getSerializationNullFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.getSink()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.getSource()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.getSplitSample()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.getStatRowOI()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.getTblDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.getTblDir()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.initializeForFetch()",1,2,2
"org.apache.hadoop.hive.ql.plan.FetchWork.isNotPartitioned()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.isPartitioned()",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.setLeastNumRows(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.setLimit(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.setPartDesc(ArrayList<PartitionDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.setPartDir(ArrayList<Path>)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.setSerializationNullFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.setSink(ListSinkOperator)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.setSource(Operator<?>)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.setSplitSample(SplitSample)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.setTblDesc(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.setTblDir(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.FetchWork.toString()",3,2,4
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.FileSinkDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.FileSinkDesc(Path,TableDesc,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.FileSinkDesc(Path,TableDesc,boolean,int,boolean,boolean,int,int,ArrayList<ExprNodeDesc>,DynamicPartitionCtx)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.canBeMerged()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.clone()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getCompressCodec()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getCompressType()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getCompressed()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getDestTableId()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getDirName()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getDpSortState()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getDynPartCtx()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getFinalDirName()",1,1,2
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getLbCtx()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getLinkedFileSinkDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getMaxStatsKeyPrefixLength()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getNumFiles()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getParentDir()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getPartitionCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getStaticSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getStatsAggPrefix()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getTableInfo()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.getTotalFiles()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.isGatherStats()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.isLinkedFileSink()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.isMultiFileSpray()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.isRemovedReduceSinkBucketSort()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.isStatsCollectRawDataSize()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.isStatsReliable()",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setCanBeMerged(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setCompressCodec(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setCompressType(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setCompressed(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setDestTableId(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setDirName(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setDpSortState(DPSortState)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setDynPartCtx(DynamicPartitionCtx)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setGatherStats(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setLbCtx(ListBucketingCtx)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setLinkedFileSink(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setLinkedFileSinkDesc(List<FileSinkDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setMaxStatsKeyPrefixLength(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setMultiFileSpray(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setNumFiles(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setParentDir(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setPartitionCols(ArrayList<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setRemovedReduceSinkBucketSort(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setStaticSpec(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setStatsAggPrefix(String)",1,1,2
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setStatsCollectRawDataSize(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setStatsReliable(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setTableInfo(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.FileSinkDesc.setTotalFiles(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.FilterDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.FilterDesc(ExprNodeDesc,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.FilterDesc(ExprNodeDesc,boolean,sampleDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.clone()",1,2,2
"org.apache.hadoop.hive.ql.plan.FilterDesc.getIsSamplingPred()",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.getPredicate()",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.getPredicateString()",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.getSampleDescr()",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.isSortedFilter()",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.sampleDesc.clone()",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.sampleDesc.getDenominator()",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.sampleDesc.getInputPruning()",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.sampleDesc.getNumerator()",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.sampleDesc.sampleDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.sampleDesc.sampleDesc(int,int,List<String>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.setIsSamplingPred(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.setPredicate(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.setSampleDescr(sampleDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.FilterDesc.setSortedFilter(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ForwardDesc.ForwardDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ForwardDesc.clone()",1,1,1
"org.apache.hadoop.hive.ql.plan.FunctionWork.FunctionWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.FunctionWork.FunctionWork(CreateFunctionDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.FunctionWork.FunctionWork(CreateMacroDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.FunctionWork.FunctionWork(DropFunctionDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.FunctionWork.FunctionWork(DropMacroDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.FunctionWork.getCreateFunctionDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.FunctionWork.getCreateMacroDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.FunctionWork.getDropFunctionDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.FunctionWork.getDropMacroDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.FunctionWork.setCreateFunctionDesc(CreateFunctionDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.FunctionWork.setDropFunctionDesc(DropFunctionDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantDesc.GrantDesc(PrivilegeObjectDesc,List<PrivilegeDesc>,List<PrincipalDesc>,String,PrincipalType,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantDesc.getGrantor()",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantDesc.getGrantorType()",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantDesc.getPrincipals()",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantDesc.getPrivilegeSubjectDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantDesc.getPrivileges()",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantDesc.isGrantOption()",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantDesc.setGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantDesc.setGrantor(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantDesc.setGrantorType(PrincipalType)",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantDesc.setPrincipals(List<PrincipalDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantDesc.setPrivilegeSubjectDesc(PrivilegeObjectDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantDesc.setPrivileges(List<PrivilegeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.GrantRevokeRoleDDL()",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.GrantRevokeRoleDDL(boolean,List<String>,List<PrincipalDesc>,String,PrincipalType,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.getGrant()",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.getGrantor()",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.getGrantorType()",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.getPrincipalDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.getRoles()",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.isGrantOption()",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.setGrant(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.setGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.setGrantor(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.setGrantorType(PrincipalType)",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.setPrincipalDesc(List<PrincipalDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.setRoles(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.GroupByDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.GroupByDesc(Mode,ArrayList<String>,ArrayList<ExprNodeDesc>,ArrayList<AggregationDesc>,boolean,boolean,float,float,List<Integer>,boolean,int,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.GroupByDesc(Mode,ArrayList<String>,ArrayList<ExprNodeDesc>,ArrayList<AggregationDesc>,boolean,float,float,List<Integer>,boolean,int,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.getAggregatorStrings()",1,2,2
"org.apache.hadoop.hive.ql.plan.GroupByDesc.getAggregators()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.getBucketGroup()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.getGroupByMemoryUsage()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.getGroupKeyNotReductionKey()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.getGroupingSetPosition()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.getKeyString()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.getKeys()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.getListGroupingSets()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.getMemoryThreshold()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.getMode()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.getModeString()",8,2,8
"org.apache.hadoop.hive.ql.plan.GroupByDesc.getOutputColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.isDistinct()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.isDistinctLike()",4,4,5
"org.apache.hadoop.hive.ql.plan.GroupByDesc.isDontResetAggrsDistinct()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.isGroupingSetsPresent()",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.setAggregators(ArrayList<AggregationDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.setBucketGroup(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.setDistinct(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.setDontResetAggrsDistinct(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.setGroupByMemoryUsage(float)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.setGroupKeyNotReductionKey(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.setGroupingSetPosition(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.setGroupingSetsPresent(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.setKeys(ArrayList<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.setListGroupingSets(List<Integer>)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.setMemoryThreshold(float)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.setMode(Mode)",1,1,1
"org.apache.hadoop.hive.ql.plan.GroupByDesc.setOutputColumnNames(ArrayList<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableDummyDesc.getTbl()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableDummyDesc.setTbl(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.HashTableSinkDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.HashTableSinkDesc(MapJoinDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getBigKeysDirMap()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getBucketMapjoinContext()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getConds()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getDumpFilePrefix()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getExprs()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getFilterMap()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getFilterMapString()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getFilters()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getHashtableMemoryUsage()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getKeyTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getKeyTblDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getKeys()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getKeysString()",1,2,2
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getOutputColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getPosBigTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getRetainList()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getReversedExprs()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getSkewKeyDefinition()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getSkewKeysValuesTables()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getSmallKeysDirMap()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getTagOrder()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getValueTblDescs()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.getValueTblFilteredDescs()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.initRetainExprList()",1,3,3
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.isHandleSkewJoin()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.isNoOuterJoin()",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setBigKeysDirMap(Map<Byte, Path>)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setBucketMapjoinContext(BucketMapJoinContext)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setConds(JoinCondDesc[])",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setDumpFilePrefix(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setExprs(Map<Byte, List<ExprNodeDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setFilterMap(int[][])",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setFilters(Map<Byte, List<ExprNodeDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setHandleSkewJoin(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setHashtableMemoryUsage(float)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setKeyTableDesc(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setKeyTblDesc(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setKeys(Map<Byte, List<ExprNodeDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setNoOuterJoin(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setOutputColumnNames(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setPosBigTable(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setRetainList(Map<Byte, List<Integer>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setReversedExprs(Map<String, Byte>)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setSkewKeyDefinition(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setSkewKeysValuesTables(Map<Byte, TableDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setSmallKeysDirMap(Map<Byte, Map<Byte, Path>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setTagOrder(Byte[])",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setValueTblDescs(List<TableDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.setValueTblFilteredDescs(List<TableDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.HiveOperation(String,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.PrivilegeAgreement.getInputColumnLevelRequiredPriv()",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.PrivilegeAgreement.getInputDBLevelRequiredPriv()",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.PrivilegeAgreement.getInputTableLevelRequiredPriv()",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.PrivilegeAgreement.getInputUserLevelRequiredPriv()",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.PrivilegeAgreement.getOutputColumnLevelRequiredPriv()",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.PrivilegeAgreement.getOutputDBLevelRequiredPriv()",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.PrivilegeAgreement.getOutputTableLevelRequiredPriv()",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.PrivilegeAgreement.getOutputUserLevelRequiredPriv()",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.PrivilegeAgreement.putColumnLevelRequiredPriv(Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.PrivilegeAgreement.putDBLevelRequiredPriv(Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.PrivilegeAgreement.putTableLevelRequiredPriv(Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.PrivilegeAgreement.putUserLevelRequiredPriv(Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.getInputRequiredPrivileges()",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.getOperationName()",1,1,1
"org.apache.hadoop.hive.ql.plan.HiveOperation.getOutputRequiredPrivileges()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinCondDesc.JoinCondDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinCondDesc.JoinCondDesc(JoinCond)",2,2,8
"org.apache.hadoop.hive.ql.plan.JoinCondDesc.JoinCondDesc(int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinCondDesc.getJoinCondString()",2,2,8
"org.apache.hadoop.hive.ql.plan.JoinCondDesc.getLeft()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinCondDesc.getPreserved()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinCondDesc.getRight()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinCondDesc.getType()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinCondDesc.setLeft(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinCondDesc.setPreserved(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinCondDesc.setRight(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinCondDesc.setType(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.JoinDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.JoinDesc(JoinDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.JoinDesc(Map<Byte, List<ExprNodeDesc>>,List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.JoinDesc(Map<Byte, List<ExprNodeDesc>>,List<String>,JoinCondDesc[])",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.JoinDesc(Map<Byte, List<ExprNodeDesc>>,List<String>,boolean,JoinCondDesc[])",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.JoinDesc(Map<Byte, List<ExprNodeDesc>>,List<String>,boolean,JoinCondDesc[],Map<Byte, List<ExprNodeDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.clone()",1,7,7
"org.apache.hadoop.hive.ql.plan.JoinDesc.compactFilter(int[][])",8,1,9
"org.apache.hadoop.hive.ql.plan.JoinDesc.convertToArray(Map<Byte, T>,Class<T>)",1,2,2
"org.apache.hadoop.hive.ql.plan.JoinDesc.getBigKeysDirMap()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getConds()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getCondsList()",2,2,3
"org.apache.hadoop.hive.ql.plan.JoinDesc.getExprs()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getExprsStringMap()",2,5,6
"org.apache.hadoop.hive.ql.plan.JoinDesc.getFilterMap()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getFilterMapString()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getFilters()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getFiltersStringMap()",3,6,9
"org.apache.hadoop.hive.ql.plan.JoinDesc.getHandleSkewJoin()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getKeyTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getNoOuterJoin()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getNullSafeString()",2,2,4
"org.apache.hadoop.hive.ql.plan.JoinDesc.getNullSafes()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getOutputColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getReversedExprs()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getSkewKeyDefinition()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getSkewKeysValuesTables()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getSmallKeysDirMap()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.getTagLength()",1,2,2
"org.apache.hadoop.hive.ql.plan.JoinDesc.getTagOrder()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.isFixedAsSorted()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.isNoOuterJoin()",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.resetOrder()",1,1,2
"org.apache.hadoop.hive.ql.plan.JoinDesc.setBigKeysDirMap(Map<Byte, Path>)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setConds(JoinCondDesc[])",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setExprs(Map<Byte, List<ExprNodeDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setFilterMap(int[][])",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setFilters(Map<Byte, List<ExprNodeDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setFixedAsSorted(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setHandleSkewJoin(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setKeyTableDesc(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setNoOuterJoin(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setNullSafes(boolean[])",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setOutputColumnNames(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setReversedExprs(Map<String, Byte>)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setSkewKeyDefinition(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setSkewKeysValuesTables(Map<Byte, TableDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setSmallKeysDirMap(Map<Byte, Map<Byte, Path>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.setTagOrder(Byte[])",1,1,1
"org.apache.hadoop.hive.ql.plan.JoinDesc.toCompactString(int[][])",4,2,5
"org.apache.hadoop.hive.ql.plan.LateralViewForwardDesc.LateralViewForwardDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.LateralViewJoinDesc.LateralViewJoinDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.LateralViewJoinDesc.LateralViewJoinDesc(int,ArrayList<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.LateralViewJoinDesc.getNumSelColumns()",1,1,1
"org.apache.hadoop.hive.ql.plan.LateralViewJoinDesc.getOutputInternalColNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.LateralViewJoinDesc.setNumSelColumns(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.LateralViewJoinDesc.setOutputInternalColNames(ArrayList<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.LimitDesc.LimitDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.LimitDesc.LimitDesc(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.LimitDesc.getLeastRows()",1,1,1
"org.apache.hadoop.hive.ql.plan.LimitDesc.getLimit()",1,1,1
"org.apache.hadoop.hive.ql.plan.LimitDesc.setLeastRows(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.LimitDesc.setLimit(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.ListBucketingCtx()",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.calculateListBucketingLevel()",1,2,2
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.calculateSkewedValueSubDirList()",1,3,3
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.getDefaultDirName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.getDefaultKey()",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.getLbLocationMap()",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.getRowSkewedIndex()",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.getSkewedColNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.getSkewedColValues()",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.getSkewedValuesDirNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.isSkewedStoredAsDir()",1,5,5
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.isStoredAsSubDirectories()",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.processRowSkewedIndex(RowSchema)",1,8,8
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.setDefaultDirName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.setDefaultKey(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.setLbLocationMap(Map<List<String>, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.setRowSkewedIndex(List<SkewedColumnPositionPair>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.setSkewedColNames(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.setSkewedColValues(List<List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.setSkewedValuesDirNames(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ListBucketingCtx.setStoredAsSubDirectories(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ListSinkDesc.ListSinkDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ListSinkDesc.ListSinkDesc(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ListSinkDesc.getSerializationNullFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.ListSinkDesc.setSerializationNullFormat(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadDesc.LoadDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadDesc.LoadDesc(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadDesc.getSourcePath()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadFileDesc.LoadFileDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadFileDesc.LoadFileDesc(CreateTableDesc,Path,Path,boolean,String,String)",1,5,5
"org.apache.hadoop.hive.ql.plan.LoadFileDesc.LoadFileDesc(Path,Path,boolean,String,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadFileDesc.getColumnTypes()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadFileDesc.getColumns()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadFileDesc.getDestinationCreateTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadFileDesc.getIsDfsDir()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadFileDesc.getTargetDir()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadFileDesc.setColumnTypes(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadFileDesc.setColumns(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadFileDesc.setIsDfsDir(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadFileDesc.setTargetDir(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.LoadMultiFilesDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.LoadMultiFilesDesc(List<Path>,List<Path>,boolean,String,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.getColumnTypes()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.getColumns()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.getIsDfsDir()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.getSourceDirs()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.getTargetDirs()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.setColumnTypes(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.setColumns(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.setIsDfsDir(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.setSourceDirs(List<Path>)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.setTargetDirs(List<Path>)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.LoadTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.LoadTableDesc(Path,TableDesc,DynamicPartitionCtx)",1,4,4
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.LoadTableDesc(Path,TableDesc,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.LoadTableDesc(Path,TableDesc,Map<String, String>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.getDPCtx()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.getHoldDDLTime()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.getInheritTableSpecs()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.getLbCtx()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.getPartitionSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.getReplace()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.getTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.init(TableDesc,Map<String, String>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.setDPCtx(DynamicPartitionCtx)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.setHoldDDLTime(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.setInheritTableSpecs(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.setLbCtx(ListBucketingCtx)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.setPartitionSpec(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.setReplace(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.LoadTableDesc.setTable(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.LockDatabaseDesc.LockDatabaseDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.LockDatabaseDesc.LockDatabaseDesc(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LockDatabaseDesc.getDatabaseName()",1,1,1
"org.apache.hadoop.hive.ql.plan.LockDatabaseDesc.getMode()",1,1,1
"org.apache.hadoop.hive.ql.plan.LockDatabaseDesc.getQueryId()",1,1,1
"org.apache.hadoop.hive.ql.plan.LockDatabaseDesc.getQueryStr()",1,1,1
"org.apache.hadoop.hive.ql.plan.LockDatabaseDesc.setDatabaseName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LockDatabaseDesc.setMode(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LockDatabaseDesc.setQueryId(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LockDatabaseDesc.setQueryStr(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LockTableDesc.LockTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.LockTableDesc.LockTableDesc(String,String,Map<String, String>,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LockTableDesc.getMode()",1,1,1
"org.apache.hadoop.hive.ql.plan.LockTableDesc.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.LockTableDesc.getQueryId()",1,1,1
"org.apache.hadoop.hive.ql.plan.LockTableDesc.getQueryStr()",1,1,1
"org.apache.hadoop.hive.ql.plan.LockTableDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.LockTableDesc.setMode(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LockTableDesc.setPartSpec(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.LockTableDesc.setQueryId(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LockTableDesc.setQueryStr(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.LockTableDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.MapJoinDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.MapJoinDesc(Map<Byte, List<ExprNodeDesc>>,TableDesc,Map<Byte, List<ExprNodeDesc>>,List<TableDesc>,List<TableDesc>,List<String>,int,JoinCondDesc[],Map<Byte, List<ExprNodeDesc>>,boolean,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.MapJoinDesc(MapJoinDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getAliasBucketFileNameMapping()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getBigTableAlias()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getBigTableBucketNumMapping()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getBigTablePartSpecToFileMapping()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getCustomBucketMapJoin()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getDumpFilePrefix()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getHashTableMemoryUsage()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getKeyCountsExplainDesc()",1,4,4
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getKeyTblDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getKeys()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getKeysString()",1,2,2
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getParentKeyCounts()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getParentToInput()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getPosBigTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getRetainList()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getValueFilteredTblDescs()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getValueIndex(byte)",1,2,2
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getValueIndices()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.getValueTblDescs()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.initRetainExprList()",1,3,3
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.isBucketMapJoin()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setAliasBucketFileNameMapping(Map<String, Map<String, List<String>>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setBigTableAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setBigTableBucketNumMapping(Map<String, Integer>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setBigTablePartSpecToFileMapping(Map<String, List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setBucketMapJoin(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setCustomBucketMapJoin(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setDumpFilePrefix(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setHashTableMemoryUsage(float)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setKeyTblDesc(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setKeys(Map<Byte, List<ExprNodeDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setParentKeyCount(Map<Integer, Long>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setParentToInput(Map<Integer, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setPosBigTable(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setRetainList(Map<Byte, List<Integer>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setValueFilteredTblDescs(List<TableDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setValueIndices(Map<Byte, int[]>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapJoinDesc.setValueTblDescs(List<TableDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.MapWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.MapWork(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.addIndexIntermediateFile(String)",1,1,2
"org.apache.hadoop.hive.ql.plan.MapWork.addMapWork(String,String,Operator<?>,PartitionDesc)",4,3,5
"org.apache.hadoop.hive.ql.plan.MapWork.configureJobConf(JobConf)",1,3,3
"org.apache.hadoop.hive.ql.plan.MapWork.deriveExplainAttributes()",1,4,4
"org.apache.hadoop.hive.ql.plan.MapWork.getAliasToPartnInfo()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getAliasToWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getAliases()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getAllRootOperators()",1,5,5
"org.apache.hadoop.hive.ql.plan.MapWork.getBucketedColsByDirectory()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getHadoopSupportsSplittable()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getIndexIntermediateFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getInputformat()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getJoinTree()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getMapLocalWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getMaxSplitSize()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getMinSplitSize()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getMinSplitSizePerNode()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getMinSplitSizePerRack()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getNameToSplitSample()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getNumMapTasks()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getOpParseCtxMap()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getPartitionDescs()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getPathToAliases()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getPathToPartitionInfo()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getPaths()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getSamplingType()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getSamplingTypeString()",1,1,3
"org.apache.hadoop.hive.ql.plan.MapWork.getSortedColsByDirectory()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getTmpHDFSPath()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.getTruncatedPathToAliases()",1,2,2
"org.apache.hadoop.hive.ql.plan.MapWork.getVectorModeOn()",1,1,2
"org.apache.hadoop.hive.ql.plan.MapWork.getWorks()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.initialize()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.isInputFormatSorted()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.isMapperCannotSpanPartns()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.isUseBucketizedHiveInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.isUseOneNullRowInputFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.logPathToAliases()",1,4,4
"org.apache.hadoop.hive.ql.plan.MapWork.mergeAliasedInput(String,String,PartitionDesc)",1,2,2
"org.apache.hadoop.hive.ql.plan.MapWork.mergingInto(MapWork)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.replaceRoots(Map<Operator<?>, Operator<?>>)",1,2,2
"org.apache.hadoop.hive.ql.plan.MapWork.resolveDynamicPartitionStoredAsSubDirsMerge(HiveConf,Path,TableDesc,ArrayList<String>,PartitionDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setAliasToPartnInfo(LinkedHashMap<String, PartitionDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setAliasToWork(LinkedHashMap<String, Operator<? extends OperatorDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setAliases()",2,2,3
"org.apache.hadoop.hive.ql.plan.MapWork.setHadoopSupportsSplittable(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setInputFormatSorted(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setInputformat(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setJoinTree(QBJoinTree)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setMapLocalWork(MapredLocalWork)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setMapperCannotSpanPartns(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setMaxSplitSize(Long)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setMinSplitSize(Long)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setMinSplitSizePerNode(Long)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setMinSplitSizePerRack(Long)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setNameToSplitSample(HashMap<String, SplitSample>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setNumMapTasks(Integer)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setOpParseCtxMap(LinkedHashMap<Operator<? extends OperatorDesc>, OpParseContext>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setPathToAliases(LinkedHashMap<String, ArrayList<String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setPathToPartitionInfo(LinkedHashMap<String, PartitionDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setSamplingType(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setTmpHDFSPath(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setUseBucketizedHiveInputFormat(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapWork.setUseOneNullRowInputFormat(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.MapredLocalWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.MapredLocalWork(LinkedHashMap<String, Operator<? extends OperatorDesc>>,LinkedHashMap<String, FetchWork>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.MapredLocalWork(MapredLocalWork)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.copyPartSpecMappingOnly()",2,3,3
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.deriveExplainAttributes()",1,3,3
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.extractDirectWorks(Map<MapJoinOperator, List<Operator<? extends OperatorDesc>>>)",2,5,5
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.getAliasToFetchWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.getAliasToWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.getBucketFileName(String)",2,4,5
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.getBucketMapjoinContext()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.getBucketMapjoinContextExplain()",1,2,3
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.getDirectFetchOp()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.getDirectWorks(Collection<List<Operator<?>>>)",1,4,4
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.getDummyParentOp()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.getFileName(String)",2,1,2
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.getInputFileChangeSensitive()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.getStageID()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.getTmpPath()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.hasStagedAlias()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.setAliasToFetchWork(LinkedHashMap<String, FetchWork>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.setAliasToWork(LinkedHashMap<String, Operator<? extends OperatorDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.setBucketMapjoinContext(BucketMapJoinContext)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.setDirectFetchOp(Map<MapJoinOperator, List<Operator<? extends OperatorDesc>>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.setDummyParentOp(List<Operator<? extends OperatorDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.setHasStagedAlias(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.setInputFileChangeSensitive(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.setStageID(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredLocalWork.setTmpPath(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredWork.configureJobConf(JobConf)",1,2,2
"org.apache.hadoop.hive.ql.plan.MapredWork.getAllOperators()",1,2,2
"org.apache.hadoop.hive.ql.plan.MapredWork.getMapWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredWork.getReduceWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredWork.isFinalMapRed()",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredWork.setFinalMapRed(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredWork.setMapWork(MapWork)",1,1,1
"org.apache.hadoop.hive.ql.plan.MapredWork.setReduceWork(ReduceWork)",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.MoveWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.MoveWork(HashSet<ReadEntity>,HashSet<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.MoveWork(HashSet<ReadEntity>,HashSet<WriteEntity>,LoadTableDesc,LoadFileDesc,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.MoveWork(HashSet<ReadEntity>,HashSet<WriteEntity>,LoadTableDesc,LoadFileDesc,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.getCheckFileFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.getInputs()",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.getLoadFileWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.getLoadMultiFilesWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.getLoadTableWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.getOutputs()",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.isSrcLocal()",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.setCheckFileFormat(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.setInputs(HashSet<ReadEntity>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.setLoadFileWork(LoadFileDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.setLoadTableWork(LoadTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.setMultiFilesDesc(LoadMultiFilesDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.setOutputs(HashSet<WriteEntity>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MoveWork.setSrcLocal(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MsckDesc.MsckDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.MsckDesc.MsckDesc(String,List<? extends Map<String, String>>,Path,boolean)",1,2,2
"org.apache.hadoop.hive.ql.plan.MsckDesc.getPartSpecs()",1,1,1
"org.apache.hadoop.hive.ql.plan.MsckDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.MsckDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.MsckDesc.isRepairPartitions()",1,1,1
"org.apache.hadoop.hive.ql.plan.MsckDesc.setPartSpecs(ArrayList<LinkedHashMap<String, String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MsckDesc.setRepairPartitions(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.MsckDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.MsckDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.MuxDesc.MuxDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.MuxDesc.MuxDesc(List<Operator<? extends OperatorDesc>>)",1,3,4
"org.apache.hadoop.hive.ql.plan.MuxDesc.getNewParentIndexToOldParentIndex()",1,1,1
"org.apache.hadoop.hive.ql.plan.MuxDesc.getParentToKeyCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.MuxDesc.getParentToOutputKeyColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.MuxDesc.getParentToOutputValueColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.MuxDesc.getParentToTag()",1,1,1
"org.apache.hadoop.hive.ql.plan.MuxDesc.getParentToValueCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.MuxDesc.setNewParentIndexToOldParentIndex(Map<Integer, Integer>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MuxDesc.setParentToKeyCols(List<List<ExprNodeDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MuxDesc.setParentToOutputKeyColumnNames(List<List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MuxDesc.setParentToOutputValueColumnNames(List<List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MuxDesc.setParentToTag(List<Integer>)",1,1,1
"org.apache.hadoop.hive.ql.plan.MuxDesc.setParentToValueCols(List<List<ExprNodeDesc>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.OpTraits.OpTraits(List<List<String>>,int)",1,1,1
"org.apache.hadoop.hive.ql.plan.OpTraits.getBucketColNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.OpTraits.getNumBuckets()",1,1,1
"org.apache.hadoop.hive.ql.plan.OpTraits.setBucketColNames(List<List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.OpTraits.setNumBuckets(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.PTFDesc.forWindowing()",1,1,2
"org.apache.hadoop.hive.ql.plan.PTFDesc.getCfg()",1,1,1
"org.apache.hadoop.hive.ql.plan.PTFDesc.getFuncDef()",1,1,1
"org.apache.hadoop.hive.ql.plan.PTFDesc.getLlInfo()",1,1,1
"org.apache.hadoop.hive.ql.plan.PTFDesc.getStartOfChain()",1,2,2
"org.apache.hadoop.hive.ql.plan.PTFDesc.isMapSide()",1,1,1
"org.apache.hadoop.hive.ql.plan.PTFDesc.setCfg(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.plan.PTFDesc.setFuncDef(PartitionedTableFunctionDef)",1,1,1
"org.apache.hadoop.hive.ql.plan.PTFDesc.setLlInfo(LeadLagInfo)",1,1,1
"org.apache.hadoop.hive.ql.plan.PTFDesc.setMapSide(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.PTFDeserializer(PTFDesc,StructObjectInspector,Configuration)",1,1,1
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.addInputColumnsToList(ShapeDetails,ArrayList<String>,ArrayList<ObjectInspector>)",1,2,2
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.addOIPropertiestoSerDePropsMap(StructObjectInspector,Map<String, String>)",2,2,5
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.alterOutputOIForStreaming(PTFDesc)",1,2,2
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.constructResolver(String)",1,1,2
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.getTypeMap(StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.initExprNodeEvaluator(ExprNodeEvaluator,ExprNodeDesc,ShapeDetails)",1,3,3
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.initialize(BoundaryDef,ShapeDetails)",1,2,2
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.initialize(PTFExpressionDef,ShapeDetails)",1,1,1
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.initialize(PTFQueryInputDef,StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.initialize(PartitionedTableFunctionDef)",1,4,4
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.initialize(ShapeDetails,StructObjectInspector)",1,2,3
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PartitionedTableFunctionDef)",1,5,5
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(WindowTableFunctionDef)",1,7,7
"org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(WindowFunctionDef)",1,3,3
"org.apache.hadoop.hive.ql.plan.PartitionDesc.PartitionDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.PartitionDesc.PartitionDesc(Partition)",1,1,1
"org.apache.hadoop.hive.ql.plan.PartitionDesc.PartitionDesc(Partition,TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.PartitionDesc.PartitionDesc(TableDesc,LinkedHashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.PartitionDesc.clone()",1,4,4
"org.apache.hadoop.hive.ql.plan.PartitionDesc.deriveBaseFileName(String)",2,1,3
"org.apache.hadoop.hive.ql.plan.PartitionDesc.getBaseFileName()",1,1,1
"org.apache.hadoop.hive.ql.plan.PartitionDesc.getDeserializer(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.plan.PartitionDesc.getDeserializerClassName()",2,1,2
"org.apache.hadoop.hive.ql.plan.PartitionDesc.getInputFileFormatClass()",1,2,3
"org.apache.hadoop.hive.ql.plan.PartitionDesc.getInputFileFormatClassName()",1,1,1
"org.apache.hadoop.hive.ql.plan.PartitionDesc.getOutputFileFormatClass()",1,2,3
"org.apache.hadoop.hive.ql.plan.PartitionDesc.getOutputFileFormatClassName()",1,1,1
"org.apache.hadoop.hive.ql.plan.PartitionDesc.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.PartitionDesc.getProperties()",2,2,3
"org.apache.hadoop.hive.ql.plan.PartitionDesc.getSerdeClassName()",1,1,1
"org.apache.hadoop.hive.ql.plan.PartitionDesc.getTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.PartitionDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.PartitionDesc.isPartitioned()",1,2,2
"org.apache.hadoop.hive.ql.plan.PartitionDesc.setBaseFileName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.PartitionDesc.setInputFileFormatClass(Class<? extends InputFormat>)",1,2,2
"org.apache.hadoop.hive.ql.plan.PartitionDesc.setOutputFileFormatClass(Class<?>)",1,3,3
"org.apache.hadoop.hive.ql.plan.PartitionDesc.setPartSpec(LinkedHashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.PartitionDesc.setProperties(Properties)",1,3,3
"org.apache.hadoop.hive.ql.plan.PartitionDesc.setTableDesc(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.PlanUtils()",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.addExprToStringBuffer(ExprNodeDesc,StringBuffer)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.addInput(Set<ReadEntity>,ReadEntity)",4,7,7
"org.apache.hadoop.hive.ql.plan.PlanUtils.addInputsForView(ParseContext)",3,2,3
"org.apache.hadoop.hive.ql.plan.PlanUtils.configureInputJobPropertiesForStorageHandler(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.configureJobConf(TableDesc,JobConf)",1,2,3
"org.apache.hadoop.hive.ql.plan.PlanUtils.configureJobPropertiesForStorageHandler(boolean,TableDesc)",2,7,9
"org.apache.hadoop.hive.ql.plan.PlanUtils.configureOutputJobPropertiesForStorageHandler(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getCountForMapJoinDumpFilePrefix()",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getDefaultQueryOutputTableDesc(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getDefaultTableDesc(CreateTableDesc,String,String)",2,11,12
"org.apache.hadoop.hive.ql.plan.PlanUtils.getDefaultTableDesc(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getDefaultTableDesc(String,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getDefaultTableDesc(String,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getDefaultTableDesc(String,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getExprListString(Collection<ExprNodeDesc>)",1,4,4
"org.apache.hadoop.hive.ql.plan.PlanUtils.getFieldSchemasFromColumnInfo(ArrayList<ColumnInfo>,String)",2,3,5
"org.apache.hadoop.hive.ql.plan.PlanUtils.getFieldSchemasFromColumnList(List<ExprNodeDesc>,List<String>,int,String)",1,2,2
"org.apache.hadoop.hive.ql.plan.PlanUtils.getFieldSchemasFromColumnList(List<ExprNodeDesc>,String)",1,2,2
"org.apache.hadoop.hive.ql.plan.PlanUtils.getFieldSchemasFromColumnListWithLength(List<ExprNodeDesc>,List<List<Integer>>,List<String>,int,String)",1,5,5
"org.apache.hadoop.hive.ql.plan.PlanUtils.getFieldSchemasFromRowSchema(RowSchema,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getIntermediateFileTableDesc(List<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getMapJoinKeyTableDesc(Configuration,List<FieldSchema>)",2,3,3
"org.apache.hadoop.hive.ql.plan.PlanUtils.getMapJoinValueTableDesc(List<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getMapRedWork()",1,1,2
"org.apache.hadoop.hive.ql.plan.PlanUtils.getParentViewInfo(String,Map<String, ReadEntity>)",3,2,4
"org.apache.hadoop.hive.ql.plan.PlanUtils.getReduceKeyTableDesc(List<FieldSchema>,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getReduceSinkDesc(ArrayList<ExprNodeDesc>,ArrayList<ExprNodeDesc>,List<String>,boolean,int,ArrayList<ExprNodeDesc>,String,int)",1,3,3
"org.apache.hadoop.hive.ql.plan.PlanUtils.getReduceSinkDesc(ArrayList<ExprNodeDesc>,ArrayList<ExprNodeDesc>,List<String>,boolean,int,int,int)",1,3,3
"org.apache.hadoop.hive.ql.plan.PlanUtils.getReduceSinkDesc(ArrayList<ExprNodeDesc>,int,ArrayList<ExprNodeDesc>,List<List<Integer>>,List<String>,List<String>,boolean,int,ArrayList<ExprNodeDesc>,String,int)",1,3,4
"org.apache.hadoop.hive.ql.plan.PlanUtils.getReduceSinkDesc(ArrayList<ExprNodeDesc>,int,ArrayList<ExprNodeDesc>,List<List<Integer>>,List<String>,List<String>,boolean,int,int,int)",1,4,4
"org.apache.hadoop.hive.ql.plan.PlanUtils.getReduceValueTableDesc(List<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getTableDesc(Class<? extends Deserializer>,String,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getTableDesc(Class<? extends Deserializer>,String,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getTableDesc(Class<? extends Deserializer>,String,String,String,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getTableDesc(Class<? extends Deserializer>,String,String,String,boolean,boolean,String)",1,5,7
"org.apache.hadoop.hive.ql.plan.PlanUtils.getTableDesc(Class<? extends Deserializer>,String,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.getTableDesc(CreateTableDesc,String,String)",1,12,12
"org.apache.hadoop.hive.ql.plan.PlanUtils.removePrefixFromWarehouseConfig(String)",1,4,4
"org.apache.hadoop.hive.ql.plan.PlanUtils.sortFieldSchemas(List<FieldSchema>)",1,1,1
"org.apache.hadoop.hive.ql.plan.PlanUtils.stripQuotes(String)",1,5,5
"org.apache.hadoop.hive.ql.plan.PrincipalDesc.PrincipalDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.PrincipalDesc.PrincipalDesc(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.ql.plan.PrincipalDesc.getName()",1,1,1
"org.apache.hadoop.hive.ql.plan.PrincipalDesc.getType()",1,1,1
"org.apache.hadoop.hive.ql.plan.PrincipalDesc.setName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.PrincipalDesc.setType(PrincipalType)",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeDesc.PrivilegeDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeDesc.PrivilegeDesc(Privilege,List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeDesc.getColumns()",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeDesc.getPrivilege()",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeDesc.setColumns(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeDesc.setPrivilege(Privilege)",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.PrivilegeObjectDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.PrivilegeObjectDesc(boolean,String,HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.getColumns()",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.getObject()",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.getTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.setColumns(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.setObject(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.setPartSpec(HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.PrivilegeObjectDesc.setTable(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.ReduceSinkDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.ReduceSinkDesc(ArrayList<ExprNodeDesc>,int,ArrayList<ExprNodeDesc>,ArrayList<String>,List<List<Integer>>,ArrayList<String>,int,ArrayList<ExprNodeDesc>,int,TableDesc,TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.clone()",1,2,2
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getBucketCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getDistinctColumnIndices()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getKeyColString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getKeyCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getKeySerializeInfo()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getNumBuckets()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getNumDistributionKeys()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getNumReducers()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getOrder()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getOutputKeyColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getOutputName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getOutputValueColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getParitionColsString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getPartitionCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getSkipTag()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getTag()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getTopN()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getTopNExplain()",1,1,2
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getTopNMemoryUsage()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getTopNMemoryUsageExplain()",1,1,3
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getValueCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getValueColsString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.getValueSerializeInfo()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.isAutoParallel()",1,1,2
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.isMapGroupBy()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.isPTFReduceSink()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setAutoParallel(boolean)",1,1,3
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setBucketCols(List<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setDistinctColumnIndices(List<List<Integer>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setKeyCols(ArrayList<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setKeySerializeInfo(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setMapGroupBy(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setNumBuckets(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setNumDistributionKeys(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setNumReducers(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setOrder(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setOutputKeyColumnNames(ArrayList<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setOutputName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setOutputValueColumnNames(ArrayList<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setPTFReduceSink(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setPartitionCols(ArrayList<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setSkipTag(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setTag(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setTopN(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setTopNMemoryUsage(float)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setValueCols(ArrayList<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.setValueSerializeInfo(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.ReduceWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.ReduceWork(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.addToReduceColumnNameMap(StructObjectInspector,int,String)",1,2,2
"org.apache.hadoop.hive.ql.plan.ReduceWork.configureJobConf(JobConf)",1,3,3
"org.apache.hadoop.hive.ql.plan.ReduceWork.fillInReduceColumnNameMap()",3,1,5
"org.apache.hadoop.hive.ql.plan.ReduceWork.getAllRootOperators()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.getKeyDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.getKeyObjectInspector()",1,2,2
"org.apache.hadoop.hive.ql.plan.ReduceWork.getMaxReduceTasks()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.getMinReduceTasks()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.getNeedsTagging()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.getNumReduceTasks()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.getObjectInspector(TableDesc)",1,1,2
"org.apache.hadoop.hive.ql.plan.ReduceWork.getReduceColumnNameMap()",4,2,4
"org.apache.hadoop.hive.ql.plan.ReduceWork.getReduceColumnNames()",4,2,4
"org.apache.hadoop.hive.ql.plan.ReduceWork.getReducer()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.getTagToInput()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.getTagToValueDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.getValueObjectInspector()",2,2,3
"org.apache.hadoop.hive.ql.plan.ReduceWork.getVectorModeOn()",1,1,2
"org.apache.hadoop.hive.ql.plan.ReduceWork.isAutoReduceParallelism()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.replaceRoots(Map<Operator<?>, Operator<?>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.setAutoReduceParallelism(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.setKeyDesc(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.setMaxReduceTasks(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.setMinReduceTasks(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.setNeedsTagging(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.setNumReduceTasks(Integer)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.setReducer(Operator<?>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.setTagToInput(Map<Integer, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReduceWork.setTagToValueDesc(List<TableDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ReducerTimeStatsPerJob.ReducerTimeStatsPerJob(List<Integer>)",2,6,6
"org.apache.hadoop.hive.ql.plan.ReducerTimeStatsPerJob.getMaximumTime()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReducerTimeStatsPerJob.getMeanTime()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReducerTimeStatsPerJob.getMinimumTime()",1,1,1
"org.apache.hadoop.hive.ql.plan.ReducerTimeStatsPerJob.getStandardDeviationTime()",1,1,1
"org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.RenamePartitionDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.RenamePartitionDesc(String,Map<String, String>,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.getLocation()",1,1,1
"org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.getNewPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.getOldPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.setLocation(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.setNewPartSpec(LinkedHashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.setOldPartSpec(LinkedHashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.RenamePartitionDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.RevokeDesc.RevokeDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.RevokeDesc.RevokeDesc(List<PrivilegeDesc>,List<PrincipalDesc>,PrivilegeObjectDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.RevokeDesc.RevokeDesc(List<PrivilegeDesc>,List<PrincipalDesc>,PrivilegeObjectDesc,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.RevokeDesc.getPrincipals()",1,1,1
"org.apache.hadoop.hive.ql.plan.RevokeDesc.getPrivilegeSubjectDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.RevokeDesc.getPrivileges()",1,1,1
"org.apache.hadoop.hive.ql.plan.RevokeDesc.isGrantOption()",1,1,1
"org.apache.hadoop.hive.ql.plan.RevokeDesc.setGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.RevokeDesc.setPrincipals(List<PrincipalDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.RevokeDesc.setPrivilegeSubjectDesc(PrivilegeObjectDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.RevokeDesc.setPrivileges(List<PrivilegeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.RoleDDLDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.RoleDDLDesc(String,PrincipalType,RoleOperation,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.RoleDDLDesc(String,RoleOperation)",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.RoleOperation.RoleOperation()",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.RoleOperation.RoleOperation(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.RoleOperation.getOperationName()",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.RoleOperation.toString()",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.getGroup()",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.getName()",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.getOperation()",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.getPrincipalType()",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.getRoleNameSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.getRoleOwnerName()",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.getRoleShowGrantSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.getShowRolePrincipalsSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.setGroup(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.setOperation(RoleOperation)",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.setPrincipalType(PrincipalType)",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.RoleDDLDesc.setRoleOwnerName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.SMBJoinDesc.SMBJoinDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.SMBJoinDesc.SMBJoinDesc(MapJoinDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.SMBJoinDesc.getAliasToSink()",1,1,1
"org.apache.hadoop.hive.ql.plan.SMBJoinDesc.getLocalWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.SMBJoinDesc.getTagToAlias()",1,1,1
"org.apache.hadoop.hive.ql.plan.SMBJoinDesc.setAliasToSink(Map<String, DummyStoreOperator>)",1,1,1
"org.apache.hadoop.hive.ql.plan.SMBJoinDesc.setLocalWork(MapredLocalWork)",1,1,1
"org.apache.hadoop.hive.ql.plan.SMBJoinDesc.setTagToAlias(HashMap<Byte, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.SchemaDesc.SchemaDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.SchemaDesc.SchemaDesc(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.SchemaDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.SchemaDesc.setSchema(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.ScriptDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.ScriptDesc(String,TableDesc,Class<? extends RecordWriter>,TableDesc,Class<? extends RecordReader>,Class<? extends RecordReader>,TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.getErrRecordReaderClass()",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.getInRecordWriterClass()",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.getOutRecordReaderClass()",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.getScriptCmd()",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.getScriptErrInfo()",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.getScriptInputInfo()",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.getScriptOutputInfo()",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.setErrRecordReaderClass(Class<? extends RecordReader>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.setInRecordWriterClass(Class<? extends RecordWriter>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.setOutRecordReaderClass(Class<? extends RecordReader>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.setScriptCmd(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.setScriptErrInfo(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.setScriptInputInfo(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ScriptDesc.setScriptOutputInfo(TableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.SelectDesc.SelectDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.SelectDesc.SelectDesc(List<ExprNodeDesc>,List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.SelectDesc.SelectDesc(List<ExprNodeDesc>,List<String>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.SelectDesc.SelectDesc(List<ExprNodeDesc>,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.SelectDesc.SelectDesc(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.SelectDesc.clone()",1,3,3
"org.apache.hadoop.hive.ql.plan.SelectDesc.explainNoCompute()",2,1,2
"org.apache.hadoop.hive.ql.plan.SelectDesc.getColList()",1,1,1
"org.apache.hadoop.hive.ql.plan.SelectDesc.getColListString()",1,1,1
"org.apache.hadoop.hive.ql.plan.SelectDesc.getOutputColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.SelectDesc.isSelStarNoCompute()",1,1,1
"org.apache.hadoop.hive.ql.plan.SelectDesc.isSelectStar()",1,1,1
"org.apache.hadoop.hive.ql.plan.SelectDesc.setColList(List<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.plan.SelectDesc.setOutputColumnNames(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.SelectDesc.setSelStarNoCompute(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.SelectDesc.setSelectStar(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.ShowColumnsDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.ShowColumnsDesc(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.ShowColumnsDesc(Path,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.getTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowColumnsDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowCompactionsDesc.ShowCompactionsDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowCompactionsDesc.ShowCompactionsDesc(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowCompactionsDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowCompactionsDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowConfDesc.ShowConfDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowConfDesc.ShowConfDesc(Path,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowConfDesc.getConfName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowConfDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowConfDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowConfDesc.setConfName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowConfDesc.setResFile(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowCreateTableDesc.ShowCreateTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowCreateTableDesc.ShowCreateTableDesc(String,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowCreateTableDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowCreateTableDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowCreateTableDesc.getTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowCreateTableDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowCreateTableDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowCreateTableDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc.ShowDatabasesDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc.ShowDatabasesDesc(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc.ShowDatabasesDesc(Path,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc.getPattern()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc.getTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc.setPattern(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowDatabasesDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc.ShowFunctionsDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc.ShowFunctionsDesc(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc.ShowFunctionsDesc(Path,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc.getPattern()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc.getTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc.setPattern(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowGrantDesc.ShowGrantDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowGrantDesc.ShowGrantDesc(String,PrincipalDesc,PrivilegeObjectDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowGrantDesc.getHiveObj()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowGrantDesc.getPrincipalDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowGrantDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowGrantDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowGrantDesc.setHiveObj(PrivilegeObjectDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowGrantDesc.setPrincipalDesc(PrincipalDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowGrantDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowIndexesDesc.ShowIndexesDesc(String,Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowIndexesDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowIndexesDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowIndexesDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowIndexesDesc.isFormatted()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowIndexesDesc.setFormatted(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.ShowLocksDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.ShowLocksDesc(Path,String,HashMap<String, String>,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.ShowLocksDesc(Path,String,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.getDatabase()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.getDbName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.getSchema()",2,1,2
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.getTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.isExt()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.isNewFormat()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.setDbName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.setExt(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.setPartSpecs(HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowLocksDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc.ShowPartitionsDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc.ShowPartitionsDesc(String,Path,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc.getTabName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc.getTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc.setPartSpec(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowPartitionsDesc.setTabName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.ShowTableStatusDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.ShowTableStatusDesc(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.ShowTableStatusDesc(String,String,String,HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.getDbName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.getPattern()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.getResFileString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.getTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.setDbName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.setPartSpec(HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.setPattern(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTableStatusDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTablesDesc.ShowTablesDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTablesDesc.ShowTablesDesc(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTablesDesc.ShowTablesDesc(Path,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTablesDesc.ShowTablesDesc(Path,String,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTablesDesc.getDbName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTablesDesc.getPattern()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTablesDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTablesDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTablesDesc.getTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTablesDesc.setDbName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTablesDesc.setPattern(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTablesDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTblPropertiesDesc.ShowTblPropertiesDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTblPropertiesDesc.ShowTblPropertiesDesc(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTblPropertiesDesc.getPropertyName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTblPropertiesDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTblPropertiesDesc.getResFileString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTblPropertiesDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTblPropertiesDesc.getTable()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTblPropertiesDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTblPropertiesDesc.setPropertyName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTblPropertiesDesc.setResFile(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTblPropertiesDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTxnsDesc.ShowTxnsDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTxnsDesc.ShowTxnsDesc(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTxnsDesc.getResFile()",1,1,1
"org.apache.hadoop.hive.ql.plan.ShowTxnsDesc.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.plan.SkewedColumnPositionPair.SkewedColumnPositionPair()",1,1,1
"org.apache.hadoop.hive.ql.plan.SkewedColumnPositionPair.SkewedColumnPositionPair(int,int)",1,1,1
"org.apache.hadoop.hive.ql.plan.SkewedColumnPositionPair.getSkewColPosition()",1,1,1
"org.apache.hadoop.hive.ql.plan.SkewedColumnPositionPair.getTblColPosition()",1,1,1
"org.apache.hadoop.hive.ql.plan.SkewedColumnPositionPair.setSkewColPosition(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.SkewedColumnPositionPair.setTblColPosition(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.Statistics()",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.Statistics(long,long)",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.addToColumnStats(List<ColStatistics>)",1,7,7
"org.apache.hadoop.hive.ql.plan.Statistics.addToDataSize(long)",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.addToNumRows(long)",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.clone()",1,3,3
"org.apache.hadoop.hive.ql.plan.Statistics.extendedToString()",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.getAvgRowSize()",2,1,2
"org.apache.hadoop.hive.ql.plan.Statistics.getBasicStatsState()",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.getColumnStatisticsForColumn(String,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.getColumnStatisticsFromColName(String)",4,2,4
"org.apache.hadoop.hive.ql.plan.Statistics.getColumnStatisticsFromFQColName(String)",2,2,2
"org.apache.hadoop.hive.ql.plan.Statistics.getColumnStats()",2,2,2
"org.apache.hadoop.hive.ql.plan.Statistics.getColumnStatsState()",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.getDataSize()",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.getNumRows()",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.setBasicStatsState(State)",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.setColumnStats(List<ColStatistics>)",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.setColumnStats(Map<String, ColStatistics>)",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.setColumnStatsState(State)",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.setDataSize(long)",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.setNumRows(long)",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.toString()",1,1,1
"org.apache.hadoop.hive.ql.plan.Statistics.updateBasicStatsState()",1,1,5
"org.apache.hadoop.hive.ql.plan.Statistics.updateColumnStatsState(State)",1,3,6
"org.apache.hadoop.hive.ql.plan.StatsNoJobWork.StatsNoJobWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsNoJobWork.StatsNoJobWork(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsNoJobWork.StatsNoJobWork(tableSpec)",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsNoJobWork.getTableSpecs()",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsNoJobWork.isStatsReliable()",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsNoJobWork.setStatsReliable(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.StatsWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.StatsWork(LoadFileDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.StatsWork(LoadTableDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.StatsWork(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.StatsWork(tableSpec)",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.getAggKey()",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.getLoadFileDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.getLoadTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.getNoStatsAggregator()",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.getSourceTask()",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.getTableSpecs()",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.isClearAggregatorStats()",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.isNoScanAnalyzeCommand()",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.isPartialScanAnalyzeCommand()",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.isStatsReliable()",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.setAggKey(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.setClearAggregatorStats(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.setNoScanAnalyzeCommand(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.setNoStatsAggregator(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.setPartialScanAnalyzeCommand(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.setSourceTask(Task)",1,1,1
"org.apache.hadoop.hive.ql.plan.StatsWork.setStatsReliable(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.SwitchDatabaseDesc.SwitchDatabaseDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.SwitchDatabaseDesc.SwitchDatabaseDesc(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.SwitchDatabaseDesc.getDatabaseName()",1,1,1
"org.apache.hadoop.hive.ql.plan.SwitchDatabaseDesc.setDatabaseName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.TableDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.TableDesc(Class<? extends InputFormat>,Class<?>,Properties)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.clone()",1,2,3
"org.apache.hadoop.hive.ql.plan.TableDesc.equals(Object)",2,9,10
"org.apache.hadoop.hive.ql.plan.TableDesc.getDeserializer()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.getDeserializerClass()",1,1,2
"org.apache.hadoop.hive.ql.plan.TableDesc.getInputFileFormatClass()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.getInputFileFormatClassName()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.getJobProperties()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.getOutputFileFormatClass()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.getOutputFileFormatClassName()",2,2,2
"org.apache.hadoop.hive.ql.plan.TableDesc.getProperties()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.getSerdeClassName()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.hashCode()",1,5,5
"org.apache.hadoop.hive.ql.plan.TableDesc.isNonNative()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.setInputFileFormatClass(Class<? extends InputFormat>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.setJobProperties(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.setOutputFileFormatClass(Class<?>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableDesc.setProperties(Properties)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.TableScanDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.TableScanDesc(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.TableScanDesc(String,List<VirtualColumn>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.addVirtualCols(List<VirtualColumn>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.clone()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getBucketFileNameMapping()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getFilterExpr()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getFilterExprString()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getFilterObject()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getIsMetadataOnly()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getMaxStatsKeyPrefixLength()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getNeededColumnIDs()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getNeededColumns()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getPartColumns()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getReferencedColumns()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getRowLimit()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getRowLimitExplain()",1,1,2
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getStatsAggPrefix()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.getVirtualCols()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.hasVirtualCols()",1,2,2
"org.apache.hadoop.hive.ql.plan.TableScanDesc.isGatherStats()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.isStatsReliable()",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setBucketFileNameMapping(Map<String, Integer>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setFilterExpr(ExprNodeGenericFuncDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setFilterObject(Serializable)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setGatherStats(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setIsMetadataOnly(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setMaxStatsKeyPrefixLength(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setNeededColumnIDs(List<Integer>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setNeededColumns(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setPartColumns(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setReferencedColumns(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setRowLimit(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setStatsAggPrefix(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setStatsReliable(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.TableScanDesc.setVirtualCols(List<VirtualColumn>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TestConditionalResolverCommonJoin.testResolvingDriverAlias()",1,1,1
"org.apache.hadoop.hive.ql.plan.TestCreateMacroDesc.setup()",1,1,1
"org.apache.hadoop.hive.ql.plan.TestCreateMacroDesc.testCreateMacroDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.TestDropMacroDesc.setup()",1,1,1
"org.apache.hadoop.hive.ql.plan.TestDropMacroDesc.testCreateMacroDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.TestReadEntityDirect.CheckInputReadEntityDirect.postAnalyze(HiveSemanticAnalyzerHookContext,List<Task<? extends Serializable>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TestReadEntityDirect.createDriver()",1,1,1
"org.apache.hadoop.hive.ql.plan.TestReadEntityDirect.onetimeSetup()",1,1,1
"org.apache.hadoop.hive.ql.plan.TestReadEntityDirect.setup()",1,1,1
"org.apache.hadoop.hive.ql.plan.TestReadEntityDirect.testSelectEntityDirect()",1,1,1
"org.apache.hadoop.hive.ql.plan.TestReadEntityDirect.testSelectEntityInDirect()",1,4,4
"org.apache.hadoop.hive.ql.plan.TestReadEntityDirect.testSelectEntityInDirectJoinAlias()",1,4,4
"org.apache.hadoop.hive.ql.plan.TestReadEntityDirect.testSelectEntityViewDirectJoin()",1,4,4
"org.apache.hadoop.hive.ql.plan.TestReadEntityDirect.testSelectEntityViewDirectUnion()",1,4,4
"org.apache.hadoop.hive.ql.plan.TestTezWork.addWork(int)",1,2,2
"org.apache.hadoop.hive.ql.plan.TestTezWork.setup()",1,1,1
"org.apache.hadoop.hive.ql.plan.TestTezWork.testAdd()",1,2,2
"org.apache.hadoop.hive.ql.plan.TestTezWork.testBroadcastConnect()",3,4,6
"org.apache.hadoop.hive.ql.plan.TestTezWork.testConnect()",3,4,6
"org.apache.hadoop.hive.ql.plan.TestTezWork.testDisconnect()",1,5,5
"org.apache.hadoop.hive.ql.plan.TestTezWork.testGetAllWork()",1,3,3
"org.apache.hadoop.hive.ql.plan.TestTezWork.testRemove()",1,1,1
"org.apache.hadoop.hive.ql.plan.TezEdgeProperty.TezEdgeProperty(EdgeType)",1,1,1
"org.apache.hadoop.hive.ql.plan.TezEdgeProperty.TezEdgeProperty(HiveConf,EdgeType,boolean,int,int,long)",1,1,1
"org.apache.hadoop.hive.ql.plan.TezEdgeProperty.TezEdgeProperty(HiveConf,EdgeType,int)",1,1,1
"org.apache.hadoop.hive.ql.plan.TezEdgeProperty.getEdgeType()",1,1,1
"org.apache.hadoop.hive.ql.plan.TezEdgeProperty.getHiveConf()",1,1,1
"org.apache.hadoop.hive.ql.plan.TezEdgeProperty.getInputSizePerReducer()",1,1,1
"org.apache.hadoop.hive.ql.plan.TezEdgeProperty.getMaxReducer()",1,1,1
"org.apache.hadoop.hive.ql.plan.TezEdgeProperty.getMinReducer()",1,1,1
"org.apache.hadoop.hive.ql.plan.TezEdgeProperty.getNumBuckets()",1,1,1
"org.apache.hadoop.hive.ql.plan.TezEdgeProperty.isAutoReduce()",1,1,1
"org.apache.hadoop.hive.ql.plan.TezWork.Dependency.compareTo(Dependency)",1,2,2
"org.apache.hadoop.hive.ql.plan.TezWork.Dependency.getName()",1,1,1
"org.apache.hadoop.hive.ql.plan.TezWork.Dependency.getType()",1,1,1
"org.apache.hadoop.hive.ql.plan.TezWork.TezWork(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.TezWork.add(BaseWork)",2,1,2
"org.apache.hadoop.hive.ql.plan.TezWork.addAll(BaseWork[])",1,2,2
"org.apache.hadoop.hive.ql.plan.TezWork.addAll(Collection<BaseWork>)",1,2,2
"org.apache.hadoop.hive.ql.plan.TezWork.configureJobConfAndExtractJars(JobConf)",1,4,6
"org.apache.hadoop.hive.ql.plan.TezWork.connect(BaseWork,BaseWork,TezEdgeProperty)",1,1,1
"org.apache.hadoop.hive.ql.plan.TezWork.disconnect(BaseWork,BaseWork)",1,3,3
"org.apache.hadoop.hive.ql.plan.TezWork.getAllWork()",1,2,2
"org.apache.hadoop.hive.ql.plan.TezWork.getAllWorkUnsorted()",1,1,1
"org.apache.hadoop.hive.ql.plan.TezWork.getChildren(BaseWork)",1,2,2
"org.apache.hadoop.hive.ql.plan.TezWork.getDependencyMap()",1,4,4
"org.apache.hadoop.hive.ql.plan.TezWork.getEdgeProperty(BaseWork,BaseWork)",1,1,1
"org.apache.hadoop.hive.ql.plan.TezWork.getEdgeType(BaseWork,BaseWork)",1,1,1
"org.apache.hadoop.hive.ql.plan.TezWork.getLeaves()",1,1,1
"org.apache.hadoop.hive.ql.plan.TezWork.getName()",1,1,1
"org.apache.hadoop.hive.ql.plan.TezWork.getParents(BaseWork)",1,2,2
"org.apache.hadoop.hive.ql.plan.TezWork.getRoots()",1,1,1
"org.apache.hadoop.hive.ql.plan.TezWork.getWorkMap()",1,2,2
"org.apache.hadoop.hive.ql.plan.TezWork.remove(BaseWork)",2,5,6
"org.apache.hadoop.hive.ql.plan.TezWork.visit(BaseWork,Set<BaseWork>,List<BaseWork>)",2,3,4
"org.apache.hadoop.hive.ql.plan.TruncateTableDesc.TruncateTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.TruncateTableDesc.TruncateTableDesc(String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TruncateTableDesc.getColumnIndexes()",1,1,1
"org.apache.hadoop.hive.ql.plan.TruncateTableDesc.getInputDir()",1,1,1
"org.apache.hadoop.hive.ql.plan.TruncateTableDesc.getLbCtx()",1,1,1
"org.apache.hadoop.hive.ql.plan.TruncateTableDesc.getOutputDir()",1,1,1
"org.apache.hadoop.hive.ql.plan.TruncateTableDesc.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.TruncateTableDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.TruncateTableDesc.setColumnIndexes(List<Integer>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TruncateTableDesc.setInputDir(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.TruncateTableDesc.setLbCtx(ListBucketingCtx)",1,1,1
"org.apache.hadoop.hive.ql.plan.TruncateTableDesc.setOutputDir(Path)",1,1,1
"org.apache.hadoop.hive.ql.plan.TruncateTableDesc.setPartSpec(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.TruncateTableDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.UDTFDesc.UDTFDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.UDTFDesc.UDTFDesc(GenericUDTF,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.UDTFDesc.getGenericUDTF()",1,1,1
"org.apache.hadoop.hive.ql.plan.UDTFDesc.getUDTFName()",1,1,1
"org.apache.hadoop.hive.ql.plan.UDTFDesc.isOuterLV()",1,1,1
"org.apache.hadoop.hive.ql.plan.UDTFDesc.isOuterLateralView()",1,1,2
"org.apache.hadoop.hive.ql.plan.UDTFDesc.setGenericUDTF(GenericUDTF)",1,1,1
"org.apache.hadoop.hive.ql.plan.UDTFDesc.setOuterLV(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.UnionDesc.UnionDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.UnionDesc.getNumInputs()",1,1,1
"org.apache.hadoop.hive.ql.plan.UnionDesc.isAllInputsInSameReducer()",1,1,1
"org.apache.hadoop.hive.ql.plan.UnionDesc.setAllInputsInSameReducer(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.UnionDesc.setNumInputs(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.UnionWork.UnionWork()",1,1,1
"org.apache.hadoop.hive.ql.plan.UnionWork.UnionWork(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.UnionWork.addUnionOperators(Collection<UnionOperator>)",1,1,1
"org.apache.hadoop.hive.ql.plan.UnionWork.configureJobConf(JobConf)",1,1,1
"org.apache.hadoop.hive.ql.plan.UnionWork.getAllRootOperators()",1,1,1
"org.apache.hadoop.hive.ql.plan.UnionWork.getName()",1,1,1
"org.apache.hadoop.hive.ql.plan.UnionWork.getUnionOperators()",1,1,1
"org.apache.hadoop.hive.ql.plan.UnionWork.replaceRoots(Map<Operator<?>, Operator<?>>)",1,1,1
"org.apache.hadoop.hive.ql.plan.UnlockDatabaseDesc.UnlockDatabaseDesc(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.UnlockDatabaseDesc.getDatabaseName()",1,1,1
"org.apache.hadoop.hive.ql.plan.UnlockDatabaseDesc.setDatabaseName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.UnlockTableDesc.UnlockTableDesc()",1,1,1
"org.apache.hadoop.hive.ql.plan.UnlockTableDesc.UnlockTableDesc(String,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.UnlockTableDesc.getPartSpec()",1,1,1
"org.apache.hadoop.hive.ql.plan.UnlockTableDesc.getTableName()",1,1,1
"org.apache.hadoop.hive.ql.plan.UnlockTableDesc.setPartSpec(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.UnlockTableDesc.setTableName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ValidationUtility.ValidationUtility()",1,1,1
"org.apache.hadoop.hive.ql.plan.ValidationUtility.validateSkewedColNameValueNumberMatch(List<String>,List<List<String>>)",3,3,3
"org.apache.hadoop.hive.ql.plan.ValidationUtility.validateSkewedColNames(List<String>,List<String>)",2,3,3
"org.apache.hadoop.hive.ql.plan.ValidationUtility.validateSkewedColumnNameUniqueness(List<String>)",3,3,3
"org.apache.hadoop.hive.ql.plan.ValidationUtility.validateSkewedInformation(List<String>,List<String>,List<List<String>>)",4,5,5
"org.apache.hadoop.hive.ql.plan.api.Adjacency.Adjacency()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.Adjacency(Adjacency)",1,3,5
"org.apache.hadoop.hive.ql.plan.api.Adjacency.Adjacency(String,List<String>,AdjacencyType)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.AdjacencyStandardScheme.read(TProtocol,Adjacency)",4,7,11
"org.apache.hadoop.hive.ql.plan.api.Adjacency.AdjacencyStandardScheme.write(TProtocol,Adjacency)",1,5,5
"org.apache.hadoop.hive.ql.plan.api.Adjacency.AdjacencyStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.AdjacencyTupleScheme.read(TProtocol,Adjacency)",1,5,5
"org.apache.hadoop.hive.ql.plan.api.Adjacency.AdjacencyTupleScheme.write(TProtocol,Adjacency)",1,8,8
"org.apache.hadoop.hive.ql.plan.api.Adjacency.AdjacencyTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.ql.plan.api.Adjacency._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.ql.plan.api.Adjacency._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.addToChildren(String)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Adjacency.clear()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.compareTo(Adjacency)",11,5,11
"org.apache.hadoop.hive.ql.plan.api.Adjacency.deepCopy()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.equals(Adjacency)",11,10,23
"org.apache.hadoop.hive.ql.plan.api.Adjacency.equals(Object)",3,2,3
"org.apache.hadoop.hive.ql.plan.api.Adjacency.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.getAdjacencyType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.getChildren()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.getChildrenIterator()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Adjacency.getChildrenSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Adjacency.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.ql.plan.api.Adjacency.getNode()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.hashCode()",1,7,7
"org.apache.hadoop.hive.ql.plan.api.Adjacency.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.ql.plan.api.Adjacency.isSetAdjacencyType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.isSetChildren()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.isSetNode()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Adjacency.setAdjacencyType(AdjacencyType)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.setAdjacencyTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Adjacency.setChildren(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.setChildrenIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Adjacency.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.ql.plan.api.Adjacency.setNode(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.setNodeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Adjacency.toString()",1,6,6
"org.apache.hadoop.hive.ql.plan.api.Adjacency.unsetAdjacencyType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.unsetChildren()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.unsetNode()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.validate()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Adjacency.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.AdjacencyType.AdjacencyType(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.AdjacencyType.findByValue(int)",4,2,4
"org.apache.hadoop.hive.ql.plan.api.AdjacencyType.getValue()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.Graph()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.Graph(Graph)",1,5,6
"org.apache.hadoop.hive.ql.plan.api.Graph.Graph(NodeType,List<String>,List<Adjacency>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.GraphStandardScheme.read(TProtocol,Graph)",4,8,12
"org.apache.hadoop.hive.ql.plan.api.Graph.GraphStandardScheme.write(TProtocol,Graph)",1,6,6
"org.apache.hadoop.hive.ql.plan.api.Graph.GraphStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.GraphTupleScheme.read(TProtocol,Graph)",1,6,6
"org.apache.hadoop.hive.ql.plan.api.Graph.GraphTupleScheme.write(TProtocol,Graph)",1,9,9
"org.apache.hadoop.hive.ql.plan.api.Graph.GraphTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.ql.plan.api.Graph._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.ql.plan.api.Graph._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.addToAdjacencyList(Adjacency)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Graph.addToRoots(String)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Graph.clear()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.compareTo(Graph)",11,5,11
"org.apache.hadoop.hive.ql.plan.api.Graph.deepCopy()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.equals(Graph)",11,10,23
"org.apache.hadoop.hive.ql.plan.api.Graph.equals(Object)",3,2,3
"org.apache.hadoop.hive.ql.plan.api.Graph.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.getAdjacencyList()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.getAdjacencyListIterator()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Graph.getAdjacencyListSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Graph.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.ql.plan.api.Graph.getNodeType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.getRoots()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.getRootsIterator()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Graph.getRootsSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Graph.hashCode()",1,7,7
"org.apache.hadoop.hive.ql.plan.api.Graph.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.ql.plan.api.Graph.isSetAdjacencyList()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.isSetNodeType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.isSetRoots()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Graph.setAdjacencyList(List<Adjacency>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.setAdjacencyListIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Graph.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.ql.plan.api.Graph.setNodeType(NodeType)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.setNodeTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Graph.setRoots(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.setRootsIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Graph.toString()",1,6,6
"org.apache.hadoop.hive.ql.plan.api.Graph.unsetAdjacencyList()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.unsetNodeType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.unsetRoots()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.validate()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Graph.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.NodeType.NodeType(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.NodeType.findByValue(int)",4,2,4
"org.apache.hadoop.hive.ql.plan.api.NodeType.getValue()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.Operator()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.Operator(Operator)",1,5,7
"org.apache.hadoop.hive.ql.plan.api.Operator.Operator(String,OperatorType,Map<String, String>,Map<String, Long>,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.OperatorStandardScheme.read(TProtocol,Operator)",4,11,18
"org.apache.hadoop.hive.ql.plan.api.Operator.OperatorStandardScheme.write(TProtocol,Operator)",1,7,7
"org.apache.hadoop.hive.ql.plan.api.Operator.OperatorStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.OperatorTupleScheme.read(TProtocol,Operator)",1,9,9
"org.apache.hadoop.hive.ql.plan.api.Operator.OperatorTupleScheme.write(TProtocol,Operator)",1,15,15
"org.apache.hadoop.hive.ql.plan.api.Operator.OperatorTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator._Fields.findByThriftId(int)",8,2,8
"org.apache.hadoop.hive.ql.plan.api.Operator._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.ql.plan.api.Operator._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.clear()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.compareTo(Operator)",20,8,20
"org.apache.hadoop.hive.ql.plan.api.Operator.deepCopy()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.equals(Object)",3,2,3
"org.apache.hadoop.hive.ql.plan.api.Operator.equals(Operator)",20,13,40
"org.apache.hadoop.hive.ql.plan.api.Operator.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.getFieldValue(_Fields)",7,7,7
"org.apache.hadoop.hive.ql.plan.api.Operator.getOperatorAttributes()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.getOperatorAttributesSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Operator.getOperatorCounters()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.getOperatorCountersSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Operator.getOperatorId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.getOperatorType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.hashCode()",1,11,11
"org.apache.hadoop.hive.ql.plan.api.Operator.isDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.isSet(_Fields)",8,7,8
"org.apache.hadoop.hive.ql.plan.api.Operator.isSetDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.isSetOperatorAttributes()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.isSetOperatorCounters()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.isSetOperatorId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.isSetOperatorType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.isSetStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.isStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.putToOperatorAttributes(String,String)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Operator.putToOperatorCounters(String,long)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Operator.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Operator.setDone(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.setDoneIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.setFieldValue(_Fields,Object)",2,8,13
"org.apache.hadoop.hive.ql.plan.api.Operator.setOperatorAttributes(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.setOperatorAttributesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Operator.setOperatorCounters(Map<String, Long>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.setOperatorCountersIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Operator.setOperatorId(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.setOperatorIdIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Operator.setOperatorType(OperatorType)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.setOperatorTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Operator.setStarted(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.setStartedIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.toString()",1,10,10
"org.apache.hadoop.hive.ql.plan.api.Operator.unsetDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.unsetOperatorAttributes()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.unsetOperatorCounters()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.unsetOperatorId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.unsetOperatorType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.unsetStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.validate()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Operator.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.OperatorType.OperatorType(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.OperatorType.findByValue(int)",23,2,23
"org.apache.hadoop.hive.ql.plan.api.OperatorType.getValue()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.Query()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.Query(Query)",1,7,10
"org.apache.hadoop.hive.ql.plan.api.Query.Query(String,String,Map<String, String>,Map<String, Long>,Graph,List<Stage>,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.QueryStandardScheme.read(TProtocol,Query)",4,14,23
"org.apache.hadoop.hive.ql.plan.api.Query.QueryStandardScheme.write(TProtocol,Query)",1,10,10
"org.apache.hadoop.hive.ql.plan.api.Query.QueryStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.QueryTupleScheme.read(TProtocol,Query)",1,12,12
"org.apache.hadoop.hive.ql.plan.api.Query.QueryTupleScheme.write(TProtocol,Query)",1,20,20
"org.apache.hadoop.hive.ql.plan.api.Query.QueryTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query._Fields.findByThriftId(int)",10,2,10
"org.apache.hadoop.hive.ql.plan.api.Query._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.ql.plan.api.Query._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.addToStageList(Stage)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Query.clear()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.compareTo(Query)",26,10,26
"org.apache.hadoop.hive.ql.plan.api.Query.deepCopy()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.equals(Object)",3,2,3
"org.apache.hadoop.hive.ql.plan.api.Query.equals(Query)",26,19,54
"org.apache.hadoop.hive.ql.plan.api.Query.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.getFieldValue(_Fields)",9,9,9
"org.apache.hadoop.hive.ql.plan.api.Query.getQueryAttributes()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.getQueryAttributesSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Query.getQueryCounters()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.getQueryCountersSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Query.getQueryId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.getQueryType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.getStageGraph()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.getStageList()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.getStageListIterator()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Query.getStageListSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Query.hashCode()",1,15,15
"org.apache.hadoop.hive.ql.plan.api.Query.isDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.isSet(_Fields)",10,9,10
"org.apache.hadoop.hive.ql.plan.api.Query.isSetDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.isSetQueryAttributes()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.isSetQueryCounters()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.isSetQueryId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.isSetQueryType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.isSetStageGraph()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.isSetStageList()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.isSetStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.isStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.putToQueryAttributes(String,String)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Query.putToQueryCounters(String,long)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Query.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Query.setDone(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.setDoneIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.setFieldValue(_Fields,Object)",2,10,17
"org.apache.hadoop.hive.ql.plan.api.Query.setQueryAttributes(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.setQueryAttributesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Query.setQueryCounters(Map<String, Long>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.setQueryCountersIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Query.setQueryId(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.setQueryIdIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Query.setQueryType(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.setQueryTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Query.setStageGraph(Graph)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.setStageGraphIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Query.setStageList(List<Stage>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.setStageListIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Query.setStarted(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.setStartedIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.toString()",1,14,14
"org.apache.hadoop.hive.ql.plan.api.Query.unsetDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.unsetQueryAttributes()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.unsetQueryCounters()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.unsetQueryId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.unsetQueryType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.unsetStageGraph()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.unsetStageList()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.unsetStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.validate()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Query.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Query.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.QueryPlan()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.QueryPlan(List<Query>,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.QueryPlan(QueryPlan)",1,3,3
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.QueryPlanStandardScheme.read(TProtocol,QueryPlan)",4,7,11
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.QueryPlanStandardScheme.write(TProtocol,QueryPlan)",1,3,3
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.QueryPlanStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.QueryPlanTupleScheme.read(TProtocol,QueryPlan)",1,5,5
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.QueryPlanTupleScheme.write(TProtocol,QueryPlan)",1,8,8
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.QueryPlanTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.ql.plan.api.QueryPlan._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.ql.plan.api.QueryPlan._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.addToQueries(Query)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.clear()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.compareTo(QueryPlan)",11,5,11
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.deepCopy()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.equals(Object)",3,2,3
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.equals(QueryPlan)",11,4,19
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.getQueries()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.getQueriesIterator()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.getQueriesSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.hashCode()",1,5,5
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.isDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.isSetDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.isSetQueries()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.isSetStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.isStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.setDone(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.setDoneIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.setQueries(List<Query>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.setQueriesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.setStarted(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.setStartedIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.toString()",1,4,4
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.unsetDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.unsetQueries()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.unsetStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.validate()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.QueryPlan.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Stage.Stage()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.Stage(Stage)",1,7,9
"org.apache.hadoop.hive.ql.plan.api.Stage.Stage(String,StageType,Map<String, String>,Map<String, Long>,List<Task>,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.StageStandardScheme.read(TProtocol,Stage)",4,13,21
"org.apache.hadoop.hive.ql.plan.api.Stage.StageStandardScheme.write(TProtocol,Stage)",1,9,9
"org.apache.hadoop.hive.ql.plan.api.Stage.StageStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.StageTupleScheme.read(TProtocol,Stage)",1,11,11
"org.apache.hadoop.hive.ql.plan.api.Stage.StageTupleScheme.write(TProtocol,Stage)",1,18,18
"org.apache.hadoop.hive.ql.plan.api.Stage.StageTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage._Fields.findByThriftId(int)",9,2,9
"org.apache.hadoop.hive.ql.plan.api.Stage._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.ql.plan.api.Stage._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.addToTaskList(Task)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Stage.clear()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.compareTo(Stage)",23,9,23
"org.apache.hadoop.hive.ql.plan.api.Stage.deepCopy()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.equals(Object)",3,2,3
"org.apache.hadoop.hive.ql.plan.api.Stage.equals(Stage)",23,16,47
"org.apache.hadoop.hive.ql.plan.api.Stage.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.getFieldValue(_Fields)",8,8,8
"org.apache.hadoop.hive.ql.plan.api.Stage.getStageAttributes()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.getStageAttributesSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Stage.getStageCounters()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.getStageCountersSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Stage.getStageId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.getStageType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.getTaskList()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.getTaskListIterator()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Stage.getTaskListSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Stage.hashCode()",1,13,13
"org.apache.hadoop.hive.ql.plan.api.Stage.isDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.isSet(_Fields)",9,8,9
"org.apache.hadoop.hive.ql.plan.api.Stage.isSetDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.isSetStageAttributes()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.isSetStageCounters()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.isSetStageId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.isSetStageType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.isSetStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.isSetTaskList()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.isStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.putToStageAttributes(String,String)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Stage.putToStageCounters(String,long)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Stage.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Stage.setDone(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.setDoneIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.setFieldValue(_Fields,Object)",2,9,15
"org.apache.hadoop.hive.ql.plan.api.Stage.setStageAttributes(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.setStageAttributesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Stage.setStageCounters(Map<String, Long>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.setStageCountersIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Stage.setStageId(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.setStageIdIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Stage.setStageType(StageType)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.setStageTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Stage.setStarted(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.setStartedIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.setTaskList(List<Task>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.setTaskListIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Stage.toString()",1,12,12
"org.apache.hadoop.hive.ql.plan.api.Stage.unsetDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.unsetStageAttributes()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.unsetStageCounters()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.unsetStageId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.unsetStageType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.unsetStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.unsetTaskList()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.validate()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Stage.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.StageType.StageType(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.StageType.findByValue(int)",14,2,14
"org.apache.hadoop.hive.ql.plan.api.StageType.getValue()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.Task()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.Task(String,TaskType,Map<String, String>,Map<String, Long>,boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.Task(Task)",1,7,10
"org.apache.hadoop.hive.ql.plan.api.Task.TaskStandardScheme.read(TProtocol,Task)",4,14,23
"org.apache.hadoop.hive.ql.plan.api.Task.TaskStandardScheme.write(TProtocol,Task)",1,12,12
"org.apache.hadoop.hive.ql.plan.api.Task.TaskStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.TaskTupleScheme.read(TProtocol,Task)",1,12,12
"org.apache.hadoop.hive.ql.plan.api.Task.TaskTupleScheme.write(TProtocol,Task)",1,20,20
"org.apache.hadoop.hive.ql.plan.api.Task.TaskTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task._Fields.findByThriftId(int)",10,2,10
"org.apache.hadoop.hive.ql.plan.api.Task._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.ql.plan.api.Task._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.addToOperatorList(Operator)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Task.clear()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.compareTo(Task)",26,10,26
"org.apache.hadoop.hive.ql.plan.api.Task.deepCopy()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.equals(Object)",3,2,3
"org.apache.hadoop.hive.ql.plan.api.Task.equals(Task)",26,19,54
"org.apache.hadoop.hive.ql.plan.api.Task.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.getFieldValue(_Fields)",9,9,9
"org.apache.hadoop.hive.ql.plan.api.Task.getOperatorGraph()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.getOperatorList()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.getOperatorListIterator()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Task.getOperatorListSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Task.getTaskAttributes()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.getTaskAttributesSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Task.getTaskCounters()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.getTaskCountersSize()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Task.getTaskId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.getTaskType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.hashCode()",1,15,15
"org.apache.hadoop.hive.ql.plan.api.Task.isDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.isSet(_Fields)",10,9,10
"org.apache.hadoop.hive.ql.plan.api.Task.isSetDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.isSetOperatorGraph()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.isSetOperatorList()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.isSetStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.isSetTaskAttributes()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.isSetTaskCounters()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.isSetTaskId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.isSetTaskType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.isStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.putToTaskAttributes(String,String)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Task.putToTaskCounters(String,long)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Task.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Task.setDone(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.setDoneIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.setFieldValue(_Fields,Object)",2,10,17
"org.apache.hadoop.hive.ql.plan.api.Task.setOperatorGraph(Graph)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.setOperatorGraphIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Task.setOperatorList(List<Operator>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.setOperatorListIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Task.setStarted(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.setStartedIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.setTaskAttributes(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.setTaskAttributesIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Task.setTaskCounters(Map<String, Long>)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.setTaskCountersIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Task.setTaskId(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.setTaskIdIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Task.setTaskType(TaskType)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.setTaskTypeIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.Task.toString()",1,16,16
"org.apache.hadoop.hive.ql.plan.api.Task.unsetDone()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.unsetOperatorGraph()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.unsetOperatorList()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.unsetStarted()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.unsetTaskAttributes()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.unsetTaskCounters()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.unsetTaskId()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.unsetTaskType()",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.validate()",1,2,2
"org.apache.hadoop.hive.ql.plan.api.Task.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.Task.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.ql.plan.api.TaskType.TaskType(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.api.TaskType.findByValue(int)",5,2,5
"org.apache.hadoop.hive.ql.plan.api.TaskType.getValue()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.BoundaryDef.getDirection()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.BoundaryDef.setDirection(Direction)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.CurrentRowDef.compareTo(BoundaryDef)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.CurrentRowDef.getAmt()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.CurrentRowDef.getDirection()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.OrderDef.OrderDef()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.OrderDef.OrderDef(PartitionDef)",1,2,2
"org.apache.hadoop.hive.ql.plan.ptf.OrderDef.addExpression(OrderExpressionDef)",1,1,2
"org.apache.hadoop.hive.ql.plan.ptf.OrderDef.getExpressions()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.OrderDef.setExpressions(ArrayList<OrderExpressionDef>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.OrderExpressionDef.OrderExpressionDef()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.OrderExpressionDef.OrderExpressionDef(PTFExpressionDef)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.OrderExpressionDef.getOrder()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.OrderExpressionDef.setOrder(Order)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.PTFExpressionDef()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.PTFExpressionDef(PTFExpressionDef)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.getExprEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.getExprNode()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.getExpressionTreeString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.getOI()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.setExprEvaluator(ExprNodeEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.setExprNode(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.setExpressionTreeString(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef.setOI(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFInputDef.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFInputDef.getExpressionTreeString()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFInputDef.getOutputShape()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFInputDef.setAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFInputDef.setExpressionTreeString(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFInputDef.setOutputShape(ShapeDetails)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFQueryInputDef.getDestination()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFQueryInputDef.getInput()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFQueryInputDef.getType()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFQueryInputDef.setDestination(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PTFQueryInputDef.setType(PTFQueryInputType)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionDef.addExpression(PTFExpressionDef)",1,1,2
"org.apache.hadoop.hive.ql.plan.ptf.PartitionDef.getExpressions()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionDef.setExpressions(List<PTFExpressionDef>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.addArg(PTFExpressionDef)",1,1,2
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.getArgs()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.getInput()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.getName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.getOrder()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.getPartition()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.getRawInputShape()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.getResolverClassName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.getStartOfChain()",2,2,2
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.getTFunction()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.isCarryForwardNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.isTransformsRawInput()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.setArgs(List<PTFExpressionDef>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.setCarryForwardNames(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.setInput(PTFInputDef)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.setName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.setOrder(OrderDef)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.setPartition(PartitionDef)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.setRawInputShape(ShapeDetails)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.setResolverClassName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.setTFunction(TableFunctionEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.PartitionedTableFunctionDef.setTransformsRawInput(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.RangeBoundaryDef.compareTo(BoundaryDef)",2,1,2
"org.apache.hadoop.hive.ql.plan.ptf.RangeBoundaryDef.getAmt()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.RangeBoundaryDef.setAmt(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.getColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.getOI()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.getRr()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.getSerde()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.getSerdeClassName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.getSerdeProps()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.getTypeCheckCtx()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.setColumnNames(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.setOI(StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.setRr(RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.setSerde(SerDe)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.setSerdeClassName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.setSerdeProps(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ShapeDetails.setTypeCheckCtx(TypeCheckCtx)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ValueBoundaryDef.compareTo(BoundaryDef)",2,1,2
"org.apache.hadoop.hive.ql.plan.ptf.ValueBoundaryDef.getAmt()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ValueBoundaryDef.getExprEvaluator()",1,2,2
"org.apache.hadoop.hive.ql.plan.ptf.ValueBoundaryDef.getExprNode()",1,2,2
"org.apache.hadoop.hive.ql.plan.ptf.ValueBoundaryDef.getExpressionDef()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ValueBoundaryDef.getOI()",1,2,2
"org.apache.hadoop.hive.ql.plan.ptf.ValueBoundaryDef.setAmt(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.ValueBoundaryDef.setExpressionDef(PTFExpressionDef)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowExpressionDef.WindowExpressionDef()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowExpressionDef.WindowExpressionDef(PTFExpressionDef)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowExpressionDef.getAlias()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowExpressionDef.setAlias(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef.getEnd()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef.getStart()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef.setEnd(BoundaryDef)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef.setStart(BoundaryDef)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.addArg(PTFExpressionDef)",1,1,2
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.getArgs()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.getName()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.getWFnEval()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.getWindowFrame()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.isDistinct()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.isPivotResult()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.isStar()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.setArgs(List<PTFExpressionDef>)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.setDistinct(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.setName(String)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.setPivotResult(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.setStar(boolean)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.setWFnEval(GenericUDAFEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowFunctionDef.setWindowFrame(WindowFrameDef)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowTableFunctionDef.getRankLimit()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowTableFunctionDef.getRankLimitFunction()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowTableFunctionDef.getWindowFunctions()",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowTableFunctionDef.setRankLimit(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowTableFunctionDef.setRankLimitFunction(int)",1,1,1
"org.apache.hadoop.hive.ql.plan.ptf.WindowTableFunctionDef.setWindowFunctions(List<WindowFunctionDef>)",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.ExprInfo.ExprInfo()",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.ExprInfo.ExprInfo(boolean,String,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.ExprWalkerInfo()",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.ExprWalkerInfo(Operator<? extends OperatorDesc>,RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.addAlias(ExprNodeDesc,String)",2,2,3
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.addConvertedNode(ExprNodeDesc,ExprNodeDesc)",1,2,2
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.addFinalCandidate(ExprNodeDesc)",1,2,2
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.addNonFinalCandidate(ExprNodeDesc)",1,2,2
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.addPushDowns(String,List<ExprNodeDesc>)",1,2,2
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.getAlias(ExprNodeDesc)",2,1,2
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.getConvertedNode(Node)",2,1,2
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.getFinalCandidates()",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.getNewToOldExprMap()",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.getNonFinalCandidates()",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.getOp()",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.getResidualPredicates(boolean)",1,4,4
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.getToRR()",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.isCandidate(ExprNodeDesc)",2,1,2
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.isDeterministic()",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.merge(ExprWalkerInfo)",2,5,6
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.setDeterministic(boolean)",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.setIsCandidate(ExprNodeDesc,boolean)",1,2,2
"org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.ColumnExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",4,4,7
"org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.DefaultExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.ExprWalkerProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.FieldExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,5,7
"org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.GenericFuncExprProcessor.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",4,7,10
"org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.extractFinalCandidates(ExprNodeDesc,ExprWalkerInfo,HiveConf)",3,6,6
"org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.extractPushdownPreds(OpWalkerInfo,Operator<? extends OperatorDesc>,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.extractPushdownPreds(OpWalkerInfo,Operator<? extends OperatorDesc>,List<ExprNodeDesc>)",1,3,3
"org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.getColumnProcessor()",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.getDefaultExprProcessor()",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.getFieldProcessor()",1,1,1
"org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.getGenericFuncProcessor()",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.DefaultPPD.getQualifiedAliases(Operator<?>,OpWalkerInfo)",5,4,8
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.DefaultPPD.logExpr(Node,ExprWalkerInfo)",1,3,3
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.DefaultPPD.mergeChildrenPred(Node,OpWalkerInfo,Set<String>,boolean)",4,7,9
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.DefaultPPD.mergeWithChildrenPred(Node,OpWalkerInfo,ExprWalkerInfo,Set<String>)",2,5,8
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.DefaultPPD.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,6,7
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.FilterPPD.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",5,10,10
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.JoinPPD.applyFilterTransitivity(JoinOperator,OpWalkerInfo)",1,15,17
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.JoinPPD.getAliases(Node,OpWalkerInfo)",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.JoinPPD.getColumnInfoFromAST(ASTNode,Map<String, RowResolver>)",5,6,7
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.JoinPPD.getQualifiedAliases(JoinOperator,RowResolver)",6,6,7
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.JoinPPD.handlePredicates(Node,ExprWalkerInfo,OpWalkerInfo)",1,2,2
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.JoinPPD.replaceColumnReference(ExprNodeDesc,String,String)",1,5,5
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.JoinerPPD.getAliases(Node,OpWalkerInfo)",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.JoinerPPD.handlePredicates(Node,ExprWalkerInfo,OpWalkerInfo)",2,2,2
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.JoinerPPD.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,6,6
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.LateralViewForwardPPD.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.OpProcFactory()",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.PTFPPD.canPushLimitToReduceSink(WindowTableFunctionDef)",5,4,7
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.PTFPPD.getLimit(WindowTableFunctionDef,List<Integer>,ExprNodeDesc)",8,3,10
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.PTFPPD.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.PTFPPD.pushRankLimit(PTFOperator,OpWalkerInfo)",5,6,14
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.PTFPPD.pushRankLimitToRedSink(PTFOperator,HiveConf,int)",2,2,4
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.PTFPPD.rankingFunctions(WindowTableFunctionDef)",1,4,4
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.ScriptPPD.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,3,3
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.TableScanPPD.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.UDTFPPD.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,3,4
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.createFilter(Operator,ExprWalkerInfo,OpWalkerInfo)",2,3,4
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.createFilter(Operator,Map<String, List<ExprNodeDesc>>,OpWalkerInfo)",6,11,14
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.getChildWalkerInfo(Operator<?>,OpWalkerInfo)",3,3,4
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.getDefaultProc()",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.getFilterProc()",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.getJoinProc()",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.getLIMProc()",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.getLVFProc()",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.getLVJProc()",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.getPTFProc()",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.getSCRProc()",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.getTSProc()",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.getUDTFProc()",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.pushFilterToStorageHandler(TableScanOperator,ExprNodeGenericFuncDesc,OpWalkerInfo,HiveConf)",4,8,9
"org.apache.hadoop.hive.ql.ppd.OpProcFactory.removeCandidates(Operator<?>,OpWalkerInfo)",1,4,4
"org.apache.hadoop.hive.ql.ppd.OpWalkerInfo.OpWalkerInfo(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpWalkerInfo.addCandidateFilterOp(FilterOperator)",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpWalkerInfo.getCandidateFilterOps()",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpWalkerInfo.getParseContext()",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpWalkerInfo.getPrunedPreds(Operator<? extends OperatorDesc>)",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpWalkerInfo.getRowResolver(Node)",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpWalkerInfo.put(Operator<? extends OperatorDesc>,OpParseContext)",1,1,1
"org.apache.hadoop.hive.ql.ppd.OpWalkerInfo.putPrunedPreds(Operator<? extends OperatorDesc>,ExprWalkerInfo)",1,1,1
"org.apache.hadoop.hive.ql.ppd.PredicatePushDown.transform(ParseContext)",1,1,1
"org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.JoinTransitive.filterExists(ReduceSinkOperator,ExprNodeDesc)",3,2,3
"org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.JoinTransitive.getTargets(CommonJoinOperator<JoinDesc>)",2,4,7
"org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.JoinTransitive.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",1,6,6
"org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.TransitiveContext.TransitiveContext()",1,1,1
"org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.TransitiveContext.getFilterPropagates()",1,1,1
"org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.TransitiveContext.getNewfilters()",1,1,1
"org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.Vectors.Vectors(int)",1,1,1
"org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.Vectors.add(int,int)",1,1,2
"org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.Vectors.toArray(Set<Integer>)",1,1,2
"org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.Vectors.traverse(Set<Integer>,int)",2,3,4
"org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.Vectors.traverse(int)",1,1,1
"org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.createFilter(Operator<?>,Operator<?>,RowResolver,ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.transform(ParseContext)",1,3,3
"org.apache.hadoop.hive.ql.processors.AddResourceProcessor.init()",1,1,1
"org.apache.hadoop.hive.ql.processors.AddResourceProcessor.run(String)",3,4,5
"org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.CommandProcessorFactory()",1,1,1
"org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.clean(HiveConf)",1,2,2
"org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.get(String)",1,1,1
"org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.get(String[],HiveConf)",4,3,5
"org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.getForHiveCommand(String[],HiveConf)",10,5,14
"org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.CommandProcessorResponse(int)",1,1,1
"org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.CommandProcessorResponse(int,String,String)",1,1,1
"org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.CommandProcessorResponse(int,String,String,Schema)",1,1,1
"org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.CommandProcessorResponse(int,String,String,Schema,Throwable)",1,1,1
"org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.CommandProcessorResponse(int,String,String,Throwable)",1,1,1
"org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.create(Exception)",1,1,1
"org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getErrorMessage()",1,1,1
"org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getException()",1,1,1
"org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getResponseCode()",1,1,1
"org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getSQLState()",1,1,1
"org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.processors.CommandUtil.authorizeCommand(SessionState,HiveOperationType,List<String>)",3,4,5
"org.apache.hadoop.hive.ql.processors.CommandUtil.authorizeCommandThrowEx(SessionState,HiveOperationType,List<String>)",1,1,1
"org.apache.hadoop.hive.ql.processors.CompileProcessor.CompileProcessorException.CompileProcessorException(String)",1,1,1
"org.apache.hadoop.hive.ql.processors.CompileProcessor.CompileProcessorException.CompileProcessorException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.ql.processors.CompileProcessor.compile(SessionState)",3,5,11
"org.apache.hadoop.hive.ql.processors.CompileProcessor.getCode()",1,1,1
"org.apache.hadoop.hive.ql.processors.CompileProcessor.getCommand()",1,1,1
"org.apache.hadoop.hive.ql.processors.CompileProcessor.getLang()",1,1,1
"org.apache.hadoop.hive.ql.processors.CompileProcessor.getNamed()",1,1,1
"org.apache.hadoop.hive.ql.processors.CompileProcessor.init()",1,1,1
"org.apache.hadoop.hive.ql.processors.CompileProcessor.parse(SessionState)",11,7,15
"org.apache.hadoop.hive.ql.processors.CompileProcessor.run(String)",2,3,4
"org.apache.hadoop.hive.ql.processors.CompileProcessor.setCode(String)",1,1,1
"org.apache.hadoop.hive.ql.processors.CompileProcessor.setLang(String)",1,1,1
"org.apache.hadoop.hive.ql.processors.CompileProcessor.setNamed(String)",1,1,1
"org.apache.hadoop.hive.ql.processors.DeleteResourceProcessor.init()",1,1,1
"org.apache.hadoop.hive.ql.processors.DeleteResourceProcessor.run(String)",3,4,5
"org.apache.hadoop.hive.ql.processors.DfsProcessor.DfsProcessor(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.processors.DfsProcessor.DfsProcessor(Configuration,boolean)",1,1,1
"org.apache.hadoop.hive.ql.processors.DfsProcessor.init()",1,1,1
"org.apache.hadoop.hive.ql.processors.DfsProcessor.run(String)",2,4,6
"org.apache.hadoop.hive.ql.processors.HiveCommand.find(String[])",6,7,8
"org.apache.hadoop.hive.ql.processors.ListResourceProcessor.init()",1,1,1
"org.apache.hadoop.hive.ql.processors.ListResourceProcessor.run(String)",2,6,6
"org.apache.hadoop.hive.ql.processors.ResetProcessor.init()",1,1,1
"org.apache.hadoop.hive.ql.processors.ResetProcessor.run(String)",3,3,5
"org.apache.hadoop.hive.ql.processors.SetProcessor.createProcessorSuccessResponse()",1,1,1
"org.apache.hadoop.hive.ql.processors.SetProcessor.dumpOption(String)",1,3,3
"org.apache.hadoop.hive.ql.processors.SetProcessor.dumpOptions(Properties)",1,6,7
"org.apache.hadoop.hive.ql.processors.SetProcessor.executeSetVariable(String,String)",1,2,3
"org.apache.hadoop.hive.ql.processors.SetProcessor.getBoolean(String)",3,3,5
"org.apache.hadoop.hive.ql.processors.SetProcessor.getSchema()",1,1,1
"org.apache.hadoop.hive.ql.processors.SetProcessor.getVariable(String)",12,12,12
"org.apache.hadoop.hive.ql.processors.SetProcessor.init()",1,1,1
"org.apache.hadoop.hive.ql.processors.SetProcessor.mapToSortedMap(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.processors.SetProcessor.propertiesToSortedMap(Properties)",1,2,2
"org.apache.hadoop.hive.ql.processors.SetProcessor.run(String)",5,7,7
"org.apache.hadoop.hive.ql.processors.SetProcessor.setConf(String,String,String,boolean)",6,6,7
"org.apache.hadoop.hive.ql.processors.SetProcessor.setVariable(String,String)",2,7,7
"org.apache.hadoop.hive.ql.processors.TestCommandProcessorFactory.setUp()",1,1,1
"org.apache.hadoop.hive.ql.processors.TestCommandProcessorFactory.testAvailableCommands()",1,5,5
"org.apache.hadoop.hive.ql.processors.TestCommandProcessorFactory.testInvalidCommands()",1,1,1
"org.apache.hadoop.hive.ql.processors.TestCompileProcessor.testSyntax()",1,1,1
"org.apache.hadoop.hive.ql.security.DummyAuthenticator.DummyAuthenticator()",1,1,1
"org.apache.hadoop.hive.ql.security.DummyAuthenticator.destroy()",1,1,1
"org.apache.hadoop.hive.ql.security.DummyAuthenticator.getConf()",1,1,1
"org.apache.hadoop.hive.ql.security.DummyAuthenticator.getGroupNames()",1,1,1
"org.apache.hadoop.hive.ql.security.DummyAuthenticator.getUserName()",1,1,1
"org.apache.hadoop.hive.ql.security.DummyAuthenticator.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.security.DummyAuthenticator.setSessionState(SessionState)",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.AuthCallContext.AuthCallContext(AuthCallContextType,List<? extends Object>,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.AuthCallContext.AuthCallContext(AuthCallContextType,Object,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.AuthCallContext.AuthCallContext(AuthCallContextType,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.authorize(Database,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.authorize(Partition,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.authorize(Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.authorize(Table,Partition,List<String>,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.authorize(Table,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.authorizeAuthorizationApiInvocation()",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.debugLog(String)",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.debugPrivPrint(Privilege[])",1,3,3
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.getAuthenticator()",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.getConf()",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.init(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.setAuthenticator(HiveAuthenticationProvider)",1,1,1
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.setConf(Configuration)",1,1,2
"org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.setMetaStoreHandler(HMSHandler)",1,1,1
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.assertExistence(String)",1,1,1
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.baseSetup()",1,1,1
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.listStatus(String)",1,2,2
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.setPermission(String)",1,1,1
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.setupDataTable()",1,1,1
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.testAlterPartition()",1,2,2
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.testCreateDb()",1,2,2
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.testCreateTable()",1,2,2
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.testCtas()",1,2,2
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.testDynamicPartitions()",1,3,3
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.testExim()",1,5,5
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.testExternalTable()",1,2,2
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.testInsert()",1,5,5
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.testLoad()",1,5,5
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.testLoadLocal()",1,5,5
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.testStaticPartition()",1,2,2
"org.apache.hadoop.hive.ql.security.FolderPermissionBase.verifyPermission(String)",1,1,1
"org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.destroy()",1,1,1
"org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.getConf()",1,1,1
"org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.getGroupNames()",1,1,1
"org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.getUserName()",1,1,1
"org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.setConf(Configuration)",2,2,4
"org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.setSessionState(SessionState)",1,1,1
"org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator.setMetaStoreHandler(HMSHandler)",1,1,1
"org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.destroy()",1,1,1
"org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.getConf()",1,1,1
"org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.getGroupNames()",2,2,2
"org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.getUserName()",2,2,2
"org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.injectGroupNames(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.injectHmapClass(Class<? extends HiveMetastoreAuthenticationProvider>)",1,1,1
"org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.injectMode(boolean)",1,1,1
"org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.injectUserName(String)",1,1,1
"org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.setConf(Configuration)",1,3,3
"org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.setMetaStoreHandler(HMSHandler)",1,1,1
"org.apache.hadoop.hive.ql.security.InjectableDummyAuthenticator.setSessionState(SessionState)",1,1,1
"org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator.setConf(Configuration)",3,3,5
"org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator.destroy()",1,1,1
"org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator.getConf()",1,1,1
"org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator.getGroupNames()",1,1,1
"org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator.getUserName()",2,2,2
"org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator.setSessionState(SessionState)",1,1,1
"org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator.destroy()",1,1,1
"org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator.getConf()",1,1,1
"org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator.getGroupNames()",1,1,1
"org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator.getUserName()",1,1,1
"org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator.setSessionState(SessionState)",1,1,1
"org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.assertAndExtractSingleObjectFromEvent(int,List<AuthCallContext>,AuthCallContextType)",1,1,1
"org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.setUp()",1,1,1
"org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.tearDown()",1,1,1
"org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.testListener()",1,2,2
"org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.validateAddPartition(Partition,Partition)",1,1,1
"org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.validateAlterPartition(Partition,Partition,String,String,List<String>,Partition)",1,1,1
"org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.validateAlterTable(Table,Table,Table,Table)",1,1,1
"org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.validateCreateDb(Database,Database)",1,1,1
"org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.validateCreateTable(Table,Table)",1,1,1
"org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.validateDropDb(Database,Database)",1,1,1
"org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.validateDropPartition(Partition,Partition)",1,1,1
"org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.validateDropTable(Table,Table)",1,1,1
"org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.validatePartition(Partition,Partition)",1,1,1
"org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.validateTable(Table,Table)",1,3,3
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.allowCreateDatabase(String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.allowCreateInDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.allowCreateInTbl(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.allowDropOnDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.allowDropOnTable(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.assertNoPrivileges(CommandProcessorResponse)",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.disallowCreateDatabase(String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.disallowCreateInDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.disallowCreateInTbl(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.getAuthorizationProvider()",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.getTestDbName()",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.getTestTableName()",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.setUp()",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.tearDown()",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.testSimplePrivileges()",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.validateCreateDb(Database,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestClientSideAuthorizationProvider.validateCreateTable(Table,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestExtendedAcls.aclEntry(AclEntryScope,AclEntryType,FsAction)",1,1,1
"org.apache.hadoop.hive.ql.security.TestExtendedAcls.aclEntry(AclEntryScope,AclEntryType,String,FsAction)",1,1,1
"org.apache.hadoop.hive.ql.security.TestExtendedAcls.getAcl(String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestExtendedAcls.setAcl(String,List<AclEntry>)",1,1,1
"org.apache.hadoop.hive.ql.security.TestExtendedAcls.setPermission(String,int)",2,2,4
"org.apache.hadoop.hive.ql.security.TestExtendedAcls.setup()",1,1,1
"org.apache.hadoop.hive.ql.security.TestExtendedAcls.verifyAcls(List<AclEntry>,List<AclEntry>)",1,5,6
"org.apache.hadoop.hive.ql.security.TestExtendedAcls.verifyPermission(String,int)",2,2,4
"org.apache.hadoop.hive.ql.security.TestFolderPermissions.setPermission(String,int)",1,1,1
"org.apache.hadoop.hive.ql.security.TestFolderPermissions.setup()",1,1,1
"org.apache.hadoop.hive.ql.security.TestFolderPermissions.verifyPermission(String,int)",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.allowCreateDatabase(String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.allowCreateInDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.allowCreateInTbl(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.allowDropOnDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.allowDropOnTable(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.assertNoPrivileges(MetaException)",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.createHiveConf()",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.disallowCreateDatabase(String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.disallowCreateInDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.disallowCreateInTbl(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.getAuthorizationProvider()",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.getTestDbName()",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.getTestTableName()",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.isTestEnabled()",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.setUp()",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.setupUser()",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.tearDown()",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.testSimplePrivileges()",2,2,5
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.validateCreateDb(Database,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.validateCreateTable(Table,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestMultiAuthorizationPreEventListener.assertAndExtractSingleObjectFromEvent(int,List<AuthCallContext>,AuthCallContextType)",1,1,1
"org.apache.hadoop.hive.ql.security.TestMultiAuthorizationPreEventListener.setUp()",1,1,1
"org.apache.hadoop.hive.ql.security.TestMultiAuthorizationPreEventListener.testMultipleAuthorizationListners()",1,1,1
"org.apache.hadoop.hive.ql.security.TestMultiAuthorizationPreEventListener.validateCreateDb(Database,Database)",1,1,1
"org.apache.hadoop.hive.ql.security.TestPasswordWithConfig.testPassword()",1,1,1
"org.apache.hadoop.hive.ql.security.TestPasswordWithCredentialProvider.doesHadoopPasswordAPIExist()",1,1,2
"org.apache.hadoop.hive.ql.security.TestPasswordWithCredentialProvider.invoke(Class,Object,String,Object...)",1,2,2
"org.apache.hadoop.hive.ql.security.TestPasswordWithCredentialProvider.testPassword()",2,2,2
"org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.allowCreateInDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.allowCreateInTbl(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.allowDropOnDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.allowDropOnTable(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.assertNoPrivileges(CommandProcessorResponse)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.disallowCreateInDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.disallowCreateInTbl(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.getAuthorizationProvider()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.getTestDbName()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.getTestTableName()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.setPermissions(String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.createHiveConf()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.dropDatabaseByOtherUser(String,int)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.dropPartitionByOtherUser(String,int)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.dropTableByOtherUser(String,int)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.getAuthorizationProvider()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.getTestDbName()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.getTestTableName()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.setPermissions(String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.setUp()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.setupFakeUser()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.setupUser()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.tearDown()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.testDropDatabase()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.testDropPartition()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.testDropTable()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.validateCreateDb(Database,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.validateCreateTable(Table,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.allowCreateInDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.allowCreateInTbl(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.allowDropOnDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.allowDropOnTable(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.assertNoPrivileges(MetaException)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.disallowCreateInDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.disallowCreateInTbl(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.getAuthorizationProvider()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.getTestDbName()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.getTestTableName()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.setPermissions(String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.aclEntry(AclEntryScope,AclEntryType,FsAction)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.aclEntry(AclEntryScope,AclEntryType,String,FsAction)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.allowCreateDatabase(String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.allowCreateInDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.allowCreateInTbl(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.allowDropOnDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.allowDropOnTable(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.allowWriteAccessViaAcl(String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.createHiveConf()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.disallowCreateDatabase(String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.disallowCreateInDb(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.disallowCreateInTbl(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.disallowWriteAccessViaAcl(String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.doesAccessAPIExist()",1,1,2
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.isTestEnabled()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.setupUser()",1,1,1
"org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.tearDown()",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationFactory.DefaultAuthorizationExceptionHandler.exception(Exception)",4,1,4
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationFactory.create(Object,Class<T>,AuthorizationExceptionHandler)",1,5,5
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.AuthorizationPreEventListener(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.PartitionWrapper.PartitionWrapper(Partition,PreEventContext)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.PartitionWrapper.PartitionWrapper(Table,Partition)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.TableWrapper.TableWrapper(Table)",1,6,6
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.authorizeAddPartition(PreAddPartitionEvent)",1,6,6
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.authorizeAlterPartition(PreAlterPartitionEvent)",1,5,5
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.authorizeAlterTable(PreAlterTableEvent)",1,4,4
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.authorizeAuthorizationAPICall()",2,4,4
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.authorizeCreateDatabase(PreCreateDatabaseEvent)",1,4,4
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.authorizeCreateTable(PreCreateTableEvent)",1,4,4
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.authorizeDropDatabase(PreDropDatabaseEvent)",1,4,4
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.authorizeDropPartition(PreDropPartitionEvent)",1,5,5
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.authorizeDropTable(PreDropTableEvent)",1,4,4
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.invalidOperationException(Exception)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.metaException(HiveException)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.onEvent(PreEventContext)",2,6,15
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getActionType(Entity)",5,3,5
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getHiveObjType(HiveObjectType)",10,2,10
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getHiveObjectRef(HiveObjectRef)",2,1,2
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getHivePrincipal(PrincipalDesc)",2,1,2
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getHivePrincipal(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getHivePrincipalType(PrincipalType)",6,2,6
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getHivePrincipals(List<PrincipalDesc>)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getHivePrivilegeObject(PrivilegeObjectDesc)",1,4,4
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getHivePrivilegeObjectType(Type)",9,2,9
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getHivePrivileges(List<PrivilegeDesc>)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getPrivObjectType(PrivilegeObjectDesc)",2,1,3
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getPrivilegeInfos(List<HiveObjectPrivilege>)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getThriftHiveObjType(HivePrivilegeObjectType)",8,2,8
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getThriftHiveObjectRef(HivePrivilegeObject)",2,1,2
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getThriftPrincipalType(HivePrincipalType)",6,2,6
"org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils.getThriftPrivilegeGrantInfo(HivePrivilege,HivePrincipal,boolean,int)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.BitSetChecker.getBitSetChecker(Privilege[],Privilege[])",1,1,5
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.authorize(Database,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.authorize(Partition,Privilege[],Privilege[])",3,3,4
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.authorize(Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.authorize(Table,Partition,List<String>,Privilege[],Privilege[])",4,8,10
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.authorize(Table,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.authorizePrivileges(PrincipalPrivilegeSet,Privilege[],boolean[],Privilege[],boolean[])",1,5,5
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.authorizeUserAndDBPriv(Database,Privilege[],Privilege[],boolean[],boolean[])",3,1,3
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.authorizeUserDBAndTable(Table,Privilege[],Privilege[],boolean[],boolean[])",3,1,3
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.authorizeUserDbAndPartition(Partition,Privilege[],Privilege[],boolean[],boolean[])",3,2,4
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.authorizeUserPriv(Privilege[],boolean[],Privilege[],boolean[])",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.booleanArrayOr(boolean[],boolean[])",1,1,4
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.checkAndThrowAuthorizationException(Privilege[],Privilege[],boolean[],boolean[],String,String,String,String)",5,5,9
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.firstFalseIndex(boolean[])",4,1,4
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.getPrivilegeStringList(Collection<List<PrivilegeGrantInfo>>)",4,5,6
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.matchPrivs(Privilege[],PrincipalPrivilegeSet,boolean[])",18,24,29
"org.apache.hadoop.hive.ql.security.authorization.BitSetCheckedAuthorizationProvider.setBooleanArray(boolean[],boolean)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider.init(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider.authorizeAuthorizationApiInvocation()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider.init(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider.setMetaStoreHandler(HMSHandler)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.HiveProxy.HiveProxy()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.HiveProxy.HiveProxy(Hive)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.HiveProxy.getDatabase(String)",2,2,4
"org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.HiveProxy.get_privilege_set(HiveObjectType,String,String,List<String>,String,String,List<String>)",2,2,4
"org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.HiveProxy.isRunFromMetaStore()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.HiveProxy.setHandler(HMSHandler)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.getAuthenticator()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.getConf()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.setAuthenticator(HiveAuthenticationProvider)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.setConf(Configuration)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly.authorize(Database,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly.authorize(Partition,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly.authorize(Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly.authorize(Table,Partition,List<String>,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly.authorize(Table,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly.authorizeAuthorizationApiInvocation()",2,1,2
"org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly.init(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly.setMetaStoreHandler(HMSHandler)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.Privilege.Privilege()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.Privilege.Privilege(PrivilegeType)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.Privilege.Privilege(PrivilegeType,EnumSet<PrivilegeScope>)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.Privilege.getPriv()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.Privilege.getScopeList()",2,2,3
"org.apache.hadoop.hive.ql.security.authorization.Privilege.setPriv(PrivilegeType)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.Privilege.supportColumnLevel()",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.Privilege.supportDBLevel()",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.Privilege.supportTableLevel()",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.Privilege.toString()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.PrivilegeRegistry.getPrivilege(PrivilegeType)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.PrivilegeRegistry.getPrivilege(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.PrivilegeRegistry.getPrivilege(int)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.PrivilegeRegistry.getPrivilegeFromRegistry(PrivilegeType)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.PrivilegeScope.PrivilegeScope(short)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.PrivilegeScope.getMode()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.PrivilegeScope.setMode(short)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.PrivilegeType.PrivilegeType(Integer,String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.PrivilegeType.getPrivTypeByName(String)",2,1,2
"org.apache.hadoop.hive.ql.security.authorization.PrivilegeType.getPrivTypeByToken(int)",2,1,2
"org.apache.hadoop.hive.ql.security.authorization.PrivilegeType.getToken()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.PrivilegeType.populateName2Type()",2,2,3
"org.apache.hadoop.hive.ql.security.authorization.PrivilegeType.populateToken2Type()",2,2,3
"org.apache.hadoop.hive.ql.security.authorization.PrivilegeType.toString()",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.DropPrivilegeExtractor.DropPrivilegeExtractor(Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.DropPrivilegeExtractor.extractDropPriv(Privilege[])",2,3,4
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.DropPrivilegeExtractor.getReadReqPriv()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.DropPrivilegeExtractor.getWriteReqPriv()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.DropPrivilegeExtractor.hasDropPrivilege()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.DropPrivilegeExtractor.setHasDropPrivilege(boolean)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.accessControlException(AccessControlException)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorizationException(Exception)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorize(Database,Privilege[],Privilege[])",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorize(Partition,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorize(Path,Privilege[],Privilege[])",2,4,5
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorize(Privilege[],Privilege[])",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorize(Table,Partition,List<String>,Privilege[],Privilege[])",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorize(Table,Partition,Privilege[],Privilege[])",1,4,4
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorize(Table,Privilege[],Privilege[])",1,8,8
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.authorizeAuthorizationApiInvocation()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.checkDeletePermission(Path,Configuration,String)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.checkPermissions(Configuration,Path,EnumSet<FsAction>)",6,4,6
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.checkPermissions(FileSystem,Path,EnumSet<FsAction>,String)",1,3,5
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.getDbLocation(Database)",2,3,3
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.getFsAction(Privilege)",11,2,11
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.getFsActions(Privilege[])",2,2,3
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.hiveException(Exception)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.init(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.initWh()",4,3,4
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.requireCreatePrivilege(Privilege[])",4,2,4
"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.setMetaStoreHandler(HMSHandler)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.DisallowTransformHook.run(HookContext)",3,1,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException.HiveAccessControlException()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException.HiveAccessControlException(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException.HiveAccessControlException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException.HiveAccessControlException(Throwable)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.HiveAuthorizerImpl(HiveAccessController,HiveAuthorizationValidator)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.applyAuthorizationConfigPolicy(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.checkPrivileges(HiveOperationType,List<HivePrivilegeObject>,List<HivePrivilegeObject>,HiveAuthzContext)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.createRole(String,HivePrincipal)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.dropRole(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.getAllRoles()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.getCurrentRoleNames()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.getPrincipalGrantInfoForRole(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.getRoleGrantInfoForPrincipal(HivePrincipal)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.getVersion()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.grantPrivileges(List<HivePrincipal>,List<HivePrivilege>,HivePrivilegeObject,HivePrincipal,boolean)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.grantRole(List<HivePrincipal>,List<String>,boolean,HivePrincipal)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.revokePrivileges(List<HivePrincipal>,List<HivePrivilege>,HivePrivilegeObject,HivePrincipal,boolean)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.revokeRole(List<HivePrincipal>,List<String>,boolean,HivePrincipal)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.setCurrentRole(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.showPrivileges(HivePrincipal,HivePrivilegeObject)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext.Builder.build()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext.Builder.getCommandString()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext.Builder.getUserIpAddress()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext.Builder.setCommandString(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext.Builder.setUserIpAddress(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext.HiveAuthzContext(Builder)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext.getCommandString()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext.getIpAddress()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext.toString()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException.HiveAuthzPluginException()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException.HiveAuthzPluginException(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException.HiveAuthzPluginException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException.HiveAuthzPluginException(Throwable)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext.Builder.Builder()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext.Builder.Builder(HiveAuthzSessionContext)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext.Builder.build()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext.Builder.getClientType()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext.Builder.getSessionString()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext.Builder.setClientType(CLIENT_TYPE)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext.Builder.setSessionString(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext.HiveAuthzSessionContext(Builder)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext.getClientType()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext.getSessionString()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext.toString()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveMetastoreClientFactoryImpl.getHiveMetastoreClient()",1,1,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrincipal.HivePrincipal(String,HivePrincipalType)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrincipal.compareTo(HivePrincipal)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrincipal.equals(Object)",8,2,8
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrincipal.getName()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrincipal.getType()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrincipal.hashCode()",1,3,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrincipal.toString()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilege.HivePrivilege(String,List<String>)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilege.HivePrivilege(String,List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilege.compare(List<String>,List<String>)",3,3,5
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilege.compareTo(HivePrivilege)",1,4,5
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilege.equals(Object)",10,3,10
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilege.getColumns()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilege.getName()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilege.getSupportedScope()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilege.hashCode()",1,3,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilege.supportsScope(PrivilegeScope)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilege.toString()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeInfo.HivePrivilegeInfo(HivePrincipal,HivePrivilege,HivePrivilegeObject,HivePrincipal,boolean,int)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeInfo.getGrantTime()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeInfo.getGrantorPrincipal()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeInfo.getObject()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeInfo.getPrincipal()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeInfo.getPrivilege()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeInfo.isGrantOption()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivilegeObject(HivePrivilegeObjectType,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivilegeObject(HivePrivilegeObjectType,String,String,HivePrivObjectActionType)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivilegeObject(HivePrivilegeObjectType,String,String,List<String>,List<String>,HivePrivObjectActionType,List<String>)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivilegeObject(HivePrivilegeObjectType,String,String,List<String>,List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivilegeObject(HivePrivilegeObjectType,String,String,List<String>,String)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.compare(Collection<String>,Collection<String>)",4,5,9
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.compareTo(HivePrivilegeObject)",1,13,17
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.createHivePrivilegeObject(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.getActionType()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.getColumns()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.getCommandParams()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.getDbObjectName(String,String)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.getDbname()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.getObjectName()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.getPartKeys()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.getType()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.toString()",3,4,10
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.HiveRoleGrant()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.HiveRoleGrant(RolePrincipalGrant)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.compareTo(HiveRoleGrant)",2,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.getGrantTime()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.getGrantor()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.getGrantorType()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.getPrincipalName()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.getPrincipalType()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.getRoleName()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.isGrantOption()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.setGrantOption(boolean)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.setGrantTime(int)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.setGrantor(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.setGrantorType(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.setPrincipalName(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.setPrincipalType(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.setRoleName(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant.toString()",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.HiveV1Authorizer(HiveConf,Hive)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.applyAuthorizationConfigPolicy(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.checkPrivileges(HiveOperationType,List<HivePrivilegeObject>,List<HivePrivilegeObject>,HiveAuthzContext)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.createRole(String,HivePrincipal)",1,2,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.dropRole(String)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.getAllRoles()",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.getCurrentRoleNames()",2,3,5
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.getPrincipalGrantInfoForRole(String)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.getRoleGrantInfoForPrincipal(HivePrincipal)",1,2,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.getVersion()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.grantOrRevokePrivs(List<HivePrincipal>,PrivilegeBag,boolean,boolean)",1,4,4
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.grantOrRevokeRole(List<HivePrincipal>,List<String>,boolean,HivePrincipal,boolean)",1,4,4
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.grantPrivileges(List<HivePrincipal>,List<HivePrivilege>,HivePrivilegeObject,HivePrincipal,boolean)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.grantRole(List<HivePrincipal>,List<String>,boolean,HivePrincipal)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.revokePrivileges(List<HivePrincipal>,List<HivePrivilege>,HivePrivilegeObject,HivePrincipal,boolean)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.revokeRole(List<HivePrincipal>,List<String>,boolean,HivePrincipal)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.setCurrentRole(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.showPrivileges(HivePrincipal,HivePrivilegeObject)",4,12,13
"org.apache.hadoop.hive.ql.security.authorization.plugin.HiveV1Authorizer.toPrivilegeBag(List<HivePrivilege>,HivePrivilegeObject,HivePrincipal,boolean)",13,20,23
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.MockedHiveAuthorizerFactory.createHiveAuthorizer(HiveMetastoreClientFactory,HiveConf,HiveAuthenticationProvider,HiveAuthzSessionContext)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.afterTests()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.assertEqualsIgnoreCase(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.beforeTest()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.checkCreateViewOrTableWithDb(String,String)",2,3,5
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.checkSingleTableInput(List<HivePrivilegeObject>)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.getHivePrivilegeObjectInputs()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.getSortedList(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.runCmd(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.testCreateTableWithDb()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.testCreateViewWithDb()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.testInputAllColumnsUsed()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.testInputNoColumnsUsed()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.testInputSomeColumnsUsed()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.testPermFunction()",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.testTempFunction()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveOperationType.checkHiveOperationTypeMatch()",1,3,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.GrantPrivAuthUtils.authorize(List<HivePrincipal>,List<HivePrivilege>,HivePrivilegeObject,boolean,IMetaStoreClient,String,List<String>,boolean)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.GrantPrivAuthUtils.checkRequiredPrivileges(RequiredPrivileges,HivePrivilegeObject,IMetaStoreClient,String,List<String>,boolean,HiveOperationType)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.GrantPrivAuthUtils.getGrantRequiredPrivileges(List<HivePrivilege>)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.PrivRequirement.PrivRequirement(SQLPrivTypeGrant[],HivePrivilegeObjectType)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.PrivRequirement.PrivRequirement(SQLPrivTypeGrant[],IOType)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.PrivRequirement.PrivRequirement(SQLPrivTypeGrant[],IOType,HivePrivObjectActionType)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.PrivRequirement.PrivRequirement(SQLPrivTypeGrant[],IOType,HivePrivObjectActionType,HivePrivilegeObjectType)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.PrivRequirement.getActionType()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.PrivRequirement.getIOType()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.PrivRequirement.getObjectType()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.PrivRequirement.getReqPrivs()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.PrivRequirement.newIOPrivRequirement(SQLPrivTypeGrant[],SQLPrivTypeGrant[])",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.PrivRequirement.newPrivRequirementList(PrivRequirement...)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.arr(PrivRequirement...)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.arr(SQLPrivTypeGrant...)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.getOperationTypes()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.getRequiredPrivs(HiveOperationType,HivePrivilegeObject,IOType)",5,5,8
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges.MissingPrivilegeCapturer.addMissingPrivilege(SQLPrivTypeGrant)",3,3,4
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges.MissingPrivilegeCapturer.getMissingPrivileges()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges.addAll(SQLPrivTypeGrant[])",2,2,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges.addPrivilege(SQLPrivTypeGrant)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges.addPrivilege(String,boolean)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges.findMissingPrivs(RequiredPrivileges)",2,4,5
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges.getPrivilegeWithGrants()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges.getRequiredPrivilegeSet()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RevokePrivAuthUtils.authorizeAndGetRevokePrivileges(List<HivePrincipal>,List<HivePrivilege>,HivePrivilegeObject,boolean,IMetaStoreClient,String)",3,9,11
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.addMissingPrivMsg(Collection<SQLPrivTypeGrant>,HivePrivilegeObject,List<String>)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.addRequiredPrivs(RequiredPrivileges,Map<String, List<PrivilegeGrantInfo>>)",2,3,4
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.assertNoDeniedPermissions(HivePrincipal,HiveOperationType,List<String>)",2,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.filterPrivsByCurrentRoles(PrincipalPrivilegeSet,List<String>)",2,5,6
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.getPluginException(String,Exception)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.getPluginObjType(HiveObjectType)",5,2,5
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.getPrivilegesFromFS(Path,HiveConf,String)",1,5,5
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.getPrivilegesFromMetaStore(IMetaStoreClient,String,HivePrivilegeObject,List<String>,boolean)",1,7,7
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.getRequiredPrivsFromThrift(PrincipalPrivilegeSet)",2,3,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.getThriftHiveObjectRef(HivePrivilegeObject)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.getThriftPrivilegeGrantInfo(HivePrivilege,HivePrincipal,boolean,int)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.getThriftPrivilegesBag(List<HivePrincipal>,List<HivePrivilege>,HivePrivilegeObject,HivePrincipal,boolean)",4,5,6
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.getValidatedPrincipal(HivePrincipal)",5,4,6
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.getValidatedPrincipals(List<HivePrincipal>)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.isOwner(IMetaStoreClient,String,List<String>,HivePrivilegeObject)",7,8,9
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.throwGetObjErr(Exception,HivePrivilegeObject)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.throwGetPrivErr(Exception,HivePrivilegeObject,String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.validatePrivileges(List<HivePrivilege>)",3,3,4
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivTypeGrant.SQLPrivTypeGrant(SQLPrivilegeType,boolean)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivTypeGrant.SQLPrivTypeGrant(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivTypeGrant.getPrivType()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivTypeGrant.getSQLPrivTypeGrant(SQLPrivilegeType,boolean)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivTypeGrant.getSQLPrivTypeGrant(String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivTypeGrant.isWithGrant()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivTypeGrant.toString()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivilegeType.getRequirePrivilege(String)",2,1,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.SQLStdHiveAccessController(HiveMetastoreClientFactory,HiveConf,HiveAuthenticationProvider,HiveAuthzSessionContext)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.applyAuthorizationConfigPolicy(HiveConf)",1,6,6
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.applyTestSettings(HiveAuthzSessionContext,HiveConf)",2,3,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.assertHiveCliAuthDisabled(HiveConf)",2,2,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.createRole(String,HivePrincipal)",3,3,5
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.doesUserHasAdminOption(List<String>)",6,4,6
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.dropRole(String)",2,2,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.ensureShowGrantAllowed(HivePrincipal)",4,4,6
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.expandAllPrivileges(List<HivePrivilege>)",1,4,4
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.expandAndValidatePrivileges(List<HivePrivilege>)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.getAllRoleAncestors(Map<String, HiveRoleGrant>,List<RolePrincipalGrant>)",1,3,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.getAllRoles()",2,2,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.getCurrentRoleNames()",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.getCurrentRoles()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.getHiveRoleGrants(IMetaStoreClient,String)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.getPluginPrivilegeObjType(HiveObjectType)",4,2,4
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.getPrincipalGrantInfoForRole(String)",2,2,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.getRoleGrantInfoForPrincipal(HivePrincipal)",1,4,4
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.getRoleGrants(String,PrincipalType)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.getRolesFromMS()",1,4,4
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.grantPrivileges(List<HivePrincipal>,List<HivePrivilege>,HivePrivilegeObject,HivePrincipal,boolean)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.grantRole(List<HivePrincipal>,List<String>,boolean,HivePrincipal)",4,6,7
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.initUserRoles()",2,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.isSupportedObjectType(HiveObjectType)",3,2,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.isUserAdmin()",3,2,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.revokePrivileges(List<HivePrincipal>,List<HivePrivilege>,HivePrivilegeObject,HivePrincipal,boolean)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.revokeRole(List<HivePrincipal>,List<String>,boolean,HivePrincipal)",4,5,6
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.setCurrentRole(String)",5,6,6
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.showPrivileges(HivePrincipal,HivePrivilegeObject)",5,7,9
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.userBelongsToRole(String)",3,2,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerForTest.SQLStdHiveAccessControllerForTest(HiveMetastoreClientFactory,HiveConf,HiveAuthenticationProvider,HiveAuthzSessionContext)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerForTest.applyAuthorizationConfigPolicy(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.SQLStdHiveAccessControllerWrapper(HiveMetastoreClientFactory,HiveConf,HiveAuthenticationProvider,HiveAuthzSessionContext)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.applyAuthorizationConfigPolicy(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.createRole(String,HivePrincipal)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.dropRole(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.getAllRoles()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.getCurrentRoleNames()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.getLowerCaseRoleNames(List<String>)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.getPrincipalGrantInfoForRole(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.getRoleGrantInfoForPrincipal(HivePrincipal)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.grantPrivileges(List<HivePrincipal>,List<HivePrivilege>,HivePrivilegeObject,HivePrincipal,boolean)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.grantRole(List<HivePrincipal>,List<String>,boolean,HivePrincipal)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.isUserAdmin()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.revokePrivileges(List<HivePrincipal>,List<HivePrivilege>,HivePrivilegeObject,HivePrincipal,boolean)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.revokeRole(List<HivePrincipal>,List<String>,boolean,HivePrincipal)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.setCurrentRole(String)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessControllerWrapper.showPrivileges(HivePrincipal,HivePrivilegeObject)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.SQLStdHiveAuthorizationValidator(HiveMetastoreClientFactory,HiveConf,HiveAuthenticationProvider,SQLStdHiveAccessControllerWrapper)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.checkPrivileges(HiveOperationType,List<HivePrivilegeObject>,IMetaStoreClient,String,IOType,List<String>)",6,4,9
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.checkPrivileges(HiveOperationType,List<HivePrivilegeObject>,List<HivePrivilegeObject>,HiveAuthzContext)",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.SQLStdHiveAuthorizationValidatorForTest(HiveMetastoreClientFactory,HiveConf,HiveAuthenticationProvider,SQLStdHiveAccessControllerWrapper)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.checkPrivileges(HiveOperationType,List<HivePrivilegeObject>,List<HivePrivilegeObject>,HiveAuthzContext)",3,2,3
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory.createHiveAuthorizer(HiveMetastoreClientFactory,HiveConf,HiveAuthenticationProvider,HiveAuthzSessionContext)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest.createHiveAuthorizer(HiveMetastoreClientFactory,HiveConf,HiveAuthenticationProvider,HiveAuthzSessionContext)",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerCLI.getCLISessionCtx()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerCLI.testAuthEnableError()",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerCLI.testConfigProcessing()",1,2,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.assertConfModificationException(HiveConf,String)",1,1,2
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.getHS2SessionCtx()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.testConfigProcessing()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.testConfigProcessingCustomSetWhitelist()",1,1,1
"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.verifyParamSettability(String[],HiveConf)",1,4,4
"org.apache.hadoop.hive.ql.session.CreateTableAutomaticGrant.create(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.session.CreateTableAutomaticGrant.getGrantMap(String)",4,8,9
"org.apache.hadoop.hive.ql.session.CreateTableAutomaticGrant.getGrantorInfoList(String)",2,3,4
"org.apache.hadoop.hive.ql.session.CreateTableAutomaticGrant.getGroupGrants()",1,1,1
"org.apache.hadoop.hive.ql.session.CreateTableAutomaticGrant.getRoleGrants()",1,1,1
"org.apache.hadoop.hive.ql.session.CreateTableAutomaticGrant.getUserGrants()",1,3,4
"org.apache.hadoop.hive.ql.session.CreateTableAutomaticGrant.validatePrivilege(String)",3,2,3
"org.apache.hadoop.hive.ql.session.LineageState.LineageState()",1,1,1
"org.apache.hadoop.hive.ql.session.LineageState.clear()",1,1,1
"org.apache.hadoop.hive.ql.session.LineageState.getLineageInfo()",1,1,1
"org.apache.hadoop.hive.ql.session.LineageState.mapDirToFop(Path,FileSinkOperator)",1,1,1
"org.apache.hadoop.hive.ql.session.LineageState.setIndex(Index)",1,1,1
"org.apache.hadoop.hive.ql.session.LineageState.setLineage(Path,DataContainer,List<FieldSchema>)",2,2,3
"org.apache.hadoop.hive.ql.session.SessionState.LogHelper.LogHelper(Log)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.LogHelper.LogHelper(Log,boolean)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.LogHelper.getChildErrStream()",1,1,3
"org.apache.hadoop.hive.ql.session.SessionState.LogHelper.getChildOutStream()",1,1,3
"org.apache.hadoop.hive.ql.session.SessionState.LogHelper.getErrStream()",1,1,3
"org.apache.hadoop.hive.ql.session.SessionState.LogHelper.getInfoStream()",1,2,3
"org.apache.hadoop.hive.ql.session.SessionState.LogHelper.getIsSilent()",1,2,2
"org.apache.hadoop.hive.ql.session.SessionState.LogHelper.getOutStream()",1,1,3
"org.apache.hadoop.hive.ql.session.SessionState.LogHelper.printError(String)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.LogHelper.printError(String,String)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.LogHelper.printInfo(String)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.LogHelper.printInfo(String,String)",1,2,2
"org.apache.hadoop.hive.ql.session.SessionState.ResourceType.postHook(Set<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.ResourceType.preHook(Set<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.SessionState(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.SessionState(HiveConf,String)",1,2,2
"org.apache.hadoop.hive.ql.session.SessionState.addLocalMapRedErrors(String,List<String>)",1,2,2
"org.apache.hadoop.hive.ql.session.SessionState.add_resource(ResourceType,String)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.add_resource(ResourceType,String,boolean)",2,2,3
"org.apache.hadoop.hive.ql.session.SessionState.add_resources(ResourceType,List<String>)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.add_resources(ResourceType,List<String>,boolean)",1,3,3
"org.apache.hadoop.hive.ql.session.SessionState.applyAuthorizationPolicy()",3,1,3
"org.apache.hadoop.hive.ql.session.SessionState.canDownloadResource(String)",1,2,2
"org.apache.hadoop.hive.ql.session.SessionState.close()",1,5,5
"org.apache.hadoop.hive.ql.session.SessionState.createPath(Configuration,Path,String)",2,2,2
"org.apache.hadoop.hive.ql.session.SessionState.createSessionPaths(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.createTempFile(HiveConf)",4,3,4
"org.apache.hadoop.hive.ql.session.SessionState.delete_resources(ResourceType)",1,3,3
"org.apache.hadoop.hive.ql.session.SessionState.delete_resources(ResourceType,List<String>)",1,3,3
"org.apache.hadoop.hive.ql.session.SessionState.detachSession()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.downloadResource(String,boolean)",5,6,10
"org.apache.hadoop.hive.ql.session.SessionState.dropSessionPaths(Configuration)",1,3,3
"org.apache.hadoop.hive.ql.session.SessionState.find_resource_type(String)",2,2,4
"org.apache.hadoop.hive.ql.session.SessionState.get()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getActiveAuthorizer()",1,2,2
"org.apache.hadoop.hive.ql.session.SessionState.getAuthenticator()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getAuthorizationMode()",3,1,3
"org.apache.hadoop.hive.ql.session.SessionState.getAuthorizer()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getAuthorizerInterface()",1,1,2
"org.apache.hadoop.hive.ql.session.SessionState.getAuthorizerV2()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getCmd()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getCommandType()",2,1,2
"org.apache.hadoop.hive.ql.session.SessionState.getConf()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getConsole()",1,2,2
"org.apache.hadoop.hive.ql.session.SessionState.getCreateTableGrants()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getCurrentDatabase()",1,1,2
"org.apache.hadoop.hive.ql.session.SessionState.getHDFSSessionPath(Configuration)",2,2,2
"org.apache.hadoop.hive.ql.session.SessionState.getHiveHistory()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getHiveOperation()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getHiveVariables()",1,1,2
"org.apache.hadoop.hive.ql.session.SessionState.getIsSilent()",2,2,2
"org.apache.hadoop.hive.ql.session.SessionState.getIsVerbose()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getLastCommand()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getLineageState()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getLocalMapRedErrors()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getLocalSessionPath(Configuration)",2,2,2
"org.apache.hadoop.hive.ql.session.SessionState.getMapRedStats()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getOverriddenConfigurations()",1,1,2
"org.apache.hadoop.hive.ql.session.SessionState.getPerfLogger(boolean)",1,3,4
"org.apache.hadoop.hive.ql.session.SessionState.getQueryId()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getResourceMap(ResourceType)",1,2,2
"org.apache.hadoop.hive.ql.session.SessionState.getSessionId()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getStackTraces()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getTempTableColStats()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getTempTableSpace()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getTempTableSpace(Configuration)",2,2,2
"org.apache.hadoop.hive.ql.session.SessionState.getTempTables()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getTezSession()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getTmpOutputFile()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getUserFromAuthenticator()",2,3,3
"org.apache.hadoop.hive.ql.session.SessionState.getUserIpAddress()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.getUserName()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.isAuthorizationModeV2()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.isHiveServerQuery()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.list_resource(ResourceType,List<String>)",3,4,5
"org.apache.hadoop.hive.ql.session.SessionState.makeSessionId()",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.registerJars(List<String>)",1,1,2
"org.apache.hadoop.hive.ql.session.SessionState.setActiveAuthorizer(Object)",4,1,4
"org.apache.hadoop.hive.ql.session.SessionState.setAuthenticator(HiveAuthenticationProvider)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setAuthorizer(HiveAuthorizationProvider)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setCmd(String)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setCommandType(HiveOperation)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setConf(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setCreateTableGrants(CreateTableAutomaticGrant)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setCurrentDatabase(String)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setCurrentSessionState(SessionState)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setHiveVariables(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setIsHiveServerQuery(boolean)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setIsSilent(boolean)",1,2,2
"org.apache.hadoop.hive.ql.session.SessionState.setIsVerbose(boolean)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setLastCommand(String)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setLocalMapRedErrors(Map<String, List<String>>)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setMapRedStats(Map<String, MapRedStats>)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setOverriddenConfigurations(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setStackTraces(Map<String, List<List<String>>>)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setTezSession(TezSessionState)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setTmpOutputFile(File)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setUserIpAddress(String)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.setupAuth()",2,3,6
"org.apache.hadoop.hive.ql.session.SessionState.start(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.session.SessionState.start(SessionState)",3,8,11
"org.apache.hadoop.hive.ql.session.SessionState.unregisterJar(List<String>)",1,2,2
"org.apache.hadoop.hive.ql.session.SessionState.validateFiles(List<String>)",3,3,5
"org.apache.hadoop.hive.ql.session.TestSessionState.RegisterJarRunnable.RegisterJarRunnable(String,SessionState)",1,1,1
"org.apache.hadoop.hive.ql.session.TestSessionState.RegisterJarRunnable.run()",1,1,1
"org.apache.hadoop.hive.ql.session.TestSessionState.TestSessionState(Boolean)",1,1,1
"org.apache.hadoop.hive.ql.session.TestSessionState.data()",1,1,1
"org.apache.hadoop.hive.ql.session.TestSessionState.setup()",1,2,2
"org.apache.hadoop.hive.ql.session.TestSessionState.testClassLoaderEquality()",1,1,1
"org.apache.hadoop.hive.ql.session.TestSessionState.testClose()",1,1,1
"org.apache.hadoop.hive.ql.session.TestSessionState.testgetDbName()",1,1,1
"org.apache.hadoop.hive.ql.stats.CounterStatsAggregator.aggregateStats(String,String)",1,1,1
"org.apache.hadoop.hive.ql.stats.CounterStatsAggregator.cleanUp(String)",1,1,1
"org.apache.hadoop.hive.ql.stats.CounterStatsAggregator.closeConnection()",1,2,2
"org.apache.hadoop.hive.ql.stats.CounterStatsAggregator.connect(Configuration,Task)",1,3,3
"org.apache.hadoop.hive.ql.stats.CounterStatsAggregator.toJobConf(Configuration)",1,1,2
"org.apache.hadoop.hive.ql.stats.CounterStatsAggregatorTez.CounterStatsAggregatorTez()",1,1,1
"org.apache.hadoop.hive.ql.stats.CounterStatsAggregatorTez.aggregateStats(String,String)",1,4,4
"org.apache.hadoop.hive.ql.stats.CounterStatsAggregatorTez.cleanUp(String)",1,1,1
"org.apache.hadoop.hive.ql.stats.CounterStatsAggregatorTez.closeConnection()",1,1,1
"org.apache.hadoop.hive.ql.stats.CounterStatsAggregatorTez.connect(Configuration,Task)",2,2,2
"org.apache.hadoop.hive.ql.stats.CounterStatsPublisher.closeConnection()",1,1,1
"org.apache.hadoop.hive.ql.stats.CounterStatsPublisher.connect(Configuration)",2,2,3
"org.apache.hadoop.hive.ql.stats.CounterStatsPublisher.init(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.stats.CounterStatsPublisher.publishStat(String,Map<String, String>)",1,3,3
"org.apache.hadoop.hive.ql.stats.DummyStatsAggregator.aggregateStats(String,String)",1,1,1
"org.apache.hadoop.hive.ql.stats.DummyStatsAggregator.cleanUp(String)",2,1,2
"org.apache.hadoop.hive.ql.stats.DummyStatsAggregator.closeConnection()",2,1,2
"org.apache.hadoop.hive.ql.stats.DummyStatsAggregator.connect(Configuration,Task)",2,1,2
"org.apache.hadoop.hive.ql.stats.DummyStatsPublisher.closeConnection()",2,1,2
"org.apache.hadoop.hive.ql.stats.DummyStatsPublisher.connect(Configuration)",2,1,2
"org.apache.hadoop.hive.ql.stats.DummyStatsPublisher.init(Configuration)",2,1,2
"org.apache.hadoop.hive.ql.stats.DummyStatsPublisher.publishStat(String,Map<String, String>)",2,1,2
"org.apache.hadoop.hive.ql.stats.KeyVerifyingStatsAggregator.aggregateStats(String,String)",1,1,1
"org.apache.hadoop.hive.ql.stats.KeyVerifyingStatsAggregator.cleanUp(String)",1,1,1
"org.apache.hadoop.hive.ql.stats.KeyVerifyingStatsAggregator.closeConnection()",1,1,1
"org.apache.hadoop.hive.ql.stats.KeyVerifyingStatsAggregator.connect(Configuration,Task)",1,1,1
"org.apache.hadoop.hive.ql.stats.StatsFactory.StatsFactory(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.stats.StatsFactory.getMaxPrefixLength(Configuration)",3,4,6
"org.apache.hadoop.hive.ql.stats.StatsFactory.getStatsAggregator()",1,1,1
"org.apache.hadoop.hive.ql.stats.StatsFactory.getStatsPublisher()",1,1,1
"org.apache.hadoop.hive.ql.stats.StatsFactory.initialize(String)",1,3,3
"org.apache.hadoop.hive.ql.stats.StatsFactory.newFactory(Configuration)",1,1,1
"org.apache.hadoop.hive.ql.stats.StatsFactory.newFactory(String,Configuration)",2,1,2
"org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(HiveConf,PrunedPartitionList,Table,TableScanOperator)",1,22,23
"org.apache.hadoop.hive.ql.stats.StatsUtils.containsNonPositives(List<Long>)",3,1,3
"org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(List<ColumnStatisticsObj>,String,Map<String, String>)",1,2,2
"org.apache.hadoop.hive.ql.stats.StatsUtils.deriveStatType(List<ColStatistics>,List<String>)",4,3,9
"org.apache.hadoop.hive.ql.stats.StatsUtils.estimateRowSizeFromSchema(HiveConf,List<ColumnInfo>,List<String>)",3,10,11
"org.apache.hadoop.hive.ql.stats.StatsUtils.getAllTableAlias(Map<String, ExprNodeDesc>)",1,3,3
"org.apache.hadoop.hive.ql.stats.StatsUtils.getAvgColLenOfFixedLengthTypes(String)",6,11,11
"org.apache.hadoop.hive.ql.stats.StatsUtils.getAvgColLenOfVariableLengthTypes(HiveConf,ObjectInspector,String)",21,15,21
"org.apache.hadoop.hive.ql.stats.StatsUtils.getBasicStatForPartitions(Table,List<Partition>,String)",1,3,4
"org.apache.hadoop.hive.ql.stats.StatsUtils.getBasicStatForTable(Table,String)",1,2,3
"org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(ColumnStatisticsObj,String,String)",11,17,17
"org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatisticsFromExprMap(HiveConf,Statistics,Map<String, ExprNodeDesc>,RowSchema)",1,4,4
"org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatisticsFromExpression(HiveConf,Statistics,ExprNodeDesc)",5,16,18
"org.apache.hadoop.hive.ql.stats.StatsUtils.getColumnInfoForColumn(String,List<ColumnInfo>)",3,2,3
"org.apache.hadoop.hive.ql.stats.StatsUtils.getDataSizeFromColumnStats(long,List<ColStatistics>)",2,17,18
"org.apache.hadoop.hive.ql.stats.StatsUtils.getFileSizeForPartitions(HiveConf,List<Partition>)",1,2,3
"org.apache.hadoop.hive.ql.stats.StatsUtils.getFileSizeForTable(HiveConf,Table)",1,1,2
"org.apache.hadoop.hive.ql.stats.StatsUtils.getFullQualifedColNameFromExprs(List<ExprNodeDesc>,Map<String, ExprNodeDesc>)",1,9,9
"org.apache.hadoop.hive.ql.stats.StatsUtils.getFullyQualifiedColumnName(String,String)",1,1,1
"org.apache.hadoop.hive.ql.stats.StatsUtils.getFullyQualifiedColumnName(String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.stats.StatsUtils.getFullyQualifiedColumnName(String,String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.stats.StatsUtils.getFullyQualifiedName(String...)",1,4,4
"org.apache.hadoop.hive.ql.stats.StatsUtils.getFullyQualifiedTableName(String,String)",1,1,1
"org.apache.hadoop.hive.ql.stats.StatsUtils.getNumRows(Table)",1,1,1
"org.apache.hadoop.hive.ql.stats.StatsUtils.getRawDataSize(Table)",1,1,1
"org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfComplexTypes(HiveConf,ObjectInspector)",2,11,16
"org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfMap(StandardConstantMapObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfPrimitiveTypeArraysFromType(String,int)",9,12,12
"org.apache.hadoop.hive.ql.stats.StatsUtils.getSumIgnoreNegatives(List<Long>)",1,1,3
"org.apache.hadoop.hive.ql.stats.StatsUtils.getTableAliasFromExprNode(ExprNodeDesc,Set<String>)",1,4,4
"org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(Table,List<ColumnInfo>,List<String>)",1,2,2
"org.apache.hadoop.hive.ql.stats.StatsUtils.getTotalSize(Table)",1,1,1
"org.apache.hadoop.hive.ql.stats.StatsUtils.getWritableSize(ObjectInspector,Object)",13,13,13
"org.apache.hadoop.hive.ql.stats.StatsUtils.processNeededColumns(List<ColumnInfo>,List<String>,Map<String, String>)",5,4,7
"org.apache.hadoop.hive.ql.stats.StatsUtils.setUnknownRcDsToAverage(List<Long>,List<Long>,int)",1,5,7
"org.apache.hadoop.hive.ql.stats.StatsUtils.stripPrefixFromColumnName(String)",1,3,3
"org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator.aggregateStats(String,String)",4,2,4
"org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator.cleanUp(String)",1,1,1
"org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator.closeConnection()",1,2,2
"org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator.connect(Configuration,Task)",1,3,3
"org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.closeConnection()",1,2,2
"org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.connect(Configuration)",1,2,2
"org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.init(Configuration)",1,2,2
"org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.publishStat(String,Map<String, String>)",1,3,3
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.JDBCStatsAggregator()",1,1,1
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.aggregateStats(String,String)",6,7,9
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.cleanUp(String)",4,7,8
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.closeConnection()",2,3,5
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.connect(Configuration,Task)",3,6,8
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.JDBCStatsPublisher()",1,1,1
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.closeConnection()",2,5,7
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.connect(Configuration)",3,6,7
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.handleSQLRecoverableException(Exception,int)",3,2,4
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.init(Configuration)",1,5,7
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.publishStat(String,Map<String, String>)",8,12,15
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.getBasicStat()",1,1,1
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.getCreate(String)",1,2,2
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.getDeleteAggr(String,String)",1,1,1
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.getIdColumnName()",1,1,1
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.getInsert(String)",1,2,2
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.getSelectAggr(String,String)",1,1,1
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.getStatColumnName(String)",1,1,1
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.getStatTableName()",1,1,1
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.getSupportedStatistics()",1,1,1
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.getTimestampColumnName()",1,1,1
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.getUpdate(String)",1,2,2
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.isValidStatistic(String)",1,1,1
"org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.isValidStatisticSet(Collection<String>)",4,2,4
"org.apache.hadoop.hive.ql.testutil.BaseScalarUdfTest.testUdf()",1,1,1
"org.apache.hadoop.hive.ql.testutil.DataBuilder.DataBuilder()",1,1,1
"org.apache.hadoop.hive.ql.testutil.DataBuilder.addRow(Object...)",1,1,1
"org.apache.hadoop.hive.ql.testutil.DataBuilder.createRows()",1,2,2
"org.apache.hadoop.hive.ql.testutil.DataBuilder.setColumnNames(String...)",1,2,2
"org.apache.hadoop.hive.ql.testutil.DataBuilder.setColumnTypes(ObjectInspector...)",1,2,2
"org.apache.hadoop.hive.ql.testutil.OperatorTestUtils.assertResults(Operator<SelectDesc>,CollectOperator,InspectableObject[],InspectableObject[])",1,3,3
"org.apache.hadoop.hive.ql.testutil.OperatorTestUtils.createOutputColumnNames(List<ExprNodeDesc>)",1,2,2
"org.apache.hadoop.hive.ql.testutil.OperatorTestUtils.getStringColumn(String)",1,1,1
"org.apache.hadoop.hive.ql.tool.TestLineageInfo.checkOutput(LineageInfo,TreeSet<String>,TreeSet<String>)",1,3,3
"org.apache.hadoop.hive.ql.tool.TestLineageInfo.testSimpleQuery()",1,2,2
"org.apache.hadoop.hive.ql.tool.TestLineageInfo.testSimpleQuery2()",1,2,2
"org.apache.hadoop.hive.ql.tool.TestLineageInfo.testSimpleQuery3()",1,2,2
"org.apache.hadoop.hive.ql.tool.TestLineageInfo.testSimpleQuery4()",1,2,2
"org.apache.hadoop.hive.ql.tool.TestLineageInfo.testSimpleQuery5()",1,2,2
"org.apache.hadoop.hive.ql.tools.LineageInfo.getInputTableList()",1,1,1
"org.apache.hadoop.hive.ql.tools.LineageInfo.getLineageInfo(String)",1,3,3
"org.apache.hadoop.hive.ql.tools.LineageInfo.getOutputTableList()",1,1,1
"org.apache.hadoop.hive.ql.tools.LineageInfo.main(String[])",1,3,3
"org.apache.hadoop.hive.ql.tools.LineageInfo.process(Node,Stack<Node>,NodeProcessorCtx,Object...)",2,3,4
"org.apache.hadoop.hive.ql.txn.compactor.Cleaner.clean(CompactionInfo)",1,3,3
"org.apache.hadoop.hive.ql.txn.compactor.Cleaner.removeFiles(String,ValidTxnList)",2,4,4
"org.apache.hadoop.hive.ql.txn.compactor.Cleaner.run()",3,8,9
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputFormat.BucketTracker.BucketTracker()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputFormat.addFileToMap(Matcher,Path,boolean,Map<Integer, BucketTracker>)",1,3,3
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputFormat.getSplits(JobConf,int)",1,7,8
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputSplit.CompactorInputSplit()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputSplit.CompactorInputSplit(Configuration,int,List<Path>,Path,Path[])",1,4,4
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputSplit.getBaseDir()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputSplit.getBucket()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputSplit.getDeltaDirs()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputSplit.getLength()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputSplit.getLocations()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputSplit.readFields(DataInput)",1,4,4
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputSplit.set(CompactorInputSplit)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputSplit.toString()",1,3,3
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorInputSplit.write(DataOutput)",1,4,4
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorMR()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorMap.close()",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorMap.configure(JobConf)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorMap.getWriter(Reporter,ObjectInspector,int)",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorMap.map(WritableComparable,CompactorInputSplit,OutputCollector<NullWritable, NullWritable>,Reporter)",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorOutputCommitter.abortJob(JobContext,int)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorOutputCommitter.abortTask(TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorOutputCommitter.commitJob(JobContext)",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorOutputCommitter.commitTask(TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorOutputCommitter.needsTaskCommit(TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorOutputCommitter.setupJob(JobContext)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorOutputCommitter.setupTask(TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorRecordReader.CompactorRecordReader(CompactorInputSplit)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorRecordReader.close()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorRecordReader.getProgress()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.CompactorRecordReader.next(NullWritable,CompactorInputSplit)",2,2,2
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.StringableList.StringableList()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.StringableList.StringableList(String)",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.StringableList.toString()",1,3,3
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.StringableMap.StringableMap(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.StringableMap.StringableMap(String)",1,4,4
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.StringableMap.toProperties()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.StringableMap.toString()",1,7,8
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.instantiate(Class<T>,String)",2,5,5
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(HiveConf,String,Table,StorageDescriptor,ValidTxnList,boolean)",2,10,10
"org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.setColumnTypes(JobConf,List<FieldSchema>)",1,3,3
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.CompactorTest()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockInputFormat.getRawReader(Configuration,boolean,int,ValidTxnList,Path,Path...)",1,6,6
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockInputFormat.getReader(InputSplit,Options)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockInputFormat.getRecordReader(InputSplit,JobConf,Reporter)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockInputFormat.getSplits(JobConf,int)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockInputFormat.validateInput(FileSystem,HiveConf,ArrayList<FileStatus>)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockOutputFormat.checkOutputSpecs(FileSystem,JobConf)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockOutputFormat.getHiveRecordWriter(JobConf,Path,Class<? extends Writable>,boolean,Properties,Progressable)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockOutputFormat.getRawRecordWriter(Path,Options)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockOutputFormat.getRecordUpdater(Path,Options)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockOutputFormat.getRecordWriter(FileSystem,JobConf,String,Progressable)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockRawReader.MockRawReader(Configuration,List<Path>)",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockRawReader.close()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockRawReader.createKey()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockRawReader.createValue()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockRawReader.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockRawReader.getPos()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockRawReader.getProgress()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockRawReader.next(RecordIdentifier,Text)",4,3,5
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockRecordWriter.MockRecordWriter(Path,Options)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockRecordWriter.close(boolean)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.MockRecordWriter.write(Writable)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.addBaseFile(HiveConf,Table,Partition,long,int)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.addBaseFile(HiveConf,Table,Partition,long,int,int,boolean)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.addDeltaFile(HiveConf,Table,Partition,long,long,int)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.addDeltaFile(HiveConf,Table,Partition,long,long,int,int,boolean)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.addFile(HiveConf,Table,Partition,long,long,int,FileType,int,boolean)",4,6,10
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.addLegacyFile(HiveConf,Table,Partition,int)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.addLegacyFile(HiveConf,Table,Partition,int,int,boolean)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.burnThroughTransactions(int)",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.getDirectories(HiveConf,Table,Partition)",1,3,3
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.getLocation(String,String)",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.newPartition(Table,String)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.newPartition(Table,String,List<Order>)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.newStorageDescriptor(String,List<Order>)",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.newTable(String,String,boolean)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.newTable(String,String,boolean,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.newTable(String,String,boolean,Map<String, String>,List<Order>)",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.openTxn()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.startCleaner(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.startInitiator(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.startThread(char,HiveConf)",2,2,5
"org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.startWorker(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.findUserToRunAs(String,Table)",2,3,3
"org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.init(BooleanPointer)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.resolvePartition(CompactionInfo)",3,4,4
"org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.resolveStorageDescriptor(Table,Partition)",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.resolveTable(CompactionInfo)",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.runJobAsSelf(String)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.setHiveConf(HiveConf)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.setThreadId(int)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.tableName(Table)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.Initiator.checkForCompaction(CompactionInfo,ValidTxnList,StorageDescriptor,String)",3,3,3
"org.apache.hadoop.hive.ql.txn.compactor.Initiator.determineCompactionType(CompactionInfo,ValidTxnList,StorageDescriptor)",8,9,14
"org.apache.hadoop.hive.ql.txn.compactor.Initiator.init(BooleanPointer)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.Initiator.lookForCurrentCompactions(ShowCompactResponse,CompactionInfo)",4,8,9
"org.apache.hadoop.hive.ql.txn.compactor.Initiator.recoverFailedCompactions(boolean)",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.Initiator.requestCompaction(CompactionInfo,String,CompactionType)",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.Initiator.run()",6,10,11
"org.apache.hadoop.hive.ql.txn.compactor.Initiator.sumDirSize(FileSystem,Path)",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.TestCleaner()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.blockedByLockPartition()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.blockedByLockTable()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.cleanupAfterMajorPartitionCompaction()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.cleanupAfterMajorPartitionCompactionNoBase()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.cleanupAfterMajorTableCompaction()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.cleanupAfterMinorPartitionCompaction()",1,4,4
"org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.cleanupAfterMinorTableCompaction()",1,4,4
"org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.nothing()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.setUpTxnDb()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.TestInitiator()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.chooseMajorOverMinorWhenBothValid()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.cleanEmptyAbortedTxns()",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.compactPartitionHighDeltaPct()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.compactPartitionTooManyDeltas()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.compactTableHighDeltaPct()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.compactTableTooManyDeltas()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.enoughDeltasNoBase()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.majorCompactOnPartitionTooManyAborts()",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.majorCompactOnTableTooManyAborts()",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.noCompactOnManyDifferentPartitionAborts()",1,3,3
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.noCompactTableDeltaPctNotHighEnough()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.noCompactTableNotEnoughDeltas()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.noCompactWhenCompactAlreadyScheduled()",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.noCompactWhenNoCompactSet()",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.nothing()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.recoverFailedLocalWorkers()",1,4,4
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.recoverFailedRemoteWorkers()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.setUpTxnDb()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.twoTxnsOnSamePartitionGenerateOneCompactionRequest()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.TestWorker()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.inputSplit()",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.inputSplitNullBase()",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.majorPartitionWithBase()",1,3,3
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.majorPartitionWithBaseMissingBuckets()",1,10,10
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.majorTableLegacy()",1,3,3
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.majorTableNoBase()",1,3,3
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.majorTableWithBase()",1,3,3
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.minorPartitionWithBase()",1,3,3
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.minorTableLegacy()",1,3,3
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.minorTableNoBase()",1,3,3
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.minorTableWithBase()",1,4,4
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.nothing()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.setUpTxnDb()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.sortedPartition()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.sortedTable()",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.stringableList()",1,5,5
"org.apache.hadoop.hive.ql.txn.compactor.TestWorker.stringableMap()",1,5,5
"org.apache.hadoop.hive.ql.txn.compactor.Worker.hostname()",1,2,2
"org.apache.hadoop.hive.ql.txn.compactor.Worker.init(BooleanPointer)",1,1,1
"org.apache.hadoop.hive.ql.txn.compactor.Worker.run()",4,12,13
"org.apache.hadoop.hive.ql.udaf.TestStreamingAvg.avgDouble(Iterator<Double>,int,int,int,Iterator<Double>)",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingAvg.avgHiveDecimal(Iterator<HiveDecimal>,int,int,int,Iterator<HiveDecimal>)",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingAvg.testDouble_0_5()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingAvg.testDouble_15_15()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingAvg.testDouble_3_0()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingAvg.testDouble_3_4()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingAvg.testDouble_7_2()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingAvg.testDouble_unb_0()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingAvg.testDouble_unb_5()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingAvg.testHiveDecimal_3_4()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMax.maxLong(Iterator<Long>,int,int,int,Iterator<Long>)",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMax.testLong_0_5()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMax.testLong_15_15()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMax.testLong_3_0()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMax.testLong_3_4()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMax.testLong_7_2()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMax.testLong_unb_0()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMax.testLong_unb_5()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMax.testLongr_1_4()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMax.testLongr_3_4()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMax.testLongr_7_2()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMin.minLong(Iterator<Long>,int,int,int,Iterator<Long>)",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMin.testLong_0_5()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMin.testLong_15_15()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMin.testLong_3_0()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMin.testLong_3_4()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMin.testLong_7_2()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMin.testLong_unb_0()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMin.testLongr_1_4()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMin.testLongr_3_4()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMin.testLongr_7_2()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingMin.testLongr_unb_5()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum._agg(GenericUDAFResolver,TypeInfo[],Iterator<T>,TypeHandler<T, TW>,TW[],ObjectInspector[],int,int,int,Iterator<T>)",1,6,8
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.sumDouble(Iterator<Double>,int,int,int,Iterator<Double>)",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.sumHiveDecimal(Iterator<HiveDecimal>,int,int,int,Iterator<HiveDecimal>)",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.sumLong(Iterator<Long>,int,int,int,Iterator<Long>)",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testDouble_0_5()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testDouble_15_15()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testDouble_3_0()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testDouble_3_4()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testDouble_7_2()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testDouble_unb_0()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testDouble_unb_5()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testHiveDecimal_0_5()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testHiveDecimal_15_15()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testHiveDecimal_3_0()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testHiveDecimal_3_4()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testHiveDecimal_7_2()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testHiveDecimal_unb_0()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testHiveDecimal_unb_5()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testLong_0_5()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testLong_15_15()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testLong_3_0()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testLong_3_4()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testLong_7_2()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testLong_unb_0()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.testLong_unb_5()",1,1,1
"org.apache.hadoop.hive.ql.udaf.TestStreamingSum.wdwFrame(int,int)",1,3,3
"org.apache.hadoop.hive.ql.udf.TestGenericUDFDate.testDateWritablepToDate()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestGenericUDFDate.testStringToDate()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestGenericUDFDate.testTimestampToDate()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestGenericUDFDateAdd.testDateWritablepToDate()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestGenericUDFDateAdd.testStringToDate()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestGenericUDFDateAdd.testTimestampToDate()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestGenericUDFDateDiff.testDateWritablepToDate()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestGenericUDFDateDiff.testStringToDate()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestGenericUDFDateDiff.testTimestampToDate()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestGenericUDFDateSub.testDateWritablepToDate()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestGenericUDFDateSub.testStringToDate()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestGenericUDFDateSub.testTimestampToDate()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestGenericUDFUtils.testFindText()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestToInteger.testTextToInteger()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFBase64.testBase64Conversion()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFHex.testHexConversion()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.createDecimal(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.testAcos()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.testAsin()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.testAtan()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.testCos()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.testDegrees()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.testExp()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.testLn()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.testLog()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.testLog10()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.testLog2()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.testRadians()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.testSin()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.testSqrt()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFMath.testTan()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFSign.testDecimalSign()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFUnbase64.testUnbase64Conversion()",1,1,1
"org.apache.hadoop.hive.ql.udf.TestUDFUnhex.testUnhexConversion()",1,2,2
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.MyComparator.compare(Entry<LongWritable, LongWritable>,Entry<LongWritable, LongWritable>)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.PercentileLongArrayEvaluator.PercentileLongArrayEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.PercentileLongArrayEvaluator.init()",1,2,2
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.PercentileLongArrayEvaluator.iterate(LongWritable,List<DoubleWritable>)",5,6,7
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.PercentileLongArrayEvaluator.merge(State)",2,2,6
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.PercentileLongArrayEvaluator.terminate()",2,6,7
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.PercentileLongArrayEvaluator.terminatePartial()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.PercentileLongEvaluator.PercentileLongEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.PercentileLongEvaluator.init()",1,2,2
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.PercentileLongEvaluator.iterate(LongWritable,Double)",4,3,7
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.PercentileLongEvaluator.merge(State)",2,2,6
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.PercentileLongEvaluator.terminate()",2,3,5
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.PercentileLongEvaluator.terminatePartial()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.getPercentile(List<Entry<LongWritable, LongWritable>>,double)",3,1,5
"org.apache.hadoop.hive.ql.udf.UDAFPercentile.increment(State,LongWritable,long)",1,2,3
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxDoubleEvaluator.MaxDoubleEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxDoubleEvaluator.init()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxDoubleEvaluator.iterate(DoubleWritable)",1,3,3
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxDoubleEvaluator.merge(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxDoubleEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxDoubleEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxFloatEvaluator.MaxFloatEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxFloatEvaluator.init()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxFloatEvaluator.iterate(FloatWritable)",1,3,3
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxFloatEvaluator.merge(FloatWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxFloatEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxFloatEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxIntEvaluator.MaxIntEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxIntEvaluator.init()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxIntEvaluator.iterate(IntWritable)",1,3,3
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxIntEvaluator.merge(IntWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxIntEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxIntEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxLongEvaluator.MaxLongEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxLongEvaluator.init()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxLongEvaluator.iterate(LongWritable)",1,3,3
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxLongEvaluator.merge(LongWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxLongEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxLongEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxShortEvaluator.MaxShortEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxShortEvaluator.init()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxShortEvaluator.iterate(ShortWritable)",1,3,3
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxShortEvaluator.merge(ShortWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxShortEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxShortEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxStringEvaluator.MaxStringEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxStringEvaluator.init()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxStringEvaluator.iterate(Text)",1,4,4
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxStringEvaluator.merge(Text)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxStringEvaluator.terminate()",1,1,2
"org.apache.hadoop.hive.ql.udf.UDAFTestMax.MaxStringEvaluator.terminatePartial()",1,1,2
"org.apache.hadoop.hive.ql.udf.UDAFWrongArgLengthForTestCase.UDAFWrongArgLengthForTestCaseEvaluator.UDAFWrongArgLengthForTestCaseEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFWrongArgLengthForTestCase.UDAFWrongArgLengthForTestCaseEvaluator.init()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFWrongArgLengthForTestCase.UDAFWrongArgLengthForTestCaseEvaluator.iterate(Object)",1,2,3
"org.apache.hadoop.hive.ql.udf.UDAFWrongArgLengthForTestCase.UDAFWrongArgLengthForTestCaseEvaluator.merge()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFWrongArgLengthForTestCase.UDAFWrongArgLengthForTestCaseEvaluator.terminate()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDAFWrongArgLengthForTestCase.UDAFWrongArgLengthForTestCaseEvaluator.terminatePartial()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFAcos.doEvaluate(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFAscii.evaluate(Text)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFAsin.doEvaluate(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFAtan.doEvaluate(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFBase64.evaluate(BytesWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFBin.evaluate(LongWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFConv.byte2char(int,int)",1,2,2
"org.apache.hadoop.hive.ql.udf.UDFConv.char2byte(int,int)",1,2,2
"org.apache.hadoop.hive.ql.udf.UDFConv.decode(long,int)",1,2,2
"org.apache.hadoop.hive.ql.udf.UDFConv.encode(int,int)",4,3,5
"org.apache.hadoop.hive.ql.udf.UDFConv.evaluate(Text,IntWritable,IntWritable)",3,5,19
"org.apache.hadoop.hive.ql.udf.UDFConv.unsignedLongDiv(long,int)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFCos.doEvaluate(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFDayOfMonth.UDFDayOfMonth()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFDayOfMonth.evaluate(DateWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFDayOfMonth.evaluate(Text)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFDayOfMonth.evaluate(TimestampWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFDegrees.doEvaluate(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFE.UDFE()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFE.evaluate()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFExp.doEvaluate(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFFileLookup.evaluate(Text)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFFileLookup.loadData()",1,3,3
"org.apache.hadoop.hive.ql.udf.UDFFindInSet.evaluate(Text,Text)",8,7,14
"org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.UDFFromUnixTime()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.eval(long,Text)",1,2,2
"org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.evaluate(IntWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.evaluate(IntWritable,Text)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.evaluate(LongWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.evaluate(LongWritable,Text)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFHex.evaluate(BytesWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFHex.evaluate(IntWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFHex.evaluate(LongWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFHex.evaluate(Text)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFHex.evaluate(byte[],int)",1,2,3
"org.apache.hadoop.hive.ql.udf.UDFHex.evaluate(long)",1,2,2
"org.apache.hadoop.hive.ql.udf.UDFHour.UDFHour()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFHour.evaluate(Text)",2,2,4
"org.apache.hadoop.hive.ql.udf.UDFHour.evaluate(TimestampWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFJson.AddingList.iterator()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFJson.AddingList.removeRange(int,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFJson.HashCache.HashCache()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFJson.HashCache.removeEldestEntry(Entry<K, V>)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFJson.UDFJson()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(String,String)",8,6,15
"org.apache.hadoop.hive.ql.udf.UDFJson.extract(Object,String)",5,7,11
"org.apache.hadoop.hive.ql.udf.UDFJson.extract_json_withindex(Object,ArrayList<String>)",8,8,12
"org.apache.hadoop.hive.ql.udf.UDFJson.extract_json_withkey(Object,String)",5,8,9
"org.apache.hadoop.hive.ql.udf.UDFLength.evaluate(BytesWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFLength.evaluate(Text)",2,2,4
"org.apache.hadoop.hive.ql.udf.UDFLike.UDFLike()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFLike.evaluate(Text,Text)",6,7,11
"org.apache.hadoop.hive.ql.udf.UDFLike.find(Text,Text,int,int)",4,1,5
"org.apache.hadoop.hive.ql.udf.UDFLike.likePatternToRegExp(String)",3,8,8
"org.apache.hadoop.hive.ql.udf.UDFLike.parseSimplePattern(String)",8,9,10
"org.apache.hadoop.hive.ql.udf.UDFLn.doEvaluate(DoubleWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFLog.doEvaluate(DoubleWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFLog.evaluate(DoubleWritable,DoubleWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFLog.evaluate(DoubleWritable,HiveDecimalWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFLog.evaluate(HiveDecimalWritable,DoubleWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFLog.evaluate(HiveDecimalWritable,HiveDecimalWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFLog.log(double,double)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFLog10.doEvaluate(DoubleWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFLog2.doEvaluate(DoubleWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFMath.evaluate(DoubleWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFMath.evaluate(HiveDecimalWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFMinute.UDFMinute()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFMinute.evaluate(Text)",2,2,4
"org.apache.hadoop.hive.ql.udf.UDFMinute.evaluate(TimestampWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFMonth.UDFMonth()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFMonth.evaluate(DateWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFMonth.evaluate(Text)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFMonth.evaluate(TimestampWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFOPBitAnd.UDFOPBitAnd()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFOPBitAnd.evaluate(ByteWritable,ByteWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFOPBitAnd.evaluate(IntWritable,IntWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFOPBitAnd.evaluate(LongWritable,LongWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFOPBitAnd.evaluate(ShortWritable,ShortWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFOPBitNot.UDFOPBitNot()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFOPBitNot.evaluate(ByteWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFOPBitNot.evaluate(IntWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFOPBitNot.evaluate(LongWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFOPBitNot.evaluate(ShortWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFOPBitOr.UDFOPBitOr()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFOPBitOr.evaluate(ByteWritable,ByteWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFOPBitOr.evaluate(IntWritable,IntWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFOPBitOr.evaluate(LongWritable,LongWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFOPBitOr.evaluate(ShortWritable,ShortWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFOPBitXor.UDFOPBitXor()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFOPBitXor.evaluate(ByteWritable,ByteWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFOPBitXor.evaluate(IntWritable,IntWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFOPBitXor.evaluate(LongWritable,LongWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFOPBitXor.evaluate(ShortWritable,ShortWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFOPLongDivide.evaluate(LongWritable,LongWritable)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFPI.UDFPI()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFPI.evaluate()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFParseUrl.UDFParseUrl()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFParseUrl.evaluate(String,String)",11,10,14
"org.apache.hadoop.hive.ql.udf.UDFParseUrl.evaluate(String,String,String)",4,3,5
"org.apache.hadoop.hive.ql.udf.UDFRadians.doEvaluate(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFRand.UDFRand()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFRand.evaluate()",1,1,2
"org.apache.hadoop.hive.ql.udf.UDFRand.evaluate(LongWritable)",1,2,2
"org.apache.hadoop.hive.ql.udf.UDFRegExp.UDFRegExp()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFRegExp.evaluate(Text,Text)",3,5,7
"org.apache.hadoop.hive.ql.udf.UDFRegExpExtract.UDFRegExpExtract()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFRegExpExtract.evaluate(String,String)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFRegExpExtract.evaluate(String,String,Integer)",3,4,6
"org.apache.hadoop.hive.ql.udf.UDFRegExpReplace.UDFRegExpReplace()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFRegExpReplace.evaluate(Text,Text,Text)",2,5,8
"org.apache.hadoop.hive.ql.udf.UDFRepeat.evaluate(Text,IntWritable)",2,3,7
"org.apache.hadoop.hive.ql.udf.UDFReverse.evaluate(Text)",2,3,4
"org.apache.hadoop.hive.ql.udf.UDFReverse.reverse(byte[],int,int)",1,1,2
"org.apache.hadoop.hive.ql.udf.UDFSecond.UDFSecond()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFSecond.evaluate(Text)",2,2,4
"org.apache.hadoop.hive.ql.udf.UDFSecond.evaluate(TimestampWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFSign.evaluate(DoubleWritable)",2,3,4
"org.apache.hadoop.hive.ql.udf.UDFSign.evaluate(HiveDecimalWritable)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFSin.doEvaluate(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFSpace.evaluate(IntWritable)",2,2,4
"org.apache.hadoop.hive.ql.udf.UDFSqrt.doEvaluate(DoubleWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFSubstr.UDFSubstr()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFSubstr.evaluate(BytesWritable,IntWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFSubstr.evaluate(BytesWritable,IntWritable,IntWritable)",4,1,6
"org.apache.hadoop.hive.ql.udf.UDFSubstr.evaluate(Text,IntWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFSubstr.evaluate(Text,IntWritable,IntWritable)",4,1,6
"org.apache.hadoop.hive.ql.udf.UDFSubstr.makeIndex(int,int,int)",2,1,5
"org.apache.hadoop.hive.ql.udf.UDFTan.doEvaluate(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFTestErrorOnFalse.evaluate(Boolean)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFTestLength.evaluate(Text)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFTestLength2.evaluate(String)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFToBoolean.UDFToBoolean()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToBoolean.evaluate(ByteWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToBoolean.evaluate(DateWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToBoolean.evaluate(DoubleWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToBoolean.evaluate(FloatWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToBoolean.evaluate(HiveDecimalWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToBoolean.evaluate(IntWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToBoolean.evaluate(LongWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToBoolean.evaluate(NullWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToBoolean.evaluate(ShortWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToBoolean.evaluate(Text)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToBoolean.evaluate(TimestampWritable)",2,3,3
"org.apache.hadoop.hive.ql.udf.UDFToByte.UDFToByte()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToByte.evaluate(BooleanWritable)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToByte.evaluate(DoubleWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToByte.evaluate(FloatWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToByte.evaluate(HiveDecimalWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToByte.evaluate(IntWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToByte.evaluate(LongWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToByte.evaluate(NullWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToByte.evaluate(ShortWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToByte.evaluate(Text)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToByte.evaluate(TimestampWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToDouble.UDFToDouble()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToDouble.evaluate(BooleanWritable)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToDouble.evaluate(ByteWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToDouble.evaluate(FloatWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToDouble.evaluate(HiveDecimalWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToDouble.evaluate(IntWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToDouble.evaluate(LongWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToDouble.evaluate(NullWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToDouble.evaluate(ShortWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToDouble.evaluate(Text)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToDouble.evaluate(TimestampWritable)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToFloat.UDFToFloat()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToFloat.evaluate(BooleanWritable)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToFloat.evaluate(ByteWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToFloat.evaluate(DoubleWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToFloat.evaluate(HiveDecimalWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToFloat.evaluate(IntWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToFloat.evaluate(LongWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToFloat.evaluate(NullWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToFloat.evaluate(ShortWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToFloat.evaluate(Text)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToFloat.evaluate(TimestampWritable)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToInteger.UDFToInteger()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToInteger.evaluate(BooleanWritable)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToInteger.evaluate(ByteWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToInteger.evaluate(DoubleWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToInteger.evaluate(FloatWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToInteger.evaluate(HiveDecimalWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToInteger.evaluate(LongWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToInteger.evaluate(NullWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToInteger.evaluate(ShortWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToInteger.evaluate(Text)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToInteger.evaluate(TimestampWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToLong.UDFToLong()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToLong.evaluate(BooleanWritable)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToLong.evaluate(ByteWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToLong.evaluate(DoubleWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToLong.evaluate(FloatWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToLong.evaluate(HiveDecimalWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToLong.evaluate(IntWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToLong.evaluate(LongWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToLong.evaluate(NullWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToLong.evaluate(ShortWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToLong.evaluate(Text)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToLong.evaluate(TimestampWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToShort.UDFToShort()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToShort.evaluate(BooleanWritable)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToShort.evaluate(ByteWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToShort.evaluate(DoubleWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToShort.evaluate(FloatWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToShort.evaluate(HiveDecimalWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToShort.evaluate(IntWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToShort.evaluate(LongWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToShort.evaluate(NullWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToShort.evaluate(Text)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToShort.evaluate(TimestampWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToString.UDFToString()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToString.evaluate(BooleanWritable)",2,2,3
"org.apache.hadoop.hive.ql.udf.UDFToString.evaluate(ByteWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToString.evaluate(BytesWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFToString.evaluate(DateWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToString.evaluate(DoubleWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToString.evaluate(FloatWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToString.evaluate(HiveDecimalWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToString.evaluate(IntWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToString.evaluate(LongWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToString.evaluate(NullWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFToString.evaluate(ShortWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFToString.evaluate(Text)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFToString.evaluate(TimestampWritable)",2,2,2
"org.apache.hadoop.hive.ql.udf.UDFUnbase64.evaluate(Text)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFUnhex.evaluate(Text)",3,3,5
"org.apache.hadoop.hive.ql.udf.UDFWeekOfYear.UDFWeekOfYear()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFWeekOfYear.evaluate(DateWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFWeekOfYear.evaluate(Text)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFWeekOfYear.evaluate(TimestampWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFYear.UDFYear()",1,1,1
"org.apache.hadoop.hive.ql.udf.UDFYear.evaluate(DateWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.UDFYear.evaluate(Text)",2,1,3
"org.apache.hadoop.hive.ql.udf.UDFYear.evaluate(TimestampWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver.getEvaluator(GenericUDAFParameterInfo)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver.getEvaluator(TypeInfo[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDFEWAHBitmapBop.AbstractGenericUDFEWAHBitmapBop(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDFEWAHBitmapBop.bitmapToWordArray(EWAHCompressedBitmap)",1,1,2
"org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDFEWAHBitmapBop.evaluate(DeferredObject[])",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDFEWAHBitmapBop.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDFEWAHBitmapBop.initialize(ObjectInspector[])",4,3,4
"org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDFEWAHBitmapBop.wordArrayToBitmap(Object)",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDFReflect.findMethod(Class,String,Class<?>,boolean)",6,10,13
"org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDFReflect.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDFReflect.setupParameterOIs(ObjectInspector[],int)",3,2,4
"org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDFReflect.setupParameters(DeferredObject[],int)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.DecimalNumDistinctValueEstimator.DecimalNumDistinctValueEstimator(String,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.DecimalNumDistinctValueEstimator.DecimalNumDistinctValueEstimator(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.DecimalNumDistinctValueEstimator.addToEstimator(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.DecimalNumDistinctValueEstimator.addToEstimatorPCSA(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.DoubleNumDistinctValueEstimator.DoubleNumDistinctValueEstimator(String,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.DoubleNumDistinctValueEstimator.DoubleNumDistinctValueEstimator(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.DoubleNumDistinctValueEstimator.addToEstimator(double)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.DoubleNumDistinctValueEstimator.addToEstimatorPCSA(double)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.DummyContextUDF.configure(MapredContext)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.DummyContextUDF.evaluate(DeferredObject[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.DummyContextUDF.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.DummyContextUDF.initialize(ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.AbstractGenericUDAFAverageEvaluator.AverageAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.AbstractGenericUDAFAverageEvaluator.init(Mode,ObjectInspector[])",2,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.AbstractGenericUDAFAverageEvaluator.iterate(AggregationBuffer,Object[])",1,4,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.AbstractGenericUDAFAverageEvaluator.merge(AggregationBuffer,Object)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.AbstractGenericUDAFAverageEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.AbstractGenericUDAFAverageEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.AbstractGenericUDAFAverageEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDecimal.deriveResultDecimalTypeInfo()",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDecimal.doIterate(AverageAggregationBuffer<HiveDecimal>,PrimitiveObjectInspector,Object)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDecimal.doMerge(AverageAggregationBuffer<HiveDecimal>,Long,ObjectInspector,Object)",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDecimal.doReset(AverageAggregationBuffer<HiveDecimal>)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDecimal.doTerminate(AverageAggregationBuffer<HiveDecimal>)",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDecimal.doTerminatePartial(AverageAggregationBuffer<HiveDecimal>)",1,2,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDecimal.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDecimal.getSumFieldJavaObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDecimal.getSumFieldWritableObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDecimal.getWindowingEvaluator(WindowFrameDef)",1,5,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDouble.doIterate(AverageAggregationBuffer<Double>,PrimitiveObjectInspector,Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDouble.doMerge(AverageAggregationBuffer<Double>,Long,ObjectInspector,Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDouble.doReset(AverageAggregationBuffer<Double>)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDouble.doTerminate(AverageAggregationBuffer<Double>)",2,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDouble.doTerminatePartial(AverageAggregationBuffer<Double>)",1,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDouble.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDouble.getSumFieldJavaObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDouble.getSumFieldWritableObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.GenericUDAFAverageEvaluatorDouble.getWindowingEvaluator(WindowFrameDef)",1,2,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.deriveSumFieldTypeInfo(int,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.getEvaluator(TypeInfo[])",6,3,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.GenericUDAFBridge(UDAF)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.GenericUDAFBridgeEvaluator.GenericUDAFBridgeEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.GenericUDAFBridgeEvaluator.GenericUDAFBridgeEvaluator(Class<? extends UDAFEvaluator>)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.GenericUDAFBridgeEvaluator.UDAFAgg.UDAFAgg(UDAFEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.GenericUDAFBridgeEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.GenericUDAFBridgeEvaluator.getUdafEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.GenericUDAFBridgeEvaluator.init(Mode,ObjectInspector[])",1,2,11
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.GenericUDAFBridgeEvaluator.iterate(AggregationBuffer,Object[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.GenericUDAFBridgeEvaluator.merge(AggregationBuffer,Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.GenericUDAFBridgeEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.GenericUDAFBridgeEvaluator.setUdafEvaluator(Class<? extends UDAFEvaluator>)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.GenericUDAFBridgeEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.GenericUDAFBridgeEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.getEvaluator(TypeInfo[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.getUDAFClass()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectList.GenericUDAFCollectList()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectList.getEvaluator(TypeInfo[])",3,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet.GenericUDAFCollectSet()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet.getEvaluator(TypeInfo[])",3,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBinaryStatsEvaluator.BinaryStatsAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBinaryStatsEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBinaryStatsEvaluator.init(Mode,ObjectInspector[])",2,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBinaryStatsEvaluator.iterate(AggregationBuffer,Object[])",1,5,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBinaryStatsEvaluator.merge(AggregationBuffer,Object)",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBinaryStatsEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBinaryStatsEvaluator.terminate(AggregationBuffer)",1,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBinaryStatsEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBooleanStatsEvaluator.BooleanStatsAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBooleanStatsEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBooleanStatsEvaluator.init(Mode,ObjectInspector[])",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBooleanStatsEvaluator.iterate(AggregationBuffer,Object[])",1,5,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBooleanStatsEvaluator.merge(AggregationBuffer,Object)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBooleanStatsEvaluator.printDebugOutput(String,AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBooleanStatsEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBooleanStatsEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFBooleanStatsEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFDecimalStatsEvaluator.DecimalStatsAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFDecimalStatsEvaluator.DecimalStatsAgg.update(Object,PrimitiveObjectInspector)",1,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFDecimalStatsEvaluator.DecimalStatsAgg.updateMax(Object,HiveDecimalObjectInspector)",1,4,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFDecimalStatsEvaluator.DecimalStatsAgg.updateMin(Object,HiveDecimalObjectInspector)",1,4,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFDecimalStatsEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFDecimalStatsEvaluator.getValueObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFDecimalStatsEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFDoubleStatsEvaluator.DoubleStatsAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFDoubleStatsEvaluator.DoubleStatsAgg.update(Object,PrimitiveObjectInspector)",1,1,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFDoubleStatsEvaluator.DoubleStatsAgg.updateMax(Object,DoubleObjectInspector)",1,4,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFDoubleStatsEvaluator.DoubleStatsAgg.updateMin(Object,DoubleObjectInspector)",1,4,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFDoubleStatsEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFDoubleStatsEvaluator.getValueObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFDoubleStatsEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFLongStatsEvaluator.LongStatsAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFLongStatsEvaluator.LongStatsAgg.update(Object,PrimitiveObjectInspector)",1,1,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFLongStatsEvaluator.LongStatsAgg.updateMax(Object,LongObjectInspector)",1,4,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFLongStatsEvaluator.LongStatsAgg.updateMin(Object,LongObjectInspector)",1,4,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFLongStatsEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFLongStatsEvaluator.getValueObjectInspector()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFLongStatsEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFNumericStatsEvaluator.NumericStatsAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFNumericStatsEvaluator.NumericStatsAgg.initNDVEstimator(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFNumericStatsEvaluator.NumericStatsAgg.reset(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFNumericStatsEvaluator.NumericStatsAgg.serialize(Object[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFNumericStatsEvaluator.NumericStatsAgg.serializeCommon(Object[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFNumericStatsEvaluator.NumericStatsAgg.serializePartial(Object[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFNumericStatsEvaluator.init(Mode,ObjectInspector[])",2,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFNumericStatsEvaluator.iterate(AggregationBuffer,Object[])",3,6,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFNumericStatsEvaluator.merge(AggregationBuffer,Object)",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFNumericStatsEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFNumericStatsEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFStringStatsEvaluator.StringStatsAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFStringStatsEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFStringStatsEvaluator.init(Mode,ObjectInspector[])",2,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFStringStatsEvaluator.initNDVEstimator(StringStatsAgg,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFStringStatsEvaluator.iterate(AggregationBuffer,Object[])",3,7,10
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFStringStatsEvaluator.merge(AggregationBuffer,Object)",1,4,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFStringStatsEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFStringStatsEvaluator.terminate(AggregationBuffer)",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.GenericUDAFStringStatsEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.getEvaluator(TypeInfo[])",10,3,10
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFContextNGrams.GenericUDAFContextNGramEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFContextNGrams.GenericUDAFContextNGramEvaluator.init(Mode,ObjectInspector[])",2,4,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFContextNGrams.GenericUDAFContextNGramEvaluator.iterate(AggregationBuffer,Object[])",8,12,17
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFContextNGrams.GenericUDAFContextNGramEvaluator.merge(AggregationBuffer,Object)",4,5,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFContextNGrams.GenericUDAFContextNGramEvaluator.processNgrams(NGramAggBuf,ArrayList<String>)",5,5,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFContextNGrams.GenericUDAFContextNGramEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFContextNGrams.GenericUDAFContextNGramEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFContextNGrams.GenericUDAFContextNGramEvaluator.terminatePartial(AggregationBuffer)",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFContextNGrams.getEvaluator(TypeInfo[])",12,12,18
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation.GenericUDAFCorrelationEvaluator.StdAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation.GenericUDAFCorrelationEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation.GenericUDAFCorrelationEvaluator.getResult()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation.GenericUDAFCorrelationEvaluator.init(Mode,ObjectInspector[])",2,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation.GenericUDAFCorrelationEvaluator.iterate(AggregationBuffer,Object[])",1,2,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation.GenericUDAFCorrelationEvaluator.merge(AggregationBuffer,Object)",1,4,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation.GenericUDAFCorrelationEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation.GenericUDAFCorrelationEvaluator.setResult(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation.GenericUDAFCorrelationEvaluator.terminate(AggregationBuffer)",2,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation.GenericUDAFCorrelationEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation.getEvaluator(TypeInfo[])",8,6,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.GenericUDAFCountEvaluator.CountAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.GenericUDAFCountEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.GenericUDAFCountEvaluator.init(Mode,ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.GenericUDAFCountEvaluator.iterate(AggregationBuffer,Object[])",5,1,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.GenericUDAFCountEvaluator.merge(AggregationBuffer,Object)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.GenericUDAFCountEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.GenericUDAFCountEvaluator.setCountAllColumns(boolean)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.GenericUDAFCountEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.GenericUDAFCountEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.getEvaluator(GenericUDAFParameterInfo)",4,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.getEvaluator(TypeInfo[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance.GenericUDAFCovarianceEvaluator.StdAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance.GenericUDAFCovarianceEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance.GenericUDAFCovarianceEvaluator.getResult()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance.GenericUDAFCovarianceEvaluator.init(Mode,ObjectInspector[])",2,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance.GenericUDAFCovarianceEvaluator.iterate(AggregationBuffer,Object[])",1,2,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance.GenericUDAFCovarianceEvaluator.merge(AggregationBuffer,Object)",1,4,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance.GenericUDAFCovarianceEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance.GenericUDAFCovarianceEvaluator.setResult(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance.GenericUDAFCovarianceEvaluator.terminate(AggregationBuffer)",2,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance.GenericUDAFCovarianceEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance.getEvaluator(TypeInfo[])",8,6,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovarianceSample.GenericUDAFCovarianceSampleEvaluator.terminate(AggregationBuffer)",2,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovarianceSample.getEvaluator(TypeInfo[])",8,6,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCumeDist.GenericUDAFCumeDistEvaluator.init(Mode,ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCumeDist.GenericUDAFCumeDistEvaluator.terminate(AggregationBuffer)",1,7,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCumeDist.createEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFDenseRank.GenericUDAFDenseRankEvaluator.nextRank(RankBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFDenseRank.createEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap.GenericUDAFEWAHBitmapEvaluator.BitmapAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap.GenericUDAFEWAHBitmapEvaluator.addBitmap(int,BitmapAgg)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap.GenericUDAFEWAHBitmapEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap.GenericUDAFEWAHBitmapEvaluator.init(Mode,ObjectInspector[])",3,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap.GenericUDAFEWAHBitmapEvaluator.iterate(AggregationBuffer,Object[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap.GenericUDAFEWAHBitmapEvaluator.merge(AggregationBuffer,Object)",1,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap.GenericUDAFEWAHBitmapEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap.GenericUDAFEWAHBitmapEvaluator.terminate(AggregationBuffer)",1,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap.GenericUDAFEWAHBitmapEvaluator.terminatePartial(AggregationBuffer)",1,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap.getEvaluator(TypeInfo[])",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.AbstractAggregationBuffer.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.GenericUDAFEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(AggregationBuffer,Object[])",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.close()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.configure(MapredContext)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.evaluate(AggregationBuffer)",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.getWindowingEvaluator(WindowFrameDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.init(Mode,ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.isEstimable(AggregationBuffer)",2,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.FirstValStreamingFixedWindow.FirstValStreamingFixedWindow(GenericUDAFEvaluator,int,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.FirstValStreamingFixedWindow.State.State(int,int,AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.FirstValStreamingFixedWindow.State.estimate()",4,1,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.FirstValStreamingFixedWindow.State.reset()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.FirstValStreamingFixedWindow.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.FirstValStreamingFixedWindow.getRowsRemainingAfterTerminate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.FirstValStreamingFixedWindow.inputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.FirstValStreamingFixedWindow.iterate(AggregationBuffer,Object[])",1,10,12
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.FirstValStreamingFixedWindow.terminate(AggregationBuffer)",1,9,10
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.FirstValueBuffer.FirstValueBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.FirstValueBuffer.init()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.GenericUDAFFirstValueEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.GenericUDAFFirstValueEvaluator.getWindowingEvaluator(WindowFrameDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.GenericUDAFFirstValueEvaluator.init(Mode,ObjectInspector[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.GenericUDAFFirstValueEvaluator.iterate(AggregationBuffer,Object[])",1,4,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.GenericUDAFFirstValueEvaluator.merge(AggregationBuffer,Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.GenericUDAFFirstValueEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.GenericUDAFFirstValueEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.GenericUDAFFirstValueEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.ValIndexPair.ValIndexPair(Object,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.createEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.getEvaluator(TypeInfo[])",3,2,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric.GenericUDAFHistogramNumericEvaluator.StdAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric.GenericUDAFHistogramNumericEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric.GenericUDAFHistogramNumericEvaluator.init(Mode,ObjectInspector[])",2,2,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric.GenericUDAFHistogramNumericEvaluator.iterate(AggregationBuffer,Object[])",4,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric.GenericUDAFHistogramNumericEvaluator.merge(AggregationBuffer,Object)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric.GenericUDAFHistogramNumericEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric.GenericUDAFHistogramNumericEvaluator.terminate(AggregationBuffer)",2,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric.GenericUDAFHistogramNumericEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFHistogramNumeric.getEvaluator(TypeInfo[])",6,5,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.GenericUDAFLagEvaluator.GenericUDAFLagEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.GenericUDAFLagEvaluator.GenericUDAFLagEvaluator(GenericUDAFLeadLagEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.GenericUDAFLagEvaluator.getNewLLBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.GenericUDAFLagEvaluator.getWindowingEvaluator(WindowFrameDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.GenericUDAFLagEvaluatorStreaming.GenericUDAFLagEvaluatorStreaming(GenericUDAFLeadLagEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.GenericUDAFLagEvaluatorStreaming.getNextResult(AggregationBuffer)",5,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.GenericUDAFLagEvaluatorStreaming.getRowsRemainingAfterTerminate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.LagBuffer.addRow(Object,Object)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.LagBuffer.initialize(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.LagBuffer.terminate()",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.createLLEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.functionName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.GenericUDAFLastValueEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.GenericUDAFLastValueEvaluator.getWindowingEvaluator(WindowFrameDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.GenericUDAFLastValueEvaluator.init(Mode,ObjectInspector[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.GenericUDAFLastValueEvaluator.iterate(AggregationBuffer,Object[])",1,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.GenericUDAFLastValueEvaluator.merge(AggregationBuffer,Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.GenericUDAFLastValueEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.GenericUDAFLastValueEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.GenericUDAFLastValueEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.LastValStreamingFixedWindow.LastValStreamingFixedWindow(GenericUDAFEvaluator,int,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.LastValStreamingFixedWindow.State.State(int,int,AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.LastValStreamingFixedWindow.State.estimate()",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.LastValStreamingFixedWindow.State.reset()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.LastValStreamingFixedWindow.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.LastValStreamingFixedWindow.getRowsRemainingAfterTerminate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.LastValStreamingFixedWindow.inputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.LastValStreamingFixedWindow.iterate(AggregationBuffer,Object[])",1,3,9
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.LastValStreamingFixedWindow.terminate(AggregationBuffer)",1,2,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.LastValueBuffer.LastValueBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.LastValueBuffer.init()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.createEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.getEvaluator(TypeInfo[])",3,2,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.GenericUDAFLeadEvaluator.GenericUDAFLeadEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.GenericUDAFLeadEvaluator.GenericUDAFLeadEvaluator(GenericUDAFLeadLagEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.GenericUDAFLeadEvaluator.getNewLLBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.GenericUDAFLeadEvaluator.getWindowingEvaluator(WindowFrameDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.GenericUDAFLeadEvaluatorStreaming.GenericUDAFLeadEvaluatorStreaming(GenericUDAFLeadLagEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.GenericUDAFLeadEvaluatorStreaming.getNextResult(AggregationBuffer)",3,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.GenericUDAFLeadEvaluatorStreaming.getRowsRemainingAfterTerminate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.LeadBuffer.addRow(Object,Object)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.LeadBuffer.initialize(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.LeadBuffer.terminate()",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.createLLEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.functionName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.GenericUDAFLeadLagEvaluator.GenericUDAFLeadLagEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.GenericUDAFLeadLagEvaluator.GenericUDAFLeadLagEvaluator(GenericUDAFLeadLagEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.GenericUDAFLeadLagEvaluator.getAmt()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.GenericUDAFLeadLagEvaluator.getFnName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.GenericUDAFLeadLagEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.GenericUDAFLeadLagEvaluator.init(Mode,ObjectInspector[])",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.GenericUDAFLeadLagEvaluator.iterate(AggregationBuffer,Object[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.GenericUDAFLeadLagEvaluator.merge(AggregationBuffer,Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.GenericUDAFLeadLagEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.GenericUDAFLeadLagEvaluator.setAmt(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.GenericUDAFLeadLagEvaluator.setFnName(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.GenericUDAFLeadLagEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.GenericUDAFLeadLagEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.getEvaluator(GenericUDAFParameterInfo)",5,6,9
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.GenericUDAFMaxEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.GenericUDAFMaxEvaluator.getWindowingEvaluator(WindowFrameDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.GenericUDAFMaxEvaluator.init(Mode,ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.GenericUDAFMaxEvaluator.iterate(AggregationBuffer,Object[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.GenericUDAFMaxEvaluator.merge(AggregationBuffer,Object)",1,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.GenericUDAFMaxEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.GenericUDAFMaxEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.GenericUDAFMaxEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.MaxStreamingFixedWindow.MaxStreamingFixedWindow(GenericUDAFEvaluator,int,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.MaxStreamingFixedWindow.State.State(int,int,AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.MaxStreamingFixedWindow.State.estimate()",4,1,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.MaxStreamingFixedWindow.State.reset()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.MaxStreamingFixedWindow.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.MaxStreamingFixedWindow.getRowsRemainingAfterTerminate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.MaxStreamingFixedWindow.inputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.MaxStreamingFixedWindow.isGreater(Object,Object)",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.MaxStreamingFixedWindow.iterate(AggregationBuffer,Object[])",3,8,9
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.MaxStreamingFixedWindow.outputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.MaxStreamingFixedWindow.removeLast(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.MaxStreamingFixedWindow.terminate(AggregationBuffer)",1,6,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.getEvaluator(TypeInfo[])",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.GenericUDAFMinEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.GenericUDAFMinEvaluator.getWindowingEvaluator(WindowFrameDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.GenericUDAFMinEvaluator.init(Mode,ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.GenericUDAFMinEvaluator.iterate(AggregationBuffer,Object[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.GenericUDAFMinEvaluator.merge(AggregationBuffer,Object)",1,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.GenericUDAFMinEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.GenericUDAFMinEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.GenericUDAFMinEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.MinStreamingFixedWindow.MinStreamingFixedWindow(GenericUDAFEvaluator,int,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.MinStreamingFixedWindow.inputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.MinStreamingFixedWindow.isLess(Object,Object)",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.MinStreamingFixedWindow.outputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.MinStreamingFixedWindow.removeLast(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.getEvaluator(TypeInfo[])",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.GenericUDAFMkCollectionEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.GenericUDAFMkCollectionEvaluator(BufferType)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.MkArrayAggregationBuffer.MkArrayAggregationBuffer()",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.getBufferType()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.init(Mode,ObjectInspector[])",3,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.iterate(AggregationBuffer,Object[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.merge(AggregationBuffer,Object)",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.putIntoCollection(Object,MkArrayAggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.setBufferType(BufferType)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFNTile.GenericUDAFNTileEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFNTile.GenericUDAFNTileEvaluator.init(Mode,ObjectInspector[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFNTile.GenericUDAFNTileEvaluator.iterate(AggregationBuffer,Object[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFNTile.GenericUDAFNTileEvaluator.merge(AggregationBuffer,Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFNTile.GenericUDAFNTileEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFNTile.GenericUDAFNTileEvaluator.terminate(AggregationBuffer)",1,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFNTile.GenericUDAFNTileEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFNTile.NTileBuffer.NTileBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFNTile.NTileBuffer.init()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFNTile.getEvaluator(TypeInfo[])",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentRank.GenericUDAFPercentRankEvaluator.init(Mode,ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentRank.GenericUDAFPercentRankEvaluator.terminate(AggregationBuffer)",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentRank.createEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.GenericUDAFMultiplePercentileApproxEvaluator.init(Mode,ObjectInspector[])",1,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.GenericUDAFMultiplePercentileApproxEvaluator.terminate(AggregationBuffer)",2,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.GenericUDAFPercentileApproxEvaluator.PercentileAggBuf.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.GenericUDAFPercentileApproxEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.GenericUDAFPercentileApproxEvaluator.getQuantileArray(ConstantObjectInspector)",3,5,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.GenericUDAFPercentileApproxEvaluator.iterate(AggregationBuffer,Object[])",2,1,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.GenericUDAFPercentileApproxEvaluator.merge(AggregationBuffer,Object)",2,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.GenericUDAFPercentileApproxEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.GenericUDAFPercentileApproxEvaluator.terminatePartial(AggregationBuffer)",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.GenericUDAFSinglePercentileApproxEvaluator.init(Mode,ObjectInspector[])",2,4,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.GenericUDAFSinglePercentileApproxEvaluator.terminate(AggregationBuffer)",2,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.getEvaluator(GenericUDAFParameterInfo)",14,12,21
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.GenericUDAFAbstractRankEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.GenericUDAFAbstractRankEvaluator.init(Mode,ObjectInspector[])",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.GenericUDAFAbstractRankEvaluator.isStreaming()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.GenericUDAFAbstractRankEvaluator.iterate(AggregationBuffer,Object[])",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.GenericUDAFAbstractRankEvaluator.merge(AggregationBuffer,Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.GenericUDAFAbstractRankEvaluator.nextRank(RankBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.GenericUDAFAbstractRankEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.GenericUDAFAbstractRankEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.GenericUDAFAbstractRankEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.GenericUDAFRankEvaluator.getNextResult(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.GenericUDAFRankEvaluator.getRowsRemainingAfterTerminate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.GenericUDAFRankEvaluator.getWindowingEvaluator(WindowFrameDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.RankBuffer.RankBuffer(int,boolean)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.RankBuffer.addRank()",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.RankBuffer.incrRowNum()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.RankBuffer.init()",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.compare(Object[],ObjectInspector[],Object[],ObjectInspector[])",3,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.copyToStandardObject(Object[],ObjectInspector[],ObjectInspectorCopyOption)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.createEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.getEvaluator(TypeInfo[])",4,2,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber.GenericUDAFRowNumberEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber.GenericUDAFRowNumberEvaluator.init(Mode,ObjectInspector[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber.GenericUDAFRowNumberEvaluator.iterate(AggregationBuffer,Object[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber.GenericUDAFRowNumberEvaluator.merge(AggregationBuffer,Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber.GenericUDAFRowNumberEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber.GenericUDAFRowNumberEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber.GenericUDAFRowNumberEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber.RowNumberBuffer.RowNumberBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber.RowNumberBuffer.incr()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber.RowNumberBuffer.init()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRowNumber.getEvaluator(TypeInfo[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd.GenericUDAFStdEvaluator.terminate(AggregationBuffer)",2,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd.getEvaluator(TypeInfo[])",5,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStdSample.GenericUDAFStdSampleEvaluator.terminate(AggregationBuffer)",2,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStdSample.getEvaluator(TypeInfo[])",5,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.GenericUDAFStreamingEvaluator(GenericUDAFEvaluator,int,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.StreamingState.StreamingState(int,int,AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.StreamingState.reset()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.SumAvgEnhancer.SumAvgEnhancer(GenericUDAFEvaluator,int,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.SumAvgEnhancer.SumAvgStreamingState.SumAvgStreamingState(int,int,AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.SumAvgEnhancer.SumAvgStreamingState.estimate()",4,1,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.SumAvgEnhancer.SumAvgStreamingState.reset()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.SumAvgEnhancer.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.SumAvgEnhancer.getRowsRemainingAfterTerminate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.SumAvgEnhancer.iterate(AggregationBuffer,Object[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.SumAvgEnhancer.terminate(AggregationBuffer)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.getNextResult(AggregationBuffer)",3,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(Mode,ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.merge(AggregationBuffer,Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumDouble.SumDoubleAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumDouble.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumDouble.getWindowingEvaluator(WindowFrameDef)",1,2,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumDouble.init(Mode,ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumDouble.iterate(AggregationBuffer,Object[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumDouble.merge(AggregationBuffer,Object)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumDouble.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumDouble.terminate(AggregationBuffer)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumDouble.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumHiveDecimal.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumHiveDecimal.getWindowingEvaluator(WindowFrameDef)",1,3,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumHiveDecimal.init(Mode,ObjectInspector[])",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumHiveDecimal.iterate(AggregationBuffer,Object[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumHiveDecimal.merge(AggregationBuffer,Object)",3,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumHiveDecimal.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumHiveDecimal.terminate(AggregationBuffer)",2,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumHiveDecimal.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumLong.SumLongAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumLong.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumLong.getWindowingEvaluator(WindowFrameDef)",1,2,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumLong.init(Mode,ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumLong.iterate(AggregationBuffer,Object[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumLong.merge(AggregationBuffer,Object)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumLong.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumLong.terminate(AggregationBuffer)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.GenericUDAFSumLong.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.getEvaluator(TypeInfo[])",7,3,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSumList.GenericUDAFSumLong.SumLongAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSumList.GenericUDAFSumLong.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSumList.GenericUDAFSumLong.init(Mode,ObjectInspector[])",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSumList.GenericUDAFSumLong.iterate(AggregationBuffer,Object[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSumList.GenericUDAFSumLong.merge(AggregationBuffer,Object)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSumList.GenericUDAFSumLong.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSumList.GenericUDAFSumLong.terminate(AggregationBuffer)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSumList.GenericUDAFSumLong.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSumList.getEvaluator(GenericUDAFParameterInfo)",4,2,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.GenericUDAFVarianceEvaluator.StdAgg.estimate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.GenericUDAFVarianceEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.GenericUDAFVarianceEvaluator.getResult()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.GenericUDAFVarianceEvaluator.init(Mode,ObjectInspector[])",2,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.GenericUDAFVarianceEvaluator.iterate(AggregationBuffer,Object[])",1,4,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.GenericUDAFVarianceEvaluator.merge(AggregationBuffer,Object)",1,4,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.GenericUDAFVarianceEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.GenericUDAFVarianceEvaluator.setResult(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.GenericUDAFVarianceEvaluator.terminate(AggregationBuffer)",2,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.GenericUDAFVarianceEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.getEvaluator(TypeInfo[])",5,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVarianceSample.GenericUDAFVarianceSampleEvaluator.terminate(AggregationBuffer)",2,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVarianceSample.getEvaluator(TypeInfo[])",5,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.GenericUDAFnGramEvaluator.getNewAggregationBuffer()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.GenericUDAFnGramEvaluator.init(Mode,ObjectInspector[])",2,4,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.GenericUDAFnGramEvaluator.iterate(AggregationBuffer,Object[])",7,10,14
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.GenericUDAFnGramEvaluator.merge(AggregationBuffer,Object)",3,2,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.GenericUDAFnGramEvaluator.processNgrams(NGramAggBuf,ArrayList<String>)",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.GenericUDAFnGramEvaluator.reset(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.GenericUDAFnGramEvaluator.terminate(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.GenericUDAFnGramEvaluator.terminatePartial(AggregationBuffer)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.getEvaluator(TypeInfo[])",12,11,18
"org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredJavaObject.DeferredJavaObject(Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredJavaObject.get()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredJavaObject.prepare(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDF.GenericUDF()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDF.close()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDF.configure(MapredContext)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDF.copyToNewInstance(Object)",2,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDF.flip()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDF.getRequiredFiles()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDF.getRequiredJars()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDF.getUdfName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(ObjectInspector[])",5,9,12
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs.evaluate(DeferredObject[])",4,3,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs.initialize(ObjectInspector[])",4,3,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFArray.evaluate(DeferredObject[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFArray.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFArray.initialize(ObjectInspector[])",3,5,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFArrayContains.evaluate(DeferredObject[])",5,4,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFArrayContains.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFArrayContains.initialize(ObjectInspector[])",5,4,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFAssertTrue.evaluate(DeferredObject[])",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFAssertTrue.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFAssertTrue.initialize(ObjectInspector[])",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.compare(DeferredObject[])",6,2,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.initialize(ObjectInspector[])",4,16,19
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.GenericUDFBaseNumeric()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.copyToNewInstance(Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.deriveResultApproxTypeInfo()",3,3,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.deriveResultDecimalTypeInfo()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.deriveResultExactTypeInfo()",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.deriveResultTypeInfo()",3,5,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.evaluate(ByteWritable,ByteWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.evaluate(DeferredObject[])",14,9,17
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.evaluate(DoubleWritable,DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.evaluate(FloatWritable,FloatWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.evaluate(HiveDecimal,HiveDecimal)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.evaluate(IntWritable,IntWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.evaluate(LongWritable,LongWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.evaluate(ShortWritable,ShortWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.initialize(ObjectInspector[])",4,4,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.isAnsiSqlArithmetic()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.isConfLookupNeeded()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.setAnsiSqlArithmetic(boolean)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.setConfLookupNeeded(boolean)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBasePad.GenericUDFBasePad(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBasePad.checkIntArguments(ObjectInspector[],int)",3,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBasePad.checkTextArguments(ObjectInspector[],int)",2,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBasePad.evaluate(DeferredObject[])",3,1,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBasePad.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBasePad.initialize(ObjectInspector[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseTrim.GenericUDFBaseTrim(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseTrim.evaluate(DeferredObject[])",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseTrim.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseTrim.initialize(ObjectInspector[])",4,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseUnary.GenericUDFBaseUnary()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseUnary.deriveResultTypeInfo(PrimitiveTypeInfo)",3,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseUnary.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseUnary.initialize(ObjectInspector[])",4,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween.evaluate(DeferredObject[])",4,4,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween.getDisplayString(String[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween.initialize(ObjectInspector[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.GenericUDFBridge()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.GenericUDFBridge(String,boolean,String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.evaluate(DeferredObject[])",1,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getDisplayString(String[])",3,4,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getRequiredFiles()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getRequiredJars()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClass()",1,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClassName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(ObjectInspector[])",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.isOperator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.setOperator(boolean)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.setUdfClassName(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.setUdfName(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFCase.evaluate(DeferredObject[])",4,4,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFCase.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFCase.initialize(ObjectInspector[])",6,6,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFCeil.GenericUDFCeil()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFCeil.evaluate(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFCeil.evaluate(HiveDecimalWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFCoalesce.evaluate(DeferredObject[])",3,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFCoalesce.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFCoalesce.initialize(ObjectInspector[])",3,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat.binaryEvaluate(DeferredObject[])",3,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat.createStringConverters()",1,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat.evaluate(DeferredObject[])",2,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat.initialize(ObjectInspector[])",7,8,21
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcat.stringEvaluate(DeferredObject[])",3,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcatWS.evaluate(DeferredObject[])",2,7,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcatWS.getDisplayString(String[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcatWS.initialize(ObjectInspector[])",4,3,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFConcatWS.isStringOrVoidType(ObjectInspector)",3,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDate.evaluate(DeferredObject[])",3,2,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDate.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDate.initialize(ObjectInspector[])",4,3,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateAdd.evaluate(DeferredObject[])",4,2,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateAdd.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateAdd.initialize(ObjectInspector[])",6,4,9
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateDiff.GenericUDFDateDiff()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateDiff.checkArguments(ObjectInspector[],int)",3,3,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateDiff.convertToDate(PrimitiveCategory,Converter,DeferredObject)",3,2,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateDiff.evaluate(Date,Date)",2,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateDiff.evaluate(DeferredObject[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateDiff.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateDiff.initialize(ObjectInspector[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateSub.evaluate(DeferredObject[])",4,2,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateSub.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateSub.initialize(ObjectInspector[])",6,4,9
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDecode.evaluate(DeferredObject[])",3,2,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDecode.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFDecode.initialize(ObjectInspector[])",4,4,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapAnd.GenericUDFEWAHBitmapAnd()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapAnd.bitmapBop(EWAHCompressedBitmap,EWAHCompressedBitmap)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapEmpty.evaluate(DeferredObject[])",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapEmpty.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapEmpty.initialize(ObjectInspector[])",3,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapOr.GenericUDFEWAHBitmapOr()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFEWAHBitmapOr.bitmapBop(EWAHCompressedBitmap,EWAHCompressedBitmap)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFElt.evaluate(DeferredObject[])",3,1,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFElt.getDisplayString(String[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFElt.initialize(ObjectInspector[])",4,5,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFEncode.evaluate(DeferredObject[])",3,2,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFEncode.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFEncode.initialize(ObjectInspector[])",4,4,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFEvaluateNPE.evaluate(DeferredObject[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFEvaluateNPE.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFEvaluateNPE.initialize(ObjectInspector[])",3,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFField.evaluate(DeferredObject[])",5,4,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFField.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFField.initialize(ObjectInspector[])",4,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFloor.GenericUDFFloor()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFloor.evaluate(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFloor.evaluate(HiveDecimalWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFloorCeilBase.GenericUDFFloorCeilBase()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFloorCeilBase.evaluate(DeferredObject[])",7,4,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFloorCeilBase.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFloorCeilBase.initialize(ObjectInspector[])",4,4,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFormatNumber.evaluate(DeferredObject[])",3,5,11
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFormatNumber.getDisplayString(String[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFormatNumber.initialize(ObjectInspector[])",6,5,10
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.applyOffset(long,Timestamp)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.evaluate(DeferredObject[])",3,3,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.getDisplayString(String[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.getName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.initialize(ObjectInspector[])",2,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.invert()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash.evaluate(DeferredObject[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFHash.initialize(ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFIf.evaluate(DeferredObject[])",2,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFIf.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFIf.initialize(ObjectInspector[])",4,6,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFIn.checkIfInSetConstant()",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFIn.evaluate(DeferredObject[])",15,13,16
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFIn.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFIn.initialize(ObjectInspector[])",4,5,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFIn.prepareInSet(DeferredObject[])",1,4,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFInFile.evaluate(DeferredObject[])",3,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFInFile.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFInFile.getRequiredFiles()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFInFile.initialize(ObjectInspector[])",4,4,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFInFile.isTypeCompatible(ObjectInspector)",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFInFile.load(InputStream)",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFIndex.evaluate(DeferredObject[])",2,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFIndex.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFIndex.initialize(ObjectInspector[])",5,5,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFInstr.evaluate(DeferredObject[])",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFInstr.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFInstr.initialize(ObjectInspector[])",4,4,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLTrim.GenericUDFLTrim()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLTrim.performOp(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLag._getFnName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLag.getIndex(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLag.getRow(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLead._getFnName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLead.getIndex(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLead.getRow(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.evaluate(DeferredObject[])",1,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.getAmt()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.getDefaultArgOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.getDefaultValueConverter()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.getExprEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.getFirstArgOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.getpItr()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.initialize(ObjectInspector[])",5,7,9
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.setAmt(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.setDefaultArgOI(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.setDefaultValueConverter(Converter)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.setExprEvaluator(ExprNodeEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.setFirstArgOI(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLeadLag.setpItr(PTFPartitionIterator<Object>)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLocate.evaluate(DeferredObject[])",4,4,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLocate.getDisplayString(String[])",1,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLocate.initialize(ObjectInspector[])",4,6,9
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLower.evaluate(DeferredObject[])",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLower.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLower.initialize(ObjectInspector[])",4,3,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLpad.GenericUDFLpad()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFLpad.performOp(byte[],byte[],byte[],int,Text,Text)",1,4,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.GenericUDFMacro()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.GenericUDFMacro(String,ExprNodeDesc,List<String>,List<TypeInfo>)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.checkNotNull(Object,String)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.evaluate(DeferredObject[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.getBody()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.getColNames()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.getColTypes()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.getMacroName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.initialize(ObjectInspector[])",2,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.isDeterministic()",2,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.isStateful()",2,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.setBody(ExprNodeDesc)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.setColNames(List<String>)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.setColTypes(List<TypeInfo>)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMacro.setMacroName(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMap.evaluate(DeferredObject[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMap.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMap.initialize(ObjectInspector[])",7,9,11
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapKeys.evaluate(DeferredObject[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapKeys.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapKeys.initialize(ObjectInspector[])",3,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapValues.evaluate(DeferredObject[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapValues.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapValues.initialize(ObjectInspector[])",3,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFNamedStruct.evaluate(DeferredObject[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFNamedStruct.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFNamedStruct.initialize(ObjectInspector[])",4,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFNvl.evaluate(DeferredObject[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFNvl.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFNvl.initialize(ObjectInspector[])",3,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.evaluate(DeferredObject[])",6,6,9
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.initialize(ObjectInspector[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.GenericUDFOPDivide()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.deriveResultApproxTypeInfo()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.deriveResultDecimalTypeInfo(int,int,int,int)",1,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.deriveResultExactTypeInfo()",2,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.deriveResultExactTypeInfoAnsiSql()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.deriveResultExactTypeInfoBackwardsCompat()",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.evaluate(DoubleWritable,DoubleWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.evaluate(HiveDecimal,HiveDecimal)",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.GenericUDFOPEqual()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual.evaluate(DeferredObject[])",6,2,13
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualNS.evaluate(DeferredObject[])",3,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.GenericUDFOPEqualOrGreaterThan()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.evaluate(DeferredObject[])",6,2,14
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.flip()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan.GenericUDFOPEqualOrLessThan()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan.evaluate(DeferredObject[])",6,2,14
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan.flip()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan.GenericUDFOPGreaterThan()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan.evaluate(DeferredObject[])",6,2,14
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan.flip()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan.GenericUDFOPLessThan()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan.evaluate(DeferredObject[])",6,2,14
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan.flip()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMinus.GenericUDFOPMinus()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMinus.deriveResultDecimalTypeInfo(int,int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMinus.evaluate(ByteWritable,ByteWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMinus.evaluate(DoubleWritable,DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMinus.evaluate(FloatWritable,FloatWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMinus.evaluate(HiveDecimal,HiveDecimal)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMinus.evaluate(IntWritable,IntWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMinus.evaluate(LongWritable,LongWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMinus.evaluate(ShortWritable,ShortWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMod.GenericUDFOPMod()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMod.deriveResultDecimalTypeInfo(int,int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMod.evaluate(ByteWritable,ByteWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMod.evaluate(DoubleWritable,DoubleWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMod.evaluate(FloatWritable,FloatWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMod.evaluate(HiveDecimal,HiveDecimal)",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMod.evaluate(IntWritable,IntWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMod.evaluate(LongWritable,LongWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMod.evaluate(ShortWritable,ShortWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMultiply.GenericUDFOPMultiply()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMultiply.deriveResultDecimalTypeInfo(int,int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMultiply.evaluate(ByteWritable,ByteWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMultiply.evaluate(DoubleWritable,DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMultiply.evaluate(FloatWritable,FloatWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMultiply.evaluate(HiveDecimal,HiveDecimal)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMultiply.evaluate(IntWritable,IntWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMultiply.evaluate(LongWritable,LongWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPMultiply.evaluate(ShortWritable,ShortWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNegative.GenericUDFOPNegative()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNegative.evaluate(DeferredObject[])",5,2,12
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNot.evaluate(DeferredObject[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNot.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNot.initialize(ObjectInspector[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.GenericUDFOPNotEqual()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.evaluate(DeferredObject[])",6,2,13
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotNull.evaluate(DeferredObject[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotNull.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotNull.initialize(ObjectInspector[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull.evaluate(DeferredObject[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull.initialize(ObjectInspector[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.evaluate(DeferredObject[])",6,6,9
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.initialize(ObjectInspector[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPlus.GenericUDFOPPlus()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPlus.deriveResultDecimalTypeInfo(int,int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPlus.evaluate(ByteWritable,ByteWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPlus.evaluate(DoubleWritable,DoubleWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPlus.evaluate(FloatWritable,FloatWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPlus.evaluate(HiveDecimal,HiveDecimal)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPlus.evaluate(IntWritable,IntWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPlus.evaluate(LongWritable,LongWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPlus.evaluate(ShortWritable,ShortWritable)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPositive.GenericUDFOPPositive()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPPositive.evaluate(DeferredObject[])",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPosMod.GenericUDFPosMod()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPosMod.deriveResultDecimalTypeInfo(int,int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPosMod.evaluate(ByteWritable,ByteWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPosMod.evaluate(DoubleWritable,DoubleWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPosMod.evaluate(FloatWritable,FloatWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPosMod.evaluate(HiveDecimal,HiveDecimal)",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPosMod.evaluate(IntWritable,IntWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPosMod.evaluate(LongWritable,LongWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPosMod.evaluate(ShortWritable,ShortWritable)",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPower.GenericUDFPower()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPower.evaluate(DeferredObject[])",5,1,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPower.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPower.initialize(ObjectInspector[])",6,5,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPrintf.evaluate(DeferredObject[])",3,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPrintf.getDisplayString(String[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFPrintf.initialize(ObjectInspector[])",6,8,9
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFRTrim.GenericUDFRTrim()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFRTrim.performOp(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect.evaluate(DeferredObject[])",3,7,10
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect.functionName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect.initialize(ObjectInspector[])",4,1,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect2.evaluate(DeferredObject[])",5,3,17
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect2.functionName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect2.getTypeFor(Class<?>)",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFReflect2.initialize(ObjectInspector[])",6,2,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFRound.evaluate(DeferredObject[])",13,14,22
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFRound.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFRound.getOutputTypeInfo(DecimalTypeInfo,int)",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFRound.initialize(ObjectInspector[])",11,5,21
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFRound.round(DoubleWritable,int)",2,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFRpad.GenericUDFRpad()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFRpad.performOp(byte[],byte[],byte[],int,Text,Text)",1,4,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFSentences.evaluate(DeferredObject[])",2,9,11
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFSentences.getDisplayString(String[])",1,1,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFSentences.initialize(ObjectInspector[])",2,2,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFSize.evaluate(DeferredObject[])",1,4,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFSize.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFSize.initialize(ObjectInspector[])",3,4,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFSortArray.evaluate(DeferredObject[])",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFSortArray.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFSortArray.initialize(ObjectInspector[])",3,2,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFSplit.evaluate(DeferredObject[])",2,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFSplit.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFSplit.initialize(ObjectInspector[])",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFStringToMap.evaluate(DeferredObject[])",1,5,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFStringToMap.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFStringToMap.initialize(ObjectInspector[])",3,5,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFStruct.evaluate(DeferredObject[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFStruct.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFStruct.initialize(ObjectInspector[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaBoolean.evaluate(DeferredObject[])",3,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaBoolean.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaBoolean.initialize(ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaString.evaluate(DeferredObject[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaString.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaString.initialize(ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate.evaluate(DeferredObject[])",4,4,11
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate.getOrdinal(int)",1,1,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate.initialize(ObjectInspector[])",4,4,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTimestamp.evaluate(DeferredObject[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTimestamp.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTimestamp.initialize(ObjectInspector[])",2,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToBinary.evaluate(DeferredObject[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToBinary.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToBinary.initialize(ObjectInspector[])",3,1,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToChar.GenericUDFToChar()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToChar.evaluate(DeferredObject[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToChar.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToChar.getTypeInfo()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToChar.initialize(ObjectInspector[])",2,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToChar.setTypeInfo(TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDate.evaluate(DeferredObject[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDate.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDate.initialize(ObjectInspector[])",3,2,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDecimal.evaluate(DeferredObject[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDecimal.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDecimal.getTypeInfo()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDecimal.initialize(ObjectInspector[])",2,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDecimal.setTypeInfo(DecimalTypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDecimal.setTypeInfo(TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.evaluate(DeferredObject[])",8,5,10
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.getName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.initialize(ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.initializeInput(ObjectInspector[])",7,7,10
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUtcTimestamp.getDisplayString(String[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUtcTimestamp.getName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUtcTimestamp.invert()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToVarchar.GenericUDFToVarchar()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToVarchar.evaluate(DeferredObject[])",2,1,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToVarchar.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToVarchar.getTypeInfo()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToVarchar.initialize(ObjectInspector[])",2,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFToVarchar.setTypeInfo(TypeInfo)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTranslate.evaluate(DeferredObject[])",2,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTranslate.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTranslate.initialize(ObjectInspector[])",5,5,9
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTranslate.populateMappings(Text,Text)",5,5,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTranslate.populateMappingsIfNecessary(Text,Text)",1,5,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTranslate.processInput(Text)",3,2,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTrim.GenericUDFTrim()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFTrim.performOp(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnion.evaluate(DeferredObject[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnion.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnion.initialize(ObjectInspector[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.evaluate(DeferredObject[])",2,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.getName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.initializeInput(ObjectInspector[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper.evaluate(DeferredObject[])",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper.initialize(ObjectInspector[])",4,3,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.ConversionHelper.ConversionHelper(Method,ObjectInspector[])",4,12,15
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.ConversionHelper.convertIfNecessary(Object...)",2,5,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.ConversionHelper.getClassFromType(Type)",3,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.GenericUDFUtils()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.ReturnObjectInspectorResolver.ReturnObjectInspectorResolver()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.ReturnObjectInspectorResolver.ReturnObjectInspectorResolver(boolean)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.ReturnObjectInspectorResolver.convertIfNecessary(Object,ObjectInspector)",3,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.ReturnObjectInspectorResolver.get()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.ReturnObjectInspectorResolver.update(ObjectInspector)",7,5,10
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.StringHelper.StringHelper(PrimitiveCategory)",2,2,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.StringHelper.getFixedStringSizeForType(PrimitiveObjectInspector)",2,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.StringHelper.setReturnValue(String)",3,2,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.findText(Text,Text,int)",5,4,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.getOrdinal(int)",1,1,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.isUtfStartByte(byte)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFWhen.evaluate(DeferredObject[])",4,5,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFWhen.getDisplayString(String[])",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDFWhen.initialize(ObjectInspector[])",6,6,6
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.configure(MapredContext)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.forward(Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.initialize(ObjectInspector[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.initialize(StructObjectInspector)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.setCollector(Collector)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode.close()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode.initialize(ObjectInspector[])",3,2,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode.process(Object[])",4,4,8
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode.toString()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFInline.GenericUDTFInline()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFInline.close()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFInline.initialize(ObjectInspector[])",4,3,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFInline.process(Object[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFInline.toString()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.HashCache.HashCache()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.HashCache.removeEldestEntry(Entry<K, V>)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.close()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.initialize(ObjectInspector[])",4,4,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.process(Object[])",5,12,14
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.reportInvalidJson(String)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.toString()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFParseUrlTuple.close()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFParseUrlTuple.evaluate(URL,int)",12,11,14
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFParseUrlTuple.evaluateQuery(String,String)",3,3,5
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFParseUrlTuple.initialize(ObjectInspector[])",4,4,7
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFParseUrlTuple.process(Object[])",3,19,20
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFParseUrlTuple.toString()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFPosExplode.close()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFPosExplode.initialize(ObjectInspector[])",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFPosExplode.process(Object[])",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFPosExplode.toString()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFStack.close()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFStack.initialize(ObjectInspector[])",7,9,11
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFStack.process(Object[])",1,4,4
"org.apache.hadoop.hive.ql.udf.generic.GenericUDTFStack.toString()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.LongNumDistinctValueEstimator.LongNumDistinctValueEstimator(String,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.LongNumDistinctValueEstimator.LongNumDistinctValueEstimator(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.LongNumDistinctValueEstimator.addToEstimator(long)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.LongNumDistinctValueEstimator.addToEstimatorPCSA(long)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NGramEstimator.NGramEstimator()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NGramEstimator.add(ArrayList<String>)",3,6,7
"org.apache.hadoop.hive.ql.udf.generic.NGramEstimator.getNGrams()",5,5,8
"org.apache.hadoop.hive.ql.udf.generic.NGramEstimator.initialize(int,int,int)",1,1,4
"org.apache.hadoop.hive.ql.udf.generic.NGramEstimator.isInitialized()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NGramEstimator.merge(List<Text>)",5,6,11
"org.apache.hadoop.hive.ql.udf.generic.NGramEstimator.reset()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NGramEstimator.serialize()",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.NGramEstimator.size()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NGramEstimator.trim(boolean)",1,2,3
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.NumDistinctValueEstimator(String,int)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.NumDistinctValueEstimator(int)",1,4,7
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.addToEstimator(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.addToEstimator(double)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.addToEstimator(long)",4,2,4
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.addToEstimatorPCSA(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.addToEstimatorPCSA(double)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.addToEstimatorPCSA(long)",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.deserialize(String,int)",1,5,9
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.estimateNumDistinctValues()",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.estimateNumDistinctValuesPCSA()",1,3,4
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.generateHash(long,int)",1,1,2
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.generateHashForPCSA(long)",1,1,2
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.getBitVector(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.getBitVectorSize()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.getnumBitVectors()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.mergeEstimators(NumDistinctValueEstimator)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.printNumDistinctValueEstimator()",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.reset()",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.NumDistinctValueEstimator.serialize()",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.Coord.compareTo(Object)",3,1,3
"org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.NumericHistogram()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.add(double)",4,6,7
"org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.allocate(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.getBin(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.getNumBins()",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.getUsedBins()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.isReady()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.merge(List<DoubleWritable>)",2,5,7
"org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.quantile(double)",4,5,7
"org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.reset()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.serialize()",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.trim()",1,5,6
"org.apache.hadoop.hive.ql.udf.generic.RoundUtils.RoundUtils()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.RoundUtils.round(HiveDecimal,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.RoundUtils.round(double,int)",2,2,3
"org.apache.hadoop.hive.ql.udf.generic.RoundUtils.round(long,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.SimpleGenericUDAFParameterInfo.SimpleGenericUDAFParameterInfo(ObjectInspector[],boolean,boolean)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.SimpleGenericUDAFParameterInfo.getParameterObjectInspectors()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.SimpleGenericUDAFParameterInfo.getParameters()",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.SimpleGenericUDAFParameterInfo.isAllColumns()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.SimpleGenericUDAFParameterInfo.isDistinct()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.StringNumDistinctValueEstimator.StringNumDistinctValueEstimator(String,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.StringNumDistinctValueEstimator.StringNumDistinctValueEstimator(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.StringNumDistinctValueEstimator.addToEstimator(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.StringNumDistinctValueEstimator.addToEstimatorPCSA(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDAFCorrelation.testCorr()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAbs.testDouble()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAbs.testFloat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAbs.testHiveDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAbs.testInt()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAbs.testLong()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAbs.testText()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFBridge.testInvalidName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFBridge.testNullName()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFCeil.testByte()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFCeil.testChar()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFCeil.testDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFCeil.testDouble()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFCeil.testFloat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFCeil.testInt()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFCeil.testLong()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFCeil.testShort()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFCeil.testString()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFCeil.testVarchar()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFConcat.getBaseTable()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFConcat.getExpectedResult()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFConcat.getExpressionList()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDecode.testDecode()",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDecode.verifyDecode(String,String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFEncode.testEncode()",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFEncode.verifyEncode(String,String)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFloor.testByte()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFloor.testChar()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFloor.testDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFloor.testDouble()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFloor.testFloat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFloor.testInt()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFloor.testLong()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFloor.testShort()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFloor.testString()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFFloor.testVarchar()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLTrim.runAndVerify(String,String,GenericUDF)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLTrim.testTrim()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLpad.runAndVerify(String,int,String,String,GenericUDF)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLpad.testLpad()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFMacro.setup()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFMacro.testNoArgsContructor()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFMacro.testUDF()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testByteDivideShort()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testDecimalDivideDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testDecimalDivideDecimal2()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testDecimalDivideDecimalSameParams()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testDecimalDivisionResultType()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testDecimalDivisionResultType(int,int,int,int,int,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testDoubleDivideLong()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testDouleDivideDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testFloatDivideFloat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testLongDivideDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testReturnTypeAnsiSql()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testReturnTypeBackwardCompat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testVarcharDivideInt()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.testByteMinusShort()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.testDecimalMinusDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.testDecimalMinusDecimalSameParams()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.testDoubleMinusLong()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.testDouleMinusDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.testFloatMinusFloat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.testLongMinusDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.testReturnTypeAnsiSql()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.testReturnTypeBackwardCompat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.testVarcharMinusInt()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMod.testDecimalModDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMod.testDecimalModDecimalSameParams()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMod.testModByZero1()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMod.testModByZero2()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMod.testModByZero3()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMod.testModByZero4()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMod.testModByZero5()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMod.testModByZero6()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMod.testModByZero8()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.testByteTimesShort()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.testDecimalTimesDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.testDecimalTimesDecimalSameParams()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.testDoubleTimesLong()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.testDouleTimesDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.testFloatTimesFloat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.testLongTimesDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.testReturnTypeAnsiSql()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.testReturnTypeBackwardCompat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.testVarcharTimesInt()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNegative.testByte()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNegative.testChar()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNegative.testDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNegative.testDouble()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNegative.testFloat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNegative.testInt()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNegative.testLong()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNegative.testShort()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNegative.testString()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNegative.testVarchar()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNumeric.TestGenericUDFOPNumeric()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNumeric.verifyReturnType(GenericUDF,String,String,String)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.testBytePlusShort()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.testDecimalPlusDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.testDecimalPlusDecimalSameParams()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.testDoublePlusLong()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.testDoulePlusDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.testFloatPlusFloat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.testLongPlusDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.testReturnTypeAnsiSql()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.testReturnTypeBackwardCompat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.testVarcharPlusInt()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPositive.testByte()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPositive.testChar()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPositive.testDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPositive.testDouble()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPositive.testFloat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPositive.testInt()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPositive.testLong()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPositive.testShort()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPositive.testString()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPositive.testVarchar()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPosMod.testDecimalPosModDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPosMod.testDecimalPosModDecimalSameParams()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPosMod.testPosModByZero1()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPosMod.testPosModByZero2()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPosMod.testPosModByZero3()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPosMod.testPosModByZero4()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPosMod.testPosModByZero5()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPosMod.testPosModByZero6()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPosMod.testPosModByZero8()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPower.testBytePowerShort()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPower.testDecimalPowerDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPower.testDoublePowerLong()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPower.testDoulePowerDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPower.testFloatPowerFloat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPower.testLongPowerDecimal()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPower.testShortPowerFloat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPower.testVarcharPowerInt()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPrintf.testCharFormat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPrintf.testCharVarcharArgs()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPrintf.testDecimalArgs()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPrintf.testVarcharFormat()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRTrim.runAndVerify(String,String,GenericUDF)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRTrim.testTrim()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRound.getBaseTable()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRound.getExpectedResult()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRound.getExpressionList()",1,3,3
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRound.testDecimalRoundingMetaData()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRound.testDecimalRoundingMetaData1()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRound.testDecimalRoundingMetaData2()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRpad.runAndVerify(String,int,String,String,GenericUDF)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRpad.testLpad()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.runAndVerify(GenericUDFToUnixTimeStamp,Object,Object)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.runAndVerify(GenericUDFToUnixTimeStamp,Object,Object,Object)",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.testDate()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.testString()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.testTimestamp()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFTrim.runAndVerify(String,String,GenericUDF)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFTrim.testTrim()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.UDFCurrentDB.configure(MapredContext)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.UDFCurrentDB.evaluate(DeferredObject[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.UDFCurrentDB.getDisplayString(String[])",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.UDFCurrentDB.initialize(ObjectInspector[])",1,2,2
"org.apache.hadoop.hive.ql.udf.generic.UDTFCollector.UDTFCollector(UDTFOperator)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.UDTFCollector.collect(Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.UDTFCollector.getCounter()",1,1,1
"org.apache.hadoop.hive.ql.udf.generic.UDTFCollector.reset()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.Chain.Chain(ArrayList<SymbolFunction>)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.Chain.isOptional()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.Chain.match(Object,PTFPartitionIterator<Object>)",5,3,5
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.MatchPathResolver.createEvaluator(PTFDesc,PartitionedTableFunctionDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.MatchPathResolver.getOutputColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.MatchPathResolver.initializeOutputOI()",1,2,3
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.MatchPathResolver.setupOutputOI()",1,3,4
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.MatchPathResolver.setupSymbolFunctionChain(MatchPath)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.MatchPathResolver.transformsRawInput()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.MatchPathResolver.validateAndSetupPatternStr(MatchPath,List<PTFExpressionDef>)",1,4,4
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.MatchPathResolver.validateAndSetupResultExprStr(MatchPath,List<PTFExpressionDef>,int)",1,4,4
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.MatchPathResolver.validateAndSetupSymbolInfo(MatchPath,List<PTFExpressionDef>,int)",1,8,8
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.Plus.Plus(SymbolFunction)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.Plus.isOptional()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.Plus.match(Object,PTFPartitionIterator<Object>)",2,4,4
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.ResultExprInfo.getResultExprNames()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.ResultExprInfo.getResultExprNodes()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.ResultExprInfo.setResultExprNames(ArrayList<String>)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.ResultExprInfo.setResultExprNodes(ArrayList<ExprNodeDesc>)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.ResultExpressionParser.ResultExpressionParser(String,RowResolver)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.ResultExpressionParser.buildExprNode(ASTNode,TypeCheckCtx)",2,2,3
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.ResultExpressionParser.buildSelectListEvaluators()",1,2,2
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.ResultExpressionParser.fixResultExprString()",1,2,3
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.ResultExpressionParser.getColumnName(String,ExprNodeDesc,int)",3,3,3
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.ResultExpressionParser.getResultExprInfo()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.ResultExpressionParser.parse()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.ResultExpressionParser.setupSelectListInputInfo()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.ResultExpressionParser.translate()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.ResultExpressionParser.validateSelectExpr()",1,2,2
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.Star.Star(SymbolFunction)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.Star.isOptional()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.Star.match(Object,PTFPartitionIterator<Object>)",1,3,3
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.Symbol.Symbol(ExprNodeEvaluator,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.Symbol.isOptional()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.Symbol.match(Object,PTFPartitionIterator<Object>)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.SymbolFunction.SymbolFunction()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.SymbolFunction.match(SymbolFunction,Object,PTFPartitionIterator<Object>)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.SymbolParser.SymbolParser(String,ArrayList<String>,ArrayList<ExprNodeEvaluator>,ArrayList<ObjectInspector>)",1,2,2
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.SymbolParser.getSymbolFunction()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.SymbolParser.parse()",3,4,7
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.SymbolsInfo.SymbolsInfo(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.SymbolsInfo.add(String,PTFExpressionDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.createSelectListOI(MatchPath,PTFInputDef)",1,3,3
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.createSelectListRR(MatchPath,PTFInputDef)",1,4,5
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.execute(PTFPartitionIterator<Object>,PTFPartition)",1,4,4
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.getInputColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.getPath(Object,ObjectInspector,PTFPartitionIterator<Object>,int)",1,3,3
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.getResultExprInfo()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.getSelectListInput(Object,ObjectInspector,PTFPartitionIterator<Object>,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.setInputColumnNames(HashMap<String, String>)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.setResultExprInfo(ResultExprInfo)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.MatchPath.throwErrorWithSignature(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.Noop.NoopResolver.carryForwardNames()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.Noop.NoopResolver.createEvaluator(PTFDesc,PartitionedTableFunctionDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.Noop.NoopResolver.getOutputColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.Noop.NoopResolver.initializeOutputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.Noop.NoopResolver.setupOutputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.Noop.NoopResolver.transformsRawInput()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.Noop.execute(PTFPartition)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.Noop.execute(PTFPartitionIterator<Object>,PTFPartition)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopStreaming.NoopStreaming()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopStreaming.NoopStreamingResolver.createEvaluator(PTFDesc,PartitionedTableFunctionDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopStreaming.initializeStreaming(Configuration,StructObjectInspector,boolean)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopStreaming.processRow(Object)",2,2,2
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMap.NoopWithMapResolver.carryForwardNames()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMap.NoopWithMapResolver.createEvaluator(PTFDesc,PartitionedTableFunctionDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMap.NoopWithMapResolver.getOutputColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMap.NoopWithMapResolver.getRawInputColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMap.NoopWithMapResolver.initializeOutputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMap.NoopWithMapResolver.initializeRawInputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMap.NoopWithMapResolver.setupOutputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMap.NoopWithMapResolver.setupRawInputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMap.NoopWithMapResolver.transformsRawInput()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMap._transformRawInput(PTFPartition)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMap.execute(PTFPartition)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMapStreaming.NoopWithMapStreaming()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMapStreaming.NoopWithMapStreamingResolver.createEvaluator(PTFDesc,PartitionedTableFunctionDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMapStreaming.initializeStreaming(Configuration,StructObjectInspector,boolean)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.NoopWithMapStreaming.processRow(Object)",2,2,2
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator._transformRawInput(PTFPartition)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.canAcceptInputAsStream()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.canIterateOutput()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.close()",1,2,2
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.execute(PTFPartition)",2,3,3
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.finishPartition()",2,2,2
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.getOutputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.getQueryDef()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.getRawInputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.getTableDef()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.initializeStreaming(Configuration,StructObjectInspector,boolean)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.isTransformsRawInput()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.iterator(PTFPartitionIterator<Object>)",3,2,3
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.processRow(Object)",2,2,2
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.setOutputOI(StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.setQueryDef(PTFDesc)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.setRawInputOI(StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.setTableDef(PartitionedTableFunctionDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.setTransformsRawInput(boolean)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.startPartition()",2,2,2
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.transformRawInput(PTFPartition)",2,2,2
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.transformRawInputIterator(PTFPartitionIterator<Object>)",2,1,2
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.carryForwardNames()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.getEvaluator()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.getPtfDesc()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.getRawInputColumnNames()",2,1,2
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.initialize(HiveConf,PTFDesc,PartitionedTableFunctionDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.initialize(PTFDesc,PartitionedTableFunctionDef,TableFunctionEvaluator)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.initializeRawInputOI()",2,1,2
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.setOutputOI(StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.setRawInputOI(StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.setupRawInputOI()",2,1,2
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.DoubleValueBoundaryScanner.DoubleValueBoundaryScanner(BoundaryDef,Order,PTFExpressionDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.DoubleValueBoundaryScanner.isEqual(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.DoubleValueBoundaryScanner.isGreater(Object,Object,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.HiveDecimalValueBoundaryScanner.HiveDecimalValueBoundaryScanner(BoundaryDef,Order,PTFExpressionDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.HiveDecimalValueBoundaryScanner.isEqual(Object,Object)",2,1,3
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.HiveDecimalValueBoundaryScanner.isGreater(Object,Object,int)",2,1,3
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.LongValueBoundaryScanner.LongValueBoundaryScanner(BoundaryDef,Order,PTFExpressionDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.LongValueBoundaryScanner.isEqual(Object,Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.LongValueBoundaryScanner.isGreater(Object,Object,int)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.Range.Range(int,int,PTFPartition)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.Range.iterator()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.RankLimit.RankLimit(RankLimit)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.RankLimit.RankLimit(int,int,List<WindowFunctionDef>)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.RankLimit.limitReached()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.RankLimit.reset()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.RankLimit.updateRank(List<Object>)",1,1,2
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.SameList.SameList(int,E)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.SameList.get(int)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.SameList.size()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.StreamingState.StreamingState(Configuration,StructObjectInspector,boolean,WindowTableFunctionDef,int,int)",1,5,6
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.StreamingState.hasOutputRow()",4,2,4
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.StreamingState.nextOutputRow()",1,4,4
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.StreamingState.rankLimitReached()",1,2,2
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.StreamingState.reset(WindowTableFunctionDef)",1,4,4
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.StringValueBoundaryScanner.StringValueBoundaryScanner(BoundaryDef,Order,PTFExpressionDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.StringValueBoundaryScanner.isEqual(Object,Object)",1,2,3
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.StringValueBoundaryScanner.isGreater(Object,Object,int)",1,3,3
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.ValueBoundaryScanner.ValueBoundaryScanner(BoundaryDef,Order,PTFExpressionDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.ValueBoundaryScanner.computeEnd(int,PTFPartition)",4,4,4
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.ValueBoundaryScanner.computeEndCurrentRow(int,PTFPartition)",2,8,8
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.ValueBoundaryScanner.computeEndFollowing(int,PTFPartition)",5,14,14
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.ValueBoundaryScanner.computeEndPreceding(int,PTFPartition)",4,10,10
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.ValueBoundaryScanner.computeStart(int,PTFPartition)",4,4,4
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.ValueBoundaryScanner.computeStartCurrentRow(int,PTFPartition)",2,7,8
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.ValueBoundaryScanner.computeStartFollowing(int,PTFPartition)",4,13,13
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.ValueBoundaryScanner.computeStartPreceding(int,PTFPartition)",5,12,14
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.ValueBoundaryScanner.computeValue(Object)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.ValueBoundaryScanner.getScanner(ValueBoundaryDef,Order)",5,5,5
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.ValueBoundaryScanner.reset(BoundaryDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingIterator.WindowingIterator(PTFPartition,ArrayList<Object>,List<?>[],int[])",1,3,5
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingIterator.hasNext()",2,2,3
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingIterator.next()",1,9,10
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingIterator.remove()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingTableFunctionResolver.carryForwardNames()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingTableFunctionResolver.createEvaluator(PTFDesc,PartitionedTableFunctionDef)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingTableFunctionResolver.getOutputColumnNames()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingTableFunctionResolver.getWdwProcessingOutputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingTableFunctionResolver.initializeOutputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingTableFunctionResolver.setWdwProcessingOutputOI(StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingTableFunctionResolver.setupOutputOI()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.WindowingTableFunctionResolver.transformsRawInput()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.canIterateOutput()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.evaluateWindowFunction(WindowFunctionDef,PTFPartitionIterator<Object>)",1,5,5
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.execute(PTFPartitionIterator<Object>,PTFPartition)",1,7,7
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.executeFnwithWindow(PTFDesc,WindowFunctionDef,PTFPartition,Order)",1,2,2
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.finishPartition()",4,12,16
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.getRange(WindowFunctionDef,int,PTFPartition,Order)",1,4,7
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.getRowBoundaryEnd(BoundaryDef,int,PTFPartition)",6,3,6
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.getRowBoundaryStart(BoundaryDef,int)",5,2,5
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.initializeStreaming(Configuration,StructObjectInspector,boolean)",2,5,6
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.iterator(PTFPartitionIterator<Object>)",1,10,11
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.processRow(Object)",4,8,12
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.processWindow(WindowFunctionDef)",3,2,4
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.setCanAcceptInputAsStream(Configuration)",7,8,15
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.startPartition()",1,1,1
"org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.streamingPossible(Configuration,WindowFunctionDef)",6,1,7
"org.apache.hadoop.hive.ql.udf.xml.GenericUDFXPath.eval(String,String)",2,3,4
"org.apache.hadoop.hive.ql.udf.xml.GenericUDFXPath.evaluate(DeferredObject[])",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.GenericUDFXPath.getDisplayString(String[])",1,2,2
"org.apache.hadoop.hive.ql.udf.xml.GenericUDFXPath.initialize(ObjectInspector[])",2,1,2
"org.apache.hadoop.hive.ql.udf.xml.TestReusableStringReader.testEmpty()",1,1,3
"org.apache.hadoop.hive.ql.udf.xml.TestReusableStringReader.testMarkReset()",1,2,2
"org.apache.hadoop.hive.ql.udf.xml.TestReusableStringReader.testSkip()",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.TestUDFXPathUtil.testEvalIllegalArgs()",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.TestUDFXPathUtil.testEvalPositive()",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathBoolean.evaluate(String,String)",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathDouble.evaluate(String,String)",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathFloat.evaluate(String,String)",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathInteger.evaluate(String,String)",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathLong.evaluate(String,String)",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathShort.evaluate(String,String)",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathString.evaluate(String,String)",2,1,2
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.ReusableStringReader.ReusableStringReader()",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.ReusableStringReader.close()",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.ReusableStringReader.ensureOpen()",2,1,2
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.ReusableStringReader.mark(int)",2,1,2
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.ReusableStringReader.markSupported()",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.ReusableStringReader.read()",2,1,2
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.ReusableStringReader.read(char[],int,int)",4,1,8
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.ReusableStringReader.ready()",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.ReusableStringReader.reset()",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.ReusableStringReader.set(String)",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.ReusableStringReader.skip(long)",2,1,2
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.eval(String,String,QName)",4,3,10
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.evalBoolean(String,String)",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.evalNode(String,String)",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.evalNodeList(String,String)",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.evalNumber(String,String)",1,1,1
"org.apache.hadoop.hive.ql.udf.xml.UDFXPathUtil.evalString(String,String)",1,1,1
"org.apache.hadoop.hive.ql.util.DosToUnix.convertWindowsScriptToUnix(File)",1,4,6
"org.apache.hadoop.hive.ql.util.DosToUnix.getUnixScriptNameFor(String)",1,2,2
"org.apache.hadoop.hive.ql.util.DosToUnix.isWindowsScript(File)",5,3,10
"org.apache.hadoop.hive.ql.util.JavaDataModel.alignUp(int,int)",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.get()",3,1,4
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthFor(NumDistinctValueEstimator)",1,2,2
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthFor(NumericHistogram)",1,2,2
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthFor(String)",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthForBooleanArrayOfSize(int)",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthForByteArrayOfSize(int)",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthForDateArrayOfSize(int)",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthForDecimalArrayOfSize(int)",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthForDoubleArrayOfSize(int)",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthForIntArrayOfSize(int)",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthForLongArrayOfSize(int)",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthForObjectArrayOfSize(int)",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthForPrimitiveArrayOfSize(int,int)",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthForRandom()",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthForStringOfLength(int)",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthForTimestampArrayOfSize(int)",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthOfBigInteger()",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthOfDate()",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthOfDecimal()",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.lengthOfTimestamp()",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.primitive1()",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.primitive2()",1,1,1
"org.apache.hadoop.hive.ql.util.JavaDataModel.round(int)",2,1,3
"org.apache.hadoop.hive.ql.util.TestDosToUnix.setUp()",1,1,1
"org.apache.hadoop.hive.ql.util.TestDosToUnix.tearDown()",2,1,2
"org.apache.hadoop.hive.ql.util.TestDosToUnix.testConvertWindowsScriptToUnix()",2,2,3
"org.apache.hadoop.hive.ql.util.TestDosToUnix.testGetUnixScriptNameFor()",1,1,1
"org.apache.hadoop.hive.ql.util.TestDosToUnix.testIsWindowsScript()",1,1,1
"org.apache.hadoop.hive.scripts.extracturl.extracturl()",1,1,1
"org.apache.hadoop.hive.scripts.extracturl.main(String[])",1,4,4
"org.apache.hadoop.hive.serde.test.InnerStruct.InnerStruct()",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.InnerStruct(InnerStruct)",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.InnerStruct(int)",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.InnerStructStandardScheme.read(TProtocol,InnerStruct)",4,4,6
"org.apache.hadoop.hive.serde.test.InnerStruct.InnerStructStandardScheme.write(TProtocol,InnerStruct)",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.InnerStructStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.InnerStructTupleScheme.read(TProtocol,InnerStruct)",1,2,2
"org.apache.hadoop.hive.serde.test.InnerStruct.InnerStructTupleScheme.write(TProtocol,InnerStruct)",1,3,3
"org.apache.hadoop.hive.serde.test.InnerStruct.InnerStructTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.serde.test.InnerStruct._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.serde.test.InnerStruct._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.clear()",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.compareTo(InnerStruct)",5,3,5
"org.apache.hadoop.hive.serde.test.InnerStruct.deepCopy()",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.equals(InnerStruct)",5,1,7
"org.apache.hadoop.hive.serde.test.InnerStruct.equals(Object)",3,2,3
"org.apache.hadoop.hive.serde.test.InnerStruct.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.getField0()",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.serde.test.InnerStruct.hashCode()",1,2,2
"org.apache.hadoop.hive.serde.test.InnerStruct.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.serde.test.InnerStruct.isSetField0()",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.serde.test.InnerStruct.setField0(int)",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.setField0IsSet(boolean)",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.serde.test.InnerStruct.toString()",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.unsetField0()",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.validate()",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.serde.test.InnerStruct.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.serde.test.ThriftTestObj.ThriftTestObj()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.ThriftTestObj(ThriftTestObj)",1,3,4
"org.apache.hadoop.hive.serde.test.ThriftTestObj.ThriftTestObj(int,String,List<InnerStruct>)",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.ThriftTestObjStandardScheme.read(TProtocol,ThriftTestObj)",4,7,11
"org.apache.hadoop.hive.serde.test.ThriftTestObj.ThriftTestObjStandardScheme.write(TProtocol,ThriftTestObj)",1,4,4
"org.apache.hadoop.hive.serde.test.ThriftTestObj.ThriftTestObjStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.ThriftTestObjTupleScheme.read(TProtocol,ThriftTestObj)",1,5,5
"org.apache.hadoop.hive.serde.test.ThriftTestObj.ThriftTestObjTupleScheme.write(TProtocol,ThriftTestObj)",1,8,8
"org.apache.hadoop.hive.serde.test.ThriftTestObj.ThriftTestObjTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.serde.test.ThriftTestObj._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.serde.test.ThriftTestObj._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.addToField3(InnerStruct)",1,1,2
"org.apache.hadoop.hive.serde.test.ThriftTestObj.clear()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.compareTo(ThriftTestObj)",11,5,11
"org.apache.hadoop.hive.serde.test.ThriftTestObj.deepCopy()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.equals(Object)",3,2,3
"org.apache.hadoop.hive.serde.test.ThriftTestObj.equals(ThriftTestObj)",11,7,21
"org.apache.hadoop.hive.serde.test.ThriftTestObj.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.getField1()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.getField2()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.getField3()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.getField3Iterator()",1,2,2
"org.apache.hadoop.hive.serde.test.ThriftTestObj.getField3Size()",1,2,2
"org.apache.hadoop.hive.serde.test.ThriftTestObj.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.serde.test.ThriftTestObj.hashCode()",1,6,6
"org.apache.hadoop.hive.serde.test.ThriftTestObj.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.serde.test.ThriftTestObj.isSetField1()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.isSetField2()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.isSetField3()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.serde.test.ThriftTestObj.setField1(int)",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.setField1IsSet(boolean)",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.setField2(String)",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.setField2IsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde.test.ThriftTestObj.setField3(List<InnerStruct>)",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.setField3IsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde.test.ThriftTestObj.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.serde.test.ThriftTestObj.toString()",1,5,5
"org.apache.hadoop.hive.serde.test.ThriftTestObj.unsetField1()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.unsetField2()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.unsetField3()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.validate()",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.serde.test.ThriftTestObj.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.serde2.AbstractEncodingAwareSerDe.deserialize(Writable)",1,2,2
"org.apache.hadoop.hive.serde2.AbstractEncodingAwareSerDe.initialize(Configuration,Properties)",1,1,1
"org.apache.hadoop.hive.serde2.AbstractEncodingAwareSerDe.serialize(Object,ObjectInspector)",1,2,2
"org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(Configuration,Properties,Properties)",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.BaseStructObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.BaseStructObjectInspector(List<String>,List<ObjectInspector>)",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.BaseStructObjectInspector(List<String>,List<ObjectInspector>,List<String>)",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.MyField.MyField(int,String,ObjectInspector,String)",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.MyField.MyField(int,StructField)",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.MyField.getFieldComment()",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.MyField.getFieldID()",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.MyField.getFieldName()",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.MyField.getFieldObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.MyField.toString()",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.createField(int,String,ObjectInspector,String)",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.getAllStructFieldRefs()",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.getStructFieldRef(String)",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.init(List<String>,List<ObjectInspector>,List<String>)",1,4,4
"org.apache.hadoop.hive.serde2.BaseStructObjectInspector.init(List<StructField>)",1,2,2
"org.apache.hadoop.hive.serde2.ByteStream.Input.Input()",1,1,1
"org.apache.hadoop.hive.serde2.ByteStream.Input.Input(byte[])",1,1,1
"org.apache.hadoop.hive.serde2.ByteStream.Input.Input(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.serde2.ByteStream.Input.getCount()",1,1,1
"org.apache.hadoop.hive.serde2.ByteStream.Input.getData()",1,1,1
"org.apache.hadoop.hive.serde2.ByteStream.Input.reset(byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.ByteStream.Output.Output()",1,1,1
"org.apache.hadoop.hive.serde2.ByteStream.Output.Output(int)",1,1,1
"org.apache.hadoop.hive.serde2.ByteStream.Output.getData()",1,1,1
"org.apache.hadoop.hive.serde2.ByteStream.Output.reserve(int)",1,2,2
"org.apache.hadoop.hive.serde2.ByteStream.Output.writeInt(long,int)",1,1,1
"org.apache.hadoop.hive.serde2.ByteStreamTypedSerDe.ByteStreamTypedSerDe(Type)",1,1,1
"org.apache.hadoop.hive.serde2.ByteStreamTypedSerDe.deserialize(Writable)",1,1,1
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.ColumnProjectionUtils()",1,1,1
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.appendReadColumnIDs(Configuration,List<Integer>)",1,1,1
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.appendReadColumnNames(Configuration,List<String>)",1,3,3
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.appendReadColumnNames(StringBuilder,List<String>)",1,3,3
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.appendReadColumns(Configuration,List<Integer>)",1,1,2
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.appendReadColumns(Configuration,List<Integer>,List<String>)",1,1,1
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.appendReadColumns(StringBuilder,List<Integer>)",1,3,3
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.appendReadColumns(StringBuilder,StringBuilder,List<Integer>,List<String>)",1,1,1
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.getReadColumnIDs(Configuration)",1,3,3
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.isReadAllColumns(Configuration)",1,1,1
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.setFullyReadColumns(Configuration)",1,1,1
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.setReadAllColumns(Configuration)",1,1,1
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.setReadColumnIDConf(Configuration,String)",1,2,2
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.setReadColumnIDs(Configuration,List<Integer>)",1,1,1
"org.apache.hadoop.hive.serde2.ColumnProjectionUtils.toReadColumnIDString(List<Integer>)",1,3,3
"org.apache.hadoop.hive.serde2.ColumnSet.ColumnSet()",1,1,1
"org.apache.hadoop.hive.serde2.ColumnSet.ColumnSet(ArrayList<String>)",1,1,1
"org.apache.hadoop.hive.serde2.ColumnSet.toString()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableListObjectInspector1.CustomNonSettableListObjectInspector1()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableListObjectInspector1.CustomNonSettableListObjectInspector1(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableListObjectInspector1.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableListObjectInspector1.getList(Object)",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableListObjectInspector1.getListElement(Object,int)",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableListObjectInspector1.getListElementObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableListObjectInspector1.getListLength(Object)",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableListObjectInspector1.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.CustomNonSettableStructObjectInspector1()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.CustomNonSettableStructObjectInspector1(List<String>,List<ObjectInspector>)",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.MyField.MyField()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.MyField.MyField(int,String,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.MyField.MyField(int,String,ObjectInspector,String)",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.MyField.getFieldComment()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.MyField.getFieldID()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.MyField.getFieldName()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.MyField.getFieldObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.MyField.toString()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.getAllStructFieldRefs()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.getStructFieldData(Object,StructField)",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.getStructFieldRef(String)",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.getStructFieldsDataAsList(Object)",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableStructObjectInspector1.init(List<String>,List<ObjectInspector>)",1,2,2
"org.apache.hadoop.hive.serde2.CustomNonSettableUnionObjectInspector1.CustomNonSettableUnionObjectInspector1(List<ObjectInspector>)",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableUnionObjectInspector1.StandardUnion.StandardUnion()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableUnionObjectInspector1.StandardUnion.StandardUnion(byte,Object)",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableUnionObjectInspector1.StandardUnion.getObject()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableUnionObjectInspector1.StandardUnion.getTag()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableUnionObjectInspector1.StandardUnion.toString()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableUnionObjectInspector1.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableUnionObjectInspector1.getField(Object)",2,1,2
"org.apache.hadoop.hive.serde2.CustomNonSettableUnionObjectInspector1.getObjectInspectors()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableUnionObjectInspector1.getTag(Object)",2,1,2
"org.apache.hadoop.hive.serde2.CustomNonSettableUnionObjectInspector1.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.CustomNonSettableUnionObjectInspector1.toString()",1,1,1
"org.apache.hadoop.hive.serde2.CustomSerDe1.deserialize(Writable)",1,1,1
"org.apache.hadoop.hive.serde2.CustomSerDe1.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.CustomSerDe1.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.serde2.CustomSerDe1.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.serde2.CustomSerDe1.initialize(Configuration,Properties)",1,4,4
"org.apache.hadoop.hive.serde2.CustomSerDe1.serialize(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.CustomSerDe2.deserialize(Writable)",1,1,1
"org.apache.hadoop.hive.serde2.CustomSerDe2.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.CustomSerDe2.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.serde2.CustomSerDe2.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.serde2.CustomSerDe2.initialize(Configuration,Properties)",1,4,4
"org.apache.hadoop.hive.serde2.CustomSerDe2.serialize(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.CustomSerDe3.initialize(Configuration,Properties)",1,4,4
"org.apache.hadoop.hive.serde2.CustomSerDe4.initialize(Configuration,Properties)",1,5,5
"org.apache.hadoop.hive.serde2.CustomSerDe5.initialize(Configuration,Properties)",1,5,5
"org.apache.hadoop.hive.serde2.CustomTextSerDe.CustomTextSerDe()",1,1,1
"org.apache.hadoop.hive.serde2.CustomTextStorageFormatDescriptor.getNames()",1,1,1
"org.apache.hadoop.hive.serde2.CustomTextStorageFormatDescriptor.getSerde()",1,1,1
"org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.DelimitedJSONSerDe()",1,1,1
"org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.doDeserialize(Writable)",1,1,1
"org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(Output,Object,ObjectInspector,SerDeParameters)",2,3,4
"org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.MetadataTypedColumnsetSerDe()",1,1,1
"org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.deserialize(ColumnSet,String,String,String,int)",1,4,4
"org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.deserialize(Writable)",2,5,7
"org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.getByteValue(String,String)",2,3,4
"org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.initialize(Configuration,Properties)",1,7,9
"org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.serialize(Object,ObjectInspector)",2,6,6
"org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.toString()",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.NullStructField.getFieldComment()",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.NullStructField.getFieldID()",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.NullStructField.getFieldName()",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.NullStructField.getFieldObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.NullStructSerDeObjectInspector.getAllStructFieldRefs()",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.NullStructSerDeObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.NullStructSerDeObjectInspector.getStructFieldData(Object,StructField)",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.NullStructSerDeObjectInspector.getStructFieldRef(String)",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.NullStructSerDeObjectInspector.getStructFieldsDataAsList(Object)",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.NullStructSerDeObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.deserialize(Writable)",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.initialize(Configuration,Properties)",1,1,1
"org.apache.hadoop.hive.serde2.NullStructSerDe.serialize(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.RegexSerDe.deserialize(Writable)",5,7,21
"org.apache.hadoop.hive.serde2.RegexSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.RegexSerDe.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.serde2.RegexSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.serde2.RegexSerDe.initialize(Configuration,Properties)",4,6,7
"org.apache.hadoop.hive.serde2.RegexSerDe.serialize(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.SerDeException.SerDeException()",1,1,1
"org.apache.hadoop.hive.serde2.SerDeException.SerDeException(String)",1,1,1
"org.apache.hadoop.hive.serde2.SerDeException.SerDeException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.serde2.SerDeException.SerDeException(Throwable)",1,1,1
"org.apache.hadoop.hive.serde2.SerDeStats.SerDeStats()",1,1,1
"org.apache.hadoop.hive.serde2.SerDeStats.getRawDataSize()",1,1,1
"org.apache.hadoop.hive.serde2.SerDeStats.getRowCount()",1,1,1
"org.apache.hadoop.hive.serde2.SerDeStats.setRawDataSize(long)",1,1,1
"org.apache.hadoop.hive.serde2.SerDeStats.setRowCount(long)",1,1,1
"org.apache.hadoop.hive.serde2.SerDeUtils.SerDeUtils()",1,1,1
"org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(StringBuilder,Object,ObjectInspector,String)",5,33,34
"org.apache.hadoop.hive.serde2.SerDeUtils.createOverlayedProperties(Properties,Properties)",1,2,2
"org.apache.hadoop.hive.serde2.SerDeUtils.escapeString(String)",2,5,11
"org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(Object,ObjectInspector,String)",1,1,1
"org.apache.hadoop.hive.serde2.SerDeUtils.hasAnyNullObject(List,StructObjectInspector,boolean[])",3,3,5
"org.apache.hadoop.hive.serde2.SerDeUtils.hasAnyNullObject(Object,ObjectInspector)",22,14,23
"org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(Deserializer,Configuration,Properties,Properties)",1,2,2
"org.apache.hadoop.hive.serde2.SerDeUtils.lightEscapeString(String)",2,3,6
"org.apache.hadoop.hive.serde2.SerDeUtils.toThriftPayload(Object,ObjectInspector,int)",4,3,5
"org.apache.hadoop.hive.serde2.SerDeUtils.transformTextFromUTF8(Text,Charset)",1,1,1
"org.apache.hadoop.hive.serde2.SerDeUtils.transformTextToUTF8(Text,Charset)",1,1,1
"org.apache.hadoop.hive.serde2.TestColumnProjectionUtils.setUp()",1,1,1
"org.apache.hadoop.hive.serde2.TestColumnProjectionUtils.testDeprecatedMethods()",1,1,1
"org.apache.hadoop.hive.serde2.TestColumnProjectionUtils.testReadAllColumns()",1,1,1
"org.apache.hadoop.hive.serde2.TestColumnProjectionUtils.testReadColumnIds()",1,1,1
"org.apache.hadoop.hive.serde2.TestSerDe.TestSerDe()",1,1,1
"org.apache.hadoop.hive.serde2.TestSerDe.deserialize(ColumnSet,String,String,String)",1,4,4
"org.apache.hadoop.hive.serde2.TestSerDe.deserialize(Writable)",2,5,7
"org.apache.hadoop.hive.serde2.TestSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.TestSerDe.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.serde2.TestSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.serde2.TestSerDe.getShortName()",1,1,1
"org.apache.hadoop.hive.serde2.TestSerDe.initialize(Configuration,Properties)",1,5,6
"org.apache.hadoop.hive.serde2.TestSerDe.serialize(Object,ObjectInspector)",2,6,6
"org.apache.hadoop.hive.serde2.TestSerDe.shortName()",1,1,1
"org.apache.hadoop.hive.serde2.TestSerDe.toString()",1,1,1
"org.apache.hadoop.hive.serde2.TestSerdeWithFieldComments.mockedStructField(String,String,String)",1,1,1
"org.apache.hadoop.hive.serde2.TestSerdeWithFieldComments.testFieldComments()",1,1,1
"org.apache.hadoop.hive.serde2.TestStatsSerde.TestStatsSerde(String)",1,1,1
"org.apache.hadoop.hive.serde2.TestStatsSerde.createProperties()",1,1,1
"org.apache.hadoop.hive.serde2.TestStatsSerde.deserializeAndSerializeColumnar(ColumnarSerDe,BytesRefArrayWritable,String[])",1,3,3
"org.apache.hadoop.hive.serde2.TestStatsSerde.deserializeAndSerializeLazyBinary(SerDe,Object[],ObjectInspector)",1,3,3
"org.apache.hadoop.hive.serde2.TestStatsSerde.deserializeAndSerializeLazySimple(LazySimpleSerDe,Text)",1,1,1
"org.apache.hadoop.hive.serde2.TestStatsSerde.testColumnarSerDe()",1,3,3
"org.apache.hadoop.hive.serde2.TestStatsSerde.testLazyBinarySerDe()",1,14,14
"org.apache.hadoop.hive.serde2.TestStatsSerde.testLazySimpleSerDe()",1,2,2
"org.apache.hadoop.hive.serde2.TestTCTLSeparatedProtocol.TestTCTLSeparatedProtocol()",1,1,1
"org.apache.hadoop.hive.serde2.TestTCTLSeparatedProtocol.test1ApacheLogFormat()",1,1,1
"org.apache.hadoop.hive.serde2.TestTCTLSeparatedProtocol.testNulls()",1,1,1
"org.apache.hadoop.hive.serde2.TestTCTLSeparatedProtocol.testQuotedWrites()",1,1,1
"org.apache.hadoop.hive.serde2.TestTCTLSeparatedProtocol.testReads()",1,1,1
"org.apache.hadoop.hive.serde2.TestTCTLSeparatedProtocol.testShouldThrowRunTimeExceptionIfUnableToInitializeTokenizer()",1,2,2
"org.apache.hadoop.hive.serde2.TestTCTLSeparatedProtocol.testWrites()",1,1,1
"org.apache.hadoop.hive.serde2.TypedSerDe.TypedSerDe(Type)",3,3,3
"org.apache.hadoop.hive.serde2.TypedSerDe.deserialize(Writable)",2,2,2
"org.apache.hadoop.hive.serde2.TypedSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.TypedSerDe.getObjectInspectorOptions()",1,1,1
"org.apache.hadoop.hive.serde2.TypedSerDe.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.serde2.TypedSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.serde2.TypedSerDe.initialize(Configuration,Properties)",1,1,1
"org.apache.hadoop.hive.serde2.TypedSerDe.serialize(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.ByteSegmentRef.ByteSegmentRef(long,int)",2,1,2
"org.apache.hadoop.hive.serde2.WriteBuffers.ByteSegmentRef.copy()",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.ByteSegmentRef.getBytes()",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.ByteSegmentRef.getLength()",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.ByteSegmentRef.getOffset()",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.WriteBuffers(int,long)",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.clear()",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.getBufferIndex(long)",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.getLength()",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.getOffset(long)",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.getReadPoint()",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.getWritePoint()",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.hashCode(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.hashCode(long,int)",2,3,3
"org.apache.hadoop.hive.serde2.WriteBuffers.isAllInOneReadBuffer(int)",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.isAllInOneWriteBuffer(int)",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.isEqual(byte[],int,long,int)",7,3,8
"org.apache.hadoop.hive.serde2.WriteBuffers.isEqual(long,int,long,int)",7,4,10
"org.apache.hadoop.hive.serde2.WriteBuffers.murmurHash(byte[],int,int)",1,1,6
"org.apache.hadoop.hive.serde2.WriteBuffers.nextBufferToWrite()",3,2,3
"org.apache.hadoop.hive.serde2.WriteBuffers.ponderNextBufferToRead()",1,2,2
"org.apache.hadoop.hive.serde2.WriteBuffers.populateValue(ByteSegmentRef)",1,4,4
"org.apache.hadoop.hive.serde2.WriteBuffers.readFiveByteULong(long)",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.readInt(long)",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.readNByteLong(long,int)",1,3,4
"org.apache.hadoop.hive.serde2.WriteBuffers.readNextByte()",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.readVLong()",2,3,6
"org.apache.hadoop.hive.serde2.WriteBuffers.reserve(int)",2,2,3
"org.apache.hadoop.hive.serde2.WriteBuffers.seal()",1,3,3
"org.apache.hadoop.hive.serde2.WriteBuffers.setByte(long,byte)",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.setReadPoint(long)",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.setWritePoint(long)",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.skipVLong()",1,2,3
"org.apache.hadoop.hive.serde2.WriteBuffers.write(byte[])",1,1,1
"org.apache.hadoop.hive.serde2.WriteBuffers.write(byte[],int,int)",1,3,3
"org.apache.hadoop.hive.serde2.WriteBuffers.write(int)",1,2,2
"org.apache.hadoop.hive.serde2.WriteBuffers.writeBytes(long,int)",1,5,5
"org.apache.hadoop.hive.serde2.WriteBuffers.writeFiveByteULong(long,long)",1,2,2
"org.apache.hadoop.hive.serde2.WriteBuffers.writeInt(long,int)",1,2,2
"org.apache.hadoop.hive.serde2.WriteBuffers.writeVLong(long)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroDeserializer.SchemaReEncoder.SchemaReEncoder(Schema,Schema)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroDeserializer.SchemaReEncoder.reencode(GenericRecord)",1,1,2
"org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserialize(List<String>,List<TypeInfo>,Writable,Schema)",2,8,9
"org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializeList(Object,Schema,Schema,ListTypeInfo)",3,7,7
"org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializeMap(Object,Schema,Schema,MapTypeInfo)",1,3,3
"org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializeNullableUnion(Object,Schema,Schema,TypeInfo)",2,3,4
"org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializePrimitive(Object,Schema,Schema,PrimitiveTypeInfo)",7,6,9
"org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializeStruct(Record,Schema,StructTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializeUnion(Object,Schema,Schema,UnionTypeInfo)",1,2,2
"org.apache.hadoop.hive.serde2.avro.AvroDeserializer.getNoEncodingNeeded()",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroDeserializer.getReEncoderCache()",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroDeserializer.worker(Object,Schema,Schema,TypeInfo)",8,8,8
"org.apache.hadoop.hive.serde2.avro.AvroDeserializer.workerBase(List<Object>,Schema,List<String>,List<TypeInfo>,GenericRecord)",1,3,3
"org.apache.hadoop.hive.serde2.avro.AvroGenericRecordWritable.AvroGenericRecordWritable()",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroGenericRecordWritable.AvroGenericRecordWritable(GenericRecord)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroGenericRecordWritable.getFileSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroGenericRecordWritable.getRecord()",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroGenericRecordWritable.getRecordReaderID()",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroGenericRecordWritable.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroGenericRecordWritable.setFileSchema(Schema)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroGenericRecordWritable.setRecord(GenericRecord)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroGenericRecordWritable.setRecordReaderID(UID)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroGenericRecordWritable.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.AvroObjectInspectorGenerator(Schema)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.createObjectInspector()",1,2,2
"org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.createObjectInspectorWorker(TypeInfo)",3,4,10
"org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.generateColumnNames(Schema)",1,2,2
"org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.getColumnNames()",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.getColumnTypes()",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.supportedCategories(TypeInfo)",1,5,5
"org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator.verifySchemaIsARecord(Schema)",2,2,2
"org.apache.hadoop.hive.serde2.avro.AvroSerDe.deserialize(Writable)",2,1,2
"org.apache.hadoop.hive.serde2.avro.AvroSerDe.getDeserializer()",1,1,2
"org.apache.hadoop.hive.serde2.avro.AvroSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerDe.getSerializer()",1,1,2
"org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(Configuration,Properties)",3,10,11
"org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(Configuration,Properties,Properties)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerDe.serialize(Object,ObjectInspector)",2,1,2
"org.apache.hadoop.hive.serde2.avro.AvroSerdeException.AvroSerdeException()",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerdeException.AvroSerdeException(String)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerdeException.AvroSerdeException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerdeException.AvroSerdeException(Throwable)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrReturnErrorSchema(Properties)",1,3,3
"org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(Properties)",4,5,8
"org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getBufferFromBytes(byte[])",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getBufferFromDecimal(HiveDecimal,int)",2,1,2
"org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getBytesFromByteBuffer(ByteBuffer)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getHiveDecimalFromByteBuffer(ByteBuffer,int)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getOtherTypeFromNullableType(Schema)",1,2,2
"org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(File)",1,2,2
"org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(InputStream)",1,1,2
"org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(String)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFromFS(String,Configuration)",1,3,3
"org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.insideMRJob(JobConf)",1,3,3
"org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.isNullableType(Schema)",1,4,4
"org.apache.hadoop.hive.serde2.avro.AvroSerializer.SerializeToAvroException.SerializeToAvroException(Schema,Record)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerializer.SerializeToAvroException.toString()",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerializer.mapHasStringKey(ObjectInspector)",1,2,2
"org.apache.hadoop.hive.serde2.avro.AvroSerializer.serialize(Object,ObjectInspector,List<String>,List<TypeInfo>,Schema)",4,4,5
"org.apache.hadoop.hive.serde2.avro.AvroSerializer.serialize(TypeInfo,ObjectInspector,Object,Schema)",4,4,10
"org.apache.hadoop.hive.serde2.avro.AvroSerializer.serializeEnum(TypeInfo,PrimitiveObjectInspector,Object,Schema)",1,1,1
"org.apache.hadoop.hive.serde2.avro.AvroSerializer.serializeList(ListTypeInfo,ListObjectInspector,Object,Schema)",1,2,2
"org.apache.hadoop.hive.serde2.avro.AvroSerializer.serializeMap(MapTypeInfo,MapObjectInspector,Object,Schema)",2,3,3
"org.apache.hadoop.hive.serde2.avro.AvroSerializer.serializePrimitive(TypeInfo,PrimitiveObjectInspector,Object,Schema)",7,6,8
"org.apache.hadoop.hive.serde2.avro.AvroSerializer.serializeStruct(StructTypeInfo,StructObjectInspector,Object,Schema)",1,2,2
"org.apache.hadoop.hive.serde2.avro.AvroSerializer.serializeUnion(UnionTypeInfo,UnionObjectInspector,Object,Schema)",1,1,1
"org.apache.hadoop.hive.serde2.avro.InstanceCache.InstanceCache()",1,1,1
"org.apache.hadoop.hive.serde2.avro.InstanceCache.retrieve(SeedObject)",2,5,5
"org.apache.hadoop.hive.serde2.avro.ReaderWriterSchemaPair.ReaderWriterSchemaPair(Schema,Schema)",1,1,1
"org.apache.hadoop.hive.serde2.avro.ReaderWriterSchemaPair.equals(Object)",5,2,6
"org.apache.hadoop.hive.serde2.avro.ReaderWriterSchemaPair.getReader()",1,1,1
"org.apache.hadoop.hive.serde2.avro.ReaderWriterSchemaPair.getWriter()",1,1,1
"org.apache.hadoop.hive.serde2.avro.ReaderWriterSchemaPair.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateArrayTypeInfo(Schema)",1,1,1
"org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateColumnTypes(Schema)",1,2,2
"org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateEnumTypeInfo(Schema)",1,1,1
"org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateMapTypeInfo(Schema)",1,1,1
"org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateRecordTypeInfo(Schema)",1,2,2
"org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfo(Schema)",2,3,5
"org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateTypeInfoWorker(Schema)",9,9,9
"org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.generateUnionTypeInfo(Schema)",1,2,2
"org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.initTypeMap()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.ResultPair.ResultPair(ObjectInspector,Object,Object)",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeArrays()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeBytes()",1,3,3
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeEnums()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeFixed()",1,3,3
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeMapWithNullablePrimitiveValues()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeMapsWithPrimitiveKeys()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeNullableEnums()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeNullableTypes()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeRecords()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeUnions()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeVoidType()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.unionTester(Schema,Record)",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.verifyCaching()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.verifyNullableType(Record,Schema,String,String)",1,2,2
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.canHandleArrays()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.canHandleBytes()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.canHandleEnums()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.canHandleFixed()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.canHandleMapsWithPrimitiveValueTypes()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.canHandleRecords()",1,2,2
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.canHandleUnions()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.convertsMapsWithNullablePrimitiveTypes()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.convertsNullableEnum()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.convertsNullableTypes()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.failOnNonRecords()",1,2,2
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.objectInspectorsAreCached()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.primitiveTypesWorkCorrectly()",1,2,2
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.verifyColumnNames(String[],List<String>)",1,2,2
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.verifyColumnTypes(TypeInfo[],List<TypeInfo>)",1,2,2
"org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.verifyMap(AvroObjectInspectorGenerator,String)",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerde.badSchemaURLProvidedReturnsErrorSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerde.bothPropertiesSetToNoneReturnsErrorSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerde.emptySchemaProvidedReturnsErrorSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerde.emptySchemaURLProvidedReturnsErrorSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerde.getSerializedClassReturnsCorrectType()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerde.gibberishSchemaProvidedReturnsErrorSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerde.initializeDoesNotReuseSchemasFromConf()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerde.noSchemaProvidedReturnsErrorSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerde.verifyErrorSchemaReturned(Properties)",1,1,3
"org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.detemineSchemaTriesToOpenUrl()",1,2,2
"org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.determineSchemaCanReadSchemaFromHDFS()",1,2,2
"org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.determineSchemaFindsLiterals()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.determineSchemaThrowsExceptionIfNoSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.getTypeFromNullableTypePositiveCase()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.isNullableTypeAcceptsNullableUnions()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.isNullableTypeIdentifiesNonUnionTypes()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.isNullableTypeIdentifiesUnionsOfMoreThanTwoTypes()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.isNullableTypeIdentifiesUnionsWithoutNulls()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.noneOptionWorksForSpecifyingSchemas()",1,4,4
"org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.testField(String,String,boolean)",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.buildSchema(String)",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeArraysWithNullableComplexElements()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeArraysWithNullablePrimitiveElements()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeBooleans()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeBytes()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeDecimals()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeDoubles()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeEnums()",1,2,2
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeFixed()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeFloats()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeInts()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeListOfDecimals()",1,2,2
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeLists()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeMapOfDecimals()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeMaps()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeMapsWithNullableComplexValues()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeMapsWithNullablePrimitiveValues()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeNullableBytes()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeNullableDecimals()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeNullableEnums()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeNullableFixed()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeNullableLists()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeNullableMaps()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeNullablePrimitiveTypes()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeNullableRecords()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeRecordsWithNullableComplexElements()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeStrings()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeStructs()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.canSerializeUnions()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.serializeAndDeserialize(String,String,Object)",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.singleFieldTest(String,Object,String)",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestGenericAvroRecordWritable.writableContractIsImplementedCorrectly()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestInstanceCache.Foo.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestInstanceCache.Wrapper.Wrapper(T)",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestInstanceCache.instanceCacheReturnsCorrectInstances()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestInstanceCache.instanceCachesOnlyCreateOneInstance()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestSchemaReEncoder.schemasCanAddFields()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestThatEvolvedSchemasActAsWeWant.resolvedSchemasShouldReturnReaderSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroBinarySchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroBooleanSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroBytesSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroDecimalSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroDoubleSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroFloatSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroIntSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroListSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroLongSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroMapSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroNestedStructSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroShortSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroStringSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroStructSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroUnionSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroUnionSchemaOfNull()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroUnionSchemaOfOne()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroUnionSchemaWithNull()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.createAvroVoidSchema()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.genSchema(String)",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.genSchemaWithoutNull(String)",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.getAvroSchemaString(TypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.setUp()",1,1,1
"org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.convert(List<String>,List<TypeInfo>,List<String>,String,String,String)",1,4,5
"org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroArray(TypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroField(String,TypeInfo,String)",1,1,1
"org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroMap(TypeInfo)",2,1,2
"org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroPrimitive(TypeInfo)",2,2,15
"org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroRecord(TypeInfo)",2,2,3
"org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroSchema(TypeInfo)",2,2,6
"org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.createAvroUnion(TypeInfo)",1,3,3
"org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.getFields(Field)",1,3,3
"org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.removeDuplicateNullSchemas(List<Schema>)",1,4,4
"org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.wrapInUnionWithNull(Schema)",2,2,4
"org.apache.hadoop.hive.serde2.avro.Utils.serializeAndDeserializeRecord(Record)",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.deserialize(InputByteBuffer,TypeInfo,boolean,Object)",31,41,75
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.deserialize(Writable)",1,2,3
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.deserializeInt(InputByteBuffer,boolean)",1,2,2
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.deserializeText(InputByteBuffer,boolean,Text)",3,6,7
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.getCharacterMaxLength(TypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.getSortOrders()",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.initialize(Configuration,Properties)",1,6,6
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.serialize(Object,ObjectInspector)",1,2,2
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.serialize(Output,Object,ObjectInspector,boolean)",24,27,33
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.serializeBytes(Output,byte[],int,boolean)",1,3,4
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.serializeInt(Output,int,boolean)",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.serializeStruct(Output,Object[],List<ObjectInspector>,boolean[])",1,2,2
"org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.writeByte(RandomAccessOutput,byte,boolean)",1,1,2
"org.apache.hadoop.hive.serde2.binarysortable.InputByteBuffer.dumpHex()",1,2,3
"org.apache.hadoop.hive.serde2.binarysortable.InputByteBuffer.getData()",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.InputByteBuffer.getEnd()",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.InputByteBuffer.read()",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.InputByteBuffer.read(boolean)",3,1,3
"org.apache.hadoop.hive.serde2.binarysortable.InputByteBuffer.reset(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.InputByteBuffer.seek(int)",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.InputByteBuffer.tell()",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.MyTestClass.MyTestClass()",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.MyTestClass.MyTestClass(Byte,Short,Integer,Long,Float,Double,String,HiveDecimal,Date,MyTestInnerStruct,List<Integer>,byte[])",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.MyTestInnerStruct.MyTestInnerStruct()",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.MyTestInnerStruct.MyTestInnerStruct(Integer,Integer)",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.getRandBA(Random,int)",1,2,2
"org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.getRandDate(Random)",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.getRandHiveDecimal(Random)",1,3,3
"org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.getRandIntegerArray(Random)",1,2,2
"org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.getRandString(Random)",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.getRandString(Random,String,int)",1,3,3
"org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.getSerDe(String,String,String)",1,1,1
"org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.hexString(BytesWritable)",1,2,3
"org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.makeHashMap(String...)",1,2,2
"org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.sort(Object[],ObjectInspector)",1,3,4
"org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.testBinarySortableSerDe()",1,14,14
"org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.testBinarySortableSerDe(Object[],ObjectInspector,SerDe,boolean)",1,6,10
"org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.BytesRefArrayWritable()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.BytesRefArrayWritable(int)",2,1,2
"org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.clear()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.compareTo(BytesRefArrayWritable)",6,2,6
"org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.ensureCapacity(int)",1,2,3
"org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.equals(Object)",2,1,3
"org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.get(int)",2,1,2
"org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.readFields(DataInput)",1,2,2
"org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.resetValid(int)",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.set(int,BytesRefWritable)",1,1,2
"org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.size()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.unCheckedGet(int)",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable.write(DataOutput)",1,2,2
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.BytesRefWritable()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.BytesRefWritable(LazyDecompressionCallback,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.BytesRefWritable(byte[])",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.BytesRefWritable(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.BytesRefWritable(int)",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.compareTo(BytesRefWritable)",3,1,4
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.equals(Object)",2,1,3
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getBytesCopy()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getData()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getLength()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.getStart()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.lazyDecompress()",1,2,3
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.readFields(DataInput)",1,1,2
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.set(LazyDecompressionCallback,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.set(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.toString()",1,4,4
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.serde2.columnar.BytesRefWritable.writeDataTo(DataOutput)",1,1,1
"org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe.ColumnarSerDe()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe.initialize(Configuration,Properties)",1,4,4
"org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe.serialize(Object,ObjectInspector)",4,11,12
"org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe.toString()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.ColumnarSerDeBase.deserialize(Writable)",2,2,2
"org.apache.hadoop.hive.serde2.columnar.ColumnarSerDeBase.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.ColumnarSerDeBase.getSerDeStats()",1,2,2
"org.apache.hadoop.hive.serde2.columnar.ColumnarSerDeBase.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.ColumnarSerDeBase.initialize(int)",1,2,2
"org.apache.hadoop.hive.serde2.columnar.ColumnarStruct.ColumnarStruct(ObjectInspector,List<Integer>,Text)",1,2,2
"org.apache.hadoop.hive.serde2.columnar.ColumnarStruct.createLazyObjectBase(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.columnar.ColumnarStruct.getLength(ObjectInspector,ByteArrayRef,int,int)",3,2,3
"org.apache.hadoop.hive.serde2.columnar.ColumnarStructBase.ColumnarStructBase(ObjectInspector,List<Integer>)",1,4,5
"org.apache.hadoop.hive.serde2.columnar.ColumnarStructBase.FieldInfo.FieldInfo(LazyObjectBase,boolean,ObjectInspector)",1,1,2
"org.apache.hadoop.hive.serde2.columnar.ColumnarStructBase.FieldInfo.getSerializedSize()",2,1,2
"org.apache.hadoop.hive.serde2.columnar.ColumnarStructBase.FieldInfo.init(BytesRefWritable)",1,1,2
"org.apache.hadoop.hive.serde2.columnar.ColumnarStructBase.FieldInfo.uncheckedGetField()",5,2,6
"org.apache.hadoop.hive.serde2.columnar.ColumnarStructBase.getField(int)",1,1,1
"org.apache.hadoop.hive.serde2.columnar.ColumnarStructBase.getFieldsAsList()",1,3,3
"org.apache.hadoop.hive.serde2.columnar.ColumnarStructBase.getRawDataSerializedSize()",1,2,2
"org.apache.hadoop.hive.serde2.columnar.ColumnarStructBase.init(BytesRefArrayWritable)",1,3,3
"org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe.initialize(Configuration,Properties)",1,4,4
"org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe.serialize(Object,ObjectInspector)",2,8,8
"org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe.toString()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarStruct.LazyBinaryColumnarStruct(ObjectInspector,List<Integer>)",1,1,1
"org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarStruct.createLazyObjectBase(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarStruct.getLength(ObjectInspector,ByteArrayRef,int,int)",4,4,6
"org.apache.hadoop.hive.serde2.columnar.TestBytesRefArrayWritable.setup()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.TestBytesRefArrayWritable.testCompareTo()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.TestLazyBinaryColumnarSerDe.InnerStruct.InnerStruct(Integer,Long)",1,1,1
"org.apache.hadoop.hive.serde2.columnar.TestLazyBinaryColumnarSerDe.testHandlingAlteredSchemas()",1,1,1
"org.apache.hadoop.hive.serde2.columnar.TestLazyBinaryColumnarSerDe.testLazyBinaryColumnarSerDeWithEmptyBinary()",1,2,2
"org.apache.hadoop.hive.serde2.columnar.TestLazyBinaryColumnarSerDe.testSerDe()",1,2,2
"org.apache.hadoop.hive.serde2.columnar.TestLazyBinaryColumnarSerDe.testSerDeEmpties()",1,2,2
"org.apache.hadoop.hive.serde2.columnar.TestLazyBinaryColumnarSerDe.testSerDeInnerNulls()",1,2,2
"org.apache.hadoop.hive.serde2.columnar.TestLazyBinaryColumnarSerDe.testSerDeOuterNulls()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.deserialize(Writable)",1,3,3
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.dynamicSerDeStructBaseToObjectInspector(DynamicSerDeTypeBase)",4,5,5
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.initialize(Configuration,Properties)",2,6,8
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.serialize(Object,ObjectInspector)",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeAsync.DynamicSerDeAsync(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeAsync.DynamicSerDeAsync(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeCommaOrSemicolon.DynamicSerDeCommaOrSemicolon(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeCommaOrSemicolon.DynamicSerDeCommaOrSemicolon(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeConst.DynamicSerDeConst(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeConst.DynamicSerDeConst(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeConstList.DynamicSerDeConstList(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeConstList.DynamicSerDeConstList(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeConstListContents.DynamicSerDeConstListContents(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeConstListContents.DynamicSerDeConstListContents(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeConstMap.DynamicSerDeConstMap(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeConstMap.DynamicSerDeConstMap(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeConstMapContents.DynamicSerDeConstMapContents(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeConstMapContents.DynamicSerDeConstMapContents(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeConstValue.DynamicSerDeConstValue(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeConstValue.DynamicSerDeConstValue(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeDefinition.DynamicSerDeDefinition(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeDefinition.DynamicSerDeDefinition(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeDefinitionType.DynamicSerDeDefinitionType(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeDefinitionType.DynamicSerDeDefinitionType(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeEnum.DynamicSerDeEnum(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeEnum.DynamicSerDeEnum(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeEnumDef.DynamicSerDeEnumDef(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeEnumDef.DynamicSerDeEnumDef(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeEnumDefList.DynamicSerDeEnumDefList(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeEnumDefList.DynamicSerDeEnumDefList(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeExtends.DynamicSerDeExtends(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeExtends.DynamicSerDeExtends(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeField.DynamicSerDeField(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeField.DynamicSerDeField(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeField.getFieldType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeField.isSkippable()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.DynamicSerDeFieldList(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.DynamicSerDeFieldList(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.deserialize(Object,TProtocol)",7,15,19
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.getChildren()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.getField(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.getFieldByFieldId(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.getFieldByName(String)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.getNumFields()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.initialize()",1,3,3
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.serialize(Object,ObjectInspector,TProtocol)",4,7,9
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.toString()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldRequiredness.DynamicSerDeFieldRequiredness(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldRequiredness.DynamicSerDeFieldRequiredness(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldRequiredness.getRequiredness()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldType.DynamicSerDeFieldType(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldType.DynamicSerDeFieldType(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldType.getMyType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldValue.DynamicSerDeFieldValue(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldValue.DynamicSerDeFieldValue(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFlagArgs.DynamicSerDeFlagArgs(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFlagArgs.DynamicSerDeFlagArgs(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFunction.DynamicSerDeFunction(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFunction.DynamicSerDeFunction(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFunction.getFieldList()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFunction.getType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFunction.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFunctionType.DynamicSerDeFunctionType(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFunctionType.DynamicSerDeFunctionType(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeHeader.DynamicSerDeHeader(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeHeader.DynamicSerDeHeader(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeHeaderList.DynamicSerDeHeaderList(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeHeaderList.DynamicSerDeHeaderList(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeInclude.DynamicSerDeInclude(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeInclude.DynamicSerDeInclude(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeNamespace.DynamicSerDeNamespace(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeNamespace.DynamicSerDeNamespace(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeSenum.DynamicSerDeSenum(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeSenum.DynamicSerDeSenum(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeSenumDef.DynamicSerDeSenumDef(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeSenumDef.DynamicSerDeSenumDef(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeSenumDefList.DynamicSerDeSenumDefList(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeSenumDefList.DynamicSerDeSenumDefList(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeService.DynamicSerDeService(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeService.DynamicSerDeService(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeSimpleNode.DynamicSerDeSimpleNode(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeSimpleNode.DynamicSerDeSimpleNode(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStart.DynamicSerDeStart(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStart.DynamicSerDeStart(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStruct.DynamicSerDeStruct(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStruct.DynamicSerDeStruct(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStruct.getFieldList()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStruct.getType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStruct.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStructBase.DynamicSerDeStructBase(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStructBase.DynamicSerDeStructBase(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStructBase.deserialize(Object,TProtocol)",1,3,3
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStructBase.getRealType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStructBase.initialize()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStructBase.isPrimitive()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStructBase.serialize(Object,ObjectInspector,TProtocol)",1,3,3
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeThrows.DynamicSerDeThrows(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeThrows.DynamicSerDeThrows(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBase.DynamicSerDeTypeBase(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBase.DynamicSerDeTypeBase(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBase.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBase.getRealType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBase.getType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBase.initialize()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBase.isList()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBase.isMap()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBase.isPrimitive()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBase.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBool.DynamicSerDeTypeBool(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBool.DynamicSerDeTypeBool(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBool.deserialize(Object,TProtocol)",2,3,4
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBool.getRealType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBool.getRealTypeInstance()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBool.getType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBool.serialize(Object,ObjectInspector,TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeBool.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeByte.DynamicSerDeTypeByte(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeByte.DynamicSerDeTypeByte(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeByte.deserialize(Object,TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeByte.deserialize(TProtocol)",2,3,4
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeByte.getType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeByte.serialize(Object,ObjectInspector,TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeByte.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeDefinition.DynamicSerDeTypeDefinition(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeDefinition.DynamicSerDeTypeDefinition(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeDouble.DynamicSerDeTypeDouble(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeDouble.DynamicSerDeTypeDouble(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeDouble.deserialize(Object,TProtocol)",2,3,4
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeDouble.getRealType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeDouble.getRealTypeInstance()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeDouble.getType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeDouble.serialize(Object,ObjectInspector,TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeDouble.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeList.DynamicSerDeTypeList(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeList.DynamicSerDeTypeList(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeList.deserialize(Object,TProtocol)",2,5,6
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeList.getElementType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeList.getRealType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeList.getType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeList.isList()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeList.isPrimitive()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeList.serialize(Object,ObjectInspector,TProtocol)",1,6,8
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeList.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeMap.DynamicSerDeTypeMap(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeMap.DynamicSerDeTypeMap(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeMap.deserialize(Object,TProtocol)",2,3,4
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeMap.getKeyType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeMap.getRealType()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeMap.getType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeMap.getValueType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeMap.isMap()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeMap.isPrimitive()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeMap.serialize(Object,ObjectInspector,TProtocol)",1,3,4
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeMap.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeSet.DynamicSerDeTypeSet(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeSet.DynamicSerDeTypeSet(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeSet.deserialize(Object,TProtocol)",2,3,4
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeSet.getElementType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeSet.getRealType()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeSet.getType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeSet.serialize(Object,ObjectInspector,TProtocol)",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeSet.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeString.DynamicSerDeTypeString(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeString.DynamicSerDeTypeString(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeString.deserialize(Object,TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeString.deserialize(TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeString.getRealType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeString.getType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeString.serialize(Object,ObjectInspector,TProtocol)",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeString.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypedef.DynamicSerDeTypedef(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypedef.DynamicSerDeTypedef(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypedef.deserialize(Object,TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypedef.getDefinitionType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypedef.getMyType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypedef.getType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypedef.serialize(Object,ObjectInspector,TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypedef.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei16.DynamicSerDeTypei16(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei16.DynamicSerDeTypei16(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei16.deserialize(Object,TProtocol)",2,3,4
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei16.getRealType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei16.getType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei16.serialize(Object,ObjectInspector,TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei16.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei32.DynamicSerDeTypei32(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei32.DynamicSerDeTypei32(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei32.deserialize(Object,TProtocol)",2,3,4
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei32.getRealType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei32.getRealTypeInstance()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei32.getType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei32.serialize(Object,ObjectInspector,TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei32.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei64.DynamicSerDeTypei64(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei64.DynamicSerDeTypei64(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei64.deserialize(Object,TProtocol)",2,3,4
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei64.getRealType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei64.getType()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei64.serialize(Object,ObjectInspector,TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypei64.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeUnflagArgs.DynamicSerDeUnflagArgs(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeUnflagArgs.DynamicSerDeUnflagArgs(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeXception.DynamicSerDeXception(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeXception.DynamicSerDeXception(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.JJTthrift_grammarState.JJTthrift_grammarState()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.JJTthrift_grammarState.clearNodeScope(Node)",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.JJTthrift_grammarState.closeNodeScope(Node,boolean)",1,3,3
"org.apache.hadoop.hive.serde2.dynamic_type.JJTthrift_grammarState.closeNodeScope(Node,int)",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.JJTthrift_grammarState.nodeArity()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.JJTthrift_grammarState.nodeCreated()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.JJTthrift_grammarState.openNodeScope(Node)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.JJTthrift_grammarState.peekNode()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.JJTthrift_grammarState.popNode()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.JJTthrift_grammarState.pushNode(Node)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.JJTthrift_grammarState.reset()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.JJTthrift_grammarState.rootNode()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.ParseException.ParseException()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.ParseException.ParseException(String)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.ParseException.ParseException(Token,int[][],String[])",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.ParseException.add_escapes(String)",4,5,14
"org.apache.hadoop.hive.serde2.dynamic_type.ParseException.getMessage()",4,6,10
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.BeginToken()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.Done()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.ExpandBuff(boolean)",1,2,3
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.FillBuff()",2,9,10
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.GetImage()",2,1,2
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.GetSuffix(int)",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.ReInit(InputStream)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.ReInit(InputStream,String)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.ReInit(InputStream,String,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.ReInit(InputStream,String,int,int,int)",1,1,2
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.ReInit(InputStream,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.ReInit(InputStream,int,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.ReInit(Reader)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.ReInit(Reader,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.ReInit(Reader,int,int,int)",1,1,3
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.SimpleCharStream(InputStream)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.SimpleCharStream(InputStream,String)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.SimpleCharStream(InputStream,String,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.SimpleCharStream(InputStream,String,int,int,int)",1,1,2
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.SimpleCharStream(InputStream,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.SimpleCharStream(InputStream,int,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.SimpleCharStream(Reader)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.SimpleCharStream(Reader,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.SimpleCharStream(Reader,int,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.UpdateLineColumn(char)",2,2,8
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.adjustBeginLineColumn(int,int)",1,1,7
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.backup(int)",1,1,2
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.getBeginColumn()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.getBeginLine()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.getColumn()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.getEndColumn()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.getEndLine()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.getLine()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.getTabSize(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.readChar()",2,2,4
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleCharStream.setTabSize(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleNode.SimpleNode(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleNode.SimpleNode(thrift_grammar,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleNode.dump(String)",1,4,4
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleNode.jjtAddChild(Node,int)",1,3,3
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleNode.jjtClose()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleNode.jjtGetChild(int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleNode.jjtGetNumChildren()",1,1,2
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleNode.jjtGetParent()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleNode.jjtOpen()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleNode.jjtSetParent(Node)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleNode.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.SimpleNode.toString(String)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.compare(Object,Object)",11,5,12
"org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.hexString(BytesWritable)",1,2,3
"org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.makeHashMap(String...)",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.sort(Object[])",1,3,4
"org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.testConfigurableTCTLSeparated()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.testDynamicSerDe()",1,6,6
"org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.testNulls1()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.testNulls2()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.testNulls3()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.testNulls4()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.testSkip()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.testStructsinStructs()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.testTBinarySortableProtocol()",1,13,13
"org.apache.hadoop.hive.serde2.dynamic_type.TestDynamicSerDe.testTBinarySortableProtocol(Object[],String,boolean)",1,6,12
"org.apache.hadoop.hive.serde2.dynamic_type.Token.newToken(int)",2,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.Token.toString()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.TokenMgrError.LexicalError(boolean,int,int,int,String,char)",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.TokenMgrError.TokenMgrError()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.TokenMgrError.TokenMgrError(String,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.TokenMgrError.TokenMgrError(boolean,int,int,int,String,char,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.TokenMgrError.addEscapes(String)",4,5,14
"org.apache.hadoop.hive.serde2.dynamic_type.TokenMgrError.getMessage()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Async()",2,4,5
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.CommaOrSemicolon()",2,4,6
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Const()",8,6,13
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.ConstList()",7,4,10
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.ConstListContents()",10,9,17
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.ConstMap()",7,4,10
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.ConstMapContents()",11,11,20
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.ConstValue()",8,6,18
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Definition()",10,6,17
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.DefinitionType()",16,6,29
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Enum()",7,4,10
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.EnumDef()",9,8,16
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.EnumDefList()",9,7,14
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Extends()",4,4,7
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Field()",9,9,17
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.FieldList()",9,7,14
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.FieldRequiredness()",6,4,11
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.FieldType()",19,7,33
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.FieldValue()",9,6,14
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.FlagArgs()",2,2,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Function()",8,6,13
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.FunctionType()",7,6,13
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Header()",9,6,15
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.HeaderList()",9,7,14
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Include()",4,4,7
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Namespace()",15,4,30
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.ReInit(InputStream)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.ReInit(InputStream,String)",1,1,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.ReInit(Reader)",1,1,2
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.ReInit(thrift_grammarTokenManager)",1,1,2
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Senum()",7,4,10
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.SenumDef()",8,6,13
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.SenumDefList()",9,7,14
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Service()",9,7,14
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Start()",10,9,17
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Struct()",7,4,10
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Throws()",7,6,12
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.TypeBool()",2,2,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.TypeByte()",2,2,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.TypeDefinition()",12,6,21
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.TypeDouble()",2,2,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.TypeList()",7,4,10
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.TypeMap()",7,4,10
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.TypeSet()",7,4,10
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.TypeString()",2,2,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Typedef()",7,4,10
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Typei16()",2,2,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Typei32()",2,2,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Typei64()",2,2,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.UnflagArgs()",2,2,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.Xception()",7,4,10
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.disable_tracing()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.enable_tracing()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.findFile(String,List<String>)",3,2,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.generateParseException()",1,4,11
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.getNextToken()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.getToken(int)",1,3,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.jj_consume_token(int)",2,2,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.jj_la1_init_0()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.jj_la1_init_1()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.jj_la1_init_2()",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.jj_ntk()",2,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.main(String[])",1,8,10
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.thrift_grammar(InputStream)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.thrift_grammar(InputStream,List<String>,boolean)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.thrift_grammar(InputStream,String)",1,1,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.thrift_grammar(Reader)",1,1,2
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammar.thrift_grammar(thrift_grammarTokenManager)",1,1,2
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.ReInit(SimpleCharStream)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.ReInit(SimpleCharStream,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.ReInitRounds()",1,1,2
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.SwitchTo(int)",2,1,3
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.getNextToken()",4,11,15
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjAddStates(int,int)",1,1,2
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjCheckNAdd(int)",1,1,2
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjCheckNAddStates(int,int)",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjCheckNAddTwoStates(int,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjFillToken()",1,2,2
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveNfa_0(int,int)",6,49,153
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa0_0()",31,31,31
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa10_0(long,long)",11,12,12
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa11_0(long,long)",12,13,16
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa12_0(long,long)",10,11,11
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa13_0(long,long)",7,8,8
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa14_0(long,long)",6,7,7
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa15_0(long,long)",6,7,9
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa16_0(long,long)",4,5,5
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa17_0(long,long)",4,5,6
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa1_0(long)",20,21,21
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa2_0(long,long)",26,27,30
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa3_0(long,long)",22,23,26
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa4_0(long,long)",21,22,23
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa5_0(long,long)",19,20,22
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa6_0(long,long)",18,19,21
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa7_0(long,long)",14,15,16
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa8_0(long,long)",13,14,15
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjMoveStringLiteralDfa9_0(long,long)",12,13,13
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjStartNfaWithStates_0(int,int,int)",1,1,2
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjStartNfa_0(int,long,long)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjStopAtPos(int,int)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.jjStopStringLiteralDfa_0(int,long,long)",48,2,48
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.setDebugStream(PrintStream)",1,1,1
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.thrift_grammarTokenManager(SimpleCharStream)",2,1,2
"org.apache.hadoop.hive.serde2.dynamic_type.thrift_grammarTokenManager.thrift_grammarTokenManager(SimpleCharStream,int)",1,1,1
"org.apache.hadoop.hive.serde2.io.ByteWritable.ByteWritable()",1,1,1
"org.apache.hadoop.hive.serde2.io.ByteWritable.ByteWritable(byte)",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.DateWritable()",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.DateWritable(Date)",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.DateWritable(DateWritable)",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.DateWritable(int)",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.compareTo(DateWritable)",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.dateToDays(Date)",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.daysToMillis(int)",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.equals(Object)",2,1,2
"org.apache.hadoop.hive.serde2.io.DateWritable.get()",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.getDays()",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.getTimeInSeconds()",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.set(Date)",2,1,2
"org.apache.hadoop.hive.serde2.io.DateWritable.set(DateWritable)",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.set(int)",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.setFromBytes(byte[],int,int,VInt)",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.timeToDate(long)",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.toString()",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.serde2.io.DateWritable.writeToByteStream(RandomAccessOutput)",1,1,1
"org.apache.hadoop.hive.serde2.io.DoubleWritable.DoubleWritable()",1,1,1
"org.apache.hadoop.hive.serde2.io.DoubleWritable.DoubleWritable(double)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveBaseCharWritable.HiveBaseCharWritable()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveBaseCharWritable.equals(Object)",2,2,3
"org.apache.hadoop.hive.serde2.io.HiveBaseCharWritable.getCharacterLength()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveBaseCharWritable.getTextValue()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveBaseCharWritable.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveBaseCharWritable.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveBaseCharWritable.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.HiveCharWritable()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.HiveCharWritable(HiveChar)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.HiveCharWritable(HiveCharWritable)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.compareTo(HiveCharWritable)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.enforceMaxLength(int)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.equals(Object)",3,2,4
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.getCharacterLength()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.getHiveChar()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.getPaddedValue()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.getStrippedValue()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.set(HiveChar)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.set(HiveChar,int)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.set(HiveCharWritable)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.set(HiveCharWritable,int)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.set(String)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.set(String,int)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveCharWritable.toString()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.HiveDecimalWritable()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.HiveDecimalWritable(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.HiveDecimalWritable(HiveDecimalWritable)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.HiveDecimalWritable(byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.compareTo(HiveDecimalWritable)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.equals(Object)",2,1,3
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.getHiveDecimal()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.getHiveDecimal(int,int)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.getInternalStorage()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.getScale()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.readFields(DataInput)",1,1,2
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.set(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.set(HiveDecimalWritable)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.set(byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.setFromBytes(byte[],int,int)",1,1,2
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.toString()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.writeToByteStream(Decimal128,Output)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.writeToByteStream(RandomAccessOutput)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.HiveVarcharWritable()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.HiveVarcharWritable(HiveVarchar)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.HiveVarcharWritable(HiveVarcharWritable)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.compareTo(HiveVarcharWritable)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.enforceMaxLength(int)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.getHiveVarchar()",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.set(HiveVarchar)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.set(HiveVarchar,int)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.set(HiveVarcharWritable)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.set(HiveVarcharWritable,int)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.set(String)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.set(String,int)",1,1,1
"org.apache.hadoop.hive.serde2.io.HiveVarcharWritable.toString()",1,1,1
"org.apache.hadoop.hive.serde2.io.ShortWritable.Comparator.Comparator()",1,1,1
"org.apache.hadoop.hive.serde2.io.ShortWritable.Comparator.compare(byte[],int,int,byte[],int,int)",1,1,1
"org.apache.hadoop.hive.serde2.io.ShortWritable.ShortWritable()",1,1,1
"org.apache.hadoop.hive.serde2.io.ShortWritable.ShortWritable(short)",1,1,1
"org.apache.hadoop.hive.serde2.io.ShortWritable.compareTo(Object)",1,1,1
"org.apache.hadoop.hive.serde2.io.ShortWritable.equals(Object)",2,2,3
"org.apache.hadoop.hive.serde2.io.ShortWritable.get()",1,1,1
"org.apache.hadoop.hive.serde2.io.ShortWritable.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.io.ShortWritable.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.serde2.io.ShortWritable.set(short)",1,1,1
"org.apache.hadoop.hive.serde2.io.ShortWritable.toString()",1,1,1
"org.apache.hadoop.hive.serde2.io.ShortWritable.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.serde2.io.TestHiveCharWritable.testComparison()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestHiveCharWritable.testConstructor()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestHiveCharWritable.testEnforceMaxLength()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestHiveCharWritable.testGetCharacterLength()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestHiveCharWritable.testGetHiveChar()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestHiveCharWritable.testSet()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestHiveDecimalWritable.doTestDecimalWithBoundsCheck(Decimal128)",2,2,3
"org.apache.hadoop.hive.serde2.io.TestHiveDecimalWritable.doTestFastStreamForHiveDecimal(String)",1,1,1
"org.apache.hadoop.hive.serde2.io.TestHiveDecimalWritable.setUp()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestHiveDecimalWritable.testFastStreamForHiveDecimal()",1,3,3
"org.apache.hadoop.hive.serde2.io.TestHiveDecimalWritable.testHive6594()",1,2,2
"org.apache.hadoop.hive.serde2.io.TestHiveVarcharWritable.testComparison()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestHiveVarcharWritable.testEnforceLength()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestHiveVarcharWritable.testStringLength()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestHiveVarcharWritable.testStringValue()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.assertTSWEquals(TimestampWritable,TimestampWritable)",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.checkTimestampWithAndWithoutNanos(Timestamp,int)",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.compareEqualLengthByteArrays(byte[],byte[])",3,1,3
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.deserializeFromBytes(byte[])",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.fromIntAndVInts(int,long...)",1,3,3
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.getSeconds(Timestamp)",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.normalizeComparisonResult(int)",1,1,3
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.normalizeTimestampStr(String)",2,2,2
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.padBytes(byte[],int)",1,1,2
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.parseToMillis(String)",1,1,2
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.randomMillis(long,long,Random)",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.randomNanos(Random)",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.randomNanos(Random,int)",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.reverseNanos(int)",3,2,6
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.serializeDeserializeAndCheckTimestamp(Timestamp)",1,2,4
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.serializeToBytes(Writable)",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.setUp()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.testBinarySortable()",4,4,5
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.testDecimalToTimestampCornerCases()",1,2,2
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.testDecimalToTimestampRandomly()",1,2,2
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.testMaxSize()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.testMillisToSeconds()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.testReverseNanos()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.testSerializationFormatDirectly()",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.testTimestampsInFullRange()",1,2,2
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.testTimestampsOutsidePositiveIntRange()",1,2,2
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.testTimestampsWithinPositiveIntRange()",1,2,2
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.testToFromDouble()",1,3,3
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.timestampToDecimal(Timestamp)",1,1,1
"org.apache.hadoop.hive.serde2.io.TestTimestampWritable.toList(byte[])",1,2,2
"org.apache.hadoop.hive.serde2.io.TimestampWritable.TimestampWritable()",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.TimestampWritable(Timestamp)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.TimestampWritable(TimestampWritable)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.TimestampWritable(byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.bytesToInt(byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.checkBytes()",1,2,2
"org.apache.hadoop.hive.serde2.io.TimestampWritable.clearTimestamp()",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.compareTo(TimestampWritable)",3,2,4
"org.apache.hadoop.hive.serde2.io.TimestampWritable.convertTimestampToBytes(Timestamp,byte[],int)",1,2,5
"org.apache.hadoop.hive.serde2.io.TimestampWritable.createTimestamp(byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.decimalToTimestamp(HiveDecimal)",1,1,2
"org.apache.hadoop.hive.serde2.io.TimestampWritable.doubleToTimestamp(double)",1,1,2
"org.apache.hadoop.hive.serde2.io.TimestampWritable.equals(Object)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.floatToTimestamp(float)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.getBinarySortable()",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.getBytes()",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.getDouble()",1,2,2
"org.apache.hadoop.hive.serde2.io.TimestampWritable.getNanos()",3,4,4
"org.apache.hadoop.hive.serde2.io.TimestampWritable.getNanos(byte[],int)",1,2,4
"org.apache.hadoop.hive.serde2.io.TimestampWritable.getSeconds()",3,3,3
"org.apache.hadoop.hive.serde2.io.TimestampWritable.getSeconds(byte[],int)",2,2,3
"org.apache.hadoop.hive.serde2.io.TimestampWritable.getTimestamp()",1,2,2
"org.apache.hadoop.hive.serde2.io.TimestampWritable.getTotalLength()",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.getTotalLength(byte[],int)",1,3,3
"org.apache.hadoop.hive.serde2.io.TimestampWritable.hasDecimal()",1,2,2
"org.apache.hadoop.hive.serde2.io.TimestampWritable.hasDecimalOrSecondVInt()",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.hasDecimalOrSecondVInt(byte)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.hasSecondVInt(byte)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.intToBytes(int,byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.millisToSeconds(long)",2,1,2
"org.apache.hadoop.hive.serde2.io.TimestampWritable.populateTimestamp()",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.readFields(DataInput)",3,5,7
"org.apache.hadoop.hive.serde2.io.TimestampWritable.readSevenByteLong(byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.set(Timestamp)",2,2,2
"org.apache.hadoop.hive.serde2.io.TimestampWritable.set(TimestampWritable)",2,3,3
"org.apache.hadoop.hive.serde2.io.TimestampWritable.set(byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.setBinarySortable(byte[],int)",1,2,5
"org.apache.hadoop.hive.serde2.io.TimestampWritable.setNanosBytes(int,byte[],int,boolean)",1,2,6
"org.apache.hadoop.hive.serde2.io.TimestampWritable.setTimestamp(Timestamp,byte[],int)",1,4,4
"org.apache.hadoop.hive.serde2.io.TimestampWritable.sevenByteLongToBytes(long,byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.toString()",4,5,5
"org.apache.hadoop.hive.serde2.io.TimestampWritable.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.write(OutputStream)",1,1,1
"org.apache.hadoop.hive.serde2.io.TimestampWritable.writeToByteStream(RandomAccessOutput)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.ByteArrayRef.getData()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.ByteArrayRef.setData(byte[])",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyArray.LazyArray(LazyListObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyArray.enlargeArrays()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyArray.getList()",3,3,5
"org.apache.hadoop.hive.serde2.lazy.LazyArray.getListElementObject(int)",2,2,4
"org.apache.hadoop.hive.serde2.lazy.LazyArray.getListLength()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyArray.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyArray.parse()",2,5,11
"org.apache.hadoop.hive.serde2.lazy.LazyArray.uncheckedGetElement(int)",3,4,5
"org.apache.hadoop.hive.serde2.lazy.LazyBinary.LazyBinary(LazyBinary)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyBinary.LazyBinary(LazyBinaryObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyBinary.init(ByteArrayRef,int,int)",1,3,3
"org.apache.hadoop.hive.serde2.lazy.LazyBoolean.LazyBoolean(LazyBoolean)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyBoolean.LazyBoolean(LazyBooleanObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyBoolean.init(ByteArrayRef,int,int)",1,16,20
"org.apache.hadoop.hive.serde2.lazy.LazyByte.LazyByte(LazyByte)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyByte.LazyByte(LazyByteObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyByte.init(ByteArrayRef,int,int)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyByte.parseByte(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyByte.parseByte(byte[],int,int,int)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.LazyDate.LazyDate(LazyDate)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyDate.LazyDate(LazyDateObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyDate.init(ByteArrayRef,int,int)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyDate.writeUTF8(OutputStream,DateWritable)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyDouble.LazyDouble(LazyDouble)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyDouble.LazyDouble(LazyDoubleObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyDouble.init(ByteArrayRef,int,int)",1,3,3
"org.apache.hadoop.hive.serde2.lazy.LazyFactory.LazyFactory()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyFactory.createColumnarStructInspector(List<String>,List<TypeInfo>,byte[],Text,boolean,byte)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObject(ObjectInspector)",6,3,6
"org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObject(ObjectInspector,boolean)",2,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(TypeInfo,byte[],int,Text,boolean,byte)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyObjectInspector(TypeInfo,byte[],int,Text,boolean,byte,boolean)",5,7,8
"org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyPrimitiveBinaryClass(PrimitiveObjectInspector)",9,2,9
"org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyPrimitiveClass(PrimitiveObjectInspector)",17,2,17
"org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyPrimitiveClass(PrimitiveObjectInspector,boolean)",2,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyStructInspector(List<String>,List<TypeInfo>,byte[],Text,boolean,boolean,byte)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyFactory.createLazyStructInspector(List<String>,List<TypeInfo>,byte[],Text,boolean,boolean,byte,boolean)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyFloat.LazyFloat(LazyFloat)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyFloat.LazyFloat(LazyFloatObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyFloat.init(ByteArrayRef,int,int)",1,3,3
"org.apache.hadoop.hive.serde2.lazy.LazyHiveChar.LazyHiveChar(LazyHiveChar)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyHiveChar.LazyHiveChar(LazyHiveCharObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyHiveChar.init(ByteArrayRef,int,int)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyHiveChar.setValue(LazyHiveChar)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyHiveDecimal.LazyHiveDecimal(LazyHiveDecimal)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyHiveDecimal.LazyHiveDecimal(LazyHiveDecimalObjectInspector)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.LazyHiveDecimal.enforcePrecisionScale(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyHiveDecimal.getWritableObject()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyHiveDecimal.init(ByteArrayRef,int,int)",1,3,3
"org.apache.hadoop.hive.serde2.lazy.LazyHiveDecimal.writeUTF8(OutputStream,HiveDecimal)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyHiveVarchar.LazyHiveVarchar(LazyHiveVarchar)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyHiveVarchar.LazyHiveVarchar(LazyHiveVarcharObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyHiveVarchar.init(ByteArrayRef,int,int)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyHiveVarchar.setValue(LazyHiveVarchar)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyInteger.LazyInteger(LazyIntObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyInteger.LazyInteger(LazyInteger)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyInteger.init(ByteArrayRef,int,int)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyInteger.parse(byte[],int,int,int,int,boolean)",10,9,10
"org.apache.hadoop.hive.serde2.lazy.LazyInteger.parseInt(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyInteger.parseInt(byte[],int,int,int)",6,3,8
"org.apache.hadoop.hive.serde2.lazy.LazyInteger.writeUTF8(OutputStream,int)",2,4,5
"org.apache.hadoop.hive.serde2.lazy.LazyInteger.writeUTF8NoException(OutputStream,int)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.LazyLong.LazyLong(LazyLong)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyLong.LazyLong(LazyLongObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyLong.init(ByteArrayRef,int,int)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyLong.parse(byte[],int,int,int,int,boolean)",9,8,10
"org.apache.hadoop.hive.serde2.lazy.LazyLong.parseLong(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyLong.parseLong(byte[],int,int,int)",6,3,8
"org.apache.hadoop.hive.serde2.lazy.LazyLong.writeUTF8(OutputStream,long)",2,4,5
"org.apache.hadoop.hive.serde2.lazy.LazyLong.writeUTF8NoException(OutputStream,long)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.LazyMap.LazyMap(LazyMapObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyMap.enlargeArrays()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyMap.getMap()",4,6,8
"org.apache.hadoop.hive.serde2.lazy.LazyMap.getMapSize()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyMap.getMapValueElement(Object)",5,5,7
"org.apache.hadoop.hive.serde2.lazy.LazyMap.getParsed()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyMap.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyMap.parse()",5,6,16
"org.apache.hadoop.hive.serde2.lazy.LazyMap.setParsed(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyMap.uncheckedGetKey(int)",3,3,5
"org.apache.hadoop.hive.serde2.lazy.LazyMap.uncheckedGetValue(int)",3,3,5
"org.apache.hadoop.hive.serde2.lazy.LazyNonPrimitive.LazyNonPrimitive(OI)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyNonPrimitive.getObject()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyNonPrimitive.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyNonPrimitive.init(ByteArrayRef,int,int)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.LazyObject.LazyObject(OI)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyObject.getInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyObject.setInspector(OI)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.LazyPrimitive(LazyPrimitive<OI, T>)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.LazyPrimitive(OI)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.equals(Object)",4,1,5
"org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.getObject()",1,1,2
"org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.getWritableObject()",1,1,2
"org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.hashCode()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.logExceptionMessage(ByteArrayRef,int,int,String)",1,3,3
"org.apache.hadoop.hive.serde2.lazy.LazyPrimitive.toString()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyShort.LazyShort(LazyShort)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyShort.LazyShort(LazyShortObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyShort.init(ByteArrayRef,int,int)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyShort.parseShort(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyShort.parseShort(byte[],int,int,int)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.LazySimpleSerDe()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.SerDeParameters.getColumnNames()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.SerDeParameters.getColumnTypes()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.SerDeParameters.getEscapeChar()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.SerDeParameters.getNeedsEscape()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.SerDeParameters.getNullSequence()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.SerDeParameters.getNullString()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.SerDeParameters.getRowTypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.SerDeParameters.getSeparators()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.SerDeParameters.isEscaped()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.SerDeParameters.isLastColumnTakesRest()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.doDeserialize(Writable)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.doSerialize(Object,ObjectInspector)",4,9,9
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getByte(String,byte)",2,4,4
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getSerDeStats()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.initSerdeParams(Configuration,Properties,String)",4,7,13
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.initialize(Configuration,Properties)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(Output,Object,ObjectInspector,byte[],int,Text,boolean,byte,boolean[])",3,13,18
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(Output,Object,ObjectInspector,SerDeParameters)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.toString()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.transformFromUTF8(Writable)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.transformToUTF8(Writable)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyString.LazyString(LazyString)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyString.LazyString(LazyStringObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyString.init(ByteArrayRef,int,int)",1,3,8
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.LazyStruct(LazySimpleStructObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.createLazyField(int,StructField)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.getField(int)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.getFieldInited()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.getFields()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.getFieldsAsList()",1,4,4
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.getObject()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.getParsed()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.getRawDataSerializedSize()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.initLazyFields(List<? extends StructField>)",2,2,3
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.parse()",4,4,17
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.setFieldInited(boolean[])",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.setFields(LazyObject[])",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.setParsed(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyStruct.uncheckedGetField(int)",2,4,5
"org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.LazyTimestamp(LazyTimestamp)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.LazyTimestamp(LazyTimestampObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.getWritableObject()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.init(ByteArrayRef,int,int)",1,4,4
"org.apache.hadoop.hive.serde2.lazy.LazyTimestamp.writeUTF8(OutputStream,TimestampWritable)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyUnion.LazyUnion(LazyUnionObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyUnion.getField()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyUnion.getTag()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.LazyUnion.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyUnion.parse()",1,1,8
"org.apache.hadoop.hive.serde2.lazy.LazyUnion.uncheckedGetField()",2,4,5
"org.apache.hadoop.hive.serde2.lazy.LazyUtils.LazyUtils()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyUtils.compare(byte[],int,int,byte[],int,int)",6,1,6
"org.apache.hadoop.hive.serde2.lazy.LazyUtils.convertToString(byte[],int,int)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.LazyUtils.createByteArray(BytesWritable)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyUtils.digit(int,int)",1,1,8
"org.apache.hadoop.hive.serde2.lazy.LazyUtils.extractColumnInfo(Properties,SerDeParameters,String)",2,7,7
"org.apache.hadoop.hive.serde2.lazy.LazyUtils.getSeparator(byte[],int)",1,1,3
"org.apache.hadoop.hive.serde2.lazy.LazyUtils.hashBytes(byte[],int,int)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.LazyUtils.writeEscaped(OutputStream,byte[],int,int,boolean,byte,boolean[])",1,6,8
"org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitive(OutputStream,Object,PrimitiveObjectInspector)",2,2,9
"org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(OutputStream,Object,PrimitiveObjectInspector,boolean,byte,boolean[])",2,17,17
"org.apache.hadoop.hive.serde2.lazy.LazyVoid.LazyVoid(LazyPrimitive<LazyVoidObjectInspector, NullWritable>)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyVoid.LazyVoid(LazyVoidObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.LazyVoid.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.checkExtendedLimitExceeded(int,Category)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.checkNotExtendedLimitExceeded(int,Category)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyArray()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyArrayNested()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyArrayNestedExceedLimit()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyArrayNestedExceedLimitNotExtended()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyMap()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyMapNested()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyMapNestedExceedLimit()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyMapNestedExceedLimitNotExtended()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyMapWithDuplicateKeys()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyStruct()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyStructNested()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyStructNestedExceedLimit()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyStructNestedExceedLimitNotExtended()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyUnion()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyUnionNested()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyUnionNestedExceedLimit()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testLazyUnionNestedExceedLimitNotExtended()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testNestedinArrayAtLevel(int,Category,Properties)",4,6,18
"org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.testNestedinArrayAtLevelExtended(int,Category)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.initLazyObject(LazyObject,byte[],int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.testLazyBinary()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.testLazyByte()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.testLazyDate()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.testLazyDouble()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.testLazyInteger()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.testLazyIntegerWrite()",1,3,3
"org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.testLazyLong()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.testLazyLongWrite()",1,3,3
"org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.testLazyShort()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.testLazyString()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.testLazyTimestamp()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.createProperties()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.deserializeAndSerialize(LazySimpleSerDe,Text,String,Object[])",1,3,3
"org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.testLazySimpleSerDe()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.testLazySimpleSerDeExtraColumns()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.testLazySimpleSerDeLastColumnTakesRest()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.testLazySimpleSerDeMissingColumns()",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.LazyListObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.LazyListObjectInspector(ObjectInspector,byte,Text,boolean,byte)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.getEscapeChar()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.getList(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.getListElement(Object,int)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.getListElementObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.getListLength(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.getNullSequence()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.getSeparator()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector.isEscaped()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.LazyMapObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.LazyMapObjectInspector(ObjectInspector,ObjectInspector,byte,byte,Text,boolean,byte)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.getEscapeChar()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.getItemSeparator()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.getKeyValueSeparator()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.getMap(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.getMapKeyObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.getMapSize(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.getMapValueElement(Object,Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.getMapValueObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.getNullSequence()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector.isEscaped()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory.LazyObjectInspectorFactory()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory.getLazySimpleListObjectInspector(ObjectInspector,byte,Text,boolean,byte)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory.getLazySimpleMapObjectInspector(ObjectInspector,ObjectInspector,byte,byte,Text,boolean,byte)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory.getLazySimpleStructObjectInspector(List<String>,List<ObjectInspector>,List<String>,byte,Text,boolean,boolean,byte)",1,3,3
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory.getLazySimpleStructObjectInspector(List<String>,List<ObjectInspector>,byte,Text,boolean,boolean,byte)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory.getLazyUnionObjectInspector(List<ObjectInspector>,byte,Text,boolean,byte)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.LazySimpleStructObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.LazySimpleStructObjectInspector(List<String>,List<ObjectInspector>,List<String>,byte,Text,boolean,boolean,byte)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.LazySimpleStructObjectInspector(List<String>,List<ObjectInspector>,byte,Text,boolean,boolean,byte)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.LazySimpleStructObjectInspector(List<StructField>,byte,Text)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.getEscapeChar()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.getLastColumnTakesRest()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.getNullSequence()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.getSeparator()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.getStructFieldData(Object,StructField)",2,2,3
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.getStructFieldsDataAsList(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.init(List<String>,List<ObjectInspector>,List<String>,byte,Text,boolean,boolean,byte)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.isEscaped()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.LazyUnionObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.LazyUnionObjectInspector(List<ObjectInspector>,byte,Text)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.LazyUnionObjectInspector(List<ObjectInspector>,byte,Text,boolean,byte)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.getEscapeChar()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.getField(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.getNullSequence()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.getObjectInspectors()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.getSeparator()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.getTag(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.init(List<ObjectInspector>,byte,Text)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.init(List<ObjectInspector>,byte,Text,boolean,byte)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector.isEscaped()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.AbstractPrimitiveLazyObjectInspector.AbstractPrimitiveLazyObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.AbstractPrimitiveLazyObjectInspector.AbstractPrimitiveLazyObjectInspector(PrimitiveTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.AbstractPrimitiveLazyObjectInspector.getPrimitiveWritableObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.AbstractPrimitiveLazyObjectInspector.preferWritable()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBinaryObjectInspector.LazyBinaryObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBinaryObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBinaryObjectInspector.getPrimitiveJavaObject(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBinaryObjectInspector.getPrimitiveWritableObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBooleanObjectInspector.LazyBooleanObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBooleanObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBooleanObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBooleanObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBooleanObjectInspector.isExtendedLiteral()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBooleanObjectInspector.setExtendedLiteral(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyByteObjectInspector.LazyByteObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyByteObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyByteObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyByteObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDateObjectInspector.LazyDateObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDateObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDateObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDoubleObjectInspector.LazyDoubleObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDoubleObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDoubleObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDoubleObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyFloatObjectInspector.LazyFloatObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyFloatObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyFloatObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyFloatObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveCharObjectInspector.LazyHiveCharObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveCharObjectInspector.LazyHiveCharObjectInspector(CharTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveCharObjectInspector.copyObject(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveCharObjectInspector.getPrimitiveJavaObject(Object)",3,2,3
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveCharObjectInspector.toString()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveDecimalObjectInspector.LazyHiveDecimalObjectInspector(DecimalTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveDecimalObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveDecimalObjectInspector.getPrimitiveJavaObject(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveVarcharObjectInspector.LazyHiveVarcharObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveVarcharObjectInspector.LazyHiveVarcharObjectInspector(VarcharTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveVarcharObjectInspector.copyObject(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveVarcharObjectInspector.getPrimitiveJavaObject(Object)",3,2,3
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveVarcharObjectInspector.toString()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector.LazyIntObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyLongObjectInspector.LazyLongObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyLongObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyLongObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyLongObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory.LazyPrimitiveObjectInspectorFactory()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory.getLazyBooleanObjectInspector(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory.getLazyObjectInspector(PrimitiveTypeInfo)",3,2,6
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory.getLazyObjectInspector(PrimitiveTypeInfo,boolean,byte)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory.getLazyObjectInspector(PrimitiveTypeInfo,boolean,byte,boolean)",4,4,4
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory.getLazyStringObjectInspector(boolean,byte)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyShortObjectInspector.LazyShortObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyShortObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyShortObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyShortObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.LazyStringObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.LazyStringObjectInspector(boolean,byte)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.getEscapeChar()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.getPrimitiveWritableObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.isEscaped()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyTimestampObjectInspector.LazyTimestampObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyTimestampObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyTimestampObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyVoidObjectInspector.LazyVoidObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyVoidObjectInspector.copyObject(Object)",1,1,1
"org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyVoidObjectInspector.getPrimitiveJavaObject(Object)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryArray.LazyBinaryArray(LazyBinaryListObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryArray.adjustArraySize(int)",1,1,3
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryArray.getList()",1,4,4
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryArray.getListElementObject(int)",2,2,4
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryArray.getListLength()",1,2,2
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryArray.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryArray.parse()",2,3,5
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryArray.uncheckedGetElement(int)",2,4,4
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryBinary.LazyBinaryBinary(LazyBinaryPrimitive<WritableBinaryObjectInspector, BytesWritable>)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryBinary.LazyBinaryBinary(WritableBinaryObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryBinary.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryBoolean.LazyBinaryBoolean(LazyBinaryBoolean)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryBoolean.LazyBinaryBoolean(WritableBooleanObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryBoolean.init(ByteArrayRef,int,int)",1,3,3
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryByte.LazyBinaryByte(LazyBinaryByte)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryByte.LazyBinaryByte(WritableByteObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryByte.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryDate.LazyBinaryDate(LazyBinaryDate)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryDate.LazyBinaryDate(WritableDateObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryDate.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryDouble.LazyBinaryDouble(LazyBinaryDouble)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryDouble.LazyBinaryDouble(WritableDoubleObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryDouble.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFactory.LazyBinaryFactory()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFactory.createColumnarStructInspector(List<String>,List<TypeInfo>)",1,2,2
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFactory.createLazyBinaryObject(ObjectInspector)",5,3,5
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFactory.createLazyBinaryPrimitiveClass(PrimitiveObjectInspector)",17,2,17
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFloat.LazyBinaryFloat(LazyBinaryFloat)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFloat.LazyBinaryFloat(WritableFloatObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryFloat.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveChar.LazyBinaryHiveChar(LazyBinaryHiveChar)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveChar.LazyBinaryHiveChar(WritableHiveCharObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveChar.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveDecimal.LazyBinaryHiveDecimal(LazyBinaryHiveDecimal)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveDecimal.LazyBinaryHiveDecimal(WritableHiveDecimalObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveDecimal.init(ByteArrayRef,int,int)",1,1,2
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveVarchar.LazyBinaryHiveVarchar(LazyBinaryHiveVarchar)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveVarchar.LazyBinaryHiveVarchar(WritableHiveVarcharObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryHiveVarchar.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryInteger.LazyBinaryInteger(LazyBinaryInteger)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryInteger.LazyBinaryInteger(WritableIntObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryInteger.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryLong.LazyBinaryLong(LazyBinaryLong)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryLong.LazyBinaryLong(WritableLongObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryLong.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.LazyBinaryMap(LazyBinaryMapObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.adjustArraySize(int)",1,1,3
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.getMap()",3,7,8
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.getMapSize()",1,2,2
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.getMapValueElement(Object)",5,5,7
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.parse()",2,5,7
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.uncheckedGetKey(int)",2,3,4
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.uncheckedGetValue(int)",2,3,4
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryNonPrimitive.LazyBinaryNonPrimitive(OI)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryNonPrimitive.getObject()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryNonPrimitive.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryNonPrimitive.init(ByteArrayRef,int,int)",3,1,3
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryObject.LazyBinaryObject(OI)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryPrimitive.LazyBinaryPrimitive(LazyBinaryPrimitive<OI, T>)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryPrimitive.LazyBinaryPrimitive(OI)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryPrimitive.equals(Object)",4,1,5
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryPrimitive.getObject()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryPrimitive.getWritableObject()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryPrimitive.hashCode()",1,2,2
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryPrimitive.toString()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.BooleanRef.BooleanRef(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.LazyBinarySerDe()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.StringWrapper.set(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.deserialize(Writable)",2,1,3
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.getSerDeStats()",1,2,2
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.getSerializedClass()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.initialize(Configuration,Properties)",1,3,3
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.serialize(Object,ObjectInspector)",2,2,2
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.serialize(RandomAccessOutput,Object,ObjectInspector,boolean,BooleanRef)",26,37,46
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.serializeStruct(RandomAccessOutput,Object,StructObjectInspector,BooleanRef)",2,2,3
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.serializeStruct(RandomAccessOutput,Object[],List<ObjectInspector>)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.serializeStruct(RandomAccessOutput,Object[],List<ObjectInspector>,BooleanRef)",1,4,6
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.serializeText(RandomAccessOutput,Text,boolean)",1,2,2
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.writeSizeAtOffset(RandomAccessOutput,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryShort.LazyBinaryShort(LazyBinaryShort)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryShort.LazyBinaryShort(WritableShortObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryShort.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryString.LazyBinaryString(LazyBinaryString)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryString.LazyBinaryString(WritableStringObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryString.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.LazyBinaryStruct(LazyBinaryStructObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.SingleFieldGetter.SingleFieldGetter(LazyBinaryStructObjectInspector,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.SingleFieldGetter.getShort()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.SingleFieldGetter.init(BinaryComparable)",1,3,5
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.getField(int)",1,2,2
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.getFieldsAsList()",1,5,5
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.getObject()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.getRawDataSerializedSize()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.parse()",1,8,13
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.uncheckedGetField(int)",2,2,3
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryTimestamp.LazyBinaryTimestamp(LazyBinaryTimestamp)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryTimestamp.LazyBinaryTimestamp(WritableTimestampObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryTimestamp.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.LazyBinaryUtils()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.RecordInfo.RecordInfo()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.RecordInfo.toString()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.VInt.VInt()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.VLong.VLong()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.byteArrayToInt(byte[],int)",1,1,2
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.byteArrayToLong(byte[],int)",1,1,2
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.byteArrayToShort(byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.checkObjectByteInfo(ObjectInspector,byte[],int,RecordInfo,VInt)",3,3,18
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.getLazyBinaryObjectInspectorFromTypeInfo(TypeInfo)",2,8,8
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.readVInt(byte[],int,VInt)",2,1,4
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.readVLong(byte[],int,VLong)",2,1,4
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.readVLongFromByteArray(byte[],int)",2,1,4
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.writeDouble(RandomAccessOutput,double)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.writeVInt(RandomAccessOutput,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.writeVLong(RandomAccessOutput,long)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.writeVLongToByteArray(byte[],int,long)",2,1,7
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.writeVLongToByteArray(byte[],long)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryVoid.LazyBinaryVoid(LazyBinaryVoid)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryVoid.LazyBinaryVoid(WritableVoidObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryVoid.init(ByteArrayRef,int,int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.MyTestClassBigger.MyTestClassBigger()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.MyTestClassBigger.MyTestClassBigger(Byte,Short,Integer,Long,Float,Double,String,HiveDecimal,Date,MyTestInnerStruct,List<Integer>,byte[],Map<String, List<MyTestInnerStruct>>)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.MyTestClassSmaller.MyTestClassSmaller()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.MyTestClassSmaller.MyTestClassSmaller(Byte,Short,Integer,Long,Float,Double,String,HiveDecimal,Date,MyTestInnerStruct)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.compareDiffSizedStructs(Object,ObjectInspector,Object,ObjectInspector)",3,2,3
"org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.getInputBytesWritable()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.getRandStructArray(Random)",1,2,2
"org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.getSerDe(String,String)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.testJavaBinaryObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.testLazyBinaryMap(Random)",7,10,10
"org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.testLazyBinaryObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.testLazyBinarySerDe()",1,14,14
"org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.testLazyBinarySerDe(Object[],ObjectInspector,SerDe)",1,4,4
"org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.testLongerSchemaDeserialization(Random)",1,14,14
"org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.testLongerSchemaDeserialization1(Random)",1,13,13
"org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.testShorterSchemaDeserialization(Random)",1,15,15
"org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.testShorterSchemaDeserialization1(Random)",1,14,14
"org.apache.hadoop.hive.serde2.lazybinary.TestLazyBinarySerDe.testWritableBinaryObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryListObjectInspector.LazyBinaryListObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryListObjectInspector.LazyBinaryListObjectInspector(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryListObjectInspector.getList(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryListObjectInspector.getListElement(Object,int)",2,1,2
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryListObjectInspector.getListLength(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.LazyBinaryMapObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.LazyBinaryMapObjectInspector(ObjectInspector,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.getMap(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.getMapSize(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.getMapValueElement(Object,Object)",2,1,3
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryObjectInspectorFactory.LazyBinaryObjectInspectorFactory()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryObjectInspectorFactory.getLazyBinaryListObjectInspector(ObjectInspector)",1,2,2
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryObjectInspectorFactory.getLazyBinaryMapObjectInspector(ObjectInspector,ObjectInspector)",1,2,2
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryObjectInspectorFactory.getLazyBinaryStructObjectInspector(List<String>,List<ObjectInspector>)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryObjectInspectorFactory.getLazyBinaryStructObjectInspector(List<String>,List<ObjectInspector>,List<String>)",1,3,3
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.LazyBinaryStructObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.LazyBinaryStructObjectInspector(List<String>,List<ObjectInspector>)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.LazyBinaryStructObjectInspector(List<String>,List<ObjectInspector>,List<String>)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.LazyBinaryStructObjectInspector(List<StructField>)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.getStructFieldData(Object,StructField)",2,2,3
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.getStructFieldRef(int)",1,1,1
"org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.getStructFieldsDataAsList(Object)",2,1,2
"org.apache.hadoop.hive.serde2.lazydio.LazyDioBoolean.LazyDioBoolean(LazyBooleanObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazydio.LazyDioBoolean.LazyDioBoolean(LazyDioBoolean)",1,1,1
"org.apache.hadoop.hive.serde2.lazydio.LazyDioBoolean.init(ByteArrayRef,int,int)",1,1,4
"org.apache.hadoop.hive.serde2.lazydio.LazyDioByte.LazyDioByte(LazyByteObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazydio.LazyDioByte.LazyDioByte(LazyDioByte)",1,1,1
"org.apache.hadoop.hive.serde2.lazydio.LazyDioByte.init(ByteArrayRef,int,int)",1,1,4
"org.apache.hadoop.hive.serde2.lazydio.LazyDioDouble.LazyDioDouble(LazyDioDouble)",1,1,1
"org.apache.hadoop.hive.serde2.lazydio.LazyDioDouble.LazyDioDouble(LazyDoubleObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazydio.LazyDioDouble.init(ByteArrayRef,int,int)",1,1,4
"org.apache.hadoop.hive.serde2.lazydio.LazyDioFloat.LazyDioFloat(LazyDioFloat)",1,1,1
"org.apache.hadoop.hive.serde2.lazydio.LazyDioFloat.LazyDioFloat(LazyFloatObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazydio.LazyDioFloat.init(ByteArrayRef,int,int)",1,1,4
"org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger.LazyDioInteger(LazyDioInteger)",1,1,1
"org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger.LazyDioInteger(LazyIntObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger.init(ByteArrayRef,int,int)",1,1,4
"org.apache.hadoop.hive.serde2.lazydio.LazyDioLong.LazyDioLong(LazyDioLong)",1,1,1
"org.apache.hadoop.hive.serde2.lazydio.LazyDioLong.LazyDioLong(LazyLongObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazydio.LazyDioLong.init(ByteArrayRef,int,int)",1,1,4
"org.apache.hadoop.hive.serde2.lazydio.LazyDioShort.LazyDioShort(LazyDioShort)",1,1,1
"org.apache.hadoop.hive.serde2.lazydio.LazyDioShort.LazyDioShort(LazyShortObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.lazydio.LazyDioShort.init(ByteArrayRef,int,int)",1,1,4
"org.apache.hadoop.hive.serde2.objectinspector.ColumnarStructObjectInspector.ColumnarStructObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ColumnarStructObjectInspector.ColumnarStructObjectInspector(List<String>,List<ObjectInspector>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ColumnarStructObjectInspector.ColumnarStructObjectInspector(List<String>,List<ObjectInspector>,List<String>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ColumnarStructObjectInspector.getStructFieldData(Object,StructField)",2,2,3
"org.apache.hadoop.hive.serde2.objectinspector.ColumnarStructObjectInspector.getStructFieldsDataAsList(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.CrossMapEqualComparer.compare(Object,MapObjectInspector,Object,MapObjectInspector)",7,3,7
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedListObjectInspector.DelegatedListObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedListObjectInspector.DelegatedListObjectInspector(ListObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedListObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedListObjectInspector.getList(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedListObjectInspector.getListElement(Object,int)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedListObjectInspector.getListElementObjectInspector()",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedListObjectInspector.getListLength(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedListObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedListObjectInspector.reset(ListObjectInspector)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedMapObjectInspector.DelegatedMapObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedMapObjectInspector.DelegatedMapObjectInspector(MapObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedMapObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedMapObjectInspector.getMap(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedMapObjectInspector.getMapKeyObjectInspector()",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedMapObjectInspector.getMapSize(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedMapObjectInspector.getMapValueElement(Object,Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedMapObjectInspector.getMapValueObjectInspector()",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedMapObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedMapObjectInspector.reset(MapObjectInspector)",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedObjectInspectorFactory.reset(ObjectInspector,ObjectInspector)",2,2,7
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedObjectInspectorFactory.wrap(ObjectInspector)",7,2,7
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.DelegatedStructField.DelegatedStructField(StructField)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.DelegatedStructField.getFieldComment()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.DelegatedStructField.getFieldID()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.DelegatedStructField.getFieldName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.DelegatedStructField.getFieldObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.DelegatedStructObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.DelegatedStructObjectInspector(StructObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.getAllStructFieldRefs()",2,3,4
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.getStructFieldData(Object,StructField)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.getStructFieldRef(String)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.getStructFieldsDataAsList(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.reset(StructObjectInspector)",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedUnionObjectInspector.DelegatedUnionObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedUnionObjectInspector.DelegatedUnionObjectInspector(UnionObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedUnionObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedUnionObjectInspector.getField(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedUnionObjectInspector.getObjectInspectors()",2,3,4
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedUnionObjectInspector.getTag(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedUnionObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.DelegatedUnionObjectInspector.reset(UnionObjectInspector)",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.FullMapEqualComparer.MapKeyComparator.MapKeyComparator(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.FullMapEqualComparer.MapKeyComparator.compare(Object,Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.FullMapEqualComparer.compare(Object,MapObjectInspector,Object,MapObjectInspector)",5,2,5
"org.apache.hadoop.hive.serde2.objectinspector.InspectableObject.InspectableObject()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.InspectableObject.InspectableObject(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.FieldComparer.FieldComparer(ObjectInspector,ObjectInspector)",1,11,13
"org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.FieldComparer.areEqual(Object,Object)",10,8,12
"org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.ListObjectsEqualComparer(ObjectInspector[],ObjectInspector[])",2,1,3
"org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.areEqual(Object[],Object[])",7,4,8
"org.apache.hadoop.hive.serde2.objectinspector.MetadataListStructObjectInspector.MetadataListStructObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.MetadataListStructObjectInspector.MetadataListStructObjectInspector(List<String>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.MetadataListStructObjectInspector.MetadataListStructObjectInspector(List<String>,List<String>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.MetadataListStructObjectInspector.getFieldObjectInspectors(int)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.MetadataListStructObjectInspector.getInstance(List<String>)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.MetadataListStructObjectInspector.getInstance(List<String>,List<String>)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.MetadataListStructObjectInspector.getStructFieldData(Object,StructField)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.MetadataListStructObjectInspector.getStructFieldsDataAsList(Object)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.IdentityConverter.convert(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.ListConverter.ListConverter(ObjectInspector,SettableListObjectInspector)",3,3,3
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.ListConverter.convert(Object)",2,3,4
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.MapConverter.MapConverter(ObjectInspector,SettableMapObjectInspector)",3,3,3
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.MapConverter.convert(Object)",2,3,4
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.ObjectInspectorConverters()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.StructConverter.StructConverter(ObjectInspector,SettableStructObjectInspector)",3,4,4
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.StructConverter.convert(Object)",2,3,4
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.UnionConverter.UnionConverter(ObjectInspector,SettableUnionObjectInspector)",3,4,4
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.UnionConverter.convert(Object)",2,3,4
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI(ObjectInspector,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI(ObjectInspector,ObjectInspector,Map<ObjectInspector, Boolean>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI(ObjectInspector,ObjectInspector,Map<ObjectInspector, Boolean>,boolean)",3,6,12
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspector,ObjectInspector)",8,3,8
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(PrimitiveObjectInspector,PrimitiveObjectInspector)",18,2,18
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.ObjectInspectorFactory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getColumnarStructObjectInspector(List<String>,List<ObjectInspector>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getColumnarStructObjectInspector(List<String>,List<ObjectInspector>,List<String>)",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspector(Type,ObjectInspectorOptions)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspectorNoCache(Type,ObjectInspectorOptions)",11,13,16
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardConstantListObjectInspector(ObjectInspector,List<?>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardConstantMapObjectInspector(ObjectInspector,ObjectInspector,Map<?, ?>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardListObjectInspector(ObjectInspector)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardMapObjectInspector(ObjectInspector,ObjectInspector)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(List<String>,List<ObjectInspector>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(List<String>,List<ObjectInspector>,List<String>)",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardUnionObjectInspector(List<ObjectInspector>)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getUnionStructObjectInspector(List<StructObjectInspector>)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.verifyObjectInspector(ObjectInspectorOptions,ObjectInspector,ObjectInspectorOptions,Class[])",4,4,4
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorUtils()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compare(Object,ObjectInspector,Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compare(Object,ObjectInspector,Object,ObjectInspector,MapEqualComparer)",34,33,48
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compare(Object[],ObjectInspector[],Object[],ObjectInspector[])",3,2,3
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compareSupported(ObjectInspector)",8,4,11
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compareTypes(ObjectInspector,ObjectInspector)",18,11,21
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyStructToArray(Object,ObjectInspector,ObjectInspectorCopyOption,Object[],int)",3,3,4
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardJavaObject(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(List<Object>,Object,StructObjectInspector,ObjectInspectorCopyOption)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(Object,ObjectInspector,ObjectInspectorCopyOption)",4,14,15
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getConstantObjectInspector(ObjectInspector,Object)",2,2,5
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getDeclaredNonStaticFields(Class<?>)",1,4,4
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getFieldNames(StructObjectInspector)",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getFieldTypes(StructObjectInspector)",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getObjectInspectorName(ObjectInspector)",7,11,11
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardObjectInspector(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardObjectInspector(ObjectInspector,ObjectInspectorCopyOption)",3,13,13
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(String,List<? extends StructField>)",4,5,6
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructTypeName(StructObjectInspector)",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardUnionTypeName(UnionObjectInspector)",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStructSize(ObjectInspector)",2,2,2
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getTypeNameFromJavaClass(Type)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getWritableConstantValue(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getWritableObjectInspector(ObjectInspector)",3,3,3
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspector,Map<ObjectInspector, Boolean>)",8,8,15
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hashCode(Object,ObjectInspector)",21,24,29
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.isConstantObjectInspector(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.isInstanceOfSettableOI(ObjectInspector)",7,3,7
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.isInstanceOfSettablePrimitiveOI(PrimitiveObjectInspector)",16,2,17
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.partialCopyToStandardObject(List<Object>,Object,int,int,StructObjectInspector,ObjectInspectorCopyOption)",4,3,4
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.setOISettablePropertiesMap(ObjectInspector,Map<ObjectInspector, Boolean>,boolean)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.supportsConstantObjectInspector(ObjectInspector)",3,2,3
"org.apache.hadoop.hive.serde2.objectinspector.ProtocolBuffersStructObjectInspector.shouldIgnoreField(String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.MyField.MyField()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.MyField.MyField(int,Field,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.MyField.getFieldComment()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.MyField.getFieldID()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.MyField.getFieldName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.MyField.getFieldObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.MyField.toString()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.ReflectionStructObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.create()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.getAllStructFieldRefs()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.getStructFieldData(Object,StructField)",3,2,4
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.getStructFieldRef(String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.getStructFieldsDataAsList(Object)",2,2,4
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.getTypeName()",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.init(Class<?>,List<ObjectInspector>)",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.setStructFieldData(Object,StructField,Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.ReflectionStructObjectInspector.shouldIgnoreField(String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.SettableStructObjectInspector.isSettable()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.SimpleMapEqualComparer.compare(Object,MapObjectInspector,Object,MapObjectInspector)",4,2,4
"org.apache.hadoop.hive.serde2.objectinspector.StandardConstantListObjectInspector.StandardConstantListObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardConstantListObjectInspector.StandardConstantListObjectInspector(ObjectInspector,List<?>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardConstantListObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardConstantMapObjectInspector.StandardConstantMapObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardConstantMapObjectInspector.StandardConstantMapObjectInspector(ObjectInspector,ObjectInspector,Map<?, ?>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardConstantMapObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.StandardListObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.StandardListObjectInspector(ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.create(int)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.getList(Object)",2,2,3
"org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.getListElement(Object,int)",5,3,7
"org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.getListElementObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.getListLength(Object)",3,2,3
"org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.resize(Object,int)",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.set(Object,int,Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.StandardMapObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.StandardMapObjectInspector(ObjectInspector,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.clear(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.create()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.getMap(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.getMapKeyObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.getMapSize(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.getMapValueElement(Object,Object)",2,1,3
"org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.getMapValueObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.put(Object,Object,Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.remove(Object,Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.MyField.MyField()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.MyField.MyField(int,String,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.MyField.MyField(int,String,ObjectInspector,String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.MyField.getFieldComment()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.MyField.getFieldID()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.MyField.getFieldName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.MyField.getFieldObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.MyField.toString()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.StandardStructObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.StandardStructObjectInspector(List<String>,List<ObjectInspector>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.StandardStructObjectInspector(List<String>,List<ObjectInspector>,List<String>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.StandardStructObjectInspector(List<StructField>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.create()",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getAllStructFieldRefs()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldData(Object,StructField)",5,7,10
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldsDataAsList(Object)",2,2,3
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.init(List<String>,List<ObjectInspector>,List<String>)",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.init(List<StructField>)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.setStructFieldData(Object,StructField,Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.StandardUnion.StandardUnion()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.StandardUnion.StandardUnion(byte,Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.StandardUnion.getObject()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.StandardUnion.getTag()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.StandardUnion.setObject(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.StandardUnion.setTag(byte)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.StandardUnion.toString()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.StandardUnionObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.StandardUnionObjectInspector(List<ObjectInspector>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.addField(Object,ObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.create()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.getField(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.getObjectInspectors()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.getTag(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.toString()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.isSettable()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector.toString()",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.SubStructObjectInspector.SubStructObjectInspector(StructObjectInspector,int,int)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.SubStructObjectInspector.getAllStructFieldRefs()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.SubStructObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.SubStructObjectInspector.getStructFieldData(Object,StructField)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.SubStructObjectInspector.getStructFieldRef(String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.SubStructObjectInspector.getStructFieldsDataAsList(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.SubStructObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestCrossMapEqualComparer.IntegerStringMapHolder.IntegerStringMapHolder()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestCrossMapEqualComparer.StringTextMapHolder.StringTextMapHolder()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestCrossMapEqualComparer.TextStringMapHolder.TextStringMapHolder()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestCrossMapEqualComparer.serializeAndDeserialize(StringTextMapHolder,StructObjectInspector,LazySimpleSerDe,SerDeParameters)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestCrossMapEqualComparer.serializeAndDeserialize(TextStringMapHolder,StructObjectInspector,LazySimpleSerDe,SerDeParameters)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestCrossMapEqualComparer.testCompatibleType()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestCrossMapEqualComparer.testIncompatibleType()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestCrossMapEqualComparer.testSameType()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestFullMapEqualComparer.IntegerIntegerMapHolder.IntegerIntegerMapHolder()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestFullMapEqualComparer.testAntiSymmetry()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestFullMapEqualComparer.testTransitivity()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorConverters.testGetConvertedOI()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorConverters.testObjectInspectorConverters()",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorUtils.testObjectInspectorUtils()",1,4,4
"org.apache.hadoop.hive.serde2.objectinspector.TestProtocolBuffersObjectInspectors.testProtocolBuffersObjectInspectors()",1,4,4
"org.apache.hadoop.hive.serde2.objectinspector.TestReflectionObjectInspectors.testReflectionObjectInspectors()",1,4,4
"org.apache.hadoop.hive.serde2.objectinspector.TestSimpleMapEqualComparer.IntegerStringMapHolder.IntegerStringMapHolder()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestSimpleMapEqualComparer.StringTextMapHolder.StringTextMapHolder()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestSimpleMapEqualComparer.TextStringMapHolder.TextStringMapHolder()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestSimpleMapEqualComparer.serializeAndDeserialize(StringTextMapHolder,StructObjectInspector,LazySimpleSerDe,SerDeParameters)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestSimpleMapEqualComparer.serializeAndDeserialize(TextStringMapHolder,StructObjectInspector,LazySimpleSerDe,SerDeParameters)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestSimpleMapEqualComparer.testCompatibleType()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestSimpleMapEqualComparer.testIncompatibleType()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestSimpleMapEqualComparer.testSameType()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.doStandardObjectInspectorTest(boolean)",1,8,8
"org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.doTestJavaPrimitiveObjectInspector(Class<?>,Class<?>,Object)",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.doTestStandardPrimitiveObjectInspector(Class<?>,Class<?>)",1,3,3
"org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.testJavaPrimitiveObjectInspector()",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.testStandardListObjectInspector()",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.testStandardMapObjectInspector()",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.testStandardPrimitiveObjectInspector()",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.testStandardStructObjectInspector()",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.testStandardUnionObjectInspector()",1,5,6
"org.apache.hadoop.hive.serde2.objectinspector.TestThriftObjectInspectors.testThriftObjectInspectors()",1,4,4
"org.apache.hadoop.hive.serde2.objectinspector.TestUnionStructObjectInspector.testUnionStructObjectInspector()",1,6,6
"org.apache.hadoop.hive.serde2.objectinspector.ThriftStructObjectInspector.shouldIgnoreField(String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.MyField.MyField()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.MyField.MyField(int,StructField)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.MyField.getFieldComment()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.MyField.getFieldID()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.MyField.getFieldName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.MyField.getFieldObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.UnionStructObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.UnionStructObjectInspector(List<StructObjectInspector>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getAllStructFieldRefs()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(Object,StructField)",2,2,3
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldRef(String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldsDataAsList(Object)",2,3,4
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.init(List<StructObjectInspector>)",1,4,4
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector.AbstractPrimitiveJavaObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector.AbstractPrimitiveJavaObjectInspector(PrimitiveTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector.copyObject(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector.getPrimitiveJavaObject(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector.preferWritable()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.AbstractPrimitiveObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.AbstractPrimitiveObjectInspector(PrimitiveTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.getJavaPrimitiveClass()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.getPrimitiveCategory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.getPrimitiveWritableClass()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.getTypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.precision()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveObjectInspector.scale()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveWritableObjectInspector.AbstractPrimitiveWritableObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveWritableObjectInspector.AbstractPrimitiveWritableObjectInspector(PrimitiveTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveWritableObjectInspector.getPrimitiveWritableObject(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveWritableObjectInspector.preferWritable()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.JavaBinaryObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.copyObject(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.create(BytesWritable)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.create(byte[])",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.getPrimitiveJavaObject(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.getPrimitiveWritableObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.set(Object,BytesWritable)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBinaryObjectInspector.set(Object,byte[])",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBooleanObjectInspector.JavaBooleanObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBooleanObjectInspector.create(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBooleanObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBooleanObjectInspector.getPrimitiveWritableObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaBooleanObjectInspector.set(Object,boolean)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaByteObjectInspector.JavaByteObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaByteObjectInspector.create(byte)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaByteObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaByteObjectInspector.getPrimitiveWritableObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaByteObjectInspector.set(Object,byte)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDateObjectInspector.JavaDateObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDateObjectInspector.create(Date)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDateObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDateObjectInspector.getPrimitiveJavaObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDateObjectInspector.getPrimitiveWritableObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDateObjectInspector.set(Object,Date)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDateObjectInspector.set(Object,DateWritable)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDoubleObjectInspector.JavaDoubleObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDoubleObjectInspector.create(double)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDoubleObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDoubleObjectInspector.getPrimitiveWritableObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDoubleObjectInspector.set(Object,double)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaFloatObjectInspector.JavaFloatObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaFloatObjectInspector.create(float)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaFloatObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaFloatObjectInspector.getPrimitiveWritableObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaFloatObjectInspector.set(Object,float)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.JavaHiveCharObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.JavaHiveCharObjectInspector(CharTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.create(HiveChar)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.getMaxLength()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.getPrimitiveJavaObject(Object)",3,1,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.getPrimitiveWithParams(HiveChar)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.getPrimitiveWritableObject(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.getWritableWithParams(HiveChar)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.set(Object,HiveChar)",2,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.set(Object,String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector.JavaHiveDecimalObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector.JavaHiveDecimalObjectInspector(DecimalTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector.create(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector.create(byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector.enforcePrecisionScale(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector.getPrimitiveJavaObject(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector.getPrimitiveWritableObject(Object)",3,2,5
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector.set(Object,HiveDecimal)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector.set(Object,HiveDecimalWritable)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveDecimalObjectInspector.set(Object,byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.JavaHiveVarcharObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.JavaHiveVarcharObjectInspector(VarcharTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.create(HiveVarchar)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.getMaxLength()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.getPrimitiveJavaObject(Object)",3,1,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.getPrimitiveWithParams(HiveVarchar)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.getPrimitiveWritableObject(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.getWritableWithParams(HiveVarchar)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.set(Object,HiveVarchar)",2,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.set(Object,String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector.JavaIntObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector.create(int)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector.getPrimitiveWritableObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector.set(Object,int)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaLongObjectInspector.JavaLongObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaLongObjectInspector.create(long)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaLongObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaLongObjectInspector.getPrimitiveWritableObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaLongObjectInspector.set(Object,long)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaShortObjectInspector.JavaShortObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaShortObjectInspector.create(short)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaShortObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaShortObjectInspector.getPrimitiveWritableObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaShortObjectInspector.set(Object,short)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector.JavaStringObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector.create(String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector.create(Text)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector.getPrimitiveWritableObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector.set(Object,String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector.set(Object,Text)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.JavaTimestampObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.create(Timestamp)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.create(byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.getPrimitiveJavaObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.getPrimitiveWritableObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.set(Object,Timestamp)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.set(Object,TimestampWritable)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaTimestampObjectInspector.set(Object,byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaVoidObjectInspector.JavaVoidObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaVoidObjectInspector.getPrimitiveWritableObject(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.BinaryConverter.BinaryConverter(PrimitiveObjectInspector,SettableBinaryObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.BinaryConverter.convert(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.BooleanConverter.BooleanConverter(PrimitiveObjectInspector,SettableBooleanObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.BooleanConverter.convert(Object)",2,1,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.ByteConverter.ByteConverter(PrimitiveObjectInspector,SettableByteObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.ByteConverter.convert(Object)",2,1,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.DateConverter.DateConverter(PrimitiveObjectInspector,SettableDateObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.DateConverter.convert(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.DoubleConverter.DoubleConverter(PrimitiveObjectInspector,SettableDoubleObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.DoubleConverter.convert(Object)",2,1,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.FloatConverter.FloatConverter(PrimitiveObjectInspector,SettableFloatObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.FloatConverter.convert(Object)",2,1,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.HiveCharConverter.HiveCharConverter(PrimitiveObjectInspector,SettableHiveCharObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.HiveCharConverter.convert(Object)",3,3,4
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.HiveDecimalConverter.HiveDecimalConverter(PrimitiveObjectInspector,SettableHiveDecimalObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.HiveDecimalConverter.convert(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.HiveVarcharConverter.HiveVarcharConverter(PrimitiveObjectInspector,SettableHiveVarcharObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.HiveVarcharConverter.convert(Object)",3,3,4
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.IntConverter.IntConverter(PrimitiveObjectInspector,SettableIntObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.IntConverter.convert(Object)",2,1,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.LongConverter.LongConverter(PrimitiveObjectInspector,SettableLongObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.LongConverter.convert(Object)",2,1,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.ShortConverter.ShortConverter(PrimitiveObjectInspector,SettableShortObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.ShortConverter.convert(Object)",2,1,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.StringConverter.StringConverter(PrimitiveObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.StringConverter.convert(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.TextConverter.TextConverter(PrimitiveObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.TextConverter.convert(Object)",4,6,23
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.TimestampConverter.TimestampConverter(PrimitiveObjectInspector,SettableTimestampObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.TimestampConverter.convert(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.PrimitiveObjectInspectorFactory()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(PrimitiveCategory)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(PrimitiveTypeInfo)",3,2,6
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.getPrimitiveObjectInspectorFromClass(Class<?>)",4,2,4
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.getPrimitiveWritableConstantObjectInspector(PrimitiveTypeInfo,Object)",17,2,17
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(PrimitiveCategory)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(PrimitiveTypeInfo)",3,2,6
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveObjectInspectorUtils()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveTypeEntry.PrimitiveTypeEntry()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveTypeEntry.PrimitiveTypeEntry(PrimitiveCategory,String,Class<?>,Class<?>,Class<?>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveTypeEntry.clone()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveTypeEntry.readFields(DataInput)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveTypeEntry.toString()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveTypeEntry.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.addParameterizedType(PrimitiveTypeEntry)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.comparePrimitiveObjects(Object,PrimitiveObjectInspector,Object,PrimitiveObjectInspector)",18,16,19
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.comparePrimitiveObjectsWithConversion(Object,PrimitiveObjectInspector,Object,PrimitiveObjectInspector)",3,2,5
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.convertPrimitiveToDouble(Object,PrimitiveObjectInspector)",12,12,13
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getBinary(Object,PrimitiveObjectInspector)",7,6,8
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getBinaryFromText(Text)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getBoolean(Object,PrimitiveObjectInspector)",2,3,14
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getByte(Object,PrimitiveObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getDate(Object,PrimitiveObjectInspector)",3,3,10
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getDouble(Object,PrimitiveObjectInspector)",2,2,15
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getFloat(Object,PrimitiveObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getHiveChar(Object,PrimitiveObjectInspector)",3,2,4
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getHiveDecimal(Object,PrimitiveObjectInspector)",3,2,16
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getHiveVarchar(Object,PrimitiveObjectInspector)",3,2,4
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getInt(Object,PrimitiveObjectInspector)",2,12,16
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getJavaPrimitiveClassFromObjectInspector(ObjectInspector)",2,1,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getLong(Object,PrimitiveObjectInspector)",2,4,16
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getPrimitiveGrouping(PrimitiveCategory)",8,2,8
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getShort(Object,PrimitiveObjectInspector)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getString(Object,PrimitiveObjectInspector)",3,2,19
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getTimestamp(Object,PrimitiveObjectInspector)",3,2,17
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getTimestampFromString(String)",1,3,4
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getTypeEntryFromPrimitiveCategory(PrimitiveCategory)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getTypeEntryFromPrimitiveJava(Class<?>)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getTypeEntryFromPrimitiveJavaClass(Class<?>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getTypeEntryFromPrimitiveJavaType(Class<?>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getTypeEntryFromPrimitiveWritableClass(Class<?>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getTypeEntryFromTypeName(String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getTypeNameFromPrimitiveJava(Class<?>)",1,2,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getTypeNameFromPrimitiveWritable(Class<?>)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.isPrimitiveJava(Class<?>)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.isPrimitiveJavaClass(Class<?>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.isPrimitiveJavaType(Class<?>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.isPrimitiveWritableClass(Class<?>)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.primitiveJavaTypeToClass(Class<?>)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.registerType(PrimitiveTypeEntry)",1,6,6
"org.apache.hadoop.hive.serde2.objectinspector.primitive.TestPrimitiveObjectInspectorFactory.testGetPrimitiveJavaObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.TestPrimitiveObjectInspectorFactory.testGetPrimitiveWritableObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.TestPrimitiveObjectInspectorUtils.testGetPrimitiveGrouping()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector.WritableBinaryObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector.copyObject(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector.create(BytesWritable)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector.create(byte[])",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector.getPrimitiveWritableObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector.set(Object,BytesWritable)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector.set(Object,byte[])",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector.WritableBooleanObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector.copyObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector.create(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector.set(Object,boolean)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector.WritableByteObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector.copyObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector.create(byte)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector.set(Object,byte)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBinaryObjectInspector.WritableConstantBinaryObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBinaryObjectInspector.WritableConstantBinaryObjectInspector(BytesWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBinaryObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBooleanObjectInspector.WritableConstantBooleanObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBooleanObjectInspector.WritableConstantBooleanObjectInspector(BooleanWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantBooleanObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantByteObjectInspector.WritableConstantByteObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantByteObjectInspector.WritableConstantByteObjectInspector(ByteWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantByteObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantByteObjectInspector.precision()",2,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantDateObjectInspector.WritableConstantDateObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantDateObjectInspector.WritableConstantDateObjectInspector(DateWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantDateObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantDoubleObjectInspector.WritableConstantDoubleObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantDoubleObjectInspector.WritableConstantDoubleObjectInspector(DoubleWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantDoubleObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantFloatObjectInspector.WritableConstantFloatObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantFloatObjectInspector.WritableConstantFloatObjectInspector(FloatWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantFloatObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveCharObjectInspector.WritableConstantHiveCharObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveCharObjectInspector.WritableConstantHiveCharObjectInspector(CharTypeInfo,HiveCharWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveCharObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.WritableConstantHiveDecimalObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.WritableConstantHiveDecimalObjectInspector(DecimalTypeInfo,HiveDecimalWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.getWritableConstantValue()",2,2,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.precision()",2,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.scale()",2,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveVarcharObjectInspector.WritableConstantHiveVarcharObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveVarcharObjectInspector.WritableConstantHiveVarcharObjectInspector(VarcharTypeInfo,HiveVarcharWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveVarcharObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantIntObjectInspector.WritableConstantIntObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantIntObjectInspector.WritableConstantIntObjectInspector(IntWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantIntObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantIntObjectInspector.precision()",2,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantLongObjectInspector.WritableConstantLongObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantLongObjectInspector.WritableConstantLongObjectInspector(LongWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantLongObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantLongObjectInspector.precision()",2,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantShortObjectInspector.WritableConstantShortObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantShortObjectInspector.WritableConstantShortObjectInspector(ShortWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantShortObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantShortObjectInspector.precision()",2,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantStringObjectInspector.WritableConstantStringObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantStringObjectInspector.WritableConstantStringObjectInspector(Text)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantStringObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantTimestampObjectInspector.WritableConstantTimestampObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantTimestampObjectInspector.WritableConstantTimestampObjectInspector(TimestampWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantTimestampObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDateObjectInspector.WritableDateObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDateObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDateObjectInspector.create(Date)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDateObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDateObjectInspector.getPrimitiveWritableObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDateObjectInspector.set(Object,Date)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDateObjectInspector.set(Object,DateWritable)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDoubleObjectInspector.WritableDoubleObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDoubleObjectInspector.copyObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDoubleObjectInspector.create(double)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDoubleObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDoubleObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableDoubleObjectInspector.set(Object,double)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableFloatObjectInspector.WritableFloatObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableFloatObjectInspector.copyObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableFloatObjectInspector.create(float)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableFloatObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableFloatObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableFloatObjectInspector.set(Object,float)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector.WritableHiveCharObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector.WritableHiveCharObjectInspector(CharTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector.copyObject(Object)",3,1,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector.create(HiveChar)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector.doesWritableMatchTypeParams(HiveCharWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector.getMaxLength()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector.getPrimitiveJavaObject(Object)",3,2,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector.getPrimitiveWithParams(HiveCharWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector.getPrimitiveWritableObject(Object)",3,1,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector.getWritableWithParams(HiveCharWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector.set(Object,HiveChar)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector.set(Object,String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.WritableHiveDecimalObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.WritableHiveDecimalObjectInspector(DecimalTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.create(HiveDecimal)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.create(byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.enforcePrecisionScale(HiveDecimal)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.enforcePrecisionScale(HiveDecimalWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.getPrimitiveJavaObject(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.getPrimitiveWritableObject(Object)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.set(Object,HiveDecimal)",2,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.set(Object,HiveDecimalWritable)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector.set(Object,byte[],int)",2,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.WritableHiveVarcharObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.WritableHiveVarcharObjectInspector(VarcharTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.copyObject(Object)",3,1,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.create(HiveVarchar)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.doesWritableMatchTypeParams(HiveVarcharWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.getMaxLength()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.getPrimitiveJavaObject(Object)",3,2,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.getPrimitiveWithParams(HiveVarcharWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.getPrimitiveWritableObject(Object)",3,1,3
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.getWritableWithParams(HiveVarcharWritable)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.set(Object,HiveVarchar)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.set(Object,String)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.WritableIntObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.copyObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.create(int)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.set(Object,int)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector.WritableLongObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector.copyObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector.create(long)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector.set(Object,long)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableShortObjectInspector.WritableShortObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableShortObjectInspector.copyObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableShortObjectInspector.create(short)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableShortObjectInspector.get(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableShortObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableShortObjectInspector.set(Object,short)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.WritableStringObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.create(String)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.create(Text)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.set(Object,String)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.set(Object,Text)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector.WritableTimestampObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector.copyObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector.create(Timestamp)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector.create(byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector.getPrimitiveJavaObject(Object)",1,2,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector.getPrimitiveWritableObject(Object)",1,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector.set(Object,Timestamp)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector.set(Object,TimestampWritable)",2,1,2
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableTimestampObjectInspector.set(Object,byte[],int)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableVoidObjectInspector.WritableVoidObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableVoidObjectInspector.copyObject(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableVoidObjectInspector.getPrimitiveJavaObject(Object)",1,1,1
"org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableVoidObjectInspector.getWritableConstantValue()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.Complex(int,String,List<Integer>,List<String>,List<IntString>)",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.getAString()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.getAint()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.getLString(int)",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.getLStringCount()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.getLStringList()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.getLint(int)",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.getLintCount()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.getLintList()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.getLintString(int)",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.getLintStringCount()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.getLintStringList()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.hasAString()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.hasAint()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.initFields()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complex.isInitialized()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.Complexpb()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.IntString.IntString(int,String,int)",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.IntString.getMyString()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.IntString.getMyint()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.IntString.getUnderscoreInt()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.IntString.hasMyString()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.IntString.hasMyint()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.IntString.hasUnderscoreInt()",1,1,1
"org.apache.hadoop.hive.serde2.proto.test.Complexpb.IntString.isInitialized()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.Factory.getProtocol(TTransport)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.TBinarySortableProtocol(TTransport)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.initialize(Configuration,Properties)",3,2,5
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.lastPrimitiveWasNull()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readBinary()",2,1,2
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readBool()",1,1,2
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readByte()",2,1,2
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readDouble()",2,1,3
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readFieldBegin()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readFieldEnd()",1,3,3
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readI16()",2,1,2
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readI32()",2,1,2
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readI64()",2,1,2
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readIsNull()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readListBegin()",2,2,3
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readListEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readMapBegin()",2,2,3
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readMapEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readMessageBegin()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readMessageEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readRawAll(byte[],int,int)",1,1,3
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readSetBegin()",2,2,3
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readSetEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readString()",4,4,8
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readStructBegin()",3,3,4
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.readStructEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeBinary(ByteBuffer)",2,3,3
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeBinary(byte[])",1,2,2
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeBinary(byte[],int,int)",1,2,2
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeBool(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeByte(byte)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeDouble(double)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeFieldBegin(TField)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeFieldEnd()",1,3,3
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeFieldStop()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeI16(short)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeI32(int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeI64(long)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeListBegin(TList)",1,2,2
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeListEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeMapBegin(TMap)",1,2,2
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeMapEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeMessageBegin(TMessage)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeMessageEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeNull()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeRawBytes(byte[],int,int)",1,2,5
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeSetBegin(TSet)",1,2,2
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeSetEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeString(String)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeStructBegin(TStruct)",1,3,3
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeStructEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeText(Text)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.writeTextBytes(byte[],int,int)",1,6,7
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.Factory.getProtocol(TTransport)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.SimpleTransportTokenizer.SimpleTransportTokenizer(TTransport,String,int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.SimpleTransportTokenizer.fillTokenizer()",3,2,5
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.SimpleTransportTokenizer.initialize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.SimpleTransportTokenizer.nextToken()",5,9,10
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.TCTLSeparatedProtocol(TTransport)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.TCTLSeparatedProtocol(TTransport,String,String,String,String,boolean,int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.TCTLSeparatedProtocol(TTransport,int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.complexSplit(String,Pattern)",3,6,7
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.getByteValue(String,String)",2,3,4
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.getMapSeparator()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.getPrimarySeparator()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.getRowSeparator()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.getSecondarySeparator()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.initialize(Configuration,Properties)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.internalInitialize()",1,2,3
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.internalWriteString(String)",1,2,2
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.lastPrimitiveWasNull()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readBinary()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readBool()",1,3,3
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readByte()",1,3,4
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readDouble()",1,3,4
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readFieldBegin()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readFieldEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readI16()",1,3,4
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readI32()",1,3,4
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readI64()",1,3,4
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readListBegin()",3,4,5
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readListEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readMapBegin()",3,4,5
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readMapEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readMessageBegin()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readMessageEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readSetBegin()",3,4,5
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readSetEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readString()",2,2,9
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readStructBegin()",1,2,3
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.readStructEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.skip(byte)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeBinary(ByteBuffer)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeBool(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeByte(byte)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeDouble(double)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeFieldBegin(TField)",1,2,2
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeFieldEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeFieldStop()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeI16(short)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeI32(int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeI64(long)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeListBegin(TList)",2,1,5
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeListEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeMapBegin(TMap)",3,1,9
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeMapEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeMessageBegin(TMessage)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeMessageEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeNull()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeSetBegin(TSet)",2,1,5
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeSetEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeString(String)",1,4,5
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeStructBegin(TStruct)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol.writeStructEnd()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TReflectionUtils.TReflectionUtils()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.TReflectionUtils.getProtocolFactoryByName(String)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.ThriftByteStreamTypedSerDe(Type,TProtocolFactory,TProtocolFactory)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.deserialize(Writable)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.getObjectInspectorOptions()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.init(TProtocolFactory,TProtocolFactory)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.ThriftByteStreamTypedSerDe.initialize(Configuration,Properties)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.ThriftDeserializer()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.deserialize(Writable)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.getObjectInspector()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.getSerDeStats()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer.initialize(Configuration,Properties)",1,1,3
"org.apache.hadoop.hive.serde2.thrift.test.Complex.Complex()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.Complex(Complex)",1,9,10
"org.apache.hadoop.hive.serde2.thrift.test.Complex.Complex(int,String,List<Integer>,List<String>,List<IntString>,Map<String, String>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.ComplexStandardScheme.read(TProtocol,Complex)",4,13,20
"org.apache.hadoop.hive.serde2.thrift.test.Complex.ComplexStandardScheme.write(TProtocol,Complex)",1,10,10
"org.apache.hadoop.hive.serde2.thrift.test.Complex.ComplexStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.ComplexTupleScheme.read(TProtocol,Complex)",1,11,11
"org.apache.hadoop.hive.serde2.thrift.test.Complex.ComplexTupleScheme.write(TProtocol,Complex)",1,17,17
"org.apache.hadoop.hive.serde2.thrift.test.Complex.ComplexTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex._Fields.findByThriftId(int)",8,2,8
"org.apache.hadoop.hive.serde2.thrift.test.Complex._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.addToLString(String)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.addToLint(int)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.addToLintString(IntString)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.clear()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.compareTo(Complex)",20,8,20
"org.apache.hadoop.hive.serde2.thrift.test.Complex.deepCopy()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.equals(Complex)",20,16,42
"org.apache.hadoop.hive.serde2.thrift.test.Complex.equals(Object)",3,2,3
"org.apache.hadoop.hive.serde2.thrift.test.Complex.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.getAString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.getAint()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.getFieldValue(_Fields)",7,7,7
"org.apache.hadoop.hive.serde2.thrift.test.Complex.getLString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.getLStringIterator()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.getLStringSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.getLint()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.getLintIterator()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.getLintSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.getLintString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.getLintStringIterator()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.getLintStringSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.getMStringString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.getMStringStringSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.hashCode()",1,12,12
"org.apache.hadoop.hive.serde2.thrift.test.Complex.isSet(_Fields)",8,7,8
"org.apache.hadoop.hive.serde2.thrift.test.Complex.isSetAString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.isSetAint()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.isSetLString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.isSetLint()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.isSetLintString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.isSetMStringString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.putToMStringString(String,String)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.setAString(String)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.setAStringIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.setAint(int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.setAintIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.setFieldValue(_Fields,Object)",2,8,13
"org.apache.hadoop.hive.serde2.thrift.test.Complex.setLString(List<String>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.setLStringIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.setLint(List<Integer>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.setLintIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.setLintString(List<IntString>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.setLintStringIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.setMStringString(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.setMStringStringIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.Complex.toString()",1,11,11
"org.apache.hadoop.hive.serde2.thrift.test.Complex.unsetAString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.unsetAint()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.unsetLString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.unsetLint()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.unsetLintString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.unsetMStringString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.validate()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.Complex.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.IntString.IntString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.IntString(IntString)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.IntString.IntString(int,String,int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.IntStringStandardScheme.read(TProtocol,IntString)",4,6,10
"org.apache.hadoop.hive.serde2.thrift.test.IntString.IntStringStandardScheme.write(TProtocol,IntString)",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.IntString.IntStringStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.IntStringTupleScheme.read(TProtocol,IntString)",1,4,4
"org.apache.hadoop.hive.serde2.thrift.test.IntString.IntStringTupleScheme.write(TProtocol,IntString)",1,7,7
"org.apache.hadoop.hive.serde2.thrift.test.IntString.IntStringTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.serde2.thrift.test.IntString._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.serde2.thrift.test.IntString._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.clear()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.compareTo(IntString)",11,5,11
"org.apache.hadoop.hive.serde2.thrift.test.IntString.deepCopy()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.equals(IntString)",11,4,19
"org.apache.hadoop.hive.serde2.thrift.test.IntString.equals(Object)",3,2,3
"org.apache.hadoop.hive.serde2.thrift.test.IntString.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.serde2.thrift.test.IntString.getMyString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.getMyint()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.getUnderscore_int()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.hashCode()",1,5,5
"org.apache.hadoop.hive.serde2.thrift.test.IntString.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.serde2.thrift.test.IntString.isSetMyString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.isSetMyint()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.isSetUnderscore_int()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.IntString.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.serde2.thrift.test.IntString.setMyString(String)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.setMyStringIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.IntString.setMyint(int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.setMyintIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.setUnderscore_int(int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.setUnderscore_intIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.toString()",1,4,4
"org.apache.hadoop.hive.serde2.thrift.test.IntString.unsetMyString()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.unsetMyint()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.unsetUnderscore_int()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.validate()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.IntString.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.MegaStruct()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.MegaStruct(MegaStruct)",1,28,29
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.MegaStructStandardScheme.read(TProtocol,MegaStruct)",4,37,58
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.MegaStructStandardScheme.write(TProtocol,MegaStruct)",1,49,49
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.MegaStructStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.MegaStructTupleScheme.read(TProtocol,MegaStruct)",1,35,35
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.MegaStructTupleScheme.write(TProtocol,MegaStruct)",1,55,55
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.MegaStructTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct._Fields.findByThriftId(int)",22,2,22
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.addToMy_enumlist(MyEnum)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.addToMy_enumset(MyEnum)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.addToMy_stringlist(String)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.addToMy_stringset(String)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.addToMy_structlist(MiniStruct)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.addToMy_structset(MiniStruct)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.bufferForMy_binary()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.clear()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.compareTo(MegaStruct)",62,22,62
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.deepCopy()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.equals(MegaStruct)",62,55,142
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.equals(Object)",3,2,3
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getFieldValue(_Fields)",21,21,21
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_16bit_int()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_32bit_int()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_64bit_int()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_binary()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_byte()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_double()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_enum_string_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_enum_string_mapSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_enum_stringlist_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_enum_stringlist_mapSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_enum_struct_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_enum_struct_mapSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_enum_structlist_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_enum_structlist_mapSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_enumlist()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_enumlistIterator()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_enumlistSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_enumset()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_enumsetIterator()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_enumsetSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_string()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_string_enum_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_string_enum_mapSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_string_string_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_string_string_mapSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_stringlist()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_stringlistIterator()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_stringlistSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_stringset()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_stringsetIterator()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_stringsetSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_structlist()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_structlistIterator()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_structlistSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_structset()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_structsetIterator()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.getMy_structsetSize()",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.hashCode()",1,41,41
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isMy_bool()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSet(_Fields)",22,21,22
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_16bit_int()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_32bit_int()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_64bit_int()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_binary()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_bool()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_byte()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_double()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_enum_string_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_enum_stringlist_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_enum_struct_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_enum_structlist_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_enumlist()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_enumset()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_string()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_string_enum_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_string_string_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_stringlist()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_stringset()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_structlist()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.isSetMy_structset()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.putToMy_enum_string_map(MyEnum,String)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.putToMy_enum_stringlist_map(MyEnum,List<String>)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.putToMy_enum_struct_map(MyEnum,MiniStruct)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.putToMy_enum_structlist_map(MyEnum,List<MiniStruct>)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.putToMy_string_enum_map(String,MyEnum)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.putToMy_string_string_map(String,String)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setFieldValue(_Fields,Object)",2,22,41
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_16bit_int(short)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_16bit_intIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_32bit_int(int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_32bit_intIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_64bit_int(long)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_64bit_intIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_binary(ByteBuffer)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_binary(byte[])",1,2,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_binaryIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_bool(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_boolIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_byte(byte)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_byteIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_double(double)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_doubleIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_enum_string_map(Map<MyEnum, String>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_enum_string_mapIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_enum_stringlist_map(Map<MyEnum, List<String>>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_enum_stringlist_mapIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_enum_struct_map(Map<MyEnum, MiniStruct>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_enum_struct_mapIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_enum_structlist_map(Map<MyEnum, List<MiniStruct>>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_enum_structlist_mapIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_enumlist(List<MyEnum>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_enumlistIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_enumset(Set<MyEnum>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_enumsetIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_string(String)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_stringIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_string_enum_map(Map<String, MyEnum>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_string_enum_mapIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_string_string_map(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_string_string_mapIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_stringlist(List<String>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_stringlistIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_stringset(Set<String>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_stringsetIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_structlist(List<MiniStruct>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_structlistIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_structset(Set<MiniStruct>)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.setMy_structsetIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.toString()",1,54,54
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_16bit_int()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_32bit_int()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_64bit_int()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_binary()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_bool()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_byte()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_double()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_enum_string_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_enum_stringlist_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_enum_struct_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_enum_structlist_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_enumlist()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_enumset()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_string()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_string_enum_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_string_string_map()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_stringlist()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_stringset()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_structlist()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.unsetMy_structset()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.validate()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MegaStruct.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.MiniStruct()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.MiniStruct(MiniStruct)",1,1,3
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.MiniStructStandardScheme.read(TProtocol,MiniStruct)",4,5,8
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.MiniStructStandardScheme.write(TProtocol,MiniStruct)",1,5,5
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.MiniStructStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.MiniStructTupleScheme.read(TProtocol,MiniStruct)",1,3,3
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.MiniStructTupleScheme.write(TProtocol,MiniStruct)",1,5,5
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.MiniStructTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.clear()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.compareTo(MiniStruct)",8,4,8
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.deepCopy()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.equals(MiniStruct)",8,7,16
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.equals(Object)",3,2,3
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.getMy_enum()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.getMy_string()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.hashCode()",1,5,5
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.isSetMy_enum()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.isSetMy_string()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.setMy_enum(MyEnum)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.setMy_enumIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.setMy_string(String)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.setMy_stringIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.toString()",1,6,6
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.unsetMy_enum()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.unsetMy_string()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.validate()",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MiniStruct.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.serde2.thrift.test.MyEnum.MyEnum(int)",1,1,1
"org.apache.hadoop.hive.serde2.thrift.test.MyEnum.findByValue(int)",4,2,4
"org.apache.hadoop.hive.serde2.thrift.test.MyEnum.getValue()",1,1,1
"org.apache.hadoop.hive.serde2.thrift_test.CreateSequenceFile.CreateSequenceFile()",1,1,1
"org.apache.hadoop.hive.serde2.thrift_test.CreateSequenceFile.ThriftSerializer.ThriftSerializer()",1,1,1
"org.apache.hadoop.hive.serde2.thrift_test.CreateSequenceFile.ThriftSerializer.serialize(TBase)",1,1,1
"org.apache.hadoop.hive.serde2.thrift_test.CreateSequenceFile.main(String[])",1,6,6
"org.apache.hadoop.hive.serde2.thrift_test.CreateSequenceFile.usage()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo.BaseCharTypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo.BaseCharTypeInfo(String,int)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo.getLength()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo.getQualifiedName()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo.getQualifiedName(String,int)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo.setLength(int)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.BaseCharUtils.doesPrimitiveMatchTypeParams(HiveBaseChar,BaseCharTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.BaseCharUtils.doesWritableMatchTypeParams(HiveBaseCharWritable,BaseCharTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.BaseCharUtils.validateCharParameter(int)",2,1,3
"org.apache.hadoop.hive.serde2.typeinfo.BaseCharUtils.validateVarcharParameter(int)",2,1,3
"org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo.CharTypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo.CharTypeInfo(int)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo.equals(Object)",2,2,4
"org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo.toString()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo.DecimalTypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo.DecimalTypeInfo(int,int)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo.accept(TypeInfo)",2,1,3
"org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo.equals(Object)",2,2,4
"org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo.getQualifiedName()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo.getQualifiedName(int,int)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo.precision()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo.scale()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo.toString()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils.enforcePrecisionScale(HiveDecimal,DecimalTypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils.enforcePrecisionScale(HiveDecimal,int,int)",4,2,5
"org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils.enforcePrecisionScale(HiveDecimalWritable,DecimalTypeInfo)",2,1,3
"org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils.enforcePrecisionScale(HiveDecimalWritable,int,int)",2,1,3
"org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils.getPrecisionForType(PrimitiveTypeInfo)",10,3,10
"org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils.getScaleForType(PrimitiveTypeInfo)",6,3,6
"org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils.validateParameter(int,int)",4,1,6
"org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.ListTypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.ListTypeInfo(TypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.equals(Object)",3,1,3
"org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.getListElementTypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.setListElementTypeInfo(TypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.MapTypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.MapTypeInfo(TypeInfo,TypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.equals(Object)",3,2,4
"org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getMapKeyTypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getMapValueTypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.setMapKeyTypeInfo(TypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.setMapValueTypeInfo(TypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.PrimitiveTypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.PrimitiveTypeInfo(String)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.equals(Object)",2,1,3
"org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getPrimitiveCategory()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getPrimitiveJavaClass()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getPrimitiveTypeEntry()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getPrimitiveWritableClass()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.setTypeName(String)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.toString()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.StructTypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.StructTypeInfo(List<String>,List<TypeInfo>)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.equals(Object)",6,4,8
"org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getAllStructFieldNames()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getAllStructFieldTypeInfos()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getStructFieldTypeInfo(String)",3,3,3
"org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getTypeName()",1,3,3
"org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.setAllStructFieldNames(ArrayList<String>)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.setAllStructFieldTypeInfos(ArrayList<TypeInfo>)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TestTypeInfoUtils.parseTypeString(String,boolean)",1,1,2
"org.apache.hadoop.hive.serde2.typeinfo.TestTypeInfoUtils.testQualifiedTypeNoParams()",1,1,3
"org.apache.hadoop.hive.serde2.typeinfo.TestTypeInfoUtils.testTypeInfoParser()",1,3,3
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.TypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.accept(TypeInfo)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.getQualifiedName()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.toString()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.TypeInfoFactory()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.createPrimitiveTypeInfo(String)",10,2,11
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getCharTypeInfo(int)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getDecimalTypeInfo(int,int)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getListTypeInfo(TypeInfo)",1,2,2
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getMapTypeInfo(TypeInfo,TypeInfo)",1,2,2
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfo(String)",3,1,3
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfoFromJavaPrimitive(Class<?>)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getPrimitiveTypeInfoFromPrimitiveWritable(Class<?>)",2,1,2
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getStructTypeInfo(List<String>,List<TypeInfo>)",1,2,2
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getUnionTypeInfo(List<TypeInfo>)",1,2,2
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory.getVarcharTypeInfo(int)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.TypeInfoParser.Token.toString()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.TypeInfoParser.TypeInfoParser(String)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.TypeInfoParser.expect(String)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.TypeInfoParser.expect(String,String)",7,10,14
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.TypeInfoParser.isTypeChar(char)",1,3,3
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.TypeInfoParser.parseParams()",3,5,6
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.TypeInfoParser.parsePrimitiveParts()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.TypeInfoParser.parseType()",20,15,25
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.TypeInfoParser.parseTypeInfos()",4,5,6
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.TypeInfoParser.peek()",2,2,2
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.TypeInfoParser.tokenize(String)",1,5,5
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.TypeInfoUtils()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.doPrimitiveCategoriesMatch(TypeInfo,TypeInfo)",3,3,4
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getArrayElementType(Type)",3,4,4
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getBaseName(String)",2,2,2
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getCharacterLengthForType(PrimitiveTypeInfo)",3,2,4
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getExtendedTypeInfoFromJavaType(Type,Method)",9,10,12
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getParameterTypeInfos(Method,int)",4,5,8
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(TypeInfo)",2,10,10
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getStandardWritableObjectInspectorFromTypeInfo(TypeInfo)",2,10,10
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfoFromObjectInspector(ObjectInspector)",3,9,10
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfoFromTypeString(String)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfosFromTypeString(String)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.hasParameters(String)",2,1,2
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.isConversionRequiredForComparison(TypeInfo,TypeInfo)",3,1,3
"org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.parsePrimitiveParts(String)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo.UnionTypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo.UnionTypeInfo(List<TypeInfo>)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo.equals(Object)",3,1,3
"org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo.getAllUnionObjectTypeInfos()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo.getCategory()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo.getTypeName()",1,3,3
"org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo.setAllUnionObjectTypeInfos(List<TypeInfo>)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo.VarcharTypeInfo()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo.VarcharTypeInfo(int)",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo.equals(Object)",2,1,3
"org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo.getTypeName()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo.hashCode()",1,1,1
"org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo.toString()",1,1,1
"org.apache.hadoop.hive.service.HiveClient.HiveClient(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.HiveClusterStatus()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.HiveClusterStatus(HiveClusterStatus)",1,1,2
"org.apache.hadoop.hive.service.HiveClusterStatus.HiveClusterStatus(int,int,int,int,int,JobTrackerState)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.HiveClusterStatusStandardScheme.read(TProtocol,HiveClusterStatus)",4,9,16
"org.apache.hadoop.hive.service.HiveClusterStatus.HiveClusterStatusStandardScheme.write(TProtocol,HiveClusterStatus)",1,2,2
"org.apache.hadoop.hive.service.HiveClusterStatus.HiveClusterStatusStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.HiveClusterStatusTupleScheme.read(TProtocol,HiveClusterStatus)",1,7,7
"org.apache.hadoop.hive.service.HiveClusterStatus.HiveClusterStatusTupleScheme.write(TProtocol,HiveClusterStatus)",1,13,13
"org.apache.hadoop.hive.service.HiveClusterStatus.HiveClusterStatusTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus._Fields.findByThriftId(int)",8,2,8
"org.apache.hadoop.hive.service.HiveClusterStatus._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.HiveClusterStatus._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.clear()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.compareTo(HiveClusterStatus)",20,8,20
"org.apache.hadoop.hive.service.HiveClusterStatus.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.equals(HiveClusterStatus)",20,4,34
"org.apache.hadoop.hive.service.HiveClusterStatus.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.HiveClusterStatus.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.getFieldValue(_Fields)",7,7,7
"org.apache.hadoop.hive.service.HiveClusterStatus.getMapTasks()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.getMaxMapTasks()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.getMaxReduceTasks()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.getReduceTasks()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.getState()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.getTaskTrackers()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.hashCode()",1,8,8
"org.apache.hadoop.hive.service.HiveClusterStatus.isSet(_Fields)",8,7,8
"org.apache.hadoop.hive.service.HiveClusterStatus.isSetMapTasks()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.isSetMaxMapTasks()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.isSetMaxReduceTasks()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.isSetReduceTasks()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.isSetState()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.isSetTaskTrackers()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.HiveClusterStatus.setFieldValue(_Fields,Object)",2,8,13
"org.apache.hadoop.hive.service.HiveClusterStatus.setMapTasks(int)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.setMapTasksIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.setMaxMapTasks(int)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.setMaxMapTasksIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.setMaxReduceTasks(int)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.setMaxReduceTasksIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.setReduceTasks(int)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.setReduceTasksIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.setState(JobTrackerState)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.setStateIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.HiveClusterStatus.setTaskTrackers(int)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.setTaskTrackersIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.toString()",1,7,7
"org.apache.hadoop.hive.service.HiveClusterStatus.unsetMapTasks()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.unsetMaxMapTasks()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.unsetMaxReduceTasks()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.unsetReduceTasks()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.unsetState()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.unsetTaskTrackers()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.validate()",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.HiveClusterStatus.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.HiveServer.HiveServerCli.HiveServerCli()",1,1,1
"org.apache.hadoop.hive.service.HiveServer.HiveServerCli.parse(String[])",1,7,7
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.HiveServerHandler()",1,1,1
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.HiveServerHandler(HiveConf)",1,1,1
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.clean()",1,3,3
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.cleanTmpFile()",1,2,2
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.execute(String)",2,5,7
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.fetchAll()",2,5,5
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.fetchN(int)",3,5,5
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.fetchOne()",4,6,6
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.getClusterStatus()",1,2,2
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.getQueryPlan()",2,2,3
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.getSchema()",3,3,5
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.getStatus()",1,1,1
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.getThriftSchema()",2,2,4
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.getVersion()",1,1,1
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.readResults(List<String>,int)",4,7,8
"org.apache.hadoop.hive.service.HiveServer.HiveServerHandler.setupSessionIO(SessionState)",1,3,3
"org.apache.hadoop.hive.service.HiveServer.ThriftHiveProcessorFactory.ThriftHiveProcessorFactory(TProcessor,HiveConf)",1,1,1
"org.apache.hadoop.hive.service.HiveServer.ThriftHiveProcessorFactory.getProcessor(TTransport)",1,2,2
"org.apache.hadoop.hive.service.HiveServer.main(String[])",1,6,6
"org.apache.hadoop.hive.service.HiveServerException.HiveServerException()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.HiveServerException(HiveServerException)",1,1,3
"org.apache.hadoop.hive.service.HiveServerException.HiveServerException(String,int,String)",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.HiveServerExceptionStandardScheme.read(TProtocol,HiveServerException)",4,6,10
"org.apache.hadoop.hive.service.HiveServerException.HiveServerExceptionStandardScheme.write(TProtocol,HiveServerException)",1,3,3
"org.apache.hadoop.hive.service.HiveServerException.HiveServerExceptionStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.HiveServerExceptionTupleScheme.read(TProtocol,HiveServerException)",1,4,4
"org.apache.hadoop.hive.service.HiveServerException.HiveServerExceptionTupleScheme.write(TProtocol,HiveServerException)",1,7,7
"org.apache.hadoop.hive.service.HiveServerException.HiveServerExceptionTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.HiveServerException._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.HiveServerException._Fields.findByThriftId(int)",5,2,5
"org.apache.hadoop.hive.service.HiveServerException._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.HiveServerException._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.clear()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.compareTo(HiveServerException)",11,5,11
"org.apache.hadoop.hive.service.HiveServerException.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.equals(HiveServerException)",11,7,21
"org.apache.hadoop.hive.service.HiveServerException.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.HiveServerException.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.getErrorCode()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.getFieldValue(_Fields)",4,4,4
"org.apache.hadoop.hive.service.HiveServerException.getMessage()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.getSQLState()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.hashCode()",1,6,6
"org.apache.hadoop.hive.service.HiveServerException.isSet(_Fields)",5,4,5
"org.apache.hadoop.hive.service.HiveServerException.isSetErrorCode()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.isSetMessage()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.isSetSQLState()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.HiveServerException.setErrorCode(int)",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.setErrorCodeIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hadoop.hive.service.HiveServerException.setMessage(String)",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.setMessageIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.HiveServerException.setSQLState(String)",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.setSQLStateIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.HiveServerException.toString()",1,5,5
"org.apache.hadoop.hive.service.HiveServerException.unsetErrorCode()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.unsetMessage()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.unsetSQLState()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.validate()",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.HiveServerException.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.JobTrackerState.JobTrackerState(int)",1,1,1
"org.apache.hadoop.hive.service.JobTrackerState.findByValue(int)",4,2,4
"org.apache.hadoop.hive.service.JobTrackerState.getValue()",1,1,1
"org.apache.hadoop.hive.service.TestHiveServer.TestHiveServer(String)",1,2,3
"org.apache.hadoop.hive.service.TestHiveServer.notestExecute()",1,1,2
"org.apache.hadoop.hive.service.TestHiveServer.setUp()",1,3,3
"org.apache.hadoop.hive.service.TestHiveServer.tearDown()",1,3,3
"org.apache.hadoop.hive.service.TestHiveServer.testAddArchiveShouldFailIfFileNotExist()",1,2,3
"org.apache.hadoop.hive.service.TestHiveServer.testAddFileShouldFailIfFileNotExist()",1,2,3
"org.apache.hadoop.hive.service.TestHiveServer.testAddJarShouldFailIfJarNotExist()",1,2,3
"org.apache.hadoop.hive.service.TestHiveServer.testDynamicSerde()",1,5,6
"org.apache.hadoop.hive.service.TestHiveServer.testExecute()",1,2,3
"org.apache.hadoop.hive.service.TestHiveServer.testFetch()",1,5,6
"org.apache.hadoop.hive.service.TestHiveServer.testGetClusterStatus()",1,2,2
"org.apache.hadoop.hive.service.TestHiveServer.testMetastore()",1,1,2
"org.apache.hadoop.hive.service.TestHiveServer.testNonHiveCommand()",1,3,4
"org.apache.hadoop.hive.service.TestHiveServer.testScratchDirShouldClearWhileStartup()",1,2,2
"org.apache.hadoop.hive.service.TestHiveServer.testScratchDirShouldNotClearWhileStartup()",1,2,2
"org.apache.hadoop.hive.service.TestHiveServerSessions.TestHiveServerSessions(String)",1,1,1
"org.apache.hadoop.hive.service.TestHiveServerSessions.findFreePort()",1,1,1
"org.apache.hadoop.hive.service.TestHiveServerSessions.setUp()",1,2,2
"org.apache.hadoop.hive.service.TestHiveServerSessions.tearDown()",1,4,5
"org.apache.hadoop.hive.service.TestHiveServerSessions.testSessionVars()",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.AsyncClient(TProtocolFactory,TAsyncClientManager,TNonblockingTransport)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.Factory.Factory(TAsyncClientManager,TProtocolFactory)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.Factory.getAsyncClient(TNonblockingTransport)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.clean(AsyncMethodCallback<clean_call>)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.clean_call.clean_call(AsyncMethodCallback<clean_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.clean_call.getResult()",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.clean_call.write_args(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.execute(String,AsyncMethodCallback<execute_call>)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.execute_call.execute_call(String,AsyncMethodCallback<execute_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.execute_call.getResult()",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.execute_call.write_args(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.fetchAll(AsyncMethodCallback<fetchAll_call>)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.fetchAll_call.fetchAll_call(AsyncMethodCallback<fetchAll_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.fetchAll_call.getResult()",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.fetchAll_call.write_args(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.fetchN(int,AsyncMethodCallback<fetchN_call>)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.fetchN_call.fetchN_call(int,AsyncMethodCallback<fetchN_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.fetchN_call.getResult()",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.fetchN_call.write_args(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.fetchOne(AsyncMethodCallback<fetchOne_call>)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.fetchOne_call.fetchOne_call(AsyncMethodCallback<fetchOne_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.fetchOne_call.getResult()",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.fetchOne_call.write_args(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getClusterStatus(AsyncMethodCallback<getClusterStatus_call>)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getClusterStatus_call.getClusterStatus_call(AsyncMethodCallback<getClusterStatus_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getClusterStatus_call.getResult()",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getClusterStatus_call.write_args(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getQueryPlan(AsyncMethodCallback<getQueryPlan_call>)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getQueryPlan_call.getQueryPlan_call(AsyncMethodCallback<getQueryPlan_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getQueryPlan_call.getResult()",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getQueryPlan_call.write_args(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getSchema(AsyncMethodCallback<getSchema_call>)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getSchema_call.getResult()",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getSchema_call.getSchema_call(AsyncMethodCallback<getSchema_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getSchema_call.write_args(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getThriftSchema(AsyncMethodCallback<getThriftSchema_call>)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getThriftSchema_call.getResult()",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getThriftSchema_call.getThriftSchema_call(AsyncMethodCallback<getThriftSchema_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.AsyncClient.getThriftSchema_call.write_args(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.Client(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.Client(TProtocol,TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.Factory.Factory()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.Factory.getClient(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.Factory.getClient(TProtocol,TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.clean()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.execute(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.fetchAll()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.fetchN(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.fetchOne()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.getClusterStatus()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.getQueryPlan()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.getSchema()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.getThriftSchema()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.recv_clean()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.recv_execute()",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.Client.recv_fetchAll()",3,1,3
"org.apache.hadoop.hive.service.ThriftHive.Client.recv_fetchN()",3,1,3
"org.apache.hadoop.hive.service.ThriftHive.Client.recv_fetchOne()",3,1,3
"org.apache.hadoop.hive.service.ThriftHive.Client.recv_getClusterStatus()",3,1,3
"org.apache.hadoop.hive.service.ThriftHive.Client.recv_getQueryPlan()",3,1,3
"org.apache.hadoop.hive.service.ThriftHive.Client.recv_getSchema()",3,1,3
"org.apache.hadoop.hive.service.ThriftHive.Client.recv_getThriftSchema()",3,1,3
"org.apache.hadoop.hive.service.ThriftHive.Client.send_clean()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.send_execute(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.send_fetchAll()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.send_fetchN(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.send_fetchOne()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.send_getClusterStatus()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.send_getQueryPlan()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.send_getSchema()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Client.send_getThriftSchema()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.Processor(I)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.Processor(I,Map<String, ProcessFunction<I, ? extends TBase>>)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.clean.clean()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.clean.getEmptyArgsInstance()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.clean.getResult(I,clean_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.clean.isOneway()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.execute.execute()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.execute.getEmptyArgsInstance()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.execute.getResult(I,execute_args)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.Processor.execute.isOneway()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.fetchAll.fetchAll()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.fetchAll.getEmptyArgsInstance()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.fetchAll.getResult(I,fetchAll_args)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.Processor.fetchAll.isOneway()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.fetchN.fetchN()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.fetchN.getEmptyArgsInstance()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.fetchN.getResult(I,fetchN_args)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.Processor.fetchN.isOneway()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.fetchOne.fetchOne()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.fetchOne.getEmptyArgsInstance()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.fetchOne.getResult(I,fetchOne_args)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.Processor.fetchOne.isOneway()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.getClusterStatus.getClusterStatus()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.getClusterStatus.getEmptyArgsInstance()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.getClusterStatus.getResult(I,getClusterStatus_args)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.Processor.getClusterStatus.isOneway()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.getProcessMap(Map<String, ProcessFunction<I, ? extends TBase>>)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.getQueryPlan.getEmptyArgsInstance()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.getQueryPlan.getQueryPlan()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.getQueryPlan.getResult(I,getQueryPlan_args)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.Processor.getQueryPlan.isOneway()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.getSchema.getEmptyArgsInstance()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.getSchema.getResult(I,getSchema_args)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.Processor.getSchema.getSchema()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.getSchema.isOneway()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.getThriftSchema.getEmptyArgsInstance()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.getThriftSchema.getResult(I,getThriftSchema_args)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.Processor.getThriftSchema.getThriftSchema()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.Processor.getThriftSchema.isOneway()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args._Fields.findByThriftId(int)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.clean_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.clean_args._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.clean_args()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.clean_args(clean_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.clean_argsStandardScheme.read(TProtocol,clean_args)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.clean_args.clean_argsStandardScheme.write(TProtocol,clean_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.clean_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.clean_argsTupleScheme.read(TProtocol,clean_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.clean_argsTupleScheme.write(TProtocol,clean_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.clean_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.compareTo(clean_args)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.clean_args.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.clean_args.equals(clean_args)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.clean_args.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.getFieldValue(_Fields)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.hashCode()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.isSet(_Fields)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.clean_args.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.clean_args.setFieldValue(_Fields,Object)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.toString()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.validate()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.clean_result._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result._Fields.findByThriftId(int)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.clean_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.clean_result._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.clean_result()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.clean_result(clean_result)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.clean_resultStandardScheme.read(TProtocol,clean_result)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.clean_result.clean_resultStandardScheme.write(TProtocol,clean_result)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.clean_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.clean_resultTupleScheme.read(TProtocol,clean_result)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.clean_resultTupleScheme.write(TProtocol,clean_result)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.clean_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.compareTo(clean_result)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.clean_result.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.clean_result.equals(clean_result)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.clean_result.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.getFieldValue(_Fields)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.hashCode()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.isSet(_Fields)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.clean_result.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.clean_result.setFieldValue(_Fields,Object)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.toString()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.validate()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.clean_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.execute_args._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.execute_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.execute_args._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.compareTo(execute_args)",5,3,5
"org.apache.hadoop.hive.service.ThriftHive.execute_args.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.execute_args.equals(execute_args)",5,4,9
"org.apache.hadoop.hive.service.ThriftHive.execute_args.execute_args()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.execute_args(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.execute_args(execute_args)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.execute_args.execute_argsStandardScheme.read(TProtocol,execute_args)",4,4,6
"org.apache.hadoop.hive.service.ThriftHive.execute_args.execute_argsStandardScheme.write(TProtocol,execute_args)",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.execute_args.execute_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.execute_argsTupleScheme.read(TProtocol,execute_args)",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.execute_args.execute_argsTupleScheme.write(TProtocol,execute_args)",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.execute_args.execute_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.execute_args.getQuery()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.hashCode()",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.execute_args.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.execute_args.isSetQuery()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.execute_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.service.ThriftHive.execute_args.setQuery(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.setQueryIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.execute_args.toString()",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.execute_args.unsetQuery()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.validate()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.execute_result._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.execute_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.execute_result._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.compareTo(execute_result)",5,3,5
"org.apache.hadoop.hive.service.ThriftHive.execute_result.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.execute_result.equals(execute_result)",5,4,9
"org.apache.hadoop.hive.service.ThriftHive.execute_result.execute_result()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.execute_result(HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.execute_result(execute_result)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.execute_result.execute_resultStandardScheme.read(TProtocol,execute_result)",4,4,6
"org.apache.hadoop.hive.service.ThriftHive.execute_result.execute_resultStandardScheme.write(TProtocol,execute_result)",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.execute_result.execute_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.execute_resultTupleScheme.read(TProtocol,execute_result)",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.execute_result.execute_resultTupleScheme.write(TProtocol,execute_result)",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.execute_result.execute_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.getEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.execute_result.hashCode()",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.execute_result.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.execute_result.isSetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.execute_result.setEx(HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.setExIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.execute_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.service.ThriftHive.execute_result.toString()",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.execute_result.unsetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.validate()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.execute_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args._Fields.findByThriftId(int)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.compareTo(fetchAll_args)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.equals(fetchAll_args)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.fetchAll_args()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.fetchAll_args(fetchAll_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.fetchAll_argsStandardScheme.read(TProtocol,fetchAll_args)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.fetchAll_argsStandardScheme.write(TProtocol,fetchAll_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.fetchAll_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.fetchAll_argsTupleScheme.read(TProtocol,fetchAll_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.fetchAll_argsTupleScheme.write(TProtocol,fetchAll_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.fetchAll_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.getFieldValue(_Fields)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.hashCode()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.isSet(_Fields)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.setFieldValue(_Fields,Object)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.toString()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.validate()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.addToSuccess(String)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.compareTo(fetchAll_result)",8,4,8
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.equals(fetchAll_result)",8,7,16
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.fetchAll_result()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.fetchAll_result(List<String>,HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.fetchAll_result(fetchAll_result)",1,3,4
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.fetchAll_resultStandardScheme.read(TProtocol,fetchAll_result)",4,6,9
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.fetchAll_resultStandardScheme.write(TProtocol,fetchAll_result)",1,4,4
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.fetchAll_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.fetchAll_resultTupleScheme.read(TProtocol,fetchAll_result)",1,4,4
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.fetchAll_resultTupleScheme.write(TProtocol,fetchAll_result)",1,6,6
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.fetchAll_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.getEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.getSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.getSuccessIterator()",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.getSuccessSize()",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.hashCode()",1,5,5
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.isSetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.isSetSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.setEx(HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.setExIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.setSuccess(List<String>)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.toString()",1,4,4
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.unsetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.unsetSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.validate()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchAll_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.compareTo(fetchN_args)",5,3,5
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.equals(fetchN_args)",5,1,7
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.fetchN_args()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.fetchN_args(fetchN_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.fetchN_args(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.fetchN_argsStandardScheme.read(TProtocol,fetchN_args)",4,4,6
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.fetchN_argsStandardScheme.write(TProtocol,fetchN_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.fetchN_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.fetchN_argsTupleScheme.read(TProtocol,fetchN_args)",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.fetchN_argsTupleScheme.write(TProtocol,fetchN_args)",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.fetchN_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.getFieldValue(_Fields)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.getNumRows()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.hashCode()",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.isSet(_Fields)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.isSetNumRows()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.setNumRows(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.setNumRowsIsSet(boolean)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.toString()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.unsetNumRows()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.validate()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.addToSuccess(String)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.compareTo(fetchN_result)",8,4,8
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.equals(fetchN_result)",8,7,16
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.fetchN_result()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.fetchN_result(List<String>,HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.fetchN_result(fetchN_result)",1,3,4
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.fetchN_resultStandardScheme.read(TProtocol,fetchN_result)",4,6,9
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.fetchN_resultStandardScheme.write(TProtocol,fetchN_result)",1,4,4
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.fetchN_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.fetchN_resultTupleScheme.read(TProtocol,fetchN_result)",1,4,4
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.fetchN_resultTupleScheme.write(TProtocol,fetchN_result)",1,6,6
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.fetchN_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.getEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.getSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.getSuccessIterator()",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.getSuccessSize()",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.hashCode()",1,5,5
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.isSetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.isSetSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.setEx(HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.setExIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.setSuccess(List<String>)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.toString()",1,4,4
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.unsetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.unsetSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.validate()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchN_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args._Fields.findByThriftId(int)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.compareTo(fetchOne_args)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.equals(fetchOne_args)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.fetchOne_args()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.fetchOne_args(fetchOne_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.fetchOne_argsStandardScheme.read(TProtocol,fetchOne_args)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.fetchOne_argsStandardScheme.write(TProtocol,fetchOne_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.fetchOne_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.fetchOne_argsTupleScheme.read(TProtocol,fetchOne_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.fetchOne_argsTupleScheme.write(TProtocol,fetchOne_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.fetchOne_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.getFieldValue(_Fields)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.hashCode()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.isSet(_Fields)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.setFieldValue(_Fields,Object)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.toString()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.validate()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.compareTo(fetchOne_result)",8,4,8
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.equals(fetchOne_result)",8,7,16
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.fetchOne_result()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.fetchOne_result(String,HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.fetchOne_result(fetchOne_result)",1,1,3
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.fetchOne_resultStandardScheme.read(TProtocol,fetchOne_result)",4,5,8
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.fetchOne_resultStandardScheme.write(TProtocol,fetchOne_result)",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.fetchOne_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.fetchOne_resultTupleScheme.read(TProtocol,fetchOne_result)",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.fetchOne_resultTupleScheme.write(TProtocol,fetchOne_result)",1,5,5
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.fetchOne_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.getEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.getSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.hashCode()",1,5,5
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.isSetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.isSetSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.setEx(HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.setExIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.setSuccess(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.toString()",1,4,4
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.unsetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.unsetSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.validate()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.fetchOne_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args._Fields.findByThriftId(int)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.compareTo(getClusterStatus_args)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.equals(getClusterStatus_args)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.getClusterStatus_args()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.getClusterStatus_args(getClusterStatus_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.getClusterStatus_argsStandardScheme.read(TProtocol,getClusterStatus_args)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.getClusterStatus_argsStandardScheme.write(TProtocol,getClusterStatus_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.getClusterStatus_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.getClusterStatus_argsTupleScheme.read(TProtocol,getClusterStatus_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.getClusterStatus_argsTupleScheme.write(TProtocol,getClusterStatus_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.getClusterStatus_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.getFieldValue(_Fields)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.hashCode()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.isSet(_Fields)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.setFieldValue(_Fields,Object)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.toString()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.validate()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.compareTo(getClusterStatus_result)",8,4,8
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.equals(getClusterStatus_result)",8,7,16
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.getClusterStatus_result()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.getClusterStatus_result(HiveClusterStatus,HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.getClusterStatus_result(getClusterStatus_result)",1,1,3
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.getClusterStatus_resultStandardScheme.read(TProtocol,getClusterStatus_result)",4,5,8
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.getClusterStatus_resultStandardScheme.write(TProtocol,getClusterStatus_result)",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.getClusterStatus_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.getClusterStatus_resultTupleScheme.read(TProtocol,getClusterStatus_result)",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.getClusterStatus_resultTupleScheme.write(TProtocol,getClusterStatus_result)",1,5,5
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.getClusterStatus_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.getEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.getSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.hashCode()",1,5,5
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.isSetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.isSetSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.setEx(HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.setExIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.setSuccess(HiveClusterStatus)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.toString()",1,4,4
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.unsetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.unsetSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.validate()",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getClusterStatus_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args._Fields.findByThriftId(int)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.compareTo(getQueryPlan_args)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.equals(getQueryPlan_args)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.getFieldValue(_Fields)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.getQueryPlan_args()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.getQueryPlan_args(getQueryPlan_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.getQueryPlan_argsStandardScheme.read(TProtocol,getQueryPlan_args)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.getQueryPlan_argsStandardScheme.write(TProtocol,getQueryPlan_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.getQueryPlan_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.getQueryPlan_argsTupleScheme.read(TProtocol,getQueryPlan_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.getQueryPlan_argsTupleScheme.write(TProtocol,getQueryPlan_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.getQueryPlan_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.hashCode()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.isSet(_Fields)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.setFieldValue(_Fields,Object)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.toString()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.validate()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.compareTo(getQueryPlan_result)",8,4,8
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.equals(getQueryPlan_result)",8,7,16
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.getEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.getQueryPlan_result()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.getQueryPlan_result(QueryPlan,HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.getQueryPlan_result(getQueryPlan_result)",1,1,3
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.getQueryPlan_resultStandardScheme.read(TProtocol,getQueryPlan_result)",4,5,8
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.getQueryPlan_resultStandardScheme.write(TProtocol,getQueryPlan_result)",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.getQueryPlan_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.getQueryPlan_resultTupleScheme.read(TProtocol,getQueryPlan_result)",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.getQueryPlan_resultTupleScheme.write(TProtocol,getQueryPlan_result)",1,5,5
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.getQueryPlan_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.getSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.hashCode()",1,5,5
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.isSetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.isSetSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.setEx(HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.setExIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.setSuccess(QueryPlan)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.toString()",1,4,4
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.unsetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.unsetSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.validate()",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getQueryPlan_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args._Fields.findByThriftId(int)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.compareTo(getSchema_args)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.equals(getSchema_args)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.getFieldValue(_Fields)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.getSchema_args()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.getSchema_args(getSchema_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.getSchema_argsStandardScheme.read(TProtocol,getSchema_args)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.getSchema_argsStandardScheme.write(TProtocol,getSchema_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.getSchema_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.getSchema_argsTupleScheme.read(TProtocol,getSchema_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.getSchema_argsTupleScheme.write(TProtocol,getSchema_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.getSchema_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.hashCode()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.isSet(_Fields)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.setFieldValue(_Fields,Object)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.toString()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.validate()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.compareTo(getSchema_result)",8,4,8
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.equals(getSchema_result)",8,7,16
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.getEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.getSchema_result()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.getSchema_result(Schema,HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.getSchema_result(getSchema_result)",1,1,3
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.getSchema_resultStandardScheme.read(TProtocol,getSchema_result)",4,5,8
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.getSchema_resultStandardScheme.write(TProtocol,getSchema_result)",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.getSchema_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.getSchema_resultTupleScheme.read(TProtocol,getSchema_result)",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.getSchema_resultTupleScheme.write(TProtocol,getSchema_result)",1,5,5
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.getSchema_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.getSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.hashCode()",1,5,5
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.isSetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.isSetSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.setEx(HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.setExIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.setSuccess(Schema)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.toString()",1,4,4
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.unsetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.unsetSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.validate()",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getSchema_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args._Fields.findByThriftId(int)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.compareTo(getThriftSchema_args)",2,2,2
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.equals(getThriftSchema_args)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.getFieldValue(_Fields)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.getThriftSchema_args()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.getThriftSchema_args(getThriftSchema_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.getThriftSchema_argsStandardScheme.read(TProtocol,getThriftSchema_args)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.getThriftSchema_argsStandardScheme.write(TProtocol,getThriftSchema_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.getThriftSchema_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.getThriftSchema_argsTupleScheme.read(TProtocol,getThriftSchema_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.getThriftSchema_argsTupleScheme.write(TProtocol,getThriftSchema_args)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.getThriftSchema_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.hashCode()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.isSet(_Fields)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.setFieldValue(_Fields,Object)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.toString()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.validate()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result._Fields._Fields(short,String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result._Fields.findByName(String)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result._Fields.findByThriftId(int)",4,2,4
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result._Fields.getFieldName()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.clear()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.compareTo(getThriftSchema_result)",8,4,8
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.deepCopy()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.equals(Object)",3,2,3
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.equals(getThriftSchema_result)",8,7,16
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.fieldForId(int)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.getEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.getFieldValue(_Fields)",3,3,3
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.getSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.getThriftSchema_result()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.getThriftSchema_result(Schema,HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.getThriftSchema_result(getThriftSchema_result)",1,1,3
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.getThriftSchema_resultStandardScheme.read(TProtocol,getThriftSchema_result)",4,5,8
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.getThriftSchema_resultStandardScheme.write(TProtocol,getThriftSchema_result)",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.getThriftSchema_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.getThriftSchema_resultTupleScheme.read(TProtocol,getThriftSchema_result)",1,3,3
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.getThriftSchema_resultTupleScheme.write(TProtocol,getThriftSchema_result)",1,5,5
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.getThriftSchema_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.hashCode()",1,5,5
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.isSet(_Fields)",4,3,4
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.isSetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.isSetSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.read(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.setEx(HiveServerException)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.setExIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.setSuccess(Schema)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.toString()",1,4,4
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.unsetEx()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.unsetSuccess()",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.validate()",1,2,2
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.write(TProtocol)",1,1,1
"org.apache.hadoop.hive.service.ThriftHive.getThriftSchema_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hadoop.hive.shims.CombineHiveKey.CombineHiveKey(Object)",1,1,1
"org.apache.hadoop.hive.shims.CombineHiveKey.compareTo(Object)",1,1,1
"org.apache.hadoop.hive.shims.CombineHiveKey.getKey()",1,1,1
"org.apache.hadoop.hive.shims.CombineHiveKey.readFields(DataInput)",1,1,1
"org.apache.hadoop.hive.shims.CombineHiveKey.setKey(Object)",1,1,1
"org.apache.hadoop.hive.shims.CombineHiveKey.write(DataOutput)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.HCatHadoopShims20S.abortJob(OutputFormat,Job)",2,2,4
"org.apache.hadoop.hive.shims.Hadoop20SShims.HCatHadoopShims20S.commitJob(OutputFormat,Job)",2,2,4
"org.apache.hadoop.hive.shims.Hadoop20SShims.HCatHadoopShims20S.createJobContext(Configuration,JobID)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.HCatHadoopShims20S.createJobContext(JobConf,JobID,Progressable)",1,1,2
"org.apache.hadoop.hive.shims.Hadoop20SShims.HCatHadoopShims20S.createTaskAttemptContext(Configuration,TaskAttemptID)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.HCatHadoopShims20S.createTaskAttemptContext(JobConf,TaskAttemptID,Progressable)",1,1,2
"org.apache.hadoop.hive.shims.Hadoop20SShims.HCatHadoopShims20S.createTaskAttemptID()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.HCatHadoopShims20S.createTaskID()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.HCatHadoopShims20S.getPropertyName(PropertyName)",6,2,6
"org.apache.hadoop.hive.shims.Hadoop20SShims.HCatHadoopShims20S.getResourceManagerAddress(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.HCatHadoopShims20S.isFileInHDFS(FileSystem,Path)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.Hadoop20SFileStatus.Hadoop20SFileStatus(FileStatus)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.Hadoop20SFileStatus.debugLog()",1,2,2
"org.apache.hadoop.hive.shims.Hadoop20SShims.Hadoop20SFileStatus.getFileStatus()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.MiniDFSShim.MiniDFSShim(MiniDFSCluster)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.MiniDFSShim.getFileSystem()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.MiniDFSShim.shutdown()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.MiniMrShim.MiniMrShim(Configuration,int,String,int)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.MiniMrShim.getJobTrackerPort()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.MiniMrShim.setupConfiguration(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.MiniMrShim.shutdown()",1,3,4
"org.apache.hadoop.hive.shims.Hadoop20SShims.createProxyFileSystem(FileSystem,URI)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getConfiguration(JobContext)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getDefaultBlockSize(FileSystem,Path)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getDefaultReplication(FileSystem,Path)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getDirectDecompressor(DirectCompressionType)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getFullFileStatus(Configuration,FileSystem,Path)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getHCatShim()",1,1,2
"org.apache.hadoop.hive.shims.Hadoop20SShims.getHadoopConfNames()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getJobLauncherHttpAddress(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getJobLauncherRpcAddress(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getJobTrackerState(ClusterStatus)",4,2,4
"org.apache.hadoop.hive.shims.Hadoop20SShims.getLocations(FileSystem,FileStatus)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getLongComparator()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getMergedCredentials(JobConf)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getMiniDfs(Configuration,int,boolean,String[])",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getMiniMrCluster(Configuration,int,String,int)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getMiniTezCluster(Configuration,int,String,int)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getNonCachedFileSystem(URI,Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getPassword(Configuration,String)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getTaskAttemptLogUrl(JobConf,String,String)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getWebHCatShim(Configuration,UserGroupInformation)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.getZeroCopyReader(FSDataInputStream,ByteBufferPoolShim)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.hflush(FSDataOutputStream)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.isLocalMode(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.listLocatedStatus(FileSystem,Path,PathFilter)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.moveToAppropriateTrash(FileSystem,Path,Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.newJobContext(Job)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.newTaskAttemptContext(Configuration,Progressable)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.newTaskAttemptID(JobID,boolean,int,int)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.setFullFileStatus(Configuration,HdfsFileStatus,FileSystem,Path)",1,2,4
"org.apache.hadoop.hive.shims.Hadoop20SShims.setJobLauncherRpcAddress(Configuration,String)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20SShims.setTotalOrderPartitionFile(JobConf,Path)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.CombineFileInputFormatShim.createPool(JobConf,PathFilter...)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.CombineFileInputFormatShim.getInputPathsShim(JobConf)",1,1,2
"org.apache.hadoop.hive.shims.Hadoop20Shims.CombineFileInputFormatShim.getInputSplitShim()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.CombineFileInputFormatShim.getRecordReader(JobConf,InputSplitShim,Reporter,Class<RecordReader<K, V>>)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.CombineFileInputFormatShim.getSplits(JobConf,int)",1,4,5
"org.apache.hadoop.hive.shims.Hadoop20Shims.CombineFileRecordReader.CombineFileRecordReader(JobConf,CombineFileSplit,Reporter,Class<RecordReader<K, V>>)",1,3,3
"org.apache.hadoop.hive.shims.Hadoop20Shims.CombineFileRecordReader.close()",1,2,2
"org.apache.hadoop.hive.shims.Hadoop20Shims.CombineFileRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.CombineFileRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.CombineFileRecordReader.doNextWithExceptionHandler(K,V)",1,2,2
"org.apache.hadoop.hive.shims.Hadoop20Shims.CombineFileRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.CombineFileRecordReader.getProgress()",1,2,2
"org.apache.hadoop.hive.shims.Hadoop20Shims.CombineFileRecordReader.initNextRecordReader(K)",2,6,8
"org.apache.hadoop.hive.shims.Hadoop20Shims.CombineFileRecordReader.next(K,V)",3,3,4
"org.apache.hadoop.hive.shims.Hadoop20Shims.Hadoop20FileStatus.Hadoop20FileStatus(FileStatus)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.Hadoop20FileStatus.debugLog()",1,2,2
"org.apache.hadoop.hive.shims.Hadoop20Shims.Hadoop20FileStatus.getFileStatus()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.InputSplitShim.InputSplitShim()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.InputSplitShim.InputSplitShim(CombineFileSplit)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.InputSplitShim.dedup(String[])",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.InputSplitShim.getShrinkedLength()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.InputSplitShim.isShrinked()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.InputSplitShim.readFields(DataInput)",1,2,2
"org.apache.hadoop.hive.shims.Hadoop20Shims.InputSplitShim.shrinkSplit(long)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.InputSplitShim.write(DataOutput)",1,2,2
"org.apache.hadoop.hive.shims.Hadoop20Shims.MiniDFSShim.MiniDFSShim(MiniDFSCluster)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.MiniDFSShim.getFileSystem()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.MiniDFSShim.shutdown()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.MiniMrShim.MiniMrShim(Configuration,int,String,int)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.MiniMrShim.getJobTrackerPort()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.MiniMrShim.setupConfiguration(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.MiniMrShim.shutdown()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.NullOutputCommitter.abortTask(TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.NullOutputCommitter.cleanupJob(JobContext)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.NullOutputCommitter.commitTask(TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.NullOutputCommitter.needsTaskCommit(TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.NullOutputCommitter.setupJob(JobContext)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.NullOutputCommitter.setupTask(TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.addServiceToToken(String,String)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.authorizeProxyAccess(String,UserGroupInformation,String,Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.checkFileAccess(FileSystem,FileStatus,FsAction)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.closeAllForUGI(UserGroupInformation)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.createDelegationTokenFile(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.createHadoopArchive(Configuration,Path,Path,String)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.createProxyFileSystem(FileSystem,URI)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.createProxyUser(String)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.createRemoteUser(String,List<String>)",1,2,2
"org.apache.hadoop.hive.shims.Hadoop20Shims.doAs(UserGroupInformation,PrivilegedExceptionAction<T>)",1,1,2
"org.apache.hadoop.hive.shims.Hadoop20Shims.getCombineFileInputFormat()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getConfiguration(JobContext)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getDefaultBlockSize(FileSystem,Path)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getDefaultReplication(FileSystem,Path)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getDirectDecompressor(DirectCompressionType)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getFullFileStatus(Configuration,FileSystem,Path)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getHCatShim()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getHadoopConfNames()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getHarUri(URI,URI,URI)",1,3,3
"org.apache.hadoop.hive.shims.Hadoop20Shims.getInputFormatClassName()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getJobLauncherHttpAddress(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getJobLauncherRpcAddress(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getJobTrackerState(ClusterStatus)",4,2,4
"org.apache.hadoop.hive.shims.Hadoop20Shims.getLocations(FileSystem,FileStatus)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getLongComparator()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getMergedCredentials(JobConf)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getMiniDfs(Configuration,int,boolean,String[])",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getMiniMrCluster(Configuration,int,String,int)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getMiniTezCluster(Configuration,int,String,int)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getNonCachedFileSystem(URI,Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getPassword(Configuration,String)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getShortUserName(UserGroupInformation)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getTaskAttemptLogUrl(JobConf,String,String)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getTokenFileLocEnvName()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getTokenStrForm(String)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getUGIForConf(Configuration)",1,2,2
"org.apache.hadoop.hive.shims.Hadoop20Shims.getWebHCatShim(Configuration,UserGroupInformation)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.getZeroCopyReader(FSDataInputStream,ByteBufferPoolShim)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.hflush(FSDataOutputStream)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.isLocalMode(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.isLoginKeytabBased()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.isSecureShimImpl()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.isSecurityEnabled()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.listLocatedStatus(FileSystem,Path,PathFilter)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.loginUserFromKeytab(String,String)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.loginUserFromKeytabAndReturnUGI(String,String)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.moveToAppropriateTrash(FileSystem,Path,Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.newJobContext(Job)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.newTaskAttemptContext(Configuration,Progressable)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.newTaskAttemptID(JobID,boolean,int,int)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.prepareJobOutput(JobConf)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.reLoginUserFromKeytab()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.run(FsShell,String[])",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.setFullFileStatus(Configuration,HdfsFileStatus,FileSystem,Path)",1,2,4
"org.apache.hadoop.hive.shims.Hadoop20Shims.setJobLauncherRpcAddress(Configuration,String)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.setTokenStr(UserGroupInformation,String,String)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.setTotalOrderPartitionFile(JobConf,Path)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.throwKerberosUnsupportedError()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop20Shims.unquoteHtmlChars(String)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.HCatHadoopShims23.abortJob(OutputFormat,Job)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.HCatHadoopShims23.commitJob(OutputFormat,Job)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.HCatHadoopShims23.createJobContext(Configuration,JobID)",1,1,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.HCatHadoopShims23.createJobContext(JobConf,JobID,Progressable)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.HCatHadoopShims23.createTaskAttemptContext(Configuration,TaskAttemptID)",1,1,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.HCatHadoopShims23.createTaskAttemptContext(JobConf,TaskAttemptID,Progressable)",1,1,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.HCatHadoopShims23.createTaskAttemptID()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.HCatHadoopShims23.createTaskID()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.HCatHadoopShims23.getPropertyName(PropertyName)",6,2,6
"org.apache.hadoop.hive.shims.Hadoop23Shims.HCatHadoopShims23.getResourceManagerAddress(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.HCatHadoopShims23.isFileInHDFS(FileSystem,Path)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.Hadoop23FileStatus.Hadoop23FileStatus(FileStatus,AclStatus)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.Hadoop23FileStatus.debugLog()",1,3,3
"org.apache.hadoop.hive.shims.Hadoop23Shims.Hadoop23FileStatus.getAclStatus()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.Hadoop23FileStatus.getFileStatus()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.Hadoop23Shims()",1,1,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.MiniDFSShim.MiniDFSShim(MiniDFSCluster)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.MiniDFSShim.getFileSystem()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.MiniDFSShim.shutdown()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.MiniMrShim.MiniMrShim()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.MiniMrShim.MiniMrShim(Configuration,int,String,int)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.MiniMrShim.getJobTrackerPort()",2,1,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.MiniMrShim.setupConfiguration(Configuration)",1,2,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.MiniMrShim.shutdown()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.MiniTezShim.MiniTezShim(Configuration,int,String,int)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.MiniTezShim.getJobTrackerPort()",2,1,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.MiniTezShim.setupConfiguration(Configuration)",1,3,3
"org.apache.hadoop.hive.shims.Hadoop23Shims.MiniTezShim.shutdown()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.ProxyFileSystem23.ProxyFileSystem23(FileSystem)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.ProxyFileSystem23.ProxyFileSystem23(FileSystem,URI)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.ProxyFileSystem23.access(Path,FsAction)",1,3,6
"org.apache.hadoop.hive.shims.Hadoop23Shims.ProxyFileSystem23.listLocatedStatus(Path)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.checkFileAccess(FileSystem,FileStatus,FsAction)",1,3,3
"org.apache.hadoop.hive.shims.Hadoop23Shims.createProxyFileSystem(FileSystem,URI)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.getConfiguration(JobContext)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.getDefaultBlockSize(FileSystem,Path)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.getDefaultReplication(FileSystem,Path)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.getDirectDecompressor(DirectCompressionType)",2,2,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.getFullFileStatus(Configuration,FileSystem,Path)",1,2,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.getHCatShim()",1,1,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.getHadoopConfNames()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.getJobLauncherHttpAddress(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.getJobLauncherRpcAddress(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.getJobTrackerState(ClusterStatus)",4,2,4
"org.apache.hadoop.hive.shims.Hadoop23Shims.getLocations(FileSystem,FileStatus)",2,2,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.getLongComparator()",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.getMergedCredentials(JobConf)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.getMiniDfs(Configuration,int,boolean,String[])",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.getMiniMrCluster(Configuration,int,String,int)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.getMiniTezCluster(Configuration,int,String,int)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.getNonCachedFileSystem(URI,Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.getPassword(Configuration,String)",3,3,4
"org.apache.hadoop.hive.shims.Hadoop23Shims.getTaskAttemptLogUrl(JobConf,String,String)",2,3,3
"org.apache.hadoop.hive.shims.Hadoop23Shims.getWebHCatShim(Configuration,UserGroupInformation)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.getZeroCopyReader(FSDataInputStream,ByteBufferPoolShim)",2,2,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.hflush(FSDataOutputStream)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.isExtendedAclEnabled(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.isLocalMode(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.listLocatedStatus(FileSystem,Path,PathFilter)",1,4,4
"org.apache.hadoop.hive.shims.Hadoop23Shims.moveToAppropriateTrash(FileSystem,Path,Configuration)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.newAclEntry(AclEntryScope,AclEntryType,FsAction)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.newJobContext(Job)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.newTaskAttemptContext(Configuration,Progressable)",1,1,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.newTaskAttemptID(JobID,boolean,int,int)",1,1,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.removeBaseAclEntries(List<AclEntry>)",2,1,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.setFullFileStatus(Configuration,HdfsFileStatus,FileSystem,Path)",1,3,5
"org.apache.hadoop.hive.shims.Hadoop23Shims.setJobLauncherRpcAddress(Configuration,String)",1,2,2
"org.apache.hadoop.hive.shims.Hadoop23Shims.setTotalOrderPartitionFile(JobConf,Path)",1,1,1
"org.apache.hadoop.hive.shims.Hadoop23Shims.wrapAccessException(Exception)",3,3,5
"org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileInputFormatShim.createPool(JobConf,PathFilter...)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileInputFormatShim.getInputPathsShim(JobConf)",1,1,2
"org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileInputFormatShim.getInputSplitShim()",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileInputFormatShim.getRecordReader(JobConf,InputSplitShim,Reporter,Class<RecordReader<K, V>>)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileInputFormatShim.getSplits(JobConf,int)",1,6,6
"org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileRecordReader.CombineFileRecordReader(JobConf,CombineFileSplit,Reporter,Class<RecordReader<K, V>>)",1,3,3
"org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileRecordReader.close()",1,2,2
"org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileRecordReader.createKey()",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileRecordReader.createValue()",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileRecordReader.doNextWithExceptionHandler(K,V)",1,2,2
"org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileRecordReader.getPos()",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileRecordReader.getProgress()",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileRecordReader.initNextRecordReader(K)",2,6,8
"org.apache.hadoop.hive.shims.HadoopShimsSecure.CombineFileRecordReader.next(K,V)",3,3,4
"org.apache.hadoop.hive.shims.HadoopShimsSecure.InputSplitShim.InputSplitShim()",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.InputSplitShim.InputSplitShim(JobConf,Path[],long[],long[],String[])",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.InputSplitShim.getShrinkedLength()",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.InputSplitShim.isShrinked()",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.InputSplitShim.readFields(DataInput)",1,2,2
"org.apache.hadoop.hive.shims.HadoopShimsSecure.InputSplitShim.shrinkSplit(long)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.InputSplitShim.write(DataOutput)",1,2,2
"org.apache.hadoop.hive.shims.HadoopShimsSecure.NullOutputCommitter.abortTask(TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.NullOutputCommitter.cleanupJob(JobContext)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.NullOutputCommitter.commitTask(TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.NullOutputCommitter.needsTaskCommit(TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.NullOutputCommitter.setupJob(JobContext)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.NullOutputCommitter.setupTask(TaskAttemptContext)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.addServiceToToken(String,String)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.authorizeProxyAccess(String,UserGroupInformation,String,Configuration)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.checkFileAccess(FileSystem,FileStatus,FsAction)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.closeAllForUGI(UserGroupInformation)",1,2,2
"org.apache.hadoop.hive.shims.HadoopShimsSecure.createDelegationTokenFile(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.createHadoopArchive(Configuration,Path,Path,String)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.createProxyUser(String)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.createRemoteUser(String,List<String>)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.createToken(String,String)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.dedup(String[])",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(UserGroupInformation,PrivilegedExceptionAction<T>)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.getCombineFileInputFormat()",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.getDirIndices(Path[],JobConf)",1,3,3
"org.apache.hadoop.hive.shims.HadoopShimsSecure.getHarUri(URI,URI,URI)",2,1,2
"org.apache.hadoop.hive.shims.HadoopShimsSecure.getInputFormatClassName()",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.getShortUserName(UserGroupInformation)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.getTokenFileLocEnvName()",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.getTokenStrForm(String)",1,2,3
"org.apache.hadoop.hive.shims.HadoopShimsSecure.getUGIForConf(Configuration)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.isLoginKeytabBased()",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.isSecureShimImpl()",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.isSecurityEnabled()",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.loginUserFromKeytab(String,String)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.loginUserFromKeytabAndReturnUGI(String,String)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.prepareJobOutput(JobConf)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.prune(Set<Integer>,List<K>)",3,2,3
"org.apache.hadoop.hive.shims.HadoopShimsSecure.reLoginUserFromKeytab()",1,2,2
"org.apache.hadoop.hive.shims.HadoopShimsSecure.run(FsShell,String[])",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.setTokenStr(UserGroupInformation,String,String)",1,1,1
"org.apache.hadoop.hive.shims.HadoopShimsSecure.unquoteHtmlChars(String)",1,1,1
"org.apache.hadoop.hive.shims.HiveEventCounter.HiveEventCounter()",1,1,1
"org.apache.hadoop.hive.shims.HiveEventCounter.activateOptions()",1,1,1
"org.apache.hadoop.hive.shims.HiveEventCounter.addFilter(Filter)",1,1,1
"org.apache.hadoop.hive.shims.HiveEventCounter.clearFilters()",1,1,1
"org.apache.hadoop.hive.shims.HiveEventCounter.close()",1,1,1
"org.apache.hadoop.hive.shims.HiveEventCounter.doAppend(LoggingEvent)",1,1,1
"org.apache.hadoop.hive.shims.HiveEventCounter.getErrorHandler()",1,1,1
"org.apache.hadoop.hive.shims.HiveEventCounter.getFilter()",1,1,1
"org.apache.hadoop.hive.shims.HiveEventCounter.getLayout()",1,1,1
"org.apache.hadoop.hive.shims.HiveEventCounter.getName()",1,1,1
"org.apache.hadoop.hive.shims.HiveEventCounter.requiresLayout()",1,1,1
"org.apache.hadoop.hive.shims.HiveEventCounter.setErrorHandler(ErrorHandler)",1,1,1
"org.apache.hadoop.hive.shims.HiveEventCounter.setLayout(Layout)",1,1,1
"org.apache.hadoop.hive.shims.HiveEventCounter.setName(String)",1,1,1
"org.apache.hadoop.hive.shims.HiveHarFileSystem.getContentSummary(Path)",2,4,4
"org.apache.hadoop.hive.shims.HiveHarFileSystem.getFileBlockLocations(FileStatus,long,long)",1,1,1
"org.apache.hadoop.hive.shims.Jetty20SShims.Server.addWar(String,String)",1,1,1
"org.apache.hadoop.hive.shims.Jetty20SShims.Server.setupListenerHostPort(String,int)",1,1,1
"org.apache.hadoop.hive.shims.Jetty20SShims.startServer(String,int)",1,1,1
"org.apache.hadoop.hive.shims.Jetty20Shims.Server.addWar(String,String)",1,1,1
"org.apache.hadoop.hive.shims.Jetty20Shims.Server.setupListenerHostPort(String,int)",1,1,1
"org.apache.hadoop.hive.shims.Jetty20Shims.startServer(String,int)",1,1,1
"org.apache.hadoop.hive.shims.Jetty23Shims.Server.addWar(String,String)",1,1,1
"org.apache.hadoop.hive.shims.Jetty23Shims.Server.setupListenerHostPort(String,int)",1,1,1
"org.apache.hadoop.hive.shims.Jetty23Shims.startServer(String,int)",1,1,1
"org.apache.hadoop.hive.shims.ShimLoader.ShimLoader()",1,1,1
"org.apache.hadoop.hive.shims.ShimLoader.createShim(String,Class<T>)",1,1,2
"org.apache.hadoop.hive.shims.ShimLoader.getEventCounter()",1,2,2
"org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims()",1,2,2
"org.apache.hadoop.hive.shims.ShimLoader.getHadoopThriftAuthBridge()",1,2,2
"org.apache.hadoop.hive.shims.ShimLoader.getJettyShims()",1,2,2
"org.apache.hadoop.hive.shims.ShimLoader.getMajorVersion()",5,3,8
"org.apache.hadoop.hive.shims.ShimLoader.loadShims(Map<String, String>,Class<T>)",1,1,1
"org.apache.hadoop.hive.shims.ZeroCopyShims.ByteBufferPoolAdapter.ByteBufferPoolAdapter(ByteBufferPoolShim)",1,1,1
"org.apache.hadoop.hive.shims.ZeroCopyShims.ByteBufferPoolAdapter.getBuffer(boolean,int)",1,1,1
"org.apache.hadoop.hive.shims.ZeroCopyShims.ByteBufferPoolAdapter.putBuffer(ByteBuffer)",1,1,1
"org.apache.hadoop.hive.shims.ZeroCopyShims.DirectDecompressorAdapter.DirectDecompressorAdapter(DirectDecompressor)",1,1,1
"org.apache.hadoop.hive.shims.ZeroCopyShims.DirectDecompressorAdapter.decompress(ByteBuffer,ByteBuffer)",1,1,1
"org.apache.hadoop.hive.shims.ZeroCopyShims.ZeroCopyAdapter.ZeroCopyAdapter(FSDataInputStream,ByteBufferPoolShim)",1,1,2
"org.apache.hadoop.hive.shims.ZeroCopyShims.ZeroCopyAdapter.readBuffer(int,boolean)",1,1,2
"org.apache.hadoop.hive.shims.ZeroCopyShims.ZeroCopyAdapter.releaseBuffer(ByteBuffer)",1,1,1
"org.apache.hadoop.hive.shims.ZeroCopyShims.getDirectDecompressor(DirectCompressionType)",3,2,5
"org.apache.hadoop.hive.shims.ZeroCopyShims.getZeroCopyReader(FSDataInputStream,ByteBufferPoolShim)",1,1,1
"org.apache.hadoop.hive.thrift.DBTokenStore.addMasterKey(String)",1,1,1
"org.apache.hadoop.hive.thrift.DBTokenStore.addToken(DelegationTokenIdentifier,DelegationTokenInformation)",1,1,2
"org.apache.hadoop.hive.thrift.DBTokenStore.close()",1,1,1
"org.apache.hadoop.hive.thrift.DBTokenStore.getAllDelegationTokenIdentifiers()",2,2,3
"org.apache.hadoop.hive.thrift.DBTokenStore.getConf()",1,1,1
"org.apache.hadoop.hive.thrift.DBTokenStore.getMasterKeys()",1,1,1
"org.apache.hadoop.hive.thrift.DBTokenStore.getToken(DelegationTokenIdentifier)",1,2,3
"org.apache.hadoop.hive.thrift.DBTokenStore.invokeOnRawStore(String,Object[],Class<?>...)",1,2,6
"org.apache.hadoop.hive.thrift.DBTokenStore.removeMasterKey(int)",1,1,1
"org.apache.hadoop.hive.thrift.DBTokenStore.removeToken(DelegationTokenIdentifier)",1,1,2
"org.apache.hadoop.hive.thrift.DBTokenStore.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.thrift.DBTokenStore.setStore(Object)",1,1,1
"org.apache.hadoop.hive.thrift.DBTokenStore.updateMasterKey(int,String)",1,1,1
"org.apache.hadoop.hive.thrift.DelegationTokenIdentifier.DelegationTokenIdentifier()",1,1,1
"org.apache.hadoop.hive.thrift.DelegationTokenIdentifier.DelegationTokenIdentifier(Text,Text,Text)",1,1,1
"org.apache.hadoop.hive.thrift.DelegationTokenIdentifier.getKind()",1,1,1
"org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.DelegationTokenSecretManager(long,long,long,long)",1,1,1
"org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.cancelDelegationToken(String)",1,1,1
"org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.createIdentifier()",1,1,1
"org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.getDelegationToken(String)",1,2,2
"org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.getUserFromToken(String)",1,1,1
"org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.renewDelegationToken(String)",1,1,1
"org.apache.hadoop.hive.thrift.DelegationTokenSelector.DelegationTokenSelector()",1,1,1
"org.apache.hadoop.hive.thrift.DelegationTokenStore.TokenStoreException.TokenStoreException(String,Throwable)",1,1,1
"org.apache.hadoop.hive.thrift.DelegationTokenStore.TokenStoreException.TokenStoreException(Throwable)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.createClient()",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.createClientWithConf(String)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.createServer(String,String)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.getCurrentUGIWithConf(String)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.getHadoopSaslProperties(Configuration)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge.getServerPrincipal(String,String)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Client.SaslClientCallbackHandler.SaslClientCallbackHandler(Token<? extends TokenIdentifier>)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Client.SaslClientCallbackHandler.encodeIdentifier(byte[])",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Client.SaslClientCallbackHandler.encodePassword(byte[])",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Client.SaslClientCallbackHandler.handle(Callback[])",6,7,12
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Client.createClientTransport(String,String,String,String,TTransport,Map<String, String>)",3,2,6
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.SaslDigestCallbackHandler.SaslDigestCallbackHandler(DelegationTokenSecretManager)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.SaslDigestCallbackHandler.encodePassword(byte[])",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.SaslDigestCallbackHandler.getPassword(DelegationTokenIdentifier)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.SaslDigestCallbackHandler.handle(Callback[])",6,7,12
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.Server()",1,1,2
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.Server(String,String)",3,3,6
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.TUGIAssumingProcessor.TUGIAssumingProcessor(TProcessor,DelegationTokenSecretManager,boolean)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.TUGIAssumingProcessor.process(TProtocol,TProtocol)",5,9,12
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.TUGIAssumingTransportFactory.TUGIAssumingTransportFactory(TTransportFactory,UserGroupInformation)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.TUGIAssumingTransportFactory.getTransport(TTransport)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.cancelDelegationToken(String)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.createTransportFactory(Map<String, String>)",2,1,2
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.getDelegationToken(String,String)",2,3,3
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.getDelegationTokenWithService(String,String,String)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.getRemoteAddress()",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.getRemoteUser()",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.getTokenStore(Configuration)",2,1,3
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.getUserFromToken(String)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.renewDelegationToken(String)",2,2,2
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.startDelegationTokenSecretManager(Configuration,Object)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.wrapNonAssumingProcessor(TProcessor)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.Server.wrapProcessor(TProcessor)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.createClient()",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.createClientWithConf(String)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.createServer(String,String)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.getCurrentUGIWithConf(String)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.getHadoopSaslProperties(Configuration)",1,1,1
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.getServerPrincipal(String,String)",2,1,2
"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge23.getHadoopSaslProperties(Configuration)",2,2,4
"org.apache.hadoop.hive.thrift.MemoryTokenStore.addMasterKey(String)",1,1,1
"org.apache.hadoop.hive.thrift.MemoryTokenStore.addToken(DelegationTokenIdentifier,DelegationTokenInformation)",1,1,1
"org.apache.hadoop.hive.thrift.MemoryTokenStore.close()",1,1,1
"org.apache.hadoop.hive.thrift.MemoryTokenStore.getAllDelegationTokenIdentifiers()",1,2,2
"org.apache.hadoop.hive.thrift.MemoryTokenStore.getConf()",1,1,1
"org.apache.hadoop.hive.thrift.MemoryTokenStore.getMasterKeys()",1,1,1
"org.apache.hadoop.hive.thrift.MemoryTokenStore.getToken(DelegationTokenIdentifier)",1,1,1
"org.apache.hadoop.hive.thrift.MemoryTokenStore.removeMasterKey(int)",1,1,1
"org.apache.hadoop.hive.thrift.MemoryTokenStore.removeToken(DelegationTokenIdentifier)",1,1,1
"org.apache.hadoop.hive.thrift.MemoryTokenStore.setConf(Configuration)",1,1,1
"org.apache.hadoop.hive.thrift.MemoryTokenStore.setStore(Object)",1,1,1
"org.apache.hadoop.hive.thrift.MemoryTokenStore.updateMasterKey(int,String)",1,1,1
"org.apache.hadoop.hive.thrift.TFilterTransport.TFilterTransport(TTransport)",1,1,1
"org.apache.hadoop.hive.thrift.TFilterTransport.close()",1,1,1
"org.apache.hadoop.hive.thrift.TFilterTransport.consumeBuffer(int)",1,1,1
"org.apache.hadoop.hive.thrift.TFilterTransport.flush()",1,1,1
"org.apache.hadoop.hive.thrift.TFilterTransport.getBuffer()",1,1,1
"org.apache.hadoop.hive.thrift.TFilterTransport.getBufferPosition()",1,1,1
"org.apache.hadoop.hive.thrift.TFilterTransport.getBytesRemainingInBuffer()",1,1,1
"org.apache.hadoop.hive.thrift.TFilterTransport.isOpen()",1,1,1
"org.apache.hadoop.hive.thrift.TFilterTransport.open()",1,1,1
"org.apache.hadoop.hive.thrift.TFilterTransport.peek()",1,1,1
"org.apache.hadoop.hive.thrift.TFilterTransport.read(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.thrift.TFilterTransport.readAll(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.thrift.TFilterTransport.write(byte[])",1,1,1
"org.apache.hadoop.hive.thrift.TFilterTransport.write(byte[],int,int)",1,1,1
"org.apache.hadoop.hive.thrift.TUGIContainingTransport.Factory.getTransport(TTransport)",3,2,3
"org.apache.hadoop.hive.thrift.TUGIContainingTransport.TUGIContainingTransport(TTransport)",1,1,1
"org.apache.hadoop.hive.thrift.TUGIContainingTransport.getClientUGI()",1,1,1
"org.apache.hadoop.hive.thrift.TUGIContainingTransport.getSocket()",2,2,2
"org.apache.hadoop.hive.thrift.TUGIContainingTransport.setClientUGI(UserGroupInformation)",1,1,1
"org.apache.hadoop.hive.thrift.TestDBTokenStore.testDBTokenStore()",1,2,2
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.MyHadoopThriftAuthBridge20S.Server.Server()",1,1,1
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.MyHadoopThriftAuthBridge20S.Server.createTransportFactory(Map<String, String>)",1,1,1
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.MyHadoopThriftAuthBridge20S.Server.getTokenStore(Configuration)",1,1,1
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.MyHadoopThriftAuthBridge20S.Server.startDelegationTokenSecretManager(Configuration,Object)",1,1,1
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.MyHadoopThriftAuthBridge20S.createServer(String,String)",1,1,1
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.configureSuperUserIPAddresses(Configuration,String)",1,4,4
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.createDBAndVerifyExistence(HiveMetaStoreClient)",1,1,1
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.findFreePort()",1,1,1
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.getDelegationTokenStr(UserGroupInformation,UserGroupInformation)",1,1,1
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.obtainTokenAndAddIntoUGI(UserGroupInformation,String)",1,2,4
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.setGroupsInConf(String[],String)",1,1,1
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.setup()",1,1,1
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.testDelegationTokenSharedStore()",1,1,2
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.testMetastoreProxyUser()",1,1,3
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.testSaslWithHiveMetaStore()",1,1,1
"org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.waitForMetastoreTokenInit()",1,2,3
"org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.createConf(String)",1,1,1
"org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.setUp()",2,1,2
"org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.tearDown()",1,2,2
"org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.testAclInvalid()",1,2,2
"org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.testAclNoAuth()",1,2,2
"org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.testAclPositive()",1,1,1
"org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.testTokenStorage()",1,1,1
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.ExpiredTokenRemover.run()",1,7,7
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.TokenStoreDelegationTokenSecretManager(long,long,long,long,DelegationTokenStore)",1,1,1
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.cancelToken(Token<DelegationTokenIdentifier>,String)",1,1,1
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.createPassword(DelegationTokenIdentifier)",2,1,2
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.decodeWritable(Writable,String)",1,1,1
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.encodeWritable(Writable)",1,1,1
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.getTokenIdentifier(Token<DelegationTokenIdentifier>)",1,1,1
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.logUpdateMasterKey(DelegationKey)",1,1,1
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.reloadKeys()",1,3,3
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.removeExpiredTokens()",1,5,5
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.renewToken(Token<DelegationTokenIdentifier>,String)",2,2,3
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.retrievePassword(DelegationTokenIdentifier)",2,1,2
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.rollMasterKeyExt()",1,4,4
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.startThreads()",1,1,2
"org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.stopThreads()",1,3,3
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.ZooKeeperTokenStore()",1,1,1
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.ZooKeeperTokenStore(String)",1,1,1
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.ZooKeeperWatcher.process(WatchedEvent)",1,3,3
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.addMasterKey(String)",1,1,3
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.addToken(DelegationTokenIdentifier,DelegationTokenInformation)",1,1,4
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.close()",1,3,3
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.createConnectedClient(String,int,long,Watcher...)",4,5,6
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.ensurePath(ZooKeeper,String,List<ACL>)",1,2,3
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getAllDelegationTokenIdentifiers()",1,3,5
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getAllKeys()",1,3,3
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getConf()",1,1,1
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getMasterKeys()",1,1,4
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getPermFromString(String)",2,3,8
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getSeq(String)",1,1,1
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getSession()",3,5,6
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(DelegationTokenIdentifier)",1,1,5
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getTokenPath(DelegationTokenIdentifier)",1,1,2
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.init()",2,3,5
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.parseACLs(String)",4,3,6
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.removeMasterKey(int)",1,1,4
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.removeToken(DelegationTokenIdentifier)",1,1,4
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.setConf(Configuration)",2,2,3
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.setStore(Object)",1,1,1
"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.updateMasterKey(int,String)",1,1,3
"org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.TUGIAssumingTransport(TTransport,UserGroupInformation)",1,1,1
"org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open()",2,3,6
"org.apache.hadoop.hive.tools.generate.RCFileGenerator.genData(String,int,String,String)",1,9,10
"org.apache.hadoop.hive.tools.generate.RCFileGenerator.getFile(String)",1,1,1
"org.apache.hadoop.hive.tools.generate.RCFileGenerator.main(String[])",1,3,3
"org.apache.hadoop.hive.tools.generate.RCFileGenerator.randomAge()",1,1,1
"org.apache.hadoop.hive.tools.generate.RCFileGenerator.randomArray()",1,3,3
"org.apache.hadoop.hive.tools.generate.RCFileGenerator.randomContribution()",1,1,1
"org.apache.hadoop.hive.tools.generate.RCFileGenerator.randomGpa()",1,1,1
"org.apache.hadoop.hive.tools.generate.RCFileGenerator.randomMap()",1,3,3
"org.apache.hadoop.hive.tools.generate.RCFileGenerator.randomName()",1,1,1
"org.apache.hadoop.hive.tools.generate.RCFileGenerator.randomRegistration()",1,1,1
"org.apache.hadoop.hive.tools.generate.RCFileGenerator.usage()",1,1,1
"org.apache.hadoop.mapred.WebHCatJTShim20S.WebHCatJTShim20S(Configuration,UserGroupInformation)",1,1,1
"org.apache.hadoop.mapred.WebHCatJTShim20S.addCacheFile(URI,Job)",1,1,1
"org.apache.hadoop.mapred.WebHCatJTShim20S.close()",1,1,1
"org.apache.hadoop.mapred.WebHCatJTShim20S.getAddress(Configuration)",1,1,1
"org.apache.hadoop.mapred.WebHCatJTShim20S.getAllJobs()",1,1,1
"org.apache.hadoop.mapred.WebHCatJTShim20S.getJobProfile(JobID)",1,1,1
"org.apache.hadoop.mapred.WebHCatJTShim20S.getJobStatus(JobID)",1,1,1
"org.apache.hadoop.mapred.WebHCatJTShim20S.killJob(JobID)",1,1,1
"org.apache.hadoop.mapred.WebHCatJTShim20S.killJobs(String,long)",1,1,1
"org.apache.hadoop.mapred.WebHCatJTShim23.WebHCatJTShim23(Configuration,UserGroupInformation)",1,1,2
"org.apache.hadoop.mapred.WebHCatJTShim23.addCacheFile(URI,Job)",1,1,1
"org.apache.hadoop.mapred.WebHCatJTShim23.close()",1,1,2
"org.apache.hadoop.mapred.WebHCatJTShim23.getAllJobs()",1,1,1
"org.apache.hadoop.mapred.WebHCatJTShim23.getJob(JobID)",2,4,4
"org.apache.hadoop.mapred.WebHCatJTShim23.getJobProfile(JobID)",2,1,2
"org.apache.hadoop.mapred.WebHCatJTShim23.getJobStatus(JobID)",2,1,2
"org.apache.hadoop.mapred.WebHCatJTShim23.getYarnChildJobs(String,long)",1,2,4
"org.apache.hadoop.mapred.WebHCatJTShim23.killJob(JobID)",2,1,2
"org.apache.hadoop.mapred.WebHCatJTShim23.killJobs(String,long)",2,3,5
"org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.HiveDelegationTokenSupport()",1,1,1
"org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.decodeDelegationTokenInformation(byte[])",1,1,1
"org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.encodeDelegationTokenInformation(DelegationTokenInformation)",1,1,2
"org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.rollMasterKey(AbstractDelegationTokenSecretManager<? extends AbstractDelegationTokenIdentifier>)",1,1,1
"org.apache.hive.TestDummy.testDummy()",1,1,1
"org.apache.hive.beeline.AbstractCommandHandler.AbstractCommandHandler(BeeLine,String[],String,Completor[])",1,2,3
"org.apache.hive.beeline.AbstractCommandHandler.getHelpText()",1,1,1
"org.apache.hive.beeline.AbstractCommandHandler.getName()",1,1,1
"org.apache.hive.beeline.AbstractCommandHandler.getNames()",1,1,1
"org.apache.hive.beeline.AbstractCommandHandler.getParameterCompletors()",1,1,1
"org.apache.hive.beeline.AbstractCommandHandler.matches(String)",5,3,7
"org.apache.hive.beeline.AbstractCommandHandler.setParameterCompletors(Completor[])",1,1,1
"org.apache.hive.beeline.AbstractOutputFormat.print(Rows)",1,2,2
"org.apache.hive.beeline.BeeLine.BeeLine()",1,1,1
"org.apache.hive.beeline.BeeLine.BeelineParser.processOption(String,ListIterator)",1,6,6
"org.apache.hive.beeline.BeeLine.assertAutoCommit()",3,3,4
"org.apache.hive.beeline.BeeLine.assertConnection()",3,5,5
"org.apache.hive.beeline.BeeLine.autocommitStatus(Connection)",1,1,1
"org.apache.hive.beeline.BeeLine.begin(String[],InputStream)",3,3,5
"org.apache.hive.beeline.BeeLine.close()",1,1,1
"org.apache.hive.beeline.BeeLine.createStatement()",1,3,3
"org.apache.hive.beeline.BeeLine.debug(String)",1,2,2
"org.apache.hive.beeline.BeeLine.dequote(String)",2,5,6
"org.apache.hive.beeline.BeeLine.dispatch(String)",11,9,13
"org.apache.hive.beeline.BeeLine.error(String)",1,1,1
"org.apache.hive.beeline.BeeLine.error(Throwable)",1,1,1
"org.apache.hive.beeline.BeeLine.execute(ConsoleReader,boolean)",3,4,5
"org.apache.hive.beeline.BeeLine.executeFile(String)",1,2,2
"org.apache.hive.beeline.BeeLine.findRegisteredDriver(String)",3,3,5
"org.apache.hive.beeline.BeeLine.getApplicationContactInformation()",1,1,1
"org.apache.hive.beeline.BeeLine.getApplicationTitle()",1,2,2
"org.apache.hive.beeline.BeeLine.getBatch()",1,1,1
"org.apache.hive.beeline.BeeLine.getColorBuffer()",1,1,1
"org.apache.hive.beeline.BeeLine.getColorBuffer(String)",1,1,1
"org.apache.hive.beeline.BeeLine.getColumnNames(DatabaseMetaData)",1,3,3
"org.apache.hive.beeline.BeeLine.getColumns(String)",2,1,2
"org.apache.hive.beeline.BeeLine.getCommandCompletor()",1,1,1
"org.apache.hive.beeline.BeeLine.getCommands()",1,1,1
"org.apache.hive.beeline.BeeLine.getConnection()",3,3,3
"org.apache.hive.beeline.BeeLine.getConnectionURLExamples()",1,1,1
"org.apache.hive.beeline.BeeLine.getConsoleReader()",1,1,1
"org.apache.hive.beeline.BeeLine.getConsoleReader(InputStream)",2,8,9
"org.apache.hive.beeline.BeeLine.getDatabaseConnection()",1,1,1
"org.apache.hive.beeline.BeeLine.getDatabaseConnections()",1,1,1
"org.apache.hive.beeline.BeeLine.getDatabaseMetaData()",3,3,3
"org.apache.hive.beeline.BeeLine.getDrivers()",1,1,1
"org.apache.hive.beeline.BeeLine.getErrorStream()",1,1,1
"org.apache.hive.beeline.BeeLine.getIsolationLevels()",1,1,1
"org.apache.hive.beeline.BeeLine.getManifest()",2,2,2
"org.apache.hive.beeline.BeeLine.getManifestAttribute(String)",4,3,6
"org.apache.hive.beeline.BeeLine.getMetadataMethodNames()",1,2,4
"org.apache.hive.beeline.BeeLine.getMoreResults(Statement)",1,1,2
"org.apache.hive.beeline.BeeLine.getOpts()",1,1,1
"org.apache.hive.beeline.BeeLine.getOutputStream()",1,1,1
"org.apache.hive.beeline.BeeLine.getPrompt()",2,3,4
"org.apache.hive.beeline.BeeLine.getPrompt(String)",1,5,6
"org.apache.hive.beeline.BeeLine.getRecordOutputFile()",1,1,1
"org.apache.hive.beeline.BeeLine.getReflector()",1,1,1
"org.apache.hive.beeline.BeeLine.getScriptOutputFile()",1,1,1
"org.apache.hive.beeline.BeeLine.getSeparator()",1,1,1
"org.apache.hive.beeline.BeeLine.getSize(ResultSet)",2,1,4
"org.apache.hive.beeline.BeeLine.getTables()",2,1,2
"org.apache.hive.beeline.BeeLine.handleException(Throwable)",1,6,6
"org.apache.hive.beeline.BeeLine.handleSQLException(SQLException)",3,6,10
"org.apache.hive.beeline.BeeLine.info(ColorBuffer)",1,2,2
"org.apache.hive.beeline.BeeLine.info(String)",1,2,2
"org.apache.hive.beeline.BeeLine.initArgs(String[])",2,11,15
"org.apache.hive.beeline.BeeLine.isComment(String)",1,2,2
"org.apache.hive.beeline.BeeLine.isExit()",1,1,1
"org.apache.hive.beeline.BeeLine.isHelpRequest(String)",1,2,2
"org.apache.hive.beeline.BeeLine.loc(String)",1,1,1
"org.apache.hive.beeline.BeeLine.loc(String,Object)",1,1,1
"org.apache.hive.beeline.BeeLine.loc(String,Object,Object)",1,1,1
"org.apache.hive.beeline.BeeLine.loc(String,Object[])",1,2,3
"org.apache.hive.beeline.BeeLine.loc(String,int)",1,1,2
"org.apache.hive.beeline.BeeLine.locElapsedTime(long)",2,2,2
"org.apache.hive.beeline.BeeLine.main(String[])",1,1,1
"org.apache.hive.beeline.BeeLine.mainWithInputRedirection(String[],InputStream)",1,2,2
"org.apache.hive.beeline.BeeLine.map(Object[])",1,2,2
"org.apache.hive.beeline.BeeLine.needsContinuation(String)",6,1,6
"org.apache.hive.beeline.BeeLine.output(ColorBuffer)",1,1,1
"org.apache.hive.beeline.BeeLine.output(ColorBuffer,boolean)",1,1,1
"org.apache.hive.beeline.BeeLine.output(ColorBuffer,boolean,PrintStream)",2,3,4
"org.apache.hive.beeline.BeeLine.output(String)",1,1,1
"org.apache.hive.beeline.BeeLine.output(String,boolean)",1,1,1
"org.apache.hive.beeline.BeeLine.output(String,boolean,PrintStream)",1,1,1
"org.apache.hive.beeline.BeeLine.print(ResultSet)",1,2,3
"org.apache.hive.beeline.BeeLine.progress(int,int)",1,3,7
"org.apache.hive.beeline.BeeLine.replace(String,String,String)",3,2,4
"org.apache.hive.beeline.BeeLine.runBatch(List<String>)",1,4,6
"org.apache.hive.beeline.BeeLine.runCommands(List<String>)",3,5,5
"org.apache.hive.beeline.BeeLine.runCommands(String[])",1,1,1
"org.apache.hive.beeline.BeeLine.runInit()",2,2,2
"org.apache.hive.beeline.BeeLine.scanDrivers(String)",1,1,1
"org.apache.hive.beeline.BeeLine.scanDrivers(boolean)",5,3,7
"org.apache.hive.beeline.BeeLine.scanDriversOLD(String)",1,10,14
"org.apache.hive.beeline.BeeLine.scanForDriver(String)",4,2,5
"org.apache.hive.beeline.BeeLine.setBatch(List<String>)",1,1,1
"org.apache.hive.beeline.BeeLine.setCompletions()",1,2,2
"org.apache.hive.beeline.BeeLine.setConsoleReader(ConsoleReader)",1,1,1
"org.apache.hive.beeline.BeeLine.setDrivers(Collection<Driver>)",1,1,1
"org.apache.hive.beeline.BeeLine.setErrorStream(PrintStream)",1,1,1
"org.apache.hive.beeline.BeeLine.setExit(boolean)",1,1,1
"org.apache.hive.beeline.BeeLine.setOutputStream(PrintStream)",1,1,1
"org.apache.hive.beeline.BeeLine.setRecordOutputFile(OutputFile)",1,1,1
"org.apache.hive.beeline.BeeLine.setScriptOutputFile(OutputFile)",1,1,1
"org.apache.hive.beeline.BeeLine.showWarnings()",2,3,4
"org.apache.hive.beeline.BeeLine.showWarnings(SQLWarning)",2,3,4
"org.apache.hive.beeline.BeeLine.split(String)",1,1,1
"org.apache.hive.beeline.BeeLine.split(String,String)",1,2,2
"org.apache.hive.beeline.BeeLine.split(String,int,String)",2,2,2
"org.apache.hive.beeline.BeeLine.usage()",1,1,1
"org.apache.hive.beeline.BeeLine.wrap(String,int,int)",1,3,4
"org.apache.hive.beeline.BeeLine.xmlattrencode(String)",1,1,1
"org.apache.hive.beeline.BeeLineCommandCompletor.BeeLineCommandCompletor(BeeLine)",1,3,4
"org.apache.hive.beeline.BeeLineCompletor.BeeLineCompletor(BeeLine)",1,1,1
"org.apache.hive.beeline.BeeLineCompletor.complete(String,int,List)",3,7,7
"org.apache.hive.beeline.BeeLineOpts.BeeLineOpts(BeeLine,Properties)",1,3,3
"org.apache.hive.beeline.BeeLineOpts.complete(String,int,List)",1,2,2
"org.apache.hive.beeline.BeeLineOpts.getAuthType()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getAutoCommit()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getAutosave()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getColor()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getDelimiterForDSV()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getFastConnect()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getForce()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getHeaderInterval()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getHistoryFile()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getHiveConfVariables()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getHiveVariables()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getIncremental()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getInitFile()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getIsolation()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getMaxColumnWidth()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getMaxHeight()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getMaxWidth()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getNullEmptyString()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getNullString()",1,1,2
"org.apache.hive.beeline.BeeLineOpts.getNumberFormat()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getOutputFormat()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getPropertiesFile()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getScriptFile()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getShowElapsedTime()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getShowHeader()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getShowNestedErrs()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getShowWarnings()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getTimeout()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getTrimScripts()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getTruncateTable()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.getVerbose()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.isAllowMultiLineCommand()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.isSilent()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.load()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.load(InputStream)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.loadProperties(Properties)",3,3,4
"org.apache.hive.beeline.BeeLineOpts.optionCompletors()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.possibleSettingValues()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.propertyNames()",4,2,5
"org.apache.hive.beeline.BeeLineOpts.save()",1,1,1
"org.apache.hive.beeline.BeeLineOpts.save(OutputStream)",1,2,2
"org.apache.hive.beeline.BeeLineOpts.saveDir()",2,2,5
"org.apache.hive.beeline.BeeLineOpts.set(String,String)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.set(String,String,boolean)",1,3,3
"org.apache.hive.beeline.BeeLineOpts.setAllowMultiLineCommand(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setAuthType(String)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setAutoCommit(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setAutosave(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setColor(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setDelimiterForDSV(char)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setFastConnect(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setForce(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setHeaderInterval(int)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setHistoryFile(String)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setHiveConfVariables(Map<String, String>)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setHiveVariables(Map<String, String>)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setIncremental(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setInitFile(String)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setIsolation(String)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setMaxColumnWidth(int)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setMaxHeight(int)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setMaxWidth(int)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setNullEmptyString(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setNumberFormat(String)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setOutputFormat(String)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setScriptFile(String)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setShowElapsedTime(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setShowHeader(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setShowNestedErrs(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setShowWarnings(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setSilent(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setTimeout(int)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setTrimScripts(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setTruncateTable(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.setVerbose(boolean)",1,1,1
"org.apache.hive.beeline.BeeLineOpts.toProperties()",1,2,3
"org.apache.hive.beeline.BooleanCompletor.BooleanCompletor()",1,1,1
"org.apache.hive.beeline.BufferedRows.BufferedRows(BeeLine,ResultSet)",1,2,2
"org.apache.hive.beeline.BufferedRows.hasNext()",1,1,1
"org.apache.hive.beeline.BufferedRows.next()",1,1,1
"org.apache.hive.beeline.BufferedRows.normalizeWidths()",1,3,5
"org.apache.hive.beeline.BufferedRows.toString()",1,1,1
"org.apache.hive.beeline.ColorBuffer.ColorAttr.ColorAttr(String)",1,1,1
"org.apache.hive.beeline.ColorBuffer.ColorAttr.toString()",1,1,1
"org.apache.hive.beeline.ColorBuffer.ColorBuffer(String,boolean)",1,1,1
"org.apache.hive.beeline.ColorBuffer.ColorBuffer(boolean)",1,1,1
"org.apache.hive.beeline.ColorBuffer.append(ColorAttr)",1,1,1
"org.apache.hive.beeline.ColorBuffer.append(ColorAttr,String)",1,1,1
"org.apache.hive.beeline.ColorBuffer.append(ColorBuffer)",1,1,1
"org.apache.hive.beeline.ColorBuffer.append(String)",1,1,1
"org.apache.hive.beeline.ColorBuffer.blue(String)",1,1,1
"org.apache.hive.beeline.ColorBuffer.bold(String)",1,1,1
"org.apache.hive.beeline.ColorBuffer.center(String,int)",1,3,3
"org.apache.hive.beeline.ColorBuffer.compareTo(Object)",1,1,1
"org.apache.hive.beeline.ColorBuffer.cyan(String)",1,1,1
"org.apache.hive.beeline.ColorBuffer.getBuffer(boolean)",3,2,4
"org.apache.hive.beeline.ColorBuffer.getColor()",1,1,1
"org.apache.hive.beeline.ColorBuffer.getMono()",1,1,1
"org.apache.hive.beeline.ColorBuffer.getVisibleLength()",1,1,1
"org.apache.hive.beeline.ColorBuffer.green(String)",1,1,1
"org.apache.hive.beeline.ColorBuffer.grey(String)",1,1,1
"org.apache.hive.beeline.ColorBuffer.lined(String)",1,1,1
"org.apache.hive.beeline.ColorBuffer.magenta(String)",1,1,1
"org.apache.hive.beeline.ColorBuffer.pad(ColorBuffer,int)",1,2,2
"org.apache.hive.beeline.ColorBuffer.pad(String,int)",1,1,2
"org.apache.hive.beeline.ColorBuffer.red(String)",1,1,1
"org.apache.hive.beeline.ColorBuffer.toString()",1,1,1
"org.apache.hive.beeline.ColorBuffer.truncate(int)",4,6,8
"org.apache.hive.beeline.ColorBuffer.yellow(String)",1,1,1
"org.apache.hive.beeline.Commands.Commands(BeeLine)",1,1,1
"org.apache.hive.beeline.Commands.all(String)",1,3,3
"org.apache.hive.beeline.Commands.arg1(String,String)",1,1,1
"org.apache.hive.beeline.Commands.arg1(String,String,String)",3,2,5
"org.apache.hive.beeline.Commands.autocommit(String)",2,3,4
"org.apache.hive.beeline.Commands.batch(String)",3,3,4
"org.apache.hive.beeline.Commands.brief(String)",1,1,1
"org.apache.hive.beeline.Commands.call(String)",1,1,1
"org.apache.hive.beeline.Commands.close(String)",2,4,5
"org.apache.hive.beeline.Commands.closeall(String)",2,2,3
"org.apache.hive.beeline.Commands.columns(String)",1,1,1
"org.apache.hive.beeline.Commands.commit(String)",3,2,4
"org.apache.hive.beeline.Commands.config(String)",1,3,3
"org.apache.hive.beeline.Commands.connect(Properties)",4,12,12
"org.apache.hive.beeline.Commands.connect(String)",3,6,11
"org.apache.hive.beeline.Commands.dbinfo(String)",2,3,4
"org.apache.hive.beeline.Commands.describe(String)",3,2,4
"org.apache.hive.beeline.Commands.dropall(String)",3,6,7
"org.apache.hive.beeline.Commands.execute(String,boolean)",4,17,22
"org.apache.hive.beeline.Commands.exportedkeys(String)",1,1,1
"org.apache.hive.beeline.Commands.getProperty(Properties,String[])",6,5,6
"org.apache.hive.beeline.Commands.go(String)",3,2,3
"org.apache.hive.beeline.Commands.help(String)",1,6,7
"org.apache.hive.beeline.Commands.history(String)",1,2,2
"org.apache.hive.beeline.Commands.importedkeys(String)",1,1,1
"org.apache.hive.beeline.Commands.indexes(String)",1,1,1
"org.apache.hive.beeline.Commands.isolation(String)",8,7,13
"org.apache.hive.beeline.Commands.list(String)",1,3,4
"org.apache.hive.beeline.Commands.load(String)",1,1,1
"org.apache.hive.beeline.Commands.manual(String)",5,5,6
"org.apache.hive.beeline.Commands.metadata(String)",2,2,3
"org.apache.hive.beeline.Commands.metadata(String,String[])",3,8,9
"org.apache.hive.beeline.Commands.nativesql(String)",1,3,3
"org.apache.hive.beeline.Commands.outputformat(String)",1,1,1
"org.apache.hive.beeline.Commands.primarykeys(String)",1,1,1
"org.apache.hive.beeline.Commands.procedures(String)",1,1,1
"org.apache.hive.beeline.Commands.properties(String)",3,3,5
"org.apache.hive.beeline.Commands.quit(String)",1,1,1
"org.apache.hive.beeline.Commands.reconnect(String)",2,4,4
"org.apache.hive.beeline.Commands.record(String)",2,2,2
"org.apache.hive.beeline.Commands.rehash(String)",2,3,4
"org.apache.hive.beeline.Commands.rollback(String)",3,2,4
"org.apache.hive.beeline.Commands.run(String)",4,7,10
"org.apache.hive.beeline.Commands.save(String)",1,1,1
"org.apache.hive.beeline.Commands.scan(String)",1,6,7
"org.apache.hive.beeline.Commands.script(String)",2,2,2
"org.apache.hive.beeline.Commands.set(String)",3,6,8
"org.apache.hive.beeline.Commands.sh(String)",4,4,6
"org.apache.hive.beeline.Commands.sql(String)",1,1,1
"org.apache.hive.beeline.Commands.startRecording(String)",3,3,4
"org.apache.hive.beeline.Commands.startScript(String)",3,3,4
"org.apache.hive.beeline.Commands.stopRecording(String)",1,2,2
"org.apache.hive.beeline.Commands.stopScript(String)",1,2,2
"org.apache.hive.beeline.Commands.tables(String)",1,1,1
"org.apache.hive.beeline.Commands.typeinfo(String)",1,1,1
"org.apache.hive.beeline.Commands.verbose(String)",1,1,1
"org.apache.hive.beeline.DatabaseConnection.DatabaseConnection(BeeLine,String,String,Properties)",1,1,1
"org.apache.hive.beeline.DatabaseConnection.Schema.Table.Column.Column(String)",1,1,1
"org.apache.hive.beeline.DatabaseConnection.Schema.Table.Table(String)",1,1,1
"org.apache.hive.beeline.DatabaseConnection.Schema.Table.getName()",1,1,1
"org.apache.hive.beeline.DatabaseConnection.Schema.getTable(String)",3,2,4
"org.apache.hive.beeline.DatabaseConnection.Schema.getTables()",2,2,5
"org.apache.hive.beeline.DatabaseConnection.close()",1,4,4
"org.apache.hive.beeline.DatabaseConnection.connect()",1,11,12
"org.apache.hive.beeline.DatabaseConnection.getConnection()",2,1,2
"org.apache.hive.beeline.DatabaseConnection.getDatabaseMetaData()",1,1,1
"org.apache.hive.beeline.DatabaseConnection.getSQLCompletor()",1,1,1
"org.apache.hive.beeline.DatabaseConnection.getSchema()",1,1,2
"org.apache.hive.beeline.DatabaseConnection.getTableNames(boolean)",1,2,3
"org.apache.hive.beeline.DatabaseConnection.getUrl()",1,1,1
"org.apache.hive.beeline.DatabaseConnection.isClosed()",1,1,1
"org.apache.hive.beeline.DatabaseConnection.reconnect()",1,1,1
"org.apache.hive.beeline.DatabaseConnection.setCompletions(boolean)",2,5,6
"org.apache.hive.beeline.DatabaseConnection.setConnection(Connection)",1,1,1
"org.apache.hive.beeline.DatabaseConnection.setDatabaseMetaData(DatabaseMetaData)",1,1,1
"org.apache.hive.beeline.DatabaseConnection.toString()",1,1,1
"org.apache.hive.beeline.DatabaseConnections.current()",2,2,2
"org.apache.hive.beeline.DatabaseConnections.getIndex()",1,1,1
"org.apache.hive.beeline.DatabaseConnections.iterator()",1,1,1
"org.apache.hive.beeline.DatabaseConnections.remove()",1,2,3
"org.apache.hive.beeline.DatabaseConnections.setConnection(DatabaseConnection)",1,2,2
"org.apache.hive.beeline.DatabaseConnections.setIndex(int)",2,2,3
"org.apache.hive.beeline.DatabaseConnections.size()",1,1,1
"org.apache.hive.beeline.DriverInfo.DriverInfo(Properties)",1,1,1
"org.apache.hive.beeline.DriverInfo.DriverInfo(String)",1,1,1
"org.apache.hive.beeline.DriverInfo.fromProperties(Properties)",1,1,1
"org.apache.hive.beeline.HiveSchemaHelper.AbstractCommandParser.cleanseCommand(String)",1,2,2
"org.apache.hive.beeline.HiveSchemaHelper.AbstractCommandParser.getDelimiter()",1,1,1
"org.apache.hive.beeline.HiveSchemaHelper.AbstractCommandParser.isNonExecCommand(String)",1,2,2
"org.apache.hive.beeline.HiveSchemaHelper.AbstractCommandParser.isPartialCommand(String)",3,3,5
"org.apache.hive.beeline.HiveSchemaHelper.AbstractCommandParser.needsQuotedIdentifier()",1,1,1
"org.apache.hive.beeline.HiveSchemaHelper.DerbyCommandParser.getScriptName(String)",3,1,3
"org.apache.hive.beeline.HiveSchemaHelper.DerbyCommandParser.isNestedScript(String)",1,1,1
"org.apache.hive.beeline.HiveSchemaHelper.MSSQLCommandParser.getScriptName(String)",2,1,2
"org.apache.hive.beeline.HiveSchemaHelper.MSSQLCommandParser.isNestedScript(String)",1,1,1
"org.apache.hive.beeline.HiveSchemaHelper.MySqlCommandParser.cleanseCommand(String)",1,1,1
"org.apache.hive.beeline.HiveSchemaHelper.MySqlCommandParser.getDelimiter()",1,1,1
"org.apache.hive.beeline.HiveSchemaHelper.MySqlCommandParser.getScriptName(String)",2,1,2
"org.apache.hive.beeline.HiveSchemaHelper.MySqlCommandParser.isNestedScript(String)",1,1,1
"org.apache.hive.beeline.HiveSchemaHelper.MySqlCommandParser.isNonExecCommand(String)",1,4,4
"org.apache.hive.beeline.HiveSchemaHelper.MySqlCommandParser.isPartialCommand(String)",3,2,3
"org.apache.hive.beeline.HiveSchemaHelper.OracleCommandParser.getScriptName(String)",2,1,2
"org.apache.hive.beeline.HiveSchemaHelper.OracleCommandParser.isNestedScript(String)",1,1,1
"org.apache.hive.beeline.HiveSchemaHelper.PostgresCommandParser.getScriptName(String)",2,1,2
"org.apache.hive.beeline.HiveSchemaHelper.PostgresCommandParser.isNestedScript(String)",1,1,1
"org.apache.hive.beeline.HiveSchemaHelper.PostgresCommandParser.needsQuotedIdentifier()",1,1,1
"org.apache.hive.beeline.HiveSchemaHelper.getDbCommandParser(String)",6,5,6
"org.apache.hive.beeline.HiveSchemaTool.HiveSchemaTool(String)",1,1,1
"org.apache.hive.beeline.HiveSchemaTool.HiveSchemaTool(String,HiveConf,String)",2,2,4
"org.apache.hive.beeline.HiveSchemaTool.buildCommand(NestedScriptParser,String,String)",4,4,7
"org.apache.hive.beeline.HiveSchemaTool.doInit()",1,1,1
"org.apache.hive.beeline.HiveSchemaTool.doInit(String)",1,2,3
"org.apache.hive.beeline.HiveSchemaTool.doUpgrade()",2,2,3
"org.apache.hive.beeline.HiveSchemaTool.doUpgrade(String)",2,4,5
"org.apache.hive.beeline.HiveSchemaTool.getConnectionToMetastore(boolean)",2,3,7
"org.apache.hive.beeline.HiveSchemaTool.getHiveConf()",1,1,1
"org.apache.hive.beeline.HiveSchemaTool.getMetaStoreSchemaVersion(Connection)",2,1,4
"org.apache.hive.beeline.HiveSchemaTool.getValidConfVar(ConfVars)",2,2,3
"org.apache.hive.beeline.HiveSchemaTool.initOptions(Options)",1,1,1
"org.apache.hive.beeline.HiveSchemaTool.main(String[])",2,20,20
"org.apache.hive.beeline.HiveSchemaTool.printAndExit(Options)",1,1,1
"org.apache.hive.beeline.HiveSchemaTool.runBeeLine(String)",2,2,3
"org.apache.hive.beeline.HiveSchemaTool.runBeeLine(String,String)",1,1,1
"org.apache.hive.beeline.HiveSchemaTool.setDryRun(boolean)",1,1,1
"org.apache.hive.beeline.HiveSchemaTool.setPassWord(String)",1,1,1
"org.apache.hive.beeline.HiveSchemaTool.setUserName(String)",1,1,1
"org.apache.hive.beeline.HiveSchemaTool.setVerbose(boolean)",1,1,1
"org.apache.hive.beeline.HiveSchemaTool.showInfo()",1,1,1
"org.apache.hive.beeline.HiveSchemaTool.testConnectionToMetastore()",1,1,2
"org.apache.hive.beeline.HiveSchemaTool.verifySchemaVersion()",3,1,3
"org.apache.hive.beeline.IncrementalRows.IncrementalRows(BeeLine,ResultSet)",1,2,2
"org.apache.hive.beeline.IncrementalRows.hasNext()",3,3,6
"org.apache.hive.beeline.IncrementalRows.next()",2,1,2
"org.apache.hive.beeline.IncrementalRows.normalizeWidths()",1,1,1
"org.apache.hive.beeline.OutputFile.OutputFile(String)",1,1,1
"org.apache.hive.beeline.OutputFile.addLine(String)",1,1,1
"org.apache.hive.beeline.OutputFile.close()",1,1,1
"org.apache.hive.beeline.OutputFile.print(String)",1,1,1
"org.apache.hive.beeline.OutputFile.println(String)",1,1,1
"org.apache.hive.beeline.OutputFile.toString()",1,1,1
"org.apache.hive.beeline.ProxyAuthTest.cleanUp()",1,2,2
"org.apache.hive.beeline.ProxyAuthTest.compareResults(File)",2,2,2
"org.apache.hive.beeline.ProxyAuthTest.exStatement(String)",1,2,2
"org.apache.hive.beeline.ProxyAuthTest.generateData()",1,2,2
"org.apache.hive.beeline.ProxyAuthTest.generateSQL(String)",1,1,1
"org.apache.hive.beeline.ProxyAuthTest.main(String[])",1,4,6
"org.apache.hive.beeline.ProxyAuthTest.runDMLs()",1,2,2
"org.apache.hive.beeline.ProxyAuthTest.runQuery(String)",1,4,4
"org.apache.hive.beeline.ProxyAuthTest.runTest()",1,3,3
"org.apache.hive.beeline.ProxyAuthTest.storeTokenInJobConf(String)",1,1,1
"org.apache.hive.beeline.ProxyAuthTest.writeArrayToByteStream(String)",1,6,6
"org.apache.hive.beeline.ProxyAuthTest.writeCmdLine(String,OutputStream)",1,1,1
"org.apache.hive.beeline.ProxyAuthTest.writeSqlLine(String,OutputStream)",1,1,1
"org.apache.hive.beeline.ReflectiveCommandHandler.ReflectiveCommandHandler(BeeLine,String[],Completor[])",1,1,1
"org.apache.hive.beeline.ReflectiveCommandHandler.execute(String)",1,4,4
"org.apache.hive.beeline.Reflector.Reflector(BeeLine)",1,1,1
"org.apache.hive.beeline.Reflector.convert(List,Class[])",1,2,2
"org.apache.hive.beeline.Reflector.convert(Object,Class)",12,16,25
"org.apache.hive.beeline.Reflector.invoke(Object,Class,String,List)",6,6,9
"org.apache.hive.beeline.Reflector.invoke(Object,String,List)",1,2,2
"org.apache.hive.beeline.Reflector.invoke(Object,String,Object[])",1,1,1
"org.apache.hive.beeline.Rows.Row.Row(int)",1,3,3
"org.apache.hive.beeline.Rows.Row.Row(int,ResultSet)",1,5,9
"org.apache.hive.beeline.Rows.Row.toString()",1,1,1
"org.apache.hive.beeline.Rows.Rows(BeeLine,ResultSet)",1,2,2
"org.apache.hive.beeline.Rows.isPrimaryKey(int)",5,9,9
"org.apache.hive.beeline.Rows.remove()",1,1,1
"org.apache.hive.beeline.SQLCompletor.SQLCompletor(BeeLine,boolean)",1,3,10
"org.apache.hive.beeline.SeparatedValuesOutputFormat.SeparatedValuesOutputFormat(BeeLine,char)",1,1,1
"org.apache.hive.beeline.SeparatedValuesOutputFormat.getFormattedStr(String[])",1,3,3
"org.apache.hive.beeline.SeparatedValuesOutputFormat.print(Rows)",1,2,2
"org.apache.hive.beeline.SeparatedValuesOutputFormat.printRow(Rows,Row)",1,1,1
"org.apache.hive.beeline.SeparatedValuesOutputFormat.updateCsvPreference()",1,3,3
"org.apache.hive.beeline.SunSignalHandler.SunSignalHandler()",1,1,1
"org.apache.hive.beeline.SunSignalHandler.handle(Signal)",1,2,3
"org.apache.hive.beeline.SunSignalHandler.setStatement(Statement)",1,1,1
"org.apache.hive.beeline.TableNameCompletor.TableNameCompletor(BeeLine)",1,1,1
"org.apache.hive.beeline.TableNameCompletor.complete(String,int,List)",2,1,2
"org.apache.hive.beeline.TableOutputFormat.TableOutputFormat(BeeLine)",1,1,1
"org.apache.hive.beeline.TableOutputFormat.getOutputString(Rows,Row)",1,1,1
"org.apache.hive.beeline.TableOutputFormat.getOutputString(Rows,Row,String)",1,9,9
"org.apache.hive.beeline.TableOutputFormat.print(Rows)",1,14,14
"org.apache.hive.beeline.TableOutputFormat.printRow(ColorBuffer,boolean)",1,2,2
"org.apache.hive.beeline.TestBeeLineWithArgs.createTable()",1,2,2
"org.apache.hive.beeline.TestBeeLineWithArgs.getBaseArgs(String)",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.postTests()",1,3,3
"org.apache.hive.beeline.TestBeeLineWithArgs.preTests()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testBeelineHiveConfVariable()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testBeelineHiveVariable()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testBeelineMultiHiveVariable()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testBeelineShellCommand()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testBreakOnErrorScriptFile()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testCommandLineScript(List<String>,InputStream)",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testEmbeddedBeelineConnection()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testGetVariableValue()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testHiveVarSubstitution()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testNPE()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testNegativeScriptFile()",1,3,3
"org.apache.hive.beeline.TestBeeLineWithArgs.testNullDefault()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testNullEmpty()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testNullEmptyCmdArg()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testNullNonEmpty()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testPositiveScriptFile()",1,1,1
"org.apache.hive.beeline.TestBeeLineWithArgs.testScriptFile(String,String,String,boolean,List<String>)",1,3,5
"org.apache.hive.beeline.TestBeeLineWithArgs.testWhitespaceBeforeCommentScriptFile()",1,1,1
"org.apache.hive.beeline.TestBeelineArgParsing.TestBeeline.dispatch(String)",1,3,3
"org.apache.hive.beeline.TestBeelineArgParsing.testBeelineOpts()",1,1,1
"org.apache.hive.beeline.TestBeelineArgParsing.testDuplicateArgs()",1,1,1
"org.apache.hive.beeline.TestBeelineArgParsing.testHelp()",1,1,1
"org.apache.hive.beeline.TestBeelineArgParsing.testHiveConfAndVars()",1,1,1
"org.apache.hive.beeline.TestBeelineArgParsing.testQueryScripts()",1,1,1
"org.apache.hive.beeline.TestBeelineArgParsing.testScriptFile()",1,1,1
"org.apache.hive.beeline.TestBeelineArgParsing.testSimpleArgs()",1,1,1
"org.apache.hive.beeline.TestBeelineArgParsing.testUnmatchedArgs()",1,1,1
"org.apache.hive.beeline.TestSchemaTool.generateTestScript(String[])",1,2,2
"org.apache.hive.beeline.TestSchemaTool.setUp()",1,1,1
"org.apache.hive.beeline.TestSchemaTool.tearDown()",1,2,2
"org.apache.hive.beeline.TestSchemaTool.testNestedScriptsForDerby()",1,1,1
"org.apache.hive.beeline.TestSchemaTool.testNestedScriptsForMySQL()",1,1,1
"org.apache.hive.beeline.TestSchemaTool.testNestedScriptsForOracle()",1,1,1
"org.apache.hive.beeline.TestSchemaTool.testSchemaInit()",1,1,1
"org.apache.hive.beeline.TestSchemaTool.testSchemaInitDryRun()",1,1,2
"org.apache.hive.beeline.TestSchemaTool.testSchemaUpgrade()",2,1,3
"org.apache.hive.beeline.TestSchemaTool.testSchemaUpgradeDryRun()",1,1,2
"org.apache.hive.beeline.TestSchemaTool.testScriptMultiRowComment()",1,1,1
"org.apache.hive.beeline.TestSchemaTool.testScriptWithDelimiter()",1,1,1
"org.apache.hive.beeline.TestSchemaTool.testScripts()",1,1,1
"org.apache.hive.beeline.VerticalOutputFormat.VerticalOutputFormat(BeeLine)",1,1,1
"org.apache.hive.beeline.VerticalOutputFormat.print(Rows)",1,2,2
"org.apache.hive.beeline.VerticalOutputFormat.printRow(Rows,Row,Row)",1,3,6
"org.apache.hive.beeline.XMLAttributeOutputFormat.XMLAttributeOutputFormat(BeeLine)",1,1,1
"org.apache.hive.beeline.XMLAttributeOutputFormat.printFooter(Row)",1,1,1
"org.apache.hive.beeline.XMLAttributeOutputFormat.printHeader(Row)",1,1,1
"org.apache.hive.beeline.XMLAttributeOutputFormat.printRow(Rows,Row,Row)",1,2,3
"org.apache.hive.beeline.XMLElementOutputFormat.XMLElementOutputFormat(BeeLine)",1,1,1
"org.apache.hive.beeline.XMLElementOutputFormat.printFooter(Row)",1,1,1
"org.apache.hive.beeline.XMLElementOutputFormat.printHeader(Row)",1,1,1
"org.apache.hive.beeline.XMLElementOutputFormat.printRow(Rows,Row,Row)",1,2,3
"org.apache.hive.beeline.util.QFileClient.QFileClient(HiveConf,String,String,String,String)",1,1,1
"org.apache.hive.beeline.util.QFileClient.RegexFilterSet.addFilter(String,String)",1,1,1
"org.apache.hive.beeline.util.QFileClient.RegexFilterSet.filter(String)",1,2,2
"org.apache.hive.beeline.util.QFileClient.cleanup()",1,5,5
"org.apache.hive.beeline.util.QFileClient.compareResults()",2,2,2
"org.apache.hive.beeline.util.QFileClient.filterResults()",1,1,1
"org.apache.hive.beeline.util.QFileClient.hasErrors()",1,1,1
"org.apache.hive.beeline.util.QFileClient.hasExpectedResults()",1,1,1
"org.apache.hive.beeline.util.QFileClient.initBeeLine()",1,1,1
"org.apache.hive.beeline.util.QFileClient.initFilterSet()",1,1,1
"org.apache.hive.beeline.util.QFileClient.overwriteResults()",1,3,3
"org.apache.hive.beeline.util.QFileClient.run()",1,1,1
"org.apache.hive.beeline.util.QFileClient.runQFileTest()",1,1,2
"org.apache.hive.beeline.util.QFileClient.setExpectedDirectory(String)",1,1,1
"org.apache.hive.beeline.util.QFileClient.setJdbcDriver(String)",1,1,1
"org.apache.hive.beeline.util.QFileClient.setJdbcUrl(String)",1,1,1
"org.apache.hive.beeline.util.QFileClient.setOutputDirectory(String)",1,1,1
"org.apache.hive.beeline.util.QFileClient.setPassword(String)",1,1,1
"org.apache.hive.beeline.util.QFileClient.setQFileDirectory(String)",1,1,1
"org.apache.hive.beeline.util.QFileClient.setQFileName(String)",1,1,1
"org.apache.hive.beeline.util.QFileClient.setTestDataDirectory(String)",1,1,1
"org.apache.hive.beeline.util.QFileClient.setTestScriptDirectory(String)",1,1,1
"org.apache.hive.beeline.util.QFileClient.setUp()",1,1,1
"org.apache.hive.beeline.util.QFileClient.setUsername(String)",1,1,1
"org.apache.hive.beeline.util.QFileClient.tearDown()",1,1,1
"org.apache.hive.common.HiveCompat.CompatLevel.CompatLevel(String,int,int)",1,1,1
"org.apache.hive.common.HiveCompat.getCompatLevel(HiveConf)",1,1,1
"org.apache.hive.common.HiveCompat.getCompatLevel(String)",3,2,4
"org.apache.hive.common.HiveCompat.getLastCompatLevel()",1,1,1
"org.apache.hive.common.util.AnnotationUtils.getAnnotation(Class<?>,Class<T>)",1,1,1
"org.apache.hive.common.util.AnnotationUtils.getAnnotation(Method,Class<T>)",1,1,1
"org.apache.hive.common.util.Decimal128FastBuffer.Decimal128FastBuffer()",1,1,1
"org.apache.hive.common.util.Decimal128FastBuffer.getByteBuffer(int)",1,1,1
"org.apache.hive.common.util.Decimal128FastBuffer.getBytes(int)",1,1,1
"org.apache.hive.common.util.HiveStringUtils.TraditionalBinaryPrefix.TraditionalBinaryPrefix(long)",1,1,1
"org.apache.hive.common.util.HiveStringUtils.TraditionalBinaryPrefix.string2long(String)",3,2,5
"org.apache.hive.common.util.HiveStringUtils.TraditionalBinaryPrefix.valueOf(char)",3,1,3
"org.apache.hive.common.util.HiveStringUtils.arrayToString(String[])",2,2,3
"org.apache.hive.common.util.HiveStringUtils.byteDesc(long)",1,1,5
"org.apache.hive.common.util.HiveStringUtils.byteToHexString(byte[])",1,1,1
"org.apache.hive.common.util.HiveStringUtils.byteToHexString(byte[],int,int)",2,2,3
"org.apache.hive.common.util.HiveStringUtils.camelize(String)",1,2,2
"org.apache.hive.common.util.HiveStringUtils.escapeHTML(String)",3,5,10
"org.apache.hive.common.util.HiveStringUtils.escapeString(String)",1,1,1
"org.apache.hive.common.util.HiveStringUtils.escapeString(String,char,char)",1,1,1
"org.apache.hive.common.util.HiveStringUtils.escapeString(String,char,char[])",2,4,5
"org.apache.hive.common.util.HiveStringUtils.findNext(String,char,char,int,StringBuilder)",3,3,5
"org.apache.hive.common.util.HiveStringUtils.formatPercent(double,int)",1,1,1
"org.apache.hive.common.util.HiveStringUtils.formatTime(long)",1,3,3
"org.apache.hive.common.util.HiveStringUtils.formatTimeDiff(long,long)",1,1,1
"org.apache.hive.common.util.HiveStringUtils.getFormattedTimeWithDiff(DateFormat,long,long)",1,3,3
"org.apache.hive.common.util.HiveStringUtils.getHostname()",1,1,2
"org.apache.hive.common.util.HiveStringUtils.getStringCollection(String)",2,2,3
"org.apache.hive.common.util.HiveStringUtils.getStrings(String)",2,1,2
"org.apache.hive.common.util.HiveStringUtils.getTextUtfLength(Text)",1,2,3
"org.apache.hive.common.util.HiveStringUtils.getTrimmedStringCollection(String)",1,1,1
"org.apache.hive.common.util.HiveStringUtils.getTrimmedStrings(String)",2,2,3
"org.apache.hive.common.util.HiveStringUtils.hasChar(char[],char)",3,1,3
"org.apache.hive.common.util.HiveStringUtils.hexStringToByte(String)",1,2,2
"org.apache.hive.common.util.HiveStringUtils.humanReadableInt(long)",2,2,4
"org.apache.hive.common.util.HiveStringUtils.isUtfStartByte(byte)",1,1,1
"org.apache.hive.common.util.HiveStringUtils.join(CharSequence,Iterable<?>)",2,2,3
"org.apache.hive.common.util.HiveStringUtils.limitDecimalTo2(double)",1,1,1
"org.apache.hive.common.util.HiveStringUtils.simpleHostname(String)",2,2,2
"org.apache.hive.common.util.HiveStringUtils.split(String)",1,1,1
"org.apache.hive.common.util.HiveStringUtils.split(String,char)",2,4,5
"org.apache.hive.common.util.HiveStringUtils.split(String,char,char)",2,4,5
"org.apache.hive.common.util.HiveStringUtils.startupShutdownMessage(Class<?>,String[],Log)",1,1,1
"org.apache.hive.common.util.HiveStringUtils.stringToPath(String[])",2,1,3
"org.apache.hive.common.util.HiveStringUtils.stringToURI(String[])",3,1,4
"org.apache.hive.common.util.HiveStringUtils.stringifyException(Throwable)",1,1,1
"org.apache.hive.common.util.HiveStringUtils.toStartupShutdownString(String,String[])",1,2,2
"org.apache.hive.common.util.HiveStringUtils.unEscapeString(String)",1,1,1
"org.apache.hive.common.util.HiveStringUtils.unEscapeString(String,char,char)",1,1,1
"org.apache.hive.common.util.HiveStringUtils.unEscapeString(String,char,char[])",7,6,9
"org.apache.hive.common.util.HiveStringUtils.uriToString(URI[])",2,2,3
"org.apache.hive.common.util.HiveTestUtils.getFileFromClasspath(String)",2,1,2
"org.apache.hive.common.util.HiveVersionInfo.getBranch()",1,2,2
"org.apache.hive.common.util.HiveVersionInfo.getBuildVersion()",1,1,1
"org.apache.hive.common.util.HiveVersionInfo.getDate()",1,2,2
"org.apache.hive.common.util.HiveVersionInfo.getPackage()",1,1,1
"org.apache.hive.common.util.HiveVersionInfo.getRevision()",1,2,2
"org.apache.hive.common.util.HiveVersionInfo.getShortVersion()",1,2,2
"org.apache.hive.common.util.HiveVersionInfo.getSrcChecksum()",1,2,2
"org.apache.hive.common.util.HiveVersionInfo.getUrl()",1,2,2
"org.apache.hive.common.util.HiveVersionInfo.getUser()",1,2,2
"org.apache.hive.common.util.HiveVersionInfo.getVersion()",1,2,2
"org.apache.hive.common.util.HiveVersionInfo.main(String[])",1,1,1
"org.apache.hive.common.util.ShutdownHookManager.HookEntry.HookEntry(Runnable,int)",1,1,1
"org.apache.hive.common.util.ShutdownHookManager.HookEntry.equals(Object)",1,1,3
"org.apache.hive.common.util.ShutdownHookManager.HookEntry.hashCode()",1,1,1
"org.apache.hive.common.util.ShutdownHookManager.ShutdownHookManager()",1,1,1
"org.apache.hive.common.util.ShutdownHookManager.addShutdownHook(Runnable,int)",1,1,1
"org.apache.hive.common.util.ShutdownHookManager.addShutdownHookInternal(Runnable,int)",3,1,3
"org.apache.hive.common.util.ShutdownHookManager.getShutdownHooksInOrder()",1,1,1
"org.apache.hive.common.util.ShutdownHookManager.getShutdownHooksInOrderInternal()",1,2,2
"org.apache.hive.common.util.ShutdownHookManager.hasShutdownHook(Runnable)",1,1,1
"org.apache.hive.common.util.ShutdownHookManager.hasShutdownHookInternal(Runnable)",1,1,1
"org.apache.hive.common.util.ShutdownHookManager.isShutdownInProgress()",1,1,1
"org.apache.hive.common.util.ShutdownHookManager.isShutdownInProgressInternal()",1,1,1
"org.apache.hive.common.util.ShutdownHookManager.removeShutdownHook(Runnable)",1,1,1
"org.apache.hive.common.util.ShutdownHookManager.removeShutdownHookInternal(Runnable)",2,1,2
"org.apache.hive.common.util.StreamPrinter.StreamPrinter(InputStream,String,PrintStream)",1,1,1
"org.apache.hive.common.util.StreamPrinter.run()",1,5,5
"org.apache.hive.common.util.TestShutdownHookManager.shutdownHookManager()",1,1,1
"org.apache.hive.hcatalog.ExitException.ExitException(int)",1,1,1
"org.apache.hive.hcatalog.ExitException.getStatus()",1,1,1
"org.apache.hive.hcatalog.HcatTestUtils.cleanupHMS(Hive,Warehouse,FsPermission)",3,3,5
"org.apache.hive.hcatalog.HcatTestUtils.createTestDataFile(String,String[])",1,3,3
"org.apache.hive.hcatalog.HcatTestUtils.getDbPath(Hive,Warehouse,String)",1,1,1
"org.apache.hive.hcatalog.MiniCluster.MiniCluster()",1,1,1
"org.apache.hive.hcatalog.MiniCluster.buildCluster()",1,2,2
"org.apache.hive.hcatalog.MiniCluster.createInputFile(FileSystem,String,String[])",2,2,3
"org.apache.hive.hcatalog.MiniCluster.createInputFile(MiniCluster,String,String[])",1,1,1
"org.apache.hive.hcatalog.MiniCluster.deleteFile(MiniCluster,String)",1,1,1
"org.apache.hive.hcatalog.MiniCluster.errorIfNotSetup()",2,1,2
"org.apache.hive.hcatalog.MiniCluster.finalize()",1,1,1
"org.apache.hive.hcatalog.MiniCluster.getFileSystem()",1,1,1
"org.apache.hive.hcatalog.MiniCluster.getProperties()",1,2,2
"org.apache.hive.hcatalog.MiniCluster.setProperty(String,String)",1,1,1
"org.apache.hive.hcatalog.MiniCluster.setupMiniDfsAndMrClusters()",1,2,3
"org.apache.hive.hcatalog.MiniCluster.shutDown()",1,1,1
"org.apache.hive.hcatalog.MiniCluster.shutdownMiniDfsAndMrClusters()",1,5,5
"org.apache.hive.hcatalog.NoExitSecurityManager.checkExit(int)",1,1,1
"org.apache.hive.hcatalog.NoExitSecurityManager.checkPermission(Permission)",1,1,1
"org.apache.hive.hcatalog.NoExitSecurityManager.checkPermission(Permission,Object)",1,1,1
"org.apache.hive.hcatalog.api.ConnectionFailureException.ConnectionFailureException(String,Throwable)",1,1,1
"org.apache.hive.hcatalog.api.HCatAddPartitionDesc.Builder.Builder(HCatPartition)",1,1,1
"org.apache.hive.hcatalog.api.HCatAddPartitionDesc.Builder.Builder(String,String,String,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.api.HCatAddPartitionDesc.Builder.build()",1,1,2
"org.apache.hive.hcatalog.api.HCatAddPartitionDesc.HCatAddPartitionDesc(HCatPartition)",1,1,1
"org.apache.hive.hcatalog.api.HCatAddPartitionDesc.HCatAddPartitionDesc(String,String,String,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.api.HCatAddPartitionDesc.create(HCatPartition)",1,1,1
"org.apache.hive.hcatalog.api.HCatAddPartitionDesc.create(String,String,String,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.api.HCatAddPartitionDesc.getDatabaseName()",1,2,2
"org.apache.hive.hcatalog.api.HCatAddPartitionDesc.getHCatPartition()",1,1,1
"org.apache.hive.hcatalog.api.HCatAddPartitionDesc.getHCatPartition(HCatTable)",1,1,1
"org.apache.hive.hcatalog.api.HCatAddPartitionDesc.getLocation()",1,2,2
"org.apache.hive.hcatalog.api.HCatAddPartitionDesc.getPartitionSpec()",1,2,2
"org.apache.hive.hcatalog.api.HCatAddPartitionDesc.getTableName()",1,2,2
"org.apache.hive.hcatalog.api.HCatAddPartitionDesc.toString()",1,1,1
"org.apache.hive.hcatalog.api.HCatClient.create(Configuration)",1,2,5
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.addPartition(HCatAddPartitionDesc)",2,4,8
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.addPartitions(List<HCatAddPartitionDesc>)",2,5,10
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.cancelDelegationToken(String)",1,1,3
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.checkDB(String)",2,1,2
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.close()",1,1,1
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.createDatabase(HCatCreateDBDesc)",2,2,6
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.createTable(HCatCreateTableDesc)",2,2,8
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.createTableLike(String,String,String,boolean,boolean,String)",3,2,8
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.deserializePartition(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.deserializePartitions(List<String>)",4,5,5
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.deserializeTable(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.dropDatabase(String,boolean,DropDBMode)",2,1,6
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.dropPartition(Partition,boolean)",2,3,3
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.dropPartitions(String,String,Map<String, String>,boolean)",1,2,5
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.dropTable(String,String,boolean)",2,1,5
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.getDatabase(String)",1,1,5
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.getDelegationToken(String,String)",1,1,3
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.getFilterString(Map<String, String>)",1,3,3
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.getHiveTableLike(String,String,String,boolean,String)",1,4,7
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.getMessageBusTopicName(String,String)",1,1,4
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.getPartition(String,String,Map<String, String>)",4,3,8
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.getPartitions(String,String)",1,2,5
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.getPartitions(String,String,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.getTable(String,String)",1,1,5
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.initialize(Configuration)",1,1,3
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.isPartitionMarkedForEvent(String,String,Map<String, String>,PartitionEventType)",1,1,6
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.listDatabaseNamesByPattern(String)",1,1,2
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.listPartitionsByFilter(String,String,String)",1,2,5
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.listTableNamesByPattern(String,String)",1,1,2
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.markPartitionForEvent(String,String,Map<String, String>,PartitionEventType)",1,1,6
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.renameTable(String,String,String)",3,2,7
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.renewDelegationToken(String)",1,1,3
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.serializePartition(HCatPartition)",1,1,1
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.serializePartitions(List<HCatPartition>)",1,2,2
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.serializeTable(HCatTable)",1,1,1
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.updateTableSchema(String,String,HCatTable)",1,1,5
"org.apache.hive.hcatalog.api.HCatClientHMSImpl.updateTableSchema(String,String,List<HCatFieldSchema>)",1,1,5
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.Builder.Builder(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.Builder.build()",2,1,2
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.Builder.comment(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.Builder.databaseProperties(Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.Builder.ifNotExists(boolean)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.Builder.location(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.HCatCreateDBDesc(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.create(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.getComments()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.getDatabaseName()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.getDatabaseProperties()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.getIfNotExists()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.getLocation()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.toHiveDb()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateDBDesc.toString()",1,1,5
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.Builder(HCatTable,boolean)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.Builder(String,String,List<HCatFieldSchema>)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.bucketCols(List<String>,int)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.build()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.collectionItemsTerminatedBy(char)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.comments(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.escapeChar(char)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.fieldsTerminatedBy(char)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.fileFormat(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.ifNotExists(boolean)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.isTableExternal(boolean)",1,1,2
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.linesTerminatedBy(char)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.location(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.mapKeysTerminatedBy(char)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.nullDefinedAs(char)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.partCols(List<HCatFieldSchema>)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.serdeParam(String,String)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.sortCols(ArrayList<Order>)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.storageHandler(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.Builder.tblProps(Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.HCatCreateTableDesc(HCatTable,boolean)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.create(HCatTable)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.create(HCatTable,boolean)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.create(String,String,List<HCatFieldSchema>)",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getBucketCols()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getCols()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getComments()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getDatabaseName()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getExternal()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getFileFormat()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getHCatTable()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getIfNotExists()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getLocation()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getNumBuckets()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getPartitionCols()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getSerdeParams()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getSortCols()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getStorageHandler()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getTableName()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.getTblProps()",1,1,1
"org.apache.hive.hcatalog.api.HCatCreateTableDesc.toString()",1,1,1
"org.apache.hive.hcatalog.api.HCatDatabase.HCatDatabase(Database)",1,1,1
"org.apache.hive.hcatalog.api.HCatDatabase.getComment()",1,1,1
"org.apache.hive.hcatalog.api.HCatDatabase.getLocation()",1,1,1
"org.apache.hive.hcatalog.api.HCatDatabase.getName()",1,1,1
"org.apache.hive.hcatalog.api.HCatDatabase.getProperties()",1,1,1
"org.apache.hive.hcatalog.api.HCatDatabase.toString()",1,1,5
"org.apache.hive.hcatalog.api.HCatPartition.HCatPartition(HCatPartition,Map<String, String>,String)",3,3,3
"org.apache.hive.hcatalog.api.HCatPartition.HCatPartition(HCatTable,Map<String, String>,String)",3,3,3
"org.apache.hive.hcatalog.api.HCatPartition.HCatPartition(HCatTable,Partition)",2,3,3
"org.apache.hive.hcatalog.api.HCatPartition.getBucketCols()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getColumns()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getColumns(StorageDescriptor)",1,2,2
"org.apache.hive.hcatalog.api.HCatPartition.getCreateTime()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getDatabaseName()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getInputFormat()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getLastAccessTime()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getLocation()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getNumBuckets()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getOutputFormat()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getParameters()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getPartitionKeyValMap()",1,2,2
"org.apache.hive.hcatalog.api.HCatPartition.getSerDe()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getSerdeParams()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getSortCols()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getStorageHandler()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getTableName()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.getValues()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.hcatTable()",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.hcatTable(HCatTable)",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.location(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatPartition.setPartitionKeyValues(Map<String, String>)",3,3,3
"org.apache.hive.hcatalog.api.HCatPartition.toHivePartition()",2,3,3
"org.apache.hive.hcatalog.api.HCatPartition.toString()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.HCatTable(String,String)",1,1,2
"org.apache.hive.hcatalog.api.HCatTable.HCatTable(Table)",1,5,5
"org.apache.hive.hcatalog.api.HCatTable.bucketCols(List<String>)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.collectionItemsTerminatedBy(char)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.cols(List<HCatFieldSchema>)",1,2,2
"org.apache.hive.hcatalog.api.HCatTable.comment()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.comment(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.dbName(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.diff(HCatTable)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.diff(HCatTable,EnumSet<TableAttribute>)",1,33,33
"org.apache.hive.hcatalog.api.HCatTable.equivalent(Map<String, String>,Map<String, String>)",1,2,2
"org.apache.hive.hcatalog.api.HCatTable.escapeChar(char)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.fieldsTerminatedBy(char)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.fileFormat()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.fileFormat(String)",1,4,4
"org.apache.hive.hcatalog.api.HCatTable.getBucketCols()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getCols()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getConf()",1,2,2
"org.apache.hive.hcatalog.api.HCatTable.getDbName()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getInputFileFormat()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getLocation()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getNumBuckets()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getOutputFileFormat()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getPartCols()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getSd()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getSerDeInfo()",1,2,2
"org.apache.hive.hcatalog.api.HCatTable.getSerdeLib()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getSerdeParams()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getSortCols()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getStorageHandler()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getTableName()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getTabletype()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.getTblProps()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.inputFileFormat(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.linesTerminatedBy(char)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.location(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.mapKeysTerminatedBy(char)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.nullDefinedAs(char)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.numBuckets(int)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.outputFileFormat(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.owner()",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.owner(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.partCol(HCatFieldSchema)",1,1,2
"org.apache.hive.hcatalog.api.HCatTable.partCols(List<HCatFieldSchema>)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.resolve(HCatTable,EnumSet<TableAttribute>)",2,9,10
"org.apache.hive.hcatalog.api.HCatTable.serdeLib(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.serdeParam(String,String)",1,2,2
"org.apache.hive.hcatalog.api.HCatTable.serdeParams(Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.setConf(Configuration)",1,2,2
"org.apache.hive.hcatalog.api.HCatTable.sortCols(List<Order>)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.storageHandler(String)",1,2,2
"org.apache.hive.hcatalog.api.HCatTable.tableName(String)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.tableType(Type)",1,1,1
"org.apache.hive.hcatalog.api.HCatTable.tblProps(Map<String, String>)",1,2,2
"org.apache.hive.hcatalog.api.HCatTable.toHiveTable()",1,7,8
"org.apache.hive.hcatalog.api.HCatTable.toString()",1,1,1
"org.apache.hive.hcatalog.api.MetadataJSONSerializer.MetadataJSONSerializer()",1,1,1
"org.apache.hive.hcatalog.api.MetadataJSONSerializer.deserializePartition(String)",1,3,3
"org.apache.hive.hcatalog.api.MetadataJSONSerializer.deserializeTable(String)",1,3,3
"org.apache.hive.hcatalog.api.MetadataJSONSerializer.serializePartition(HCatPartition)",1,1,2
"org.apache.hive.hcatalog.api.MetadataJSONSerializer.serializeTable(HCatTable)",1,1,2
"org.apache.hive.hcatalog.api.MetadataSerializer.MetadataSerializer()",1,1,1
"org.apache.hive.hcatalog.api.MetadataSerializer.get()",1,1,1
"org.apache.hive.hcatalog.api.ObjectNotFoundException.ObjectNotFoundException(String,Throwable)",1,1,1
"org.apache.hive.hcatalog.api.TestHCatClient.RunMS.RunMS(String)",1,1,1
"org.apache.hive.hcatalog.api.TestHCatClient.RunMS.arg(String)",1,1,1
"org.apache.hive.hcatalog.api.TestHCatClient.RunMS.run()",1,2,2
"org.apache.hive.hcatalog.api.TestHCatClient.fixPath(String)",2,1,3
"org.apache.hive.hcatalog.api.TestHCatClient.mapEqualsContainedIn(Map<String, String>,Map<String, String>)",1,2,2
"org.apache.hive.hcatalog.api.TestHCatClient.startMetaStoreServer()",1,1,1
"org.apache.hive.hcatalog.api.TestHCatClient.startReplicationTargetMetaStoreIfRequired()",1,2,2
"org.apache.hive.hcatalog.api.TestHCatClient.tearDown()",1,1,1
"org.apache.hive.hcatalog.api.TestHCatClient.testBasicDDLCommands()",1,2,2
"org.apache.hive.hcatalog.api.TestHCatClient.testCreateTableLike()",1,1,1
"org.apache.hive.hcatalog.api.TestHCatClient.testDatabaseLocation()",1,1,1
"org.apache.hive.hcatalog.api.TestHCatClient.testDropPartitionsWithPartialSpec()",1,2,2
"org.apache.hive.hcatalog.api.TestHCatClient.testDropTableException()",1,2,2
"org.apache.hive.hcatalog.api.TestHCatClient.testEmptyTableInstantiation()",1,1,1
"org.apache.hive.hcatalog.api.TestHCatClient.testGetMessageBusTopicName()",1,2,2
"org.apache.hive.hcatalog.api.TestHCatClient.testGetPartitionsWithPartialSpec()",1,2,2
"org.apache.hive.hcatalog.api.TestHCatClient.testObjectNotFoundException()",1,6,6
"org.apache.hive.hcatalog.api.TestHCatClient.testOtherFailure()",1,2,2
"org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema()",1,4,4
"org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSchema()",1,2,2
"org.apache.hive.hcatalog.api.TestHCatClient.testPartitionsHCatClientImpl()",1,1,1
"org.apache.hive.hcatalog.api.TestHCatClient.testRenameTable()",1,2,2
"org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation()",1,2,2
"org.apache.hive.hcatalog.api.TestHCatClient.testTransportFailure()",1,2,2
"org.apache.hive.hcatalog.api.TestHCatClient.testUpdateTableSchema()",1,2,2
"org.apache.hive.hcatalog.cli.HCatCli.main(String[])",1,12,14
"org.apache.hive.hcatalog.cli.HCatCli.printUsage(Options,OutputStream)",1,1,1
"org.apache.hive.hcatalog.cli.HCatCli.processCmd(String)",3,9,10
"org.apache.hive.hcatalog.cli.HCatCli.processFile(String)",1,4,4
"org.apache.hive.hcatalog.cli.HCatCli.processLine(String)",4,3,4
"org.apache.hive.hcatalog.cli.HCatCli.setConfProperties(HiveConf,Properties)",1,2,2
"org.apache.hive.hcatalog.cli.HCatCli.sysExit(SessionState,int)",1,2,2
"org.apache.hive.hcatalog.cli.HCatCli.validatePermissions(CliSessionState,HiveConf,String)",1,8,8
"org.apache.hive.hcatalog.cli.HCatDriver.run(String)",1,3,3
"org.apache.hive.hcatalog.cli.HCatDriver.setFSPermsNGrp(SessionState)",5,15,18
"org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateDatabaseHook.authorizeDDLWork(HiveSemanticAnalyzerHookContext,Hive,DDLWork)",1,2,2
"org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateDatabaseHook.postAnalyze(HiveSemanticAnalyzerHookContext,List<Task<? extends Serializable>>)",1,1,1
"org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateDatabaseHook.preAnalyze(HiveSemanticAnalyzerHookContext,ASTNode)",4,4,7
"org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook.postAnalyze(HiveSemanticAnalyzerHookContext,List<Task<? extends Serializable>>)",5,8,13
"org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook.preAnalyze(HiveSemanticAnalyzerHookContext,ASTNode)",10,9,18
"org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.authorizeDDLWork(HiveSemanticAnalyzerHookContext,Hive,DDLWork)",4,20,21
"org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.extractTableName(String)",1,1,1
"org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.postAnalyze(HiveSemanticAnalyzerHookContext,List<Task<? extends Serializable>>)",4,5,12
"org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.preAnalyze(HiveSemanticAnalyzerHookContext,ASTNode)",6,4,8
"org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.authorize(Database,Privilege)",1,1,2
"org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.authorize(Partition,Privilege)",1,1,2
"org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.authorize(Privilege[],Privilege[])",1,1,2
"org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.authorize(Table,Privilege)",1,1,2
"org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.authorizeDDL(HiveSemanticAnalyzerHookContext,List<Task<? extends Serializable>>)",2,4,8
"org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.authorizeDDLWork(HiveSemanticAnalyzerHookContext,Hive,DDLWork)",1,1,1
"org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.authorizeTable(Hive,String,Privilege)",1,1,2
"org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.getAuthProvider()",1,2,2
"org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzerBase.postAnalyze(HiveSemanticAnalyzerHookContext,List<Task<? extends Serializable>>)",1,1,1
"org.apache.hive.hcatalog.cli.TestPermsGrp.callHCatCli(String[])",1,1,1
"org.apache.hive.hcatalog.cli.TestPermsGrp.cleanupTbl(String,String,String)",1,1,1
"org.apache.hive.hcatalog.cli.TestPermsGrp.getTable(String,String,String)",1,1,1
"org.apache.hive.hcatalog.cli.TestPermsGrp.setUp()",2,1,2
"org.apache.hive.hcatalog.cli.TestPermsGrp.silentDropDatabase(String)",1,2,3
"org.apache.hive.hcatalog.cli.TestPermsGrp.tearDown()",1,1,1
"org.apache.hive.hcatalog.cli.TestPermsGrp.testCustomPerms()",1,9,9
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.setUpHCatDriver()",1,2,2
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testAddDriverInfo()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testAddPartFail()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testAddPartPass()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testAddReplaceCols()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testAlterTableSetFF()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testAlterTblClusteredBy()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testAlterTblFFpart()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testAlterTblTouch()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testCTAS()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testCTLFail()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testCTLPass()",1,2,2
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testChangeColumns()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testCreateTableIfNotExists()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testCreateTblWithLowerCasePartNames()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testDatabaseOperations()",1,3,3
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testDescDB()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testInvalidateClusteredBy()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testInvalidateNonStringPartition()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testInvalidateSeqFileStoredAs()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testInvalidateTextFileStoredAs()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testStoredAs()",1,1,1
"org.apache.hive.hcatalog.cli.TestSemanticAnalysis.testUsNonExistentDB()",1,1,1
"org.apache.hive.hcatalog.cli.TestUseDatabase.setUp()",1,1,1
"org.apache.hive.hcatalog.cli.TestUseDatabase.testAlterTablePass()",1,1,1
"org.apache.hive.hcatalog.common.ErrorType.ErrorType(int,String)",1,1,1
"org.apache.hive.hcatalog.common.ErrorType.ErrorType(int,String,boolean)",1,1,1
"org.apache.hive.hcatalog.common.ErrorType.ErrorType(int,String,boolean,boolean)",1,1,1
"org.apache.hive.hcatalog.common.ErrorType.appendCauseMessage()",1,1,1
"org.apache.hive.hcatalog.common.ErrorType.getErrorCode()",1,1,1
"org.apache.hive.hcatalog.common.ErrorType.getErrorMessage()",1,1,1
"org.apache.hive.hcatalog.common.ErrorType.isRetriable()",1,1,1
"org.apache.hive.hcatalog.common.HCatConstants.HCatConstants()",1,1,1
"org.apache.hive.hcatalog.common.HCatContext.getConf()",1,1,1
"org.apache.hive.hcatalog.common.HCatContext.setConf(Configuration)",2,4,5
"org.apache.hive.hcatalog.common.HCatException.HCatException(ErrorType)",1,1,1
"org.apache.hive.hcatalog.common.HCatException.HCatException(ErrorType,String)",1,1,1
"org.apache.hive.hcatalog.common.HCatException.HCatException(ErrorType,String,Throwable)",1,1,1
"org.apache.hive.hcatalog.common.HCatException.HCatException(ErrorType,Throwable)",1,1,1
"org.apache.hive.hcatalog.common.HCatException.HCatException(String)",1,1,1
"org.apache.hive.hcatalog.common.HCatException.HCatException(String,Throwable)",1,1,1
"org.apache.hive.hcatalog.common.HCatException.buildErrorMessage(ErrorType,String,Throwable)",1,4,4
"org.apache.hive.hcatalog.common.HCatException.getErrorCode()",1,1,1
"org.apache.hive.hcatalog.common.HCatException.getErrorType()",1,1,1
"org.apache.hive.hcatalog.common.HCatException.isRetriable()",1,1,1
"org.apache.hive.hcatalog.common.HCatException.toString()",1,1,1
"org.apache.hive.hcatalog.common.HCatUtil.assertNotNull(Object,String,Logger)",2,3,3
"org.apache.hive.hcatalog.common.HCatUtil.checkJobContextIfRunningFromBackend(JobContext)",2,2,3
"org.apache.hive.hcatalog.common.HCatUtil.closeHiveClientQuietly(HiveMetaStoreClient)",1,3,3
"org.apache.hive.hcatalog.common.HCatUtil.configureOutputStorageHandler(HiveStorageHandler,Configuration,OutputJobInfo)",1,8,9
"org.apache.hive.hcatalog.common.HCatUtil.copyConf(Configuration,Configuration)",1,2,2
"org.apache.hive.hcatalog.common.HCatUtil.copyJobPropertiesToJobConf(Map<String, String>,JobConf)",1,2,2
"org.apache.hive.hcatalog.common.HCatUtil.decodeBytes(String)",1,2,2
"org.apache.hive.hcatalog.common.HCatUtil.deserialize(String)",2,3,4
"org.apache.hive.hcatalog.common.HCatUtil.encodeBytes(byte[])",1,2,2
"org.apache.hive.hcatalog.common.HCatUtil.extractSchema(Partition)",1,1,1
"org.apache.hive.hcatalog.common.HCatUtil.extractSchema(Table)",1,1,1
"org.apache.hive.hcatalog.common.HCatUtil.extractThriftToken(String,String)",1,1,1
"org.apache.hive.hcatalog.common.HCatUtil.getDbAndTableName(String)",3,1,3
"org.apache.hive.hcatalog.common.HCatUtil.getFieldSchemaList(List<HCatFieldSchema>)",2,3,3
"org.apache.hive.hcatalog.common.HCatUtil.getHCatFieldSchemaList(FieldSchema...)",1,2,2
"org.apache.hive.hcatalog.common.HCatUtil.getHCatFieldSchemaList(List<FieldSchema>)",2,3,3
"org.apache.hive.hcatalog.common.HCatUtil.getHiveClient(HiveConf)",2,2,5
"org.apache.hive.hcatalog.common.HCatUtil.getHiveConf(Configuration)",1,9,9
"org.apache.hive.hcatalog.common.HCatUtil.getInputJobProperties(HiveStorageHandler,InputJobInfo)",1,2,3
"org.apache.hive.hcatalog.common.HCatUtil.getJobConfFromContext(JobContext)",1,1,1
"org.apache.hive.hcatalog.common.HCatUtil.getJobTrackerDelegationToken(Configuration,String)",1,1,1
"org.apache.hive.hcatalog.common.HCatUtil.getNonCachedHiveClient(HiveConf)",1,1,1
"org.apache.hive.hcatalog.common.HCatUtil.getPartitionColumns(Table)",1,3,3
"org.apache.hive.hcatalog.common.HCatUtil.getStorageHandler(Configuration,PartInfo)",1,1,1
"org.apache.hive.hcatalog.common.HCatUtil.getStorageHandler(Configuration,StorerInfo)",1,1,1
"org.apache.hive.hcatalog.common.HCatUtil.getStorageHandler(Configuration,String,String,String,String)",2,4,5
"org.apache.hive.hcatalog.common.HCatUtil.getTable(HiveMetaStoreClient,String,String)",1,1,1
"org.apache.hive.hcatalog.common.HCatUtil.getTableSchemaWithPtnCols(Table)",1,3,3
"org.apache.hive.hcatalog.common.HCatUtil.isHadoop23()",2,2,3
"org.apache.hive.hcatalog.common.HCatUtil.makePathASafeFileName(String)",1,1,1
"org.apache.hive.hcatalog.common.HCatUtil.serialize(Serializable)",2,2,3
"org.apache.hive.hcatalog.common.HCatUtil.validateExecuteBitPresentIfReadOrWrite(FsAction)",2,1,4
"org.apache.hive.hcatalog.common.HCatUtil.validateMorePermissive(FsAction,FsAction)",5,2,10
"org.apache.hive.hcatalog.common.HCatUtil.validatePartitionSchema(Table,HCatSchema)",7,8,8
"org.apache.hive.hcatalog.common.HiveClientCache.CacheableHiveMetaStoreClient.CacheableHiveMetaStoreClient(HiveConf,int)",1,1,1
"org.apache.hive.hcatalog.common.HiveClientCache.CacheableHiveMetaStoreClient.acquire()",1,1,1
"org.apache.hive.hcatalog.common.HiveClientCache.CacheableHiveMetaStoreClient.close()",1,2,2
"org.apache.hive.hcatalog.common.HiveClientCache.CacheableHiveMetaStoreClient.finalize()",1,1,1
"org.apache.hive.hcatalog.common.HiveClientCache.CacheableHiveMetaStoreClient.isClosed()",1,1,1
"org.apache.hive.hcatalog.common.HiveClientCache.CacheableHiveMetaStoreClient.isOpen()",1,1,2
"org.apache.hive.hcatalog.common.HiveClientCache.CacheableHiveMetaStoreClient.release()",1,1,1
"org.apache.hive.hcatalog.common.HiveClientCache.CacheableHiveMetaStoreClient.setExpiredFromCache()",1,1,1
"org.apache.hive.hcatalog.common.HiveClientCache.CacheableHiveMetaStoreClient.tearDown()",1,3,3
"org.apache.hive.hcatalog.common.HiveClientCache.CacheableHiveMetaStoreClient.tearDownIfUnused()",1,3,3
"org.apache.hive.hcatalog.common.HiveClientCache.HiveClientCache(HiveConf)",1,1,1
"org.apache.hive.hcatalog.common.HiveClientCache.HiveClientCache(int)",1,2,3
"org.apache.hive.hcatalog.common.HiveClientCache.HiveClientCacheKey.HiveClientCacheKey(HiveConf,int)",1,1,1
"org.apache.hive.hcatalog.common.HiveClientCache.HiveClientCacheKey.equals(Object)",3,2,4
"org.apache.hive.hcatalog.common.HiveClientCache.HiveClientCacheKey.fromHiveConf(HiveConf,int)",1,1,1
"org.apache.hive.hcatalog.common.HiveClientCache.HiveClientCacheKey.getHiveConf()",1,1,1
"org.apache.hive.hcatalog.common.HiveClientCache.HiveClientCacheKey.hashCode()",1,1,1
"org.apache.hive.hcatalog.common.HiveClientCache.cleanup()",1,1,1
"org.apache.hive.hcatalog.common.HiveClientCache.closeAllClientsQuietly()",1,3,3
"org.apache.hive.hcatalog.common.HiveClientCache.get(HiveConf)",1,2,2
"org.apache.hive.hcatalog.common.HiveClientCache.getNonCachedHiveClient(HiveConf)",1,1,1
"org.apache.hive.hcatalog.common.HiveClientCache.getOrCreate(HiveClientCacheKey)",4,2,5
"org.apache.hive.hcatalog.common.HiveClientCache.getThreadId()",1,1,1
"org.apache.hive.hcatalog.common.TestHCatUtil.assertConsistentFsPermissionBehaviour(FsAction,boolean,boolean,boolean,boolean,boolean,boolean,boolean,boolean)",1,1,1
"org.apache.hive.hcatalog.common.TestHCatUtil.assertFsPermissionTransformationIsGood(String)",1,1,1
"org.apache.hive.hcatalog.common.TestHCatUtil.testExecutePermissionsCheck()",1,1,1
"org.apache.hive.hcatalog.common.TestHCatUtil.testFsPermissionOperation()",1,5,5
"org.apache.hive.hcatalog.common.TestHCatUtil.testGetTableSchemaWithPtnColsApi()",1,1,1
"org.apache.hive.hcatalog.common.TestHCatUtil.testGetTableSchemaWithPtnColsSerDeReportedFields()",1,1,1
"org.apache.hive.hcatalog.common.TestHCatUtil.testValidateMorePermissive()",1,1,1
"org.apache.hive.hcatalog.common.TestHiveClientCache.LocalMetaServer.LocalMetaServer()",1,1,1
"org.apache.hive.hcatalog.common.TestHiveClientCache.LocalMetaServer.getHiveConf()",1,1,1
"org.apache.hive.hcatalog.common.TestHiveClientCache.LocalMetaServer.run()",1,2,2
"org.apache.hive.hcatalog.common.TestHiveClientCache.LocalMetaServer.shutDown()",1,1,1
"org.apache.hive.hcatalog.common.TestHiveClientCache.LocalMetaServer.start()",1,1,1
"org.apache.hive.hcatalog.common.TestHiveClientCache.setUp()",1,1,1
"org.apache.hive.hcatalog.common.TestHiveClientCache.tearDown()",1,1,1
"org.apache.hive.hcatalog.common.TestHiveClientCache.testCacheExpiry()",1,1,1
"org.apache.hive.hcatalog.common.TestHiveClientCache.testCacheHit()",1,1,1
"org.apache.hive.hcatalog.common.TestHiveClientCache.testCacheMiss()",1,1,1
"org.apache.hive.hcatalog.common.TestHiveClientCache.testCloseAllClients()",1,1,1
"org.apache.hive.hcatalog.common.TestHiveClientCache.testHMSCBreakability()",1,1,4
"org.apache.hive.hcatalog.common.TestHiveClientCache.testMultipleThreadAccess()",1,1,1
"org.apache.hive.hcatalog.data.DataType.compare(Object,Object)",1,1,1
"org.apache.hive.hcatalog.data.DataType.compare(Object,Object,byte,byte)",27,24,29
"org.apache.hive.hcatalog.data.DataType.compareByteArray(byte[],byte[])",6,1,6
"org.apache.hive.hcatalog.data.DataType.findType(Object)",18,1,18
"org.apache.hive.hcatalog.data.DefaultHCatRecord.DefaultHCatRecord()",1,1,1
"org.apache.hive.hcatalog.data.DefaultHCatRecord.DefaultHCatRecord(List<Object>)",1,1,1
"org.apache.hive.hcatalog.data.DefaultHCatRecord.DefaultHCatRecord(int)",1,2,2
"org.apache.hive.hcatalog.data.DefaultHCatRecord.copy(HCatRecord)",1,1,1
"org.apache.hive.hcatalog.data.DefaultHCatRecord.get(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.DefaultHCatRecord.get(int)",1,1,1
"org.apache.hive.hcatalog.data.DefaultHCatRecord.getAll()",1,1,1
"org.apache.hive.hcatalog.data.DefaultHCatRecord.hashCode()",1,3,3
"org.apache.hive.hcatalog.data.DefaultHCatRecord.readFields(DataInput)",1,2,2
"org.apache.hive.hcatalog.data.DefaultHCatRecord.remove(int)",1,1,1
"org.apache.hive.hcatalog.data.DefaultHCatRecord.set(String,HCatSchema,Object)",1,1,1
"org.apache.hive.hcatalog.data.DefaultHCatRecord.set(int,Object)",1,1,1
"org.apache.hive.hcatalog.data.DefaultHCatRecord.size()",1,1,1
"org.apache.hive.hcatalog.data.DefaultHCatRecord.toString()",1,2,2
"org.apache.hive.hcatalog.data.DefaultHCatRecord.write(DataOutput)",1,2,2
"org.apache.hive.hcatalog.data.HCatDataCheckUtil.compareRecordContents(List<Object>,List<Object>,StringBuilder)",5,7,7
"org.apache.hive.hcatalog.data.HCatDataCheckUtil.compareRecords(HCatRecord,HCatRecord)",1,1,1
"org.apache.hive.hcatalog.data.HCatDataCheckUtil.compareRecords(HCatRecord,HCatRecord,StringBuilder)",1,1,1
"org.apache.hive.hcatalog.data.HCatDataCheckUtil.createTable(Driver,String,String)",2,1,2
"org.apache.hive.hcatalog.data.HCatDataCheckUtil.dropTable(Driver,String)",1,1,1
"org.apache.hive.hcatalog.data.HCatDataCheckUtil.formattedRun(Driver,String,String)",1,1,1
"org.apache.hive.hcatalog.data.HCatDataCheckUtil.generateDataFile(MiniCluster,String)",1,1,2
"org.apache.hive.hcatalog.data.HCatDataCheckUtil.instantiateDriver(MiniCluster)",1,2,2
"org.apache.hive.hcatalog.data.HCatDataCheckUtil.recordsEqual(HCatRecord,HCatRecord)",1,1,1
"org.apache.hive.hcatalog.data.HCatDataCheckUtil.recordsEqual(HCatRecord,HCatRecord,StringBuilder)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.get(String,HCatSchema,Class)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getBoolean(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getByte(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getByteArray(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getChar(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getDate(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getDecimal(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getDouble(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getFloat(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getInteger(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getList(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getLong(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getMap(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getShort(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getString(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getStruct(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getTimestamp(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.getVarchar(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setBoolean(String,HCatSchema,Boolean)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setByte(String,HCatSchema,Byte)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setByteArray(String,HCatSchema,byte[])",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setChar(String,HCatSchema,HiveChar)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setDate(String,HCatSchema,Date)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setDecimal(String,HCatSchema,HiveDecimal)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setDouble(String,HCatSchema,Double)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setFloat(String,HCatSchema,Float)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setInteger(String,HCatSchema,Integer)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setList(String,HCatSchema,List<?>)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setLong(String,HCatSchema,Long)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setMap(String,HCatSchema,Map<?, ?>)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setShort(String,HCatSchema,Short)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setString(String,HCatSchema,String)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setStruct(String,HCatSchema,List<? extends Object>)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setTimestamp(String,HCatSchema,Timestamp)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecord.setVarchar(String,HCatSchema,HiveVarchar)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecordObjectInspector.HCatRecordObjectInspector(List<String>,List<ObjectInspector>)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecordObjectInspector.getStructFieldData(Object,StructField)",3,2,4
"org.apache.hive.hcatalog.data.HCatRecordObjectInspector.getStructFieldsDataAsList(Object)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory.getHCatRecordObjectInspector(StructTypeInfo)",3,4,5
"org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory.getStandardObjectInspectorFromTypeInfo(TypeInfo)",2,4,8
"org.apache.hive.hcatalog.data.HCatRecordSerDe.HCatRecordSerDe()",1,1,1
"org.apache.hive.hcatalog.data.HCatRecordSerDe.deserialize(Writable)",2,2,2
"org.apache.hive.hcatalog.data.HCatRecordSerDe.getObjectInspector()",1,1,1
"org.apache.hive.hcatalog.data.HCatRecordSerDe.getSerDeStats()",1,1,1
"org.apache.hive.hcatalog.data.HCatRecordSerDe.getSerializedClass()",1,1,1
"org.apache.hive.hcatalog.data.HCatRecordSerDe.initialize(Configuration,Properties)",1,3,3
"org.apache.hive.hcatalog.data.HCatRecordSerDe.initialize(HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.HCatRecordSerDe.serialize(Object,ObjectInspector)",2,2,2
"org.apache.hive.hcatalog.data.HCatRecordSerDe.serializeField(Object,ObjectInspector)",5,5,5
"org.apache.hive.hcatalog.data.HCatRecordSerDe.serializeList(Object,ListObjectInspector)",6,9,10
"org.apache.hive.hcatalog.data.HCatRecordSerDe.serializeMap(Object,MapObjectInspector)",2,3,3
"org.apache.hive.hcatalog.data.HCatRecordSerDe.serializePrimitiveField(Object,ObjectInspector)",6,8,11
"org.apache.hive.hcatalog.data.HCatRecordSerDe.serializeStruct(Object,StructObjectInspector)",2,3,4
"org.apache.hive.hcatalog.data.JsonSerDe.appendWithQuotes(StringBuilder,String)",1,2,2
"org.apache.hive.hcatalog.data.JsonSerDe.buildJSONString(StringBuilder,Object,ObjectInspector)",6,30,36
"org.apache.hive.hcatalog.data.JsonSerDe.deserialize(Writable)",2,5,6
"org.apache.hive.hcatalog.data.JsonSerDe.extractCurrentField(JsonParser,HCatFieldSchema,boolean)",6,21,44
"org.apache.hive.hcatalog.data.JsonSerDe.getHiveInternalColumnName(int)",1,1,1
"org.apache.hive.hcatalog.data.JsonSerDe.getObjectInspector()",1,1,1
"org.apache.hive.hcatalog.data.JsonSerDe.getObjectOfCorrespondingPrimitiveType(String,PrimitiveTypeInfo)",15,13,15
"org.apache.hive.hcatalog.data.JsonSerDe.getPositionFromHiveInternalColumnName(String)",2,2,2
"org.apache.hive.hcatalog.data.JsonSerDe.getSerDeStats()",1,1,1
"org.apache.hive.hcatalog.data.JsonSerDe.getSerializedClass()",1,1,1
"org.apache.hive.hcatalog.data.JsonSerDe.initialize(Configuration,Properties)",1,3,4
"org.apache.hive.hcatalog.data.JsonSerDe.populateRecord(List<Object>,JsonToken,JsonParser,HCatSchema)",5,4,5
"org.apache.hive.hcatalog.data.JsonSerDe.serialize(Object,ObjectInspector)",1,5,5
"org.apache.hive.hcatalog.data.JsonSerDe.skipValue(JsonParser)",1,2,3
"org.apache.hive.hcatalog.data.LazyHCatRecord.LazyHCatRecord(Object,ObjectInspector)",2,2,2
"org.apache.hive.hcatalog.data.LazyHCatRecord.copy(HCatRecord)",1,1,1
"org.apache.hive.hcatalog.data.LazyHCatRecord.get(String,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.LazyHCatRecord.get(int)",1,1,2
"org.apache.hive.hcatalog.data.LazyHCatRecord.getAll()",1,2,2
"org.apache.hive.hcatalog.data.LazyHCatRecord.getWritable()",1,1,1
"org.apache.hive.hcatalog.data.LazyHCatRecord.readFields(DataInput)",1,1,1
"org.apache.hive.hcatalog.data.LazyHCatRecord.remove(int)",1,1,1
"org.apache.hive.hcatalog.data.LazyHCatRecord.set(String,HCatSchema,Object)",1,1,1
"org.apache.hive.hcatalog.data.LazyHCatRecord.set(int,Object)",1,1,1
"org.apache.hive.hcatalog.data.LazyHCatRecord.size()",1,1,1
"org.apache.hive.hcatalog.data.LazyHCatRecord.toString()",1,2,2
"org.apache.hive.hcatalog.data.LazyHCatRecord.write(DataOutput)",1,1,1
"org.apache.hive.hcatalog.data.Pair.Pair(T,U)",1,1,1
"org.apache.hive.hcatalog.data.Pair.equals(Object)",8,2,9
"org.apache.hive.hcatalog.data.Pair.hashCode()",1,3,3
"org.apache.hive.hcatalog.data.Pair.toString()",1,1,1
"org.apache.hive.hcatalog.data.ReaderWriter.readDatum(DataInput)",8,9,21
"org.apache.hive.hcatalog.data.ReaderWriter.writeDatum(DataOutput,Object)",3,4,21
"org.apache.hive.hcatalog.data.TestDefaultHCatRecord.getByteArray()",1,1,1
"org.apache.hive.hcatalog.data.TestDefaultHCatRecord.getGetSet2InpRec()",1,1,1
"org.apache.hive.hcatalog.data.TestDefaultHCatRecord.getHCat13TypesComplexRecord()",1,1,1
"org.apache.hive.hcatalog.data.TestDefaultHCatRecord.getHCat13TypesRecord()",1,1,1
"org.apache.hive.hcatalog.data.TestDefaultHCatRecord.getHCatRecords()",1,1,1
"org.apache.hive.hcatalog.data.TestDefaultHCatRecord.getList()",1,1,1
"org.apache.hive.hcatalog.data.TestDefaultHCatRecord.getStruct()",1,1,1
"org.apache.hive.hcatalog.data.TestDefaultHCatRecord.testCompareTo()",1,1,1
"org.apache.hive.hcatalog.data.TestDefaultHCatRecord.testEqualsObject()",1,1,1
"org.apache.hive.hcatalog.data.TestDefaultHCatRecord.testGetSetByType1()",1,1,1
"org.apache.hive.hcatalog.data.TestDefaultHCatRecord.testGetSetByType2()",1,1,1
"org.apache.hive.hcatalog.data.TestDefaultHCatRecord.testGetSetByType3()",1,1,1
"org.apache.hive.hcatalog.data.TestDefaultHCatRecord.testRYW()",1,3,3
"org.apache.hive.hcatalog.data.TestHCatRecordSerDe.getData()",1,1,1
"org.apache.hive.hcatalog.data.TestHCatRecordSerDe.testRW()",1,2,2
"org.apache.hive.hcatalog.data.TestJsonSerDe.getData()",1,1,1
"org.apache.hive.hcatalog.data.TestJsonSerDe.getInternalNames(String)",3,2,4
"org.apache.hive.hcatalog.data.TestJsonSerDe.testLooseJsonReadability()",1,1,1
"org.apache.hive.hcatalog.data.TestJsonSerDe.testRW()",1,2,2
"org.apache.hive.hcatalog.data.TestJsonSerDe.testRobustRead()",1,4,4
"org.apache.hive.hcatalog.data.TestJsonSerDe.testUpperCaseKey()",1,1,1
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.getHCatRecord()",1,1,1
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.getObjectInspector()",1,1,1
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.getObjectInspector(TypeInfo)",1,1,1
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.getTypeInfo()",1,1,1
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.testCopy()",1,1,2
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.testGet()",1,1,1
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.testGetAll()",1,1,1
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.testGetWithName()",1,1,1
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.testGetWritable()",1,1,1
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.testReadFields()",1,1,2
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.testRemove()",1,1,2
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.testSet()",1,1,2
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.testSetWithName()",1,1,2
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.testSize()",1,1,1
"org.apache.hive.hcatalog.data.TestLazyHCatRecord.testWrite()",1,1,2
"org.apache.hive.hcatalog.data.TestReaderWriter.HCatRecordItr.hasNext()",1,1,2
"org.apache.hive.hcatalog.data.TestReaderWriter.HCatRecordItr.next()",1,1,1
"org.apache.hive.hcatalog.data.TestReaderWriter.HCatRecordItr.remove()",1,1,1
"org.apache.hive.hcatalog.data.TestReaderWriter.commit(Map<String, String>,boolean,WriterContext)",1,2,2
"org.apache.hive.hcatalog.data.TestReaderWriter.getRecord(int)",1,1,1
"org.apache.hive.hcatalog.data.TestReaderWriter.runsInMaster(Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.data.TestReaderWriter.runsInMaster(Map<String, String>,boolean)",1,1,1
"org.apache.hive.hcatalog.data.TestReaderWriter.runsInSlave(ReaderContext,int)",1,2,2
"org.apache.hive.hcatalog.data.TestReaderWriter.runsInSlave(WriterContext)",1,1,1
"org.apache.hive.hcatalog.data.TestReaderWriter.test()",1,3,3
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.Category.fromType(Type)",4,1,4
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.HCatFieldSchema()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.HCatFieldSchema(String,PrimitiveTypeInfo,String)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.HCatFieldSchema(String,Type,HCatSchema,String)",1,2,2
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.HCatFieldSchema(String,Type,String)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.HCatFieldSchema(String,Type,Type,HCatSchema,String)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.Type.Type(Category)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.Type.Type(PrimitiveCategory)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.Type.getCategory()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.Type.getPrimitiveCategory()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.Type.getPrimitiveHType(PrimitiveTypeInfo)",3,2,3
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.Type.numPrimitiveTypes()",1,1,3
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.assertTypeInCategory(Type,Category,String)",2,1,2
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.assertTypeNotInCategory(Type,Category)",2,1,2
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.createMapTypeFieldSchema(String,PrimitiveTypeInfo,HCatSchema,String)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.equals(Object)",11,3,11
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.getArrayElementSchema()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.getCategory()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.getComment()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.getMapKeyType()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.getMapKeyTypeInfo()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.getMapValueSchema()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.getName()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.getStructSubSchema()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.getType()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.getTypeInfo()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.getTypeString()",2,7,8
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.hashCode()",1,4,4
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.isComplex()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.setName(String)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatFieldSchema.toString()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchema.HCatSchema(List<HCatFieldSchema>)",4,3,4
"org.apache.hive.hcatalog.data.schema.HCatSchema.append(HCatFieldSchema)",3,1,3
"org.apache.hive.hcatalog.data.schema.HCatSchema.equals(Object)",5,1,5
"org.apache.hive.hcatalog.data.schema.HCatSchema.get(String)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchema.get(int)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchema.getFieldNames()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchema.getFields()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchema.getPosition(String)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchema.getSchemaAsTypeString()",1,4,4
"org.apache.hive.hcatalog.data.schema.HCatSchema.hashCode()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchema.normalizeName(String)",1,2,2
"org.apache.hive.hcatalog.data.schema.HCatSchema.reAlignPositionMap(int,int)",1,3,3
"org.apache.hive.hcatalog.data.schema.HCatSchema.remove(HCatFieldSchema)",2,1,2
"org.apache.hive.hcatalog.data.schema.HCatSchema.size()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchema.toString()",1,4,4
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.CollectionBuilder.CollectionBuilder()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.CollectionBuilder.addField(FieldSchema)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.CollectionBuilder.addField(HCatFieldSchema)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.CollectionBuilder.build()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.MapBuilder.build()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.MapBuilder.withKeyType(PrimitiveTypeInfo)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.MapBuilder.withValueSchema(HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.constructHCatSchema(StructTypeInfo)",1,2,2
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getFieldSchema(HCatFieldSchema)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getFieldSchemas(List<HCatFieldSchema>)",1,2,2
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(FieldSchema)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(String,TypeInfo)",5,5,5
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatSchema(List<? extends FieldSchema>)",1,2,2
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatSchema(Schema)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatSchema(String)",2,2,3
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatSchema(TypeInfo)",5,5,5
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatSchemaFromTypeString(String)",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getListSchemaBuilder()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getMapSchemaBuilder()",1,1,1
"org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getStructSchemaBuilder()",1,1,1
"org.apache.hive.hcatalog.data.schema.TestHCatSchema.testCannotAddFieldMoreThanOnce()",1,3,3
"org.apache.hive.hcatalog.data.schema.TestHCatSchema.testCannotInstantiateSchemaWithRepeatedFieldNames()",1,2,2
"org.apache.hive.hcatalog.data.schema.TestHCatSchema.testHashCodeEquals()",1,1,1
"org.apache.hive.hcatalog.data.schema.TestHCatSchema.testRemoveAddField()",1,2,2
"org.apache.hive.hcatalog.data.schema.TestHCatSchema.testRemoveAddField2()",1,4,4
"org.apache.hive.hcatalog.data.schema.TestHCatSchemaUtils.pretty_print(PrintStream,HCatFieldSchema,String)",1,4,4
"org.apache.hive.hcatalog.data.schema.TestHCatSchemaUtils.pretty_print(PrintStream,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.data.schema.TestHCatSchemaUtils.pretty_print(PrintStream,HCatSchema,String)",1,3,3
"org.apache.hive.hcatalog.data.schema.TestHCatSchemaUtils.testSimpleOperation()",1,1,1
"org.apache.hive.hcatalog.data.transfer.DataTransferFactory.getHCatReader(ReadEntity,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.data.transfer.DataTransferFactory.getHCatReader(ReaderContext,int)",1,1,1
"org.apache.hive.hcatalog.data.transfer.DataTransferFactory.getHCatReader(ReaderContext,int,StateProvider)",1,1,1
"org.apache.hive.hcatalog.data.transfer.DataTransferFactory.getHCatWriter(WriteEntity,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.data.transfer.DataTransferFactory.getHCatWriter(WriterContext)",1,1,1
"org.apache.hive.hcatalog.data.transfer.DataTransferFactory.getHCatWriter(WriterContext,StateProvider)",1,1,1
"org.apache.hive.hcatalog.data.transfer.EntityBase.Entity.getDbName()",1,1,1
"org.apache.hive.hcatalog.data.transfer.EntityBase.Entity.getPartitionKVs()",1,1,1
"org.apache.hive.hcatalog.data.transfer.EntityBase.Entity.getRegion()",1,1,1
"org.apache.hive.hcatalog.data.transfer.EntityBase.Entity.getTableName()",1,1,1
"org.apache.hive.hcatalog.data.transfer.HCatReader.HCatReader(Configuration,StateProvider)",1,1,1
"org.apache.hive.hcatalog.data.transfer.HCatReader.HCatReader(Map<String, String>)",1,3,3
"org.apache.hive.hcatalog.data.transfer.HCatReader.HCatReader(ReadEntity,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.data.transfer.HCatWriter.HCatWriter(Configuration,StateProvider)",1,1,1
"org.apache.hive.hcatalog.data.transfer.HCatWriter.HCatWriter(Map<String, String>)",1,3,3
"org.apache.hive.hcatalog.data.transfer.HCatWriter.HCatWriter(WriteEntity,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.data.transfer.ReadEntity.Builder.build()",1,1,1
"org.apache.hive.hcatalog.data.transfer.ReadEntity.Builder.withDatabase(String)",1,1,1
"org.apache.hive.hcatalog.data.transfer.ReadEntity.Builder.withFilter(String)",1,1,1
"org.apache.hive.hcatalog.data.transfer.ReadEntity.Builder.withPartition(Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.data.transfer.ReadEntity.Builder.withRegion(String)",1,1,1
"org.apache.hive.hcatalog.data.transfer.ReadEntity.Builder.withTable(String)",1,1,1
"org.apache.hive.hcatalog.data.transfer.ReadEntity.ReadEntity()",1,1,1
"org.apache.hive.hcatalog.data.transfer.ReadEntity.ReadEntity(Builder)",1,1,1
"org.apache.hive.hcatalog.data.transfer.ReadEntity.getFilterString()",1,1,1
"org.apache.hive.hcatalog.data.transfer.WriteEntity.Builder.build()",1,1,1
"org.apache.hive.hcatalog.data.transfer.WriteEntity.Builder.withDatabase(String)",1,1,1
"org.apache.hive.hcatalog.data.transfer.WriteEntity.Builder.withPartition(Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.data.transfer.WriteEntity.Builder.withRegion(String)",1,1,1
"org.apache.hive.hcatalog.data.transfer.WriteEntity.Builder.withTable(String)",1,1,1
"org.apache.hive.hcatalog.data.transfer.WriteEntity.WriteEntity()",1,1,1
"org.apache.hive.hcatalog.data.transfer.WriteEntity.WriteEntity(Builder)",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader.HCatInputFormatReader(ReadEntity,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader.HCatInputFormatReader(ReaderContext,int,StateProvider)",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader.HCatRecordItr.HCatRecordItr(RecordReader<WritableComparable, HCatRecord>)",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader.HCatRecordItr.hasNext()",2,1,4
"org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader.HCatRecordItr.next()",1,1,3
"org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader.HCatRecordItr.remove()",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader.prepareRead()",1,1,3
"org.apache.hive.hcatalog.data.transfer.impl.HCatInputFormatReader.read()",1,1,3
"org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter.HCatOutputFormatWriter(WriteEntity,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter.HCatOutputFormatWriter(WriterContext,StateProvider)",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter.abort(WriterContext)",1,1,3
"org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter.commit(WriterContext)",1,1,3
"org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter.prepareWrite()",1,1,3
"org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter.setVarsInConf(int)",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.HCatOutputFormatWriter.write(Iterator<HCatRecord>)",3,7,9
"org.apache.hive.hcatalog.data.transfer.impl.ReaderContextImpl.ReaderContextImpl()",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.ReaderContextImpl.getConf()",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.ReaderContextImpl.getSplits()",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.ReaderContextImpl.numSplits()",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.ReaderContextImpl.readExternal(ObjectInput)",1,2,2
"org.apache.hive.hcatalog.data.transfer.impl.ReaderContextImpl.setConf(Configuration)",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.ReaderContextImpl.setInputSplits(List<InputSplit>)",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.ReaderContextImpl.writeExternal(ObjectOutput)",1,2,2
"org.apache.hive.hcatalog.data.transfer.impl.WriterContextImpl.WriterContextImpl()",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.WriterContextImpl.getConf()",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.WriterContextImpl.readExternal(ObjectInput)",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.WriterContextImpl.setConf(Configuration)",1,1,1
"org.apache.hive.hcatalog.data.transfer.impl.WriterContextImpl.writeExternal(ObjectOutput)",1,1,1
"org.apache.hive.hcatalog.data.transfer.state.DefaultStateProvider.get()",1,1,2
"org.apache.hive.hcatalog.data.transfer.state.DefaultStateProvider.getId()",1,1,1
"org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor.exec(JobContext,Partition,Path)",1,1,1
"org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor.getParentFSPath(Path)",1,1,1
"org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor.getProcessedLocation(Path)",1,1,1
"org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor.harFile(Path)",1,1,1
"org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor.isEnabled()",1,1,1
"org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor.makeHar(JobContext,String,String)",2,3,5
"org.apache.hive.hcatalog.har.HarOutputCommitterPostProcessor.setEnabled(boolean)",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.Builder.Builder(File)",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.Builder.build()",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.Builder.hbaseConf(Configuration)",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.Builder.hiveConf(HiveConf)",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.Builder.jobConf(JobConf)",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.Builder.miniHBaseClusterEnabled(boolean)",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.Builder.miniHiveMetastoreEnabled(boolean)",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.Builder.miniMRClusterEnabled(boolean)",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.Builder.miniZookeeperClusterEnabled(boolean)",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.Builder.numTaskTrackers(int)",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.ManyMiniCluster(Builder)",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.create(File)",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.findFreePort()",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.getFileSystem()",1,1,2
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.getHBaseConf()",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.getHiveConf()",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.getHiveMetaStoreClient()",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.getJobConf()",1,1,1
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.setUpMetastore()",1,2,2
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.setupHBaseCluster()",1,2,3
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.setupMRCluster()",1,1,3
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.setupZookeeper()",1,1,2
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.start()",1,6,8
"org.apache.hive.hcatalog.hbase.ManyMiniCluster.stop()",1,10,10
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.Context.Context(String)",1,1,1
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.Context.getCluster()",1,1,1
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.Context.getFileSystem()",1,1,1
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.Context.getHbaseConf()",1,1,1
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.Context.getHiveConf()",1,1,1
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.Context.getJobConf()",1,1,1
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.Context.getTestDir()",1,1,1
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.Context.start()",1,3,3
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.Context.stop()",2,2,3
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.createTable(String,String[])",1,3,3
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.getCluster()",1,1,1
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.getContextHandle()",1,1,1
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.getFileSystem()",1,1,1
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.getHbaseConf()",1,1,1
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.getHiveConf()",1,1,1
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.getJobConf()",1,1,1
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.getTestDir()",1,1,1
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.newTableName(String)",2,4,4
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.setup()",1,2,2
"org.apache.hive.hcatalog.hbase.SkeletonHBaseTest.tearDown()",1,1,1
"org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.Initialize()",1,3,3
"org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.createTestDataFile(String)",1,3,3
"org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.generatePuts(String)",1,2,2
"org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.populateHBaseTable(String)",1,1,1
"org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.testPigFilterProjection()",1,2,2
"org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.testPigHBaseSchema()",1,1,1
"org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.testPigPopulation()",1,3,3
"org.apache.hive.hcatalog.listener.NotificationListener.NotificationListener(Configuration)",1,1,1
"org.apache.hive.hcatalog.listener.NotificationListener.createConnection()",1,4,4
"org.apache.hive.hcatalog.listener.NotificationListener.createProducer(Destination)",1,1,1
"org.apache.hive.hcatalog.listener.NotificationListener.createSession()",1,1,1
"org.apache.hive.hcatalog.listener.NotificationListener.createTopic(String)",1,1,1
"org.apache.hive.hcatalog.listener.NotificationListener.finalize()",1,3,3
"org.apache.hive.hcatalog.listener.NotificationListener.getTopicName(Table,ListenerEvent)",1,1,1
"org.apache.hive.hcatalog.listener.NotificationListener.getTopicPrefix(Configuration)",1,1,1
"org.apache.hive.hcatalog.listener.NotificationListener.isConnectionHealthy()",1,1,2
"org.apache.hive.hcatalog.listener.NotificationListener.onAddPartition(AddPartitionEvent)",1,4,4
"org.apache.hive.hcatalog.listener.NotificationListener.onAlterPartition(AlterPartitionEvent)",1,1,1
"org.apache.hive.hcatalog.listener.NotificationListener.onAlterTable(AlterTableEvent)",1,1,1
"org.apache.hive.hcatalog.listener.NotificationListener.onCreateDatabase(CreateDatabaseEvent)",1,2,2
"org.apache.hive.hcatalog.listener.NotificationListener.onCreateTable(CreateTableEvent)",2,4,4
"org.apache.hive.hcatalog.listener.NotificationListener.onDropDatabase(DropDatabaseEvent)",1,2,2
"org.apache.hive.hcatalog.listener.NotificationListener.onDropPartition(DropPartitionEvent)",1,4,4
"org.apache.hive.hcatalog.listener.NotificationListener.onDropTable(DropTableEvent)",1,2,2
"org.apache.hive.hcatalog.listener.NotificationListener.onLoadPartitionDone(LoadPartitionDoneEvent)",1,1,1
"org.apache.hive.hcatalog.listener.NotificationListener.send(HCatEventMessage,String)",1,1,1
"org.apache.hive.hcatalog.listener.NotificationListener.send(HCatEventMessage,String,int)",2,3,4
"org.apache.hive.hcatalog.listener.NotificationListener.testAndCreateConnection()",1,5,5
"org.apache.hive.hcatalog.listener.TestMsgBusConnection.connectClient()",1,1,1
"org.apache.hive.hcatalog.listener.TestMsgBusConnection.setUp()",1,1,1
"org.apache.hive.hcatalog.listener.TestMsgBusConnection.testConnection()",1,3,3
"org.apache.hive.hcatalog.listener.TestNotificationListener.onMessage(Message)",1,9,9
"org.apache.hive.hcatalog.listener.TestNotificationListener.setUp()",1,1,1
"org.apache.hive.hcatalog.listener.TestNotificationListener.tearDown()",1,1,1
"org.apache.hive.hcatalog.listener.TestNotificationListener.testAMQListener()",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultOutputCommitterContainer.DefaultOutputCommitterContainer(JobContext,OutputCommitter)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultOutputCommitterContainer.abortJob(JobContext,State)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultOutputCommitterContainer.abortTask(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultOutputCommitterContainer.cleanupJob(JobContext)",1,4,4
"org.apache.hive.hcatalog.mapreduce.DefaultOutputCommitterContainer.commitJob(JobContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultOutputCommitterContainer.commitTask(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultOutputCommitterContainer.needsTaskCommit(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultOutputCommitterContainer.setupJob(JobContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultOutputCommitterContainer.setupTask(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultOutputFormatContainer.DefaultOutputFormatContainer(OutputFormat<WritableComparable<?>, Writable>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultOutputFormatContainer.checkOutputSpecs(JobContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultOutputFormatContainer.getOutputCommitter(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultOutputFormatContainer.getOutputName(int)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultOutputFormatContainer.getRecordWriter(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultRecordWriterContainer.DefaultRecordWriterContainer(TaskAttemptContext,RecordWriter<? super WritableComparable<?>, ? super Writable>)",1,1,2
"org.apache.hive.hcatalog.mapreduce.DefaultRecordWriterContainer.close(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DefaultRecordWriterContainer.write(WritableComparable<?>,HCatRecord)",1,1,2
"org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.DynamicPartitionFileRecordWriterContainer(RecordWriter<? super WritableComparable<?>, ? super Writable>,TaskAttemptContext)",2,1,2
"org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.close(TaskAttemptContext)",1,4,4
"org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.configureDynamicStorageHandler(JobContext,List<String>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.getLocalFileWriter(HCatRecord)",3,4,6
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.FileOutputCommitterContainer(JobContext,OutputCommitter)",1,4,5
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.abortJob(JobContext,State)",4,9,10
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.abortTask(TaskAttemptContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.applyGroupAndPerms(FileSystem,Path,FsPermission,String,boolean)",1,5,5
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.cancelDelegationTokens(JobContext)",1,5,5
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.cleanupJob(JobContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.commitJob(JobContext)",1,8,8
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.commitTask(TaskAttemptContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.constructPartialPartPath(Path,String,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.constructPartition(JobContext,OutputJobInfo,String,String,Map<String, String>,HCatSchema,Map<String, String>,Table,FileSystem,String,FsPermission)",1,12,12
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.discoverPartitions(JobContext)",4,5,7
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.getCustomPartitionRootLocation(OutputJobInfo,Configuration)",1,4,4
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.getFinalDynamicPartitionDestination(Table,Map<String, String>,OutputJobInfo)",2,5,5
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.getFinalPath(FileSystem,Path,Path,Path,boolean)",5,6,9
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.getOutputDirMarking(Configuration)",1,1,1
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.getPartitionRootLocation(String,int)",2,3,4
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.getStorerParameterMap(StorerInfo)",1,2,2
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.moveCustomLocationTaskOutputs(FileSystem,Table,Configuration)",1,3,3
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.moveTaskOutputs(FileSystem,Path,Path,Path,boolean,boolean)",18,26,34
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.needsTaskCommit(TaskAttemptContext)",2,2,2
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.registerPartitions(JobContext)",11,28,32
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.setupJob(JobContext)",1,3,3
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.setupTask(TaskAttemptContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.updateTableSchema(HiveMetaStoreClient,Table,HCatSchema)",1,2,2
"org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer.FileOutputFormatContainer(OutputFormat<? super WritableComparable<?>, ? super Writable>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer.checkOutputSpecs(JobContext)",1,2,4
"org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer.getOutputCommitter(TaskAttemptContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer.getPartitionValueList(Table,Map<String, String>)",4,4,4
"org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer.getRecordWriter(TaskAttemptContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer.handleDuplicatePublish(JobContext,OutputJobInfo,HiveMetaStoreClient,Table)",6,4,6
"org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer.setWorkOutputPath(TaskAttemptContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.FileRecordWriterContainer(RecordWriter<? super WritableComparable<?>, ? super Writable>,TaskAttemptContext)",2,1,3
"org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.LocalFileWriter.LocalFileWriter(RecordWriter,ObjectInspector,SerDe,OutputJobInfo)",1,1,1
"org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.LocalFileWriter.getLocalJobInfo()",1,1,1
"org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.LocalFileWriter.getLocalObjectInspector()",1,1,1
"org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.LocalFileWriter.getLocalSerDe()",1,1,1
"org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.LocalFileWriter.getLocalWriter()",1,1,1
"org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.getStorageHandler()",1,1,1
"org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(WritableComparable<?>,HCatRecord)",1,2,3
"org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.FosterStorageHandler(Class<? extends InputFormat>,Class<? extends OutputFormat>,Class<? extends SerDe>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.FosterStorageHandler(String,String,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.configureInputJobProperties(TableDesc,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.configureJobConf(TableDesc,JobConf)",1,1,1
"org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.configureOutputJobProperties(TableDesc,Map<String, String>)",1,19,20
"org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.configureTableJobProperties(TableDesc,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.getAuthorizationProvider()",1,1,1
"org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.getConf()",1,1,1
"org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.getInputFormatClass()",1,1,1
"org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.getMetaHook()",1,1,1
"org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.getOutputFormatClass()",1,1,1
"org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.getOutputFormatContainer(OutputFormat)",1,1,1
"org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.getSerDeClass()",1,1,1
"org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.setConf(Configuration)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.createRecordReader(InputSplit,TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.getColValsNotInDataColumns(HCatSchema,PartInfo)",1,4,4
"org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.getJobInfo(Configuration)",2,1,2
"org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.getMapRedInputFormat(JobConf,Class)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.getOutputSchema(Configuration)",2,2,2
"org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.getSplits(JobContext)",2,5,7
"org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.getTableSchema(Configuration)",1,3,3
"org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.setInputPath(JobConf,String)",2,5,10
"org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.setOutputSchema(Job,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.checkOutputSpecs(JobContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.configureOutputStorageHandler(JobContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.configureOutputStorageHandler(JobContext,List<String>)",4,4,6
"org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.configureOutputStorageHandler(JobContext,OutputJobInfo,Map<String, String>)",1,4,4
"org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.getJobInfo(Configuration)",2,1,2
"org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.getOutputFormat(JobContext)",2,2,2
"org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.getTableSchema(Configuration)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatBaseOutputFormat.setPartDetails(OutputJobInfo,HCatSchema,Map<String, String>)",1,6,6
"org.apache.hive.hcatalog.mapreduce.HCatBaseTest.createPigServer(boolean)",2,2,2
"org.apache.hive.hcatalog.mapreduce.HCatBaseTest.logAndRegister(PigServer,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatBaseTest.logAndRegister(PigServer,String,int)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatBaseTest.setUp()",1,2,2
"org.apache.hive.hcatalog.mapreduce.HCatBaseTest.setUpHiveConf()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatBaseTest.setUpTestDataDir()",1,2,2
"org.apache.hive.hcatalog.mapreduce.HCatFileUtil.getPartKeyValuesForCustomLocation(Map<String, String>,OutputJobInfo,String)",1,3,3
"org.apache.hive.hcatalog.mapreduce.HCatFileUtil.resolveCustomPath(OutputJobInfo,Map<String, String>,boolean)",2,8,8
"org.apache.hive.hcatalog.mapreduce.HCatFileUtil.setCustomPath(String,OutputJobInfo)",3,3,4
"org.apache.hive.hcatalog.mapreduce.HCatInputFormat.getDataColumns(Configuration)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatInputFormat.getPartitionColumns(Configuration)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setFilter(String)",2,2,3
"org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setInput(Configuration,String,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setInput(Configuration,String,String,String)",1,1,2
"org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setInput(Job,String,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setInput(Job,String,String,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setProperties(Properties)",1,1,2
"org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil.createJobContext(JobConf,JobID,Progressable)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil.createJobContext(JobContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil.createTaskAttemptContext(Configuration,TaskAttemptID)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil.createTaskAttemptContext(JobConf,TaskAttemptID,Progressable)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil.createTaskAttemptContext(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatMapRedUtil.createTaskAttemptID(JobID,boolean,int,int)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.HCatMapReduceTest(String,String,String,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.MapCreate.map(LongWritable,Text,Context)",1,2,2
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.MapRead.map(WritableComparable,HCatRecord,Context)",1,2,2
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.createInputFile(Path,int)",1,3,3
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.createTable()",1,5,7
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.deleteTable()",1,5,6
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.generateParameters()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.getTableSchema()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.isTableExternal()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.isTableImmutable()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.runMRCreate(Map<String, String>,List<HCatFieldSchema>,List<HCatRecord>,int,boolean)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.runMRCreate(Map<String, String>,List<HCatFieldSchema>,List<HCatRecord>,int,boolean,boolean,String)",1,8,8
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.runMRRead(int)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.runMRRead(int,String)",1,2,2
"org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.setUpOneTime()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.getHarRequested(HiveConf)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.getMaxDynamicPartitions(HiveConf)",1,2,2
"org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.getOutputCommitter(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.getRecordWriter(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.setOutput(Configuration,Credentials,OutputJobInfo)",11,17,25
"org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.setOutput(Job,OutputJobInfo)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.setSchema(Configuration,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.setSchema(Job,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatRecordReader.HCatRecordReader(HiveStorageHandler,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatRecordReader.InputErrorTracker.InputErrorTracker(Configuration)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatRecordReader.InputErrorTracker.incErrors(Throwable)",4,2,6
"org.apache.hive.hcatalog.mapreduce.HCatRecordReader.InputErrorTracker.incRecords()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatRecordReader.close()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatRecordReader.createBaseRecordReader(HCatSplit,HiveStorageHandler,TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatRecordReader.createDeserializer(HCatSplit,HiveStorageHandler,TaskAttemptContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.HCatRecordReader.getCurrentKey()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatRecordReader.getCurrentValue()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatRecordReader.getProgress()",1,2,2
"org.apache.hive.hcatalog.mapreduce.HCatRecordReader.initialize(InputSplit,TaskAttemptContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.HCatRecordReader.nextKeyValue()",3,6,7
"org.apache.hive.hcatalog.mapreduce.HCatSplit.HCatSplit()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatSplit.HCatSplit(PartInfo,InputSplit,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatSplit.getBaseSplit()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatSplit.getDataSchema()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatSplit.getLength()",1,2,2
"org.apache.hive.hcatalog.mapreduce.HCatSplit.getLocations()",1,2,2
"org.apache.hive.hcatalog.mapreduce.HCatSplit.getPartitionInfo()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatSplit.getTableSchema()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatSplit.readFields(DataInput)",1,1,2
"org.apache.hive.hcatalog.mapreduce.HCatSplit.write(DataOutput)",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatTableInfo.HCatTableInfo(String,String,HCatSchema,HCatSchema,StorerInfo,Table)",1,1,2
"org.apache.hive.hcatalog.mapreduce.HCatTableInfo.equals(Object)",9,8,16
"org.apache.hive.hcatalog.mapreduce.HCatTableInfo.getDataColumns()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatTableInfo.getDatabaseName()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatTableInfo.getPartitionColumns()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatTableInfo.getStorerInfo()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatTableInfo.getTable()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatTableInfo.getTableLocation()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatTableInfo.getTableName()",1,1,1
"org.apache.hive.hcatalog.mapreduce.HCatTableInfo.hashCode()",1,7,7
"org.apache.hive.hcatalog.mapreduce.HCatTableInfo.valueOf(Table)",1,1,1
"org.apache.hive.hcatalog.mapreduce.InitializeInput.extractPartInfo(HCatSchema,StorageDescriptor,Map<String, String>,Configuration,InputJobInfo)",1,2,2
"org.apache.hive.hcatalog.mapreduce.InitializeInput.getInputJobInfo(Configuration,InputJobInfo,String)",3,6,6
"org.apache.hive.hcatalog.mapreduce.InitializeInput.setInput(Configuration,InputJobInfo)",1,1,1
"org.apache.hive.hcatalog.mapreduce.InitializeInput.setInput(Job,InputJobInfo)",1,1,1
"org.apache.hive.hcatalog.mapreduce.InputJobInfo.InputJobInfo(String,String,String,Properties)",1,1,3
"org.apache.hive.hcatalog.mapreduce.InputJobInfo.create(String,String,String,Properties)",1,1,1
"org.apache.hive.hcatalog.mapreduce.InputJobInfo.getDatabaseName()",1,1,1
"org.apache.hive.hcatalog.mapreduce.InputJobInfo.getFilter()",1,1,1
"org.apache.hive.hcatalog.mapreduce.InputJobInfo.getPartitions()",1,1,1
"org.apache.hive.hcatalog.mapreduce.InputJobInfo.getProperties()",1,1,1
"org.apache.hive.hcatalog.mapreduce.InputJobInfo.getTableInfo()",1,1,1
"org.apache.hive.hcatalog.mapreduce.InputJobInfo.getTableName()",1,1,1
"org.apache.hive.hcatalog.mapreduce.InputJobInfo.readObject(ObjectInputStream)",1,1,1
"org.apache.hive.hcatalog.mapreduce.InputJobInfo.setPartitions(List<PartInfo>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.InputJobInfo.setTableInfo(HCatTableInfo)",1,1,1
"org.apache.hive.hcatalog.mapreduce.InputJobInfo.writeObject(ObjectOutputStream)",1,1,1
"org.apache.hive.hcatalog.mapreduce.InternalUtil.castToHCatSplit(InputSplit)",2,2,2
"org.apache.hive.hcatalog.mapreduce.InternalUtil.createPtnKeyValueMap(Table,Partition)",2,3,3
"org.apache.hive.hcatalog.mapreduce.InternalUtil.createReporter(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.InternalUtil.createStructObjectInspector(HCatSchema)",2,2,3
"org.apache.hive.hcatalog.mapreduce.InternalUtil.extractStorerInfo(StorageDescriptor,Map<String, String>)",1,3,3
"org.apache.hive.hcatalog.mapreduce.InternalUtil.getObjectInspector(TypeInfo)",2,3,7
"org.apache.hive.hcatalog.mapreduce.InternalUtil.getSerdeProperties(HCatTableInfo,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.mapreduce.InternalUtil.initializeDeserializer(Deserializer,Configuration,HCatTableInfo,HCatSchema)",1,1,1
"org.apache.hive.hcatalog.mapreduce.InternalUtil.initializeOutputSerDe(SerDe,Configuration,OutputJobInfo)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.BaseOutputCommitterContainer.BaseOutputCommitterContainer(OutputCommitter,TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.BaseOutputCommitterContainer.getBaseCommitter()",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.BaseOutputCommitterContainer.getContext()",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.BaseRecordWriterContainer.BaseRecordWriterContainer(RecordWriter,TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.BaseRecordWriterContainer.getContext()",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.BaseRecordWriterContainer.getRecordWriter()",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.JobConfigurer.JobConfigurer(Job)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.JobConfigurer.addOutputFormat(String,Class<? extends OutputFormat>,Class<?>,Class<?>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.JobConfigurer.configure()",1,2,2
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.JobConfigurer.create(Job)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.JobConfigurer.getJob(String)",2,1,2
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.KeyValue.KeyValue(K,V)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.KeyValue.getKey()",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.KeyValue.getValue()",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.KeyValue.readFields(DataInput)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.KeyValue.write(DataOutput)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.MultiOutputCommitter.MultiOutputCommitter(TaskAttemptContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.MultiOutputCommitter.abortJob(JobContext,State)",1,2,2
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.MultiOutputCommitter.abortTask(TaskAttemptContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.MultiOutputCommitter.commitJob(JobContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.MultiOutputCommitter.commitTask(TaskAttemptContext)",1,3,3
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.MultiOutputCommitter.needsTaskCommit(TaskAttemptContext)",1,3,3
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.MultiOutputCommitter.setupJob(JobContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.MultiOutputCommitter.setupTask(TaskAttemptContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.MultiRecordWriter.MultiRecordWriter(TaskAttemptContext)",1,4,4
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.MultiRecordWriter.close(TaskAttemptContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.MultiRecordWriter.write(Writable,Writable)",2,1,2
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.addToConfig(String,Configuration)",1,2,2
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.checkOutputSpecs(JobContext)",1,2,2
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.createConfigurer(Job)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.getAliasConfName(String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.getJobContext(String,JobContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.getMergedConfValue(String,String,String)",2,2,3
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.getOutputCommitter(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.getOutputFormatAliases(JobContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.getOutputFormatInstance(JobContext)",1,1,2
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.getRecordWriter(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.getTaskAttemptContext(String,TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.setAliasConf(String,JobContext,JobContext)",1,7,7
"org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.write(String,K,V,TaskInputOutputContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputCommitterContainer.OutputCommitterContainer(JobContext,OutputCommitter)",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputCommitterContainer.getBaseOutputCommitter()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputFormatContainer.OutputFormatContainer(OutputFormat<? super WritableComparable<?>, ? super Writable>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputFormatContainer.getBaseOutputFormat()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.OutputJobInfo(String,String,Map<String, String>)",1,1,2
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.create(String,String,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.getCustomDynamicPath()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.getCustomDynamicRoot()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.getDatabaseName()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.getDynamicPartitioningKeys()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.getHarRequested()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.getLocation()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.getMaxDynamicPartitions()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.getOutputSchema()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.getPartitionValues()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.getPosOfDynPartCols()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.getPosOfPartCols()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.getProperties()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.getTableInfo()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.getTableName()",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.isDynamicPartitioningUsed()",1,2,2
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.setCustomDynamicLocation(String,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.setDynamicPartitioningKeys(List<String>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.setHarRequested(boolean)",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.setLocation(String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.setMaximumDynamicPartitions(int)",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.setOutputSchema(HCatSchema)",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.setPartitionValues(Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.setPosOfDynPartCols(List<Integer>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.setPosOfPartCols(List<Integer>)",1,1,3
"org.apache.hive.hcatalog.mapreduce.OutputJobInfo.setTableInfo(HCatTableInfo)",1,1,1
"org.apache.hive.hcatalog.mapreduce.PartInfo.PartInfo(HCatSchema,HiveStorageHandler,String,Properties,Map<String, String>,HCatTableInfo)",1,1,1
"org.apache.hive.hcatalog.mapreduce.PartInfo.getInputFormatClassName()",1,1,1
"org.apache.hive.hcatalog.mapreduce.PartInfo.getInputStorageHandlerProperties()",1,1,1
"org.apache.hive.hcatalog.mapreduce.PartInfo.getJobProperties()",1,1,1
"org.apache.hive.hcatalog.mapreduce.PartInfo.getLocation()",1,1,1
"org.apache.hive.hcatalog.mapreduce.PartInfo.getOutputFormatClassName()",1,1,1
"org.apache.hive.hcatalog.mapreduce.PartInfo.getPartitionSchema()",1,1,1
"org.apache.hive.hcatalog.mapreduce.PartInfo.getPartitionValues()",1,1,1
"org.apache.hive.hcatalog.mapreduce.PartInfo.getSerdeClassName()",1,1,1
"org.apache.hive.hcatalog.mapreduce.PartInfo.getStorageHandlerClassName()",1,1,1
"org.apache.hive.hcatalog.mapreduce.PartInfo.getTableInfo()",1,1,1
"org.apache.hive.hcatalog.mapreduce.PartInfo.setPartitionValues(Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.ProgressReporter.ProgressReporter(TaskAttemptContext)",1,1,2
"org.apache.hive.hcatalog.mapreduce.ProgressReporter.getCounter(Enum<?>)",1,2,2
"org.apache.hive.hcatalog.mapreduce.ProgressReporter.getCounter(String,String)",1,2,2
"org.apache.hive.hcatalog.mapreduce.ProgressReporter.getInputSplit()",1,1,1
"org.apache.hive.hcatalog.mapreduce.ProgressReporter.getProgress()",1,1,1
"org.apache.hive.hcatalog.mapreduce.ProgressReporter.incrCounter(Enum<?>,long)",1,2,2
"org.apache.hive.hcatalog.mapreduce.ProgressReporter.incrCounter(String,String,long)",1,2,2
"org.apache.hive.hcatalog.mapreduce.ProgressReporter.progress()",1,2,2
"org.apache.hive.hcatalog.mapreduce.ProgressReporter.setStatus(String)",1,2,2
"org.apache.hive.hcatalog.mapreduce.RecordWriterContainer.RecordWriterContainer(TaskAttemptContext,RecordWriter<? super WritableComparable<?>, ? super Writable>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.RecordWriterContainer.getBaseRecordWriter()",1,1,1
"org.apache.hive.hcatalog.mapreduce.Security.cancelToken(HiveMetaStoreClient,JobContext)",2,4,4
"org.apache.hive.hcatalog.mapreduce.Security.getInstance()",1,1,1
"org.apache.hive.hcatalog.mapreduce.Security.getTokenSignature(OutputJobInfo)",1,5,5
"org.apache.hive.hcatalog.mapreduce.Security.handleSecurity(Credentials,OutputJobInfo,HiveMetaStoreClient,Configuration,boolean)",1,5,5
"org.apache.hive.hcatalog.mapreduce.Security.handleSecurity(Job,OutputJobInfo,HiveMetaStoreClient,Configuration,boolean)",1,1,1
"org.apache.hive.hcatalog.mapreduce.Security.isSecurityEnabled()",1,4,4
"org.apache.hive.hcatalog.mapreduce.SpecialCases.addSpecialCasesParametersForHCatLoader(Configuration,HCatTableInfo)",2,4,5
"org.apache.hive.hcatalog.mapreduce.SpecialCases.addSpecialCasesParametersToOutputJobProperties(Map<String, String>,OutputJobInfo,Class<? extends OutputFormat>)",1,5,5
"org.apache.hive.hcatalog.mapreduce.StaticPartitionFileRecordWriterContainer.StaticPartitionFileRecordWriterContainer(RecordWriter<? super WritableComparable<?>, ? super Writable>,TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.StaticPartitionFileRecordWriterContainer.close(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.StaticPartitionFileRecordWriterContainer.getLocalFileWriter(HCatRecord)",1,1,1
"org.apache.hive.hcatalog.mapreduce.StorerInfo.StorerInfo(String,String,String,String,Properties)",1,1,1
"org.apache.hive.hcatalog.mapreduce.StorerInfo.getIfClass()",1,1,1
"org.apache.hive.hcatalog.mapreduce.StorerInfo.getOfClass()",1,1,1
"org.apache.hive.hcatalog.mapreduce.StorerInfo.getProperties()",1,1,1
"org.apache.hive.hcatalog.mapreduce.StorerInfo.getSerdeClass()",1,1,1
"org.apache.hive.hcatalog.mapreduce.StorerInfo.getStorageHandlerClass()",1,1,1
"org.apache.hive.hcatalog.mapreduce.StorerInfo.setIfClass(String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.StorerInfo.setProperties(Properties)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.TestHCatDynamicPartitioned(String,String,String,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned._testHCatDynamicPartitionMaxPartitions()",1,2,3
"org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.generateDataColumns()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.generateWriteRecords(int,int,int)",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.getPartitionKeys()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.getTableColumns()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.runHCatDynamicPartitionedTable(boolean,String)",4,5,9
"org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.testHCatDynamicPartitionedTable()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.testHCatDynamicPartitionedTableMultipleTask()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatExternalDynamicPartitioned.TestHCatExternalDynamicPartitioned(String,String,String,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatExternalDynamicPartitioned.isTableExternal()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatExternalDynamicPartitioned.testHCatExternalDynamicCustomLocation()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatExternalNonPartitioned.TestHCatExternalNonPartitioned(String,String,String,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatExternalNonPartitioned.isTableExternal()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatExternalPartitioned.TestHCatExternalPartitioned(String,String,String,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatExternalPartitioned.isTableExternal()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatHiveCompatibility.createInputData()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestHCatHiveCompatibility.testPartedRead()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestHCatHiveCompatibility.testUnpartedReadWrite()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestHCatHiveThriftCompatibility.setUp()",2,1,2
"org.apache.hive.hcatalog.mapreduce.TestHCatHiveThriftCompatibility.testDynamicCols()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatInputFormat.MyMapper.map(NullWritable,HCatRecord,Context)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatInputFormat.runJob(float)",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestHCatInputFormat.setUp()",2,3,4
"org.apache.hive.hcatalog.mapreduce.TestHCatInputFormat.testBadRecordHandlingFails()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatInputFormat.testBadRecordHandlingPasses()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatInputFormatMethods.setUp()",2,1,2
"org.apache.hive.hcatalog.mapreduce.TestHCatInputFormatMethods.testGetPartitionAndDataColumns()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.MyMapper.map(LongWritable,Text,Context)",2,2,5
"org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.RunMS.run()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.createInputFile()",1,3,3
"org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.createTable(String,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.getTableData(String,String)",1,4,4
"org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.initalizeTables()",1,4,5
"org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.initializeSetup()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.setup()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.tearDown()",1,3,3
"org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.testOutputFormat()",1,4,4
"org.apache.hive.hcatalog.mapreduce.TestHCatMutableDynamicPartitioned.TestHCatMutableDynamicPartitioned(String,String,String,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatMutableDynamicPartitioned.isTableImmutable()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatMutableNonPartitioned.TestHCatMutableNonPartitioned(String,String,String,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatMutableNonPartitioned.isTableImmutable()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatMutablePartitioned.TestHCatMutablePartitioned(String,String,String,String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatMutablePartitioned.isTableImmutable()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatNonPartitioned.TestHCatNonPartitioned(String,String,String,String)",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestHCatNonPartitioned.getPartitionKeys()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatNonPartitioned.getTableColumns()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatNonPartitioned.hiveReadTest()",2,2,3
"org.apache.hive.hcatalog.mapreduce.TestHCatNonPartitioned.testHCatNonPartitionedTable()",1,5,5
"org.apache.hive.hcatalog.mapreduce.TestHCatOutputFormat.initTable()",1,1,3
"org.apache.hive.hcatalog.mapreduce.TestHCatOutputFormat.publishTest(Job)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatOutputFormat.setUp()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestHCatOutputFormat.tearDown()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestHCatOutputFormat.testSetOutput()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.MapFail.map(LongWritable,Text,Context)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.createInputFile(Path,int)",1,3,3
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.createTable(String,String)",1,1,3
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.getPartitionKeys()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.getTableColumns()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.handleWorkDir()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.runMRCreateFail(String,String,Map<String, String>,List<HCatFieldSchema>)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.setup()",2,1,2
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.tearDown()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.testPartitionPublish()",1,3,3
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.TestHCatPartitioned(String,String,String,String)",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.columnOrderChangeTest()",1,4,5
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.getPartitionKeys()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.getTableColumns()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.hiveReadTest()",2,2,3
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.tableSchemaTest()",1,4,6
"org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.testHCatPartitionedTable()",1,6,8
"org.apache.hive.hcatalog.mapreduce.TestInputJobInfo.test4ArgCreate()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.MultiOutWordCountReducer.reduce(Text,Iterable<IntWritable>,Context)",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.MultiOutWordIndexMapper.map(LongWritable,Text,Context)",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.NullOutputFormat.getOutputCommitter(TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.WordCountMapper.map(LongWritable,Text,Context)",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.createInputFile(String)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.createWorkDir()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.readFully(Path)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.setup()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.tearDown()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.testMultiOutputFormatWithReduce()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.testMultiOutputFormatWithoutReduce()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestPassProperties.Initialize()",1,1,2
"org.apache.hive.hcatalog.mapreduce.TestPassProperties.Map.map(LongWritable,Text,Context)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestPassProperties.getSchema()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestPassProperties.testSequenceTableWriteReadMR()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.Map.map(LongWritable,Text,Context)",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.getSchema()",1,1,1
"org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.setup()",2,1,3
"org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.teardown()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.testSequenceTableWriteRead()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.testSequenceTableWriteReadMR()",1,3,3
"org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.testTextTableWriteRead()",1,2,2
"org.apache.hive.hcatalog.mapreduce.TestSequenceFileReadWrite.testTextTableWriteReadMR()",1,3,3
"org.apache.hive.hcatalog.messaging.AddPartitionMessage.AddPartitionMessage()",1,1,1
"org.apache.hive.hcatalog.messaging.AddPartitionMessage.checkValid()",3,1,3
"org.apache.hive.hcatalog.messaging.CreateDatabaseMessage.CreateDatabaseMessage()",1,1,1
"org.apache.hive.hcatalog.messaging.CreateTableMessage.CreateTableMessage()",1,1,1
"org.apache.hive.hcatalog.messaging.CreateTableMessage.checkValid()",2,1,2
"org.apache.hive.hcatalog.messaging.DropDatabaseMessage.DropDatabaseMessage()",1,1,1
"org.apache.hive.hcatalog.messaging.DropPartitionMessage.DropPartitionMessage()",1,1,1
"org.apache.hive.hcatalog.messaging.DropPartitionMessage.checkValid()",3,1,3
"org.apache.hive.hcatalog.messaging.DropTableMessage.DropTableMessage()",1,1,1
"org.apache.hive.hcatalog.messaging.DropTableMessage.checkValid()",2,1,2
"org.apache.hive.hcatalog.messaging.HCatEventMessage.EventType.EventType(String)",1,1,1
"org.apache.hive.hcatalog.messaging.HCatEventMessage.EventType.toString()",1,1,1
"org.apache.hive.hcatalog.messaging.HCatEventMessage.HCatEventMessage(EventType)",1,1,1
"org.apache.hive.hcatalog.messaging.HCatEventMessage.checkValid()",4,2,5
"org.apache.hive.hcatalog.messaging.HCatEventMessage.getEventType()",1,1,1
"org.apache.hive.hcatalog.messaging.MessageDeserializer.MessageDeserializer()",1,1,1
"org.apache.hive.hcatalog.messaging.MessageDeserializer.getHCatEventMessage(String,String)",8,8,8
"org.apache.hive.hcatalog.messaging.MessageFactory.getDeserializer(String,String)",1,1,1
"org.apache.hive.hcatalog.messaging.MessageFactory.getInstance()",1,2,2
"org.apache.hive.hcatalog.messaging.MessageFactory.getInstance(String)",1,1,2
"org.apache.hive.hcatalog.messaging.jms.MessagingUtils.MessagingUtils()",1,1,1
"org.apache.hive.hcatalog.messaging.jms.MessagingUtils.getMessage(Message)",2,2,4
"org.apache.hive.hcatalog.messaging.json.JSONAddPartitionMessage.JSONAddPartitionMessage()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONAddPartitionMessage.JSONAddPartitionMessage(String,String,String,String,List<Map<String, String>>,Long)",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONAddPartitionMessage.getDB()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONAddPartitionMessage.getPartitions()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONAddPartitionMessage.getServer()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONAddPartitionMessage.getServicePrincipal()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONAddPartitionMessage.getTable()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONAddPartitionMessage.getTimestamp()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONAddPartitionMessage.toString()",1,1,2
"org.apache.hive.hcatalog.messaging.json.JSONCreateDatabaseMessage.JSONCreateDatabaseMessage()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONCreateDatabaseMessage.JSONCreateDatabaseMessage(String,String,String,Long)",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONCreateDatabaseMessage.getDB()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONCreateDatabaseMessage.getServer()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONCreateDatabaseMessage.getServicePrincipal()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONCreateDatabaseMessage.getTimestamp()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONCreateDatabaseMessage.toString()",1,1,2
"org.apache.hive.hcatalog.messaging.json.JSONCreateTableMessage.JSONCreateTableMessage()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONCreateTableMessage.JSONCreateTableMessage(String,String,String,String,Long)",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONCreateTableMessage.getDB()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONCreateTableMessage.getServer()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONCreateTableMessage.getServicePrincipal()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONCreateTableMessage.getTable()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONCreateTableMessage.getTimestamp()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONCreateTableMessage.toString()",1,1,2
"org.apache.hive.hcatalog.messaging.json.JSONDropDatabaseMessage.JSONDropDatabaseMessage()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropDatabaseMessage.JSONDropDatabaseMessage(String,String,String,Long)",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropDatabaseMessage.getDB()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropDatabaseMessage.getServer()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropDatabaseMessage.getServicePrincipal()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropDatabaseMessage.getTimestamp()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropDatabaseMessage.toString()",1,1,2
"org.apache.hive.hcatalog.messaging.json.JSONDropPartitionMessage.JSONDropPartitionMessage()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropPartitionMessage.JSONDropPartitionMessage(String,String,String,String,List<Map<String, String>>,Long)",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropPartitionMessage.getDB()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropPartitionMessage.getPartitions()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropPartitionMessage.getServer()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropPartitionMessage.getServicePrincipal()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropPartitionMessage.getTable()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropPartitionMessage.getTimestamp()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropPartitionMessage.toString()",1,1,2
"org.apache.hive.hcatalog.messaging.json.JSONDropTableMessage.JSONDropTableMessage()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropTableMessage.JSONDropTableMessage(String,String,String,String,Long)",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropTableMessage.getDB()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropTableMessage.getServer()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropTableMessage.getServicePrincipal()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropTableMessage.getTable()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropTableMessage.getTimestamp()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONDropTableMessage.toString()",1,1,2
"org.apache.hive.hcatalog.messaging.json.JSONMessageDeserializer.getAddPartitionMessage(String)",1,1,2
"org.apache.hive.hcatalog.messaging.json.JSONMessageDeserializer.getCreateDatabaseMessage(String)",1,1,2
"org.apache.hive.hcatalog.messaging.json.JSONMessageDeserializer.getCreateTableMessage(String)",1,1,2
"org.apache.hive.hcatalog.messaging.json.JSONMessageDeserializer.getDropDatabaseMessage(String)",1,1,2
"org.apache.hive.hcatalog.messaging.json.JSONMessageDeserializer.getDropPartitionMessage(String)",1,1,2
"org.apache.hive.hcatalog.messaging.json.JSONMessageDeserializer.getDropTableMessage(String)",1,1,2
"org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.buildAddPartitionMessage(Table,List<Partition>)",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.buildCreateDatabaseMessage(Database)",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.buildCreateTableMessage(Table)",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.buildDropDatabaseMessage(Database)",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.buildDropPartitionMessage(Table,Partition)",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.buildDropTableMessage(Table)",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.getDeserializer()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.getMessageFormat()",1,1,1
"org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.getPartitionKeyValues(Table,List<Partition>)",1,2,2
"org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.getPartitionKeyValues(Table,Partition)",1,2,2
"org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.getVersion()",1,1,1
"org.apache.hive.hcatalog.oozie.JavaAction.main(String[])",1,1,1
"org.apache.hive.hcatalog.pig.HCatBaseLoader.getFeatures()",1,1,1
"org.apache.hive.hcatalog.pig.HCatBaseLoader.getNext()",1,2,4
"org.apache.hive.hcatalog.pig.HCatBaseLoader.getSizeInBytes(InputJobInfo)",1,5,6
"org.apache.hive.hcatalog.pig.HCatBaseLoader.getStatistics(String,Job)",1,1,1
"org.apache.hive.hcatalog.pig.HCatBaseLoader.prepareToRead(RecordReader,PigSplit)",1,1,1
"org.apache.hive.hcatalog.pig.HCatBaseLoader.pushProjection(RequiredFieldList)",1,1,1
"org.apache.hive.hcatalog.pig.HCatBaseLoader.setUDFContextSignature(String)",1,1,1
"org.apache.hive.hcatalog.pig.HCatBaseLoader.storeInUDFContext(String,String,Object)",1,1,1
"org.apache.hive.hcatalog.pig.HCatBaseStorer.DataLossLogger.getColumnTypeKey(HCatFieldSchema)",1,2,2
"org.apache.hive.hcatalog.pig.HCatBaseStorer.DataLossLogger.logDataLossMsg(HCatFieldSchema,Object,String)",1,2,2
"org.apache.hive.hcatalog.pig.HCatBaseStorer.HCatBaseStorer(String,String)",4,7,7
"org.apache.hive.hcatalog.pig.HCatBaseStorer.checkSchema(ResourceSchema)",3,3,3
"org.apache.hive.hcatalog.pig.HCatBaseStorer.cleanupOnFailure(String,Job)",1,1,1
"org.apache.hive.hcatalog.pig.HCatBaseStorer.convertPigSchemaToHCatSchema(Schema,HCatSchema)",2,4,4
"org.apache.hive.hcatalog.pig.HCatBaseStorer.doSchemaValidations(Schema,HCatSchema)",1,3,3
"org.apache.hive.hcatalog.pig.HCatBaseStorer.getColFromSchema(String,HCatSchema)",4,5,6
"org.apache.hive.hcatalog.pig.HCatBaseStorer.getDefaultValue()",1,1,1
"org.apache.hive.hcatalog.pig.HCatBaseStorer.getHCatFSFromPigFS(FieldSchema,HCatFieldSchema,Schema,HCatSchema)",18,18,30
"org.apache.hive.hcatalog.pig.HCatBaseStorer.getJavaObj(Object,HCatFieldSchema)",17,20,34
"org.apache.hive.hcatalog.pig.HCatBaseStorer.handleOutOfRangeValue(Object,HCatFieldSchema)",1,1,1
"org.apache.hive.hcatalog.pig.HCatBaseStorer.handleOutOfRangeValue(Object,HCatFieldSchema,String)",3,3,6
"org.apache.hive.hcatalog.pig.HCatBaseStorer.prepareToWrite(RecordWriter)",1,1,1
"org.apache.hive.hcatalog.pig.HCatBaseStorer.putNext(Tuple)",1,2,3
"org.apache.hive.hcatalog.pig.HCatBaseStorer.relToAbsPathForStoreLocation(String,Path)",1,1,1
"org.apache.hive.hcatalog.pig.HCatBaseStorer.removeTupleFromBag(HCatFieldSchema,FieldSchema)",3,6,8
"org.apache.hive.hcatalog.pig.HCatBaseStorer.setStoreFuncUDFContextSignature(String)",1,1,1
"org.apache.hive.hcatalog.pig.HCatBaseStorer.storeStatistics(ResourceStatistics,String,Job)",1,1,1
"org.apache.hive.hcatalog.pig.HCatBaseStorer.throwTypeMismatchException(byte,List<Type>,HCatFieldSchema,int)",2,2,2
"org.apache.hive.hcatalog.pig.HCatBaseStorer.validateAlias(String)",3,1,3
"org.apache.hive.hcatalog.pig.HCatBaseStorer.validateSchema(FieldSchema,HCatFieldSchema,Schema,HCatSchema,int)",8,10,25
"org.apache.hive.hcatalog.pig.HCatLoader.getHCatComparisonString(Expression)",3,3,4
"org.apache.hive.hcatalog.pig.HCatLoader.getInputFormat()",1,1,2
"org.apache.hive.hcatalog.pig.HCatLoader.getPartitionFilterString()",1,2,2
"org.apache.hive.hcatalog.pig.HCatLoader.getPartitionKeys(String,Job)",1,3,3
"org.apache.hive.hcatalog.pig.HCatLoader.getSchema(String,Job)",1,3,3
"org.apache.hive.hcatalog.pig.HCatLoader.getStatistics(String,Job)",1,1,2
"org.apache.hive.hcatalog.pig.HCatLoader.relativeToAbsolutePath(String,Path)",1,1,1
"org.apache.hive.hcatalog.pig.HCatLoader.setLocation(String,Job)",3,11,13
"org.apache.hive.hcatalog.pig.HCatLoader.setPartitionFilter(Expression)",1,1,1
"org.apache.hive.hcatalog.pig.HCatStorer.HCatStorer()",1,1,1
"org.apache.hive.hcatalog.pig.HCatStorer.HCatStorer(String)",1,1,1
"org.apache.hive.hcatalog.pig.HCatStorer.HCatStorer(String,String)",1,1,1
"org.apache.hive.hcatalog.pig.HCatStorer.HCatStorer(String,String,String)",1,3,3
"org.apache.hive.hcatalog.pig.HCatStorer.cleanupOnFailure(String,Job)",1,1,1
"org.apache.hive.hcatalog.pig.HCatStorer.getOutputFormat()",1,1,1
"org.apache.hive.hcatalog.pig.HCatStorer.isValidOOROption(String)",4,2,4
"org.apache.hive.hcatalog.pig.HCatStorer.populateValidOptions()",1,1,1
"org.apache.hive.hcatalog.pig.HCatStorer.setStoreLocation(String,Job)",5,12,14
"org.apache.hive.hcatalog.pig.HCatStorer.storeSchema(ResourceSchema,String,Job)",1,1,1
"org.apache.hive.hcatalog.pig.HCatStorerWrapper.HCatStorerWrapper(String)",1,1,1
"org.apache.hive.hcatalog.pig.HCatStorerWrapper.HCatStorerWrapper(String,String)",1,1,1
"org.apache.hive.hcatalog.pig.HCatStorerWrapper.HCatStorerWrapper(String,String,String)",1,1,1
"org.apache.hive.hcatalog.pig.HCatStorerWrapper.setStoreFuncUDFContextSignature(String)",1,1,1
"org.apache.hive.hcatalog.pig.HCatStorerWrapper.setStoreLocation(String,Job)",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockInputFormat.MockInputFormat(String)",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockInputFormat.createRecordReader(InputSplit,TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockInputFormat.getSplits(JobContext)",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockInputSplit.MockInputSplit()",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockInputSplit.MockInputSplit(String)",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockInputSplit.equals(Object)",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockInputSplit.getLength()",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockInputSplit.getLocations()",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockInputSplit.hashCode()",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockInputSplit.readFields(DataInput)",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockInputSplit.write(DataOutput)",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockRecordReader.close()",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockRecordReader.getCurrentKey()",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockRecordReader.getCurrentValue()",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockRecordReader.getProgress()",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockRecordReader.initialize(InputSplit,TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.MockRecordReader.nextKeyValue()",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.getInputFormat()",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.getNext()",2,2,3
"org.apache.hive.hcatalog.pig.MockLoader.prepareToRead(RecordReader,PigSplit)",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.relativeToAbsolutePath(String,Path)",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.setData(String,Iterable<Tuple>)",1,1,1
"org.apache.hive.hcatalog.pig.MockLoader.setLocation(String,Job)",3,1,3
"org.apache.hive.hcatalog.pig.MyPigStorage.MyPigStorage(String,String)",1,1,1
"org.apache.hive.hcatalog.pig.MyPigStorage.putNext(Tuple)",1,1,1
"org.apache.hive.hcatalog.pig.PigHCatUtil.extractPigObject(Object,HCatFieldSchema)",3,2,12
"org.apache.hive.hcatalog.pig.PigHCatUtil.getBagSubSchema(HCatFieldSchema)",1,7,7
"org.apache.hive.hcatalog.pig.PigHCatUtil.getConfigFromUDFProperties(Properties,Configuration,String)",1,2,2
"org.apache.hive.hcatalog.pig.PigHCatUtil.getDBTableNames(String)",1,1,2
"org.apache.hive.hcatalog.pig.PigHCatUtil.getHCatSchema(List<RequiredField>,String,Class<?>)",2,2,3
"org.apache.hive.hcatalog.pig.PigHCatUtil.getHCatServerPrincipal(Job)",1,1,1
"org.apache.hive.hcatalog.pig.PigHCatUtil.getHCatServerUri(Job)",1,1,1
"org.apache.hive.hcatalog.pig.PigHCatUtil.getHiveMetaClient(String,String,Class<?>)",1,3,4
"org.apache.hive.hcatalog.pig.PigHCatUtil.getPigType(HCatFieldSchema)",1,1,1
"org.apache.hive.hcatalog.pig.PigHCatUtil.getPigType(Type)",13,1,19
"org.apache.hive.hcatalog.pig.PigHCatUtil.getResourceSchema(HCatSchema)",1,2,2
"org.apache.hive.hcatalog.pig.PigHCatUtil.getResourceSchemaFromFieldSchema(HCatFieldSchema)",1,3,3
"org.apache.hive.hcatalog.pig.PigHCatUtil.getTable(String,String,String)",2,2,4
"org.apache.hive.hcatalog.pig.PigHCatUtil.getTupleSubSchema(HCatFieldSchema)",1,2,2
"org.apache.hive.hcatalog.pig.PigHCatUtil.pigHasBooleanSupport()",1,1,1
"org.apache.hive.hcatalog.pig.PigHCatUtil.saveConfigIntoUDFProperties(Properties,Configuration,String)",1,2,2
"org.apache.hive.hcatalog.pig.PigHCatUtil.transformToBag(List<?>,HCatFieldSchema)",2,3,4
"org.apache.hive.hcatalog.pig.PigHCatUtil.transformToPigMap(Map<?, ?>,HCatFieldSchema)",2,2,3
"org.apache.hive.hcatalog.pig.PigHCatUtil.transformToTuple(HCatRecord,HCatSchema)",2,1,2
"org.apache.hive.hcatalog.pig.PigHCatUtil.transformToTuple(List<?>,HCatFieldSchema)",2,3,3
"org.apache.hive.hcatalog.pig.PigHCatUtil.transformToTuple(List<?>,HCatSchema)",2,2,3
"org.apache.hive.hcatalog.pig.PigHCatUtil.validateHCatSchemaFollowsPigRules(HCatSchema)",1,2,2
"org.apache.hive.hcatalog.pig.PigHCatUtil.validateHCatTableSchemaFollowsPigRules(HCatSchema)",1,1,1
"org.apache.hive.hcatalog.pig.PigHCatUtil.validateHcatFieldFollowsPigRules(HCatFieldSchema)",3,3,8
"org.apache.hive.hcatalog.pig.TestE2EScenarios.copyTable(String,String)",1,4,4
"org.apache.hive.hcatalog.pig.TestE2EScenarios.createTable(String,String,String,String)",1,2,4
"org.apache.hive.hcatalog.pig.TestE2EScenarios.createTaskAttemptContext(Configuration)",1,1,2
"org.apache.hive.hcatalog.pig.TestE2EScenarios.driverRun(String)",2,1,2
"org.apache.hive.hcatalog.pig.TestE2EScenarios.dropTable(String)",1,1,1
"org.apache.hive.hcatalog.pig.TestE2EScenarios.pigDump(String)",1,3,3
"org.apache.hive.hcatalog.pig.TestE2EScenarios.setUp()",2,2,3
"org.apache.hive.hcatalog.pig.TestE2EScenarios.storageFormat()",1,1,1
"org.apache.hive.hcatalog.pig.TestE2EScenarios.tearDown()",1,1,1
"org.apache.hive.hcatalog.pig.TestE2EScenarios.testReadOrcAndRCFromPig()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoader.AllTypesTable.setupAllTypesTable(Driver)",1,3,5
"org.apache.hive.hcatalog.pig.TestHCatLoader.AllTypesTable.testReadDataPrimitiveTypes()",1,5,5
"org.apache.hive.hcatalog.pig.TestHCatLoader.AllTypesTable.testSchemaLoadPrimitiveTypes()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoader.checkProjection(FieldSchema,String,byte)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoader.createTable(String,String)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoader.createTable(String,String,String)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoader.createTable(String,String,String,Driver,String)",1,2,3
"org.apache.hive.hcatalog.pig.TestHCatLoader.dropTable(String)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoader.dropTable(String,Driver)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoader.executeStatementOnDriver(String,Driver)",2,2,2
"org.apache.hive.hcatalog.pig.TestHCatLoader.setup()",2,4,5
"org.apache.hive.hcatalog.pig.TestHCatLoader.storageFormat()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoader.tearDown()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoader.testColumnarStorePushdown()",1,2,2
"org.apache.hive.hcatalog.pig.TestHCatLoader.testConvertBooleanToInt()",1,2,2
"org.apache.hive.hcatalog.pig.TestHCatLoader.testGetInputBytes()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoader.testProjectionsBasic()",1,3,3
"org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataBasic()",1,2,2
"org.apache.hive.hcatalog.pig.TestHCatLoader.testReadDataPrimitiveTypes()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoader.testReadPartitionedBasic()",1,5,5
"org.apache.hive.hcatalog.pig.TestHCatLoader.testSchemaLoadBasic()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoader.testSchemaLoadComplex()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoader.testSchemaLoadPrimitiveTypes()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.b(Tuple...)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.compareIgnoreFiledNames(Schema,Schema)",4,4,7
"org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.compareTuples(Tuple,Tuple)",1,2,2
"org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.createTable(String,String)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.createTable(String,String,String)",2,3,4
"org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.dropTable(String)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.noOrder(String)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.setUpBeforeClass()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.storageFormat()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.t(Object...)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.testMapWithComplexData()",1,2,2
"org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.testSyntheticComplexSchema()",1,2,2
"org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.testTupleInBagInTupleInBag()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.verifyWriteRead(String,String,String,List<Tuple>,boolean)",2,3,4
"org.apache.hive.hcatalog.pig.TestHCatLoaderStorer.smallTinyIntBoundsCheckHelper(String,JOB_STATUS)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatLoaderStorer.testReadWrite()",1,2,2
"org.apache.hive.hcatalog.pig.TestHCatLoaderStorer.testSmallTinyInt()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.FailEvalFunc.exec(Tuple)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.dumpFile(String)",1,2,2
"org.apache.hive.hcatalog.pig.TestHCatStorer.getStorageFormat()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.pigValueRangeTest(String,String,String,OOR_VALUE_OPT_VALUES,String,String)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.pigValueRangeTest(String,String,String,OOR_VALUE_OPT_VALUES,String,String,String)",3,9,12
"org.apache.hive.hcatalog.pig.TestHCatStorer.pigValueRangeTestOverflow(String,String,String,OOR_VALUE_OPT_VALUES,String)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.pigValueRangeTestOverflow(String,String,String,OOR_VALUE_OPT_VALUES,String,String)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.testBagNStruct()",2,1,2
"org.apache.hive.hcatalog.pig.TestHCatStorer.testDateCharTypes()",1,5,6
"org.apache.hive.hcatalog.pig.TestHCatStorer.testDynamicPartitioningMultiPartColsInDataNoSpec()",2,1,2
"org.apache.hive.hcatalog.pig.TestHCatStorer.testDynamicPartitioningMultiPartColsInDataPartialSpec()",2,1,2
"org.apache.hive.hcatalog.pig.TestHCatStorer.testDynamicPartitioningMultiPartColsNoDataInDataNoSpec()",2,1,2
"org.apache.hive.hcatalog.pig.TestHCatStorer.testEmptyStore()",2,1,4
"org.apache.hive.hcatalog.pig.TestHCatStorer.testMultiPartColsInData()",2,1,2
"org.apache.hive.hcatalog.pig.TestHCatStorer.testNoAlias()",2,3,4
"org.apache.hive.hcatalog.pig.TestHCatStorer.testPartColsInData()",2,2,4
"org.apache.hive.hcatalog.pig.TestHCatStorer.testPartitionPublish()",3,1,4
"org.apache.hive.hcatalog.pig.TestHCatStorer.testStoreFuncAllSimpleTypes()",2,3,4
"org.apache.hive.hcatalog.pig.TestHCatStorer.testStoreFuncSimple()",2,3,6
"org.apache.hive.hcatalog.pig.TestHCatStorer.testStoreInPartiitonedTbl()",2,2,4
"org.apache.hive.hcatalog.pig.TestHCatStorer.testStoreMultiTables()",3,2,6
"org.apache.hive.hcatalog.pig.TestHCatStorer.testStoreWithNoCtorArgs()",2,2,5
"org.apache.hive.hcatalog.pig.TestHCatStorer.testStoreWithNoSchema()",2,2,5
"org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteChar()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteDate()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteDate2()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteDate3()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteDecimal()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteDecimalX()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteDecimalXY()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteSmallint()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteTimestamp()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteTinyint()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorer.testWriteVarchar()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorerMulti.cleanup()",1,2,2
"org.apache.hive.hcatalog.pig.TestHCatStorerMulti.createTable(String,String)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorerMulti.createTable(String,String,String)",2,2,4
"org.apache.hive.hcatalog.pig.TestHCatStorerMulti.dropTable(String)",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorerMulti.populateBasicFile()",1,3,3
"org.apache.hive.hcatalog.pig.TestHCatStorerMulti.setUp()",1,2,2
"org.apache.hive.hcatalog.pig.TestHCatStorerMulti.storageFormat()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorerMulti.tearDown()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorerMulti.testStoreBasicTable()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorerMulti.testStorePartitionedTable()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorerMulti.testStoreTableMulti()",1,1,1
"org.apache.hive.hcatalog.pig.TestHCatStorerWrapper.testStoreExternalTableWithExternalDir()",1,5,8
"org.apache.hive.hcatalog.pig.TestOrcHCatLoader.storageFormat()",1,1,1
"org.apache.hive.hcatalog.pig.TestOrcHCatLoaderComplexSchema.storageFormat()",1,1,1
"org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.getStorageFormat()",1,1,1
"org.apache.hive.hcatalog.pig.TestOrcHCatStorer.storageFormat()",1,1,1
"org.apache.hive.hcatalog.pig.TestPigHCatUtil.testGetBagSubSchema()",1,1,1
"org.apache.hive.hcatalog.pig.TestPigHCatUtil.testGetBagSubSchemaConfigured()",1,1,1
"org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat.createRecordReader(InputSplit,TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.rcfile.RCFileMapReduceInputFormat.getSplits(JobContext)",1,1,1
"org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat.getRecordWriter(TaskAttemptContext)",1,3,3
"org.apache.hive.hcatalog.rcfile.RCFileMapReduceOutputFormat.setColumnNumber(Configuration,int)",1,1,1
"org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader.close()",1,1,1
"org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader.getCurrentKey()",1,1,1
"org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader.getCurrentValue()",1,1,1
"org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader.getProgress()",2,2,2
"org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader.initialize(InputSplit,TaskAttemptContext)",1,2,2
"org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader.next(LongWritable)",4,1,4
"org.apache.hive.hcatalog.rcfile.RCFileMapReduceRecordReader.nextKeyValue()",1,2,2
"org.apache.hive.hcatalog.rcfile.TestRCFileMapReduceInputFormat.createProperties()",1,1,1
"org.apache.hive.hcatalog.rcfile.TestRCFileMapReduceInputFormat.main(String[])",3,6,6
"org.apache.hive.hcatalog.rcfile.TestRCFileMapReduceInputFormat.splitAfterSync()",1,1,1
"org.apache.hive.hcatalog.rcfile.TestRCFileMapReduceInputFormat.splitBeforeSync()",1,1,1
"org.apache.hive.hcatalog.rcfile.TestRCFileMapReduceInputFormat.splitInMiddleOfSync()",1,1,1
"org.apache.hive.hcatalog.rcfile.TestRCFileMapReduceInputFormat.splitRightAfterSync()",1,1,1
"org.apache.hive.hcatalog.rcfile.TestRCFileMapReduceInputFormat.splitRightBeforeSync()",1,1,1
"org.apache.hive.hcatalog.rcfile.TestRCFileMapReduceInputFormat.testSynAndSplit()",1,1,1
"org.apache.hive.hcatalog.rcfile.TestRCFileMapReduceInputFormat.writeThenReadByRecordReader(int,int,int,long,CompressionCodec)",1,4,5
"org.apache.hive.hcatalog.storagehandler.DummyHCatAuthProvider.authorize(Database,Privilege[],Privilege[])",1,1,1
"org.apache.hive.hcatalog.storagehandler.DummyHCatAuthProvider.authorize(Partition,Privilege[],Privilege[])",1,1,1
"org.apache.hive.hcatalog.storagehandler.DummyHCatAuthProvider.authorize(Privilege[],Privilege[])",1,1,1
"org.apache.hive.hcatalog.storagehandler.DummyHCatAuthProvider.authorize(Table,Partition,List<String>,Privilege[],Privilege[])",1,1,1
"org.apache.hive.hcatalog.storagehandler.DummyHCatAuthProvider.authorize(Table,Privilege[],Privilege[])",1,1,1
"org.apache.hive.hcatalog.storagehandler.DummyHCatAuthProvider.getAuthenticator()",1,1,1
"org.apache.hive.hcatalog.storagehandler.DummyHCatAuthProvider.getConf()",1,1,1
"org.apache.hive.hcatalog.storagehandler.DummyHCatAuthProvider.init(Configuration)",1,1,1
"org.apache.hive.hcatalog.storagehandler.DummyHCatAuthProvider.setAuthenticator(HiveAuthenticationProvider)",1,1,1
"org.apache.hive.hcatalog.storagehandler.DummyHCatAuthProvider.setConf(Configuration)",1,1,1
"org.apache.hive.hcatalog.streaming.AbstractRecordWriter.AbstractRecordWriter(HiveEndPoint)",1,1,1
"org.apache.hive.hcatalog.streaming.AbstractRecordWriter.AbstractRecordWriter(HiveEndPoint,HiveConf)",2,4,7
"org.apache.hive.hcatalog.streaming.AbstractRecordWriter.clear()",1,1,1
"org.apache.hive.hcatalog.streaming.AbstractRecordWriter.closeBatch()",1,1,2
"org.apache.hive.hcatalog.streaming.AbstractRecordWriter.createRecordUpdater(int,Long,Long)",1,2,2
"org.apache.hive.hcatalog.streaming.AbstractRecordWriter.flush()",1,1,2
"org.apache.hive.hcatalog.streaming.AbstractRecordWriter.getPathForEndPoint(HiveMetaStoreClient,HiveEndPoint)",1,4,4
"org.apache.hive.hcatalog.streaming.AbstractRecordWriter.newBatch(Long,Long)",1,2,2
"org.apache.hive.hcatalog.streaming.ConnectionError.ConnectionError(HiveEndPoint,Exception)",1,1,1
"org.apache.hive.hcatalog.streaming.ConnectionError.ConnectionError(String,Exception)",1,1,1
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.DelimitedInputWriter(String[],String,HiveEndPoint)",1,1,1
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.DelimitedInputWriter(String[],String,HiveEndPoint,HiveConf)",1,1,1
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.DelimitedInputWriter(String[],String,HiveEndPoint,HiveConf,char)",1,1,1
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.areFieldsInColOrder(int[])",3,1,3
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.createSerde(Table,HiveConf)",1,1,2
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.encode(byte[])",1,1,2
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.getCols(Table)",1,2,2
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.getFieldReordering(String[],List<String>)",6,2,7
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.getSerde()",2,1,2
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.getSerdeSeparator()",1,1,1
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.getTableColumns()",1,1,1
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.isReorderingNeeded(String,ArrayList<String>)",1,3,3
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.join(String[],char)",2,5,5
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.reorderFields(byte[])",2,1,4
"org.apache.hive.hcatalog.streaming.DelimitedInputWriter.write(long,byte[])",1,1,2
"org.apache.hive.hcatalog.streaming.HeartBeatFailure.HeartBeatFailure(Collection<Long>,Set<Long>)",1,1,1
"org.apache.hive.hcatalog.streaming.HiveEndPoint.ConnectionImpl.ConnectionImpl(HiveEndPoint,String,UserGroupInformation,HiveConf,boolean)",1,4,4
"org.apache.hive.hcatalog.streaming.HiveEndPoint.ConnectionImpl.close()",2,4,4
"org.apache.hive.hcatalog.streaming.HiveEndPoint.ConnectionImpl.createPartitionIfNotExists(HiveEndPoint,IMetaStoreClient,HiveConf)",2,7,8
"org.apache.hive.hcatalog.streaming.HiveEndPoint.ConnectionImpl.fetchTransactionBatch(int,RecordWriter)",2,2,3
"org.apache.hive.hcatalog.streaming.HiveEndPoint.ConnectionImpl.fetchTransactionBatchImpl(int,RecordWriter)",1,1,1
"org.apache.hive.hcatalog.streaming.HiveEndPoint.ConnectionImpl.getMetaStoreClient(HiveEndPoint,HiveConf)",1,2,3
"org.apache.hive.hcatalog.streaming.HiveEndPoint.ConnectionImpl.partSpecStr(List<FieldSchema>,ArrayList<String>)",2,3,4
"org.apache.hive.hcatalog.streaming.HiveEndPoint.ConnectionImpl.runDDL(Driver,String)",3,3,5
"org.apache.hive.hcatalog.streaming.HiveEndPoint.HiveEndPoint(String,String,String,List<String>)",3,1,4
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.TransactionBatchImpl(String,UserGroupInformation,HiveEndPoint,int,IMetaStoreClient,RecordWriter)",1,3,4
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.abort()",2,3,3
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.abortImpl()",1,3,3
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.beginNextTransaction()",2,2,3
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.beginNextTransactionImpl()",3,1,4
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.close()",2,2,3
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.commit()",2,3,3
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.commitImpl()",1,3,4
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.createLockRequest(HiveEndPoint,String,String,long)",1,3,3
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.getCurrentTransactionState()",1,1,1
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.getCurrentTxnId()",1,1,1
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.heartbeat()",2,3,4
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.remainingTransactions()",2,2,2
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.toString()",2,2,3
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.write(Collection<byte[]>)",2,3,3
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.write(byte[])",2,3,3
"org.apache.hive.hcatalog.streaming.HiveEndPoint.TransactionBatchImpl.writeImpl(Collection<byte[]>)",1,2,2
"org.apache.hive.hcatalog.streaming.HiveEndPoint.createHiveConf(Class<?>,String)",1,2,2
"org.apache.hive.hcatalog.streaming.HiveEndPoint.equals(Object)",7,5,11
"org.apache.hive.hcatalog.streaming.HiveEndPoint.getUserGroupInfo(String)",1,2,2
"org.apache.hive.hcatalog.streaming.HiveEndPoint.hashCode()",1,4,4
"org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(String,boolean,HiveConf)",2,3,4
"org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(boolean)",1,1,1
"org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(boolean,HiveConf)",1,1,1
"org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnectionImpl(String,UserGroupInformation,boolean,HiveConf)",1,1,1
"org.apache.hive.hcatalog.streaming.HiveEndPoint.toString()",1,1,1
"org.apache.hive.hcatalog.streaming.ImpersonationFailed.ImpersonationFailed(String,Exception)",1,1,1
"org.apache.hive.hcatalog.streaming.InvalidColumn.InvalidColumn(String)",1,1,1
"org.apache.hive.hcatalog.streaming.InvalidPartition.InvalidPartition(String,String)",1,1,1
"org.apache.hive.hcatalog.streaming.InvalidTable.InvalidTable(String,String)",1,1,1
"org.apache.hive.hcatalog.streaming.InvalidTable.makeMsg(String,String)",1,1,1
"org.apache.hive.hcatalog.streaming.InvalidTrasactionState.InvalidTrasactionState(String)",1,1,1
"org.apache.hive.hcatalog.streaming.PartitionCreationFailed.PartitionCreationFailed(HiveEndPoint,Exception)",1,1,1
"org.apache.hive.hcatalog.streaming.QueryFailedException.QueryFailedException(String,CommandNeedRetryException)",1,1,1
"org.apache.hive.hcatalog.streaming.SerializationError.SerializationError(String,Exception)",1,1,1
"org.apache.hive.hcatalog.streaming.StreamingException.StreamingException(String)",1,1,1
"org.apache.hive.hcatalog.streaming.StreamingException.StreamingException(String,Exception)",1,1,1
"org.apache.hive.hcatalog.streaming.StreamingIOFailure.StreamingIOFailure(String)",1,1,1
"org.apache.hive.hcatalog.streaming.StreamingIOFailure.StreamingIOFailure(String,Exception)",1,1,1
"org.apache.hive.hcatalog.streaming.StreamingIntegrationTester.StreamingIntegrationTester(String,String,String,int,int,int,int,int,float,String[],String[],String[],boolean)",1,1,1
"org.apache.hive.hcatalog.streaming.StreamingIntegrationTester.Writer.Writer(HiveEndPoint,int,int,int,int,int,float,String[],String[],boolean)",1,1,1
"org.apache.hive.hcatalog.streaming.StreamingIntegrationTester.Writer.generateColumn(String)",6,6,7
"org.apache.hive.hcatalog.streaming.StreamingIntegrationTester.Writer.generateRecord(String[],String[])",1,2,2
"org.apache.hive.hcatalog.streaming.StreamingIntegrationTester.Writer.run()",1,10,10
"org.apache.hive.hcatalog.streaming.StreamingIntegrationTester.go()",1,4,4
"org.apache.hive.hcatalog.streaming.StreamingIntegrationTester.main(String[])",1,3,3
"org.apache.hive.hcatalog.streaming.StreamingIntegrationTester.usage(Options)",1,1,1
"org.apache.hive.hcatalog.streaming.StrictJsonWriter.StrictJsonWriter(HiveEndPoint)",1,1,1
"org.apache.hive.hcatalog.streaming.StrictJsonWriter.StrictJsonWriter(HiveEndPoint,HiveConf)",1,1,1
"org.apache.hive.hcatalog.streaming.StrictJsonWriter.createSerde(Table,HiveConf)",1,2,2
"org.apache.hive.hcatalog.streaming.StrictJsonWriter.encode(byte[])",1,1,2
"org.apache.hive.hcatalog.streaming.StrictJsonWriter.getSerde()",2,1,2
"org.apache.hive.hcatalog.streaming.StrictJsonWriter.write(long,byte[])",1,1,2
"org.apache.hive.hcatalog.streaming.TestDelimitedInputWriter.testFieldReordering()",1,1,3
"org.apache.hive.hcatalog.streaming.TestStreaming.RawFileSystem.getFileStatus(Path)",2,1,5
"org.apache.hive.hcatalog.streaming.TestStreaming.RawFileSystem.getUri()",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.TestStreaming()",1,2,2
"org.apache.hive.hcatalog.streaming.TestStreaming.WriterThd.WriterThd(HiveEndPoint,String)",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.WriterThd.run()",2,4,6
"org.apache.hive.hcatalog.streaming.TestStreaming.addPartition(IMetaStoreClient,Table,List<String>)",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.checkDataWritten(long,long,int,int,String...)",1,6,6
"org.apache.hive.hcatalog.streaming.TestStreaming.checkNothingWritten()",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.createDbAndTable(IMetaStoreClient,String,String,List<String>)",1,1,2
"org.apache.hive.hcatalog.streaming.TestStreaming.dropDB(IMetaStoreClient,String)",1,2,3
"org.apache.hive.hcatalog.streaming.TestStreaming.getPartitionKeys()",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.getTableColumns()",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.makePartPath(List<FieldSchema>,List<String>)",2,3,4
"org.apache.hive.hcatalog.streaming.TestStreaming.setup()",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.testAddPartition()",1,1,2
"org.apache.hive.hcatalog.streaming.TestStreaming.testConcurrentTransactionBatchCommits()",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.testEndpointConnection()",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.testInterleavedTransactionBatchCommits()",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.testMultipleTransactionBatchCommits()",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.testRemainingTransactions()",1,5,5
"org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchAbort()",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchAbortAndCommit()",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Delimited()",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Json()",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyAbort()",1,1,1
"org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyCommit()",1,1,1
"org.apache.hive.hcatalog.streaming.TransactionBatchUnAvailable.TransactionBatchUnAvailable(HiveEndPoint,Exception)",1,1,1
"org.apache.hive.hcatalog.streaming.TransactionError.TransactionError(String)",1,1,1
"org.apache.hive.hcatalog.streaming.TransactionError.TransactionError(String,Exception)",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.AppConfig()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.clusterHadoop()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.clusterHcat()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.clusterPython()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.controllerMRChildOpts()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.dumpEnvironent()",1,4,4
"org.apache.hive.hcatalog.templeton.AppConfig.getHadoopConfDir()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.getTempletonDir()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.getWebhcatConfDir()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.hadoopQueueName()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.handleHiveProperties()",5,6,8
"org.apache.hive.hcatalog.templeton.AppConfig.hiveArchive()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.hivePath()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.hiveProps()",1,2,2
"org.apache.hive.hcatalog.templeton.AppConfig.init()",1,5,5
"org.apache.hive.hcatalog.templeton.AppConfig.kerberosKeytab()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.kerberosPrincipal()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.kerberosSecret()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.libJars()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.loadOneClasspathConfig(String)",2,2,2
"org.apache.hive.hcatalog.templeton.AppConfig.loadOneFileConfig(String,String)",3,3,3
"org.apache.hive.hcatalog.templeton.AppConfig.logConfigLoadAttempt(String)",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.mapperMemoryMb()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.overrideJars()",2,2,2
"org.apache.hive.hcatalog.templeton.AppConfig.overrideJarsString()",2,2,2
"org.apache.hive.hcatalog.templeton.AppConfig.pigArchive()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.pigPath()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.sqoopArchive()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.sqoopPath()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.startCleanup()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.streamingJar()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.zkCleanupInterval()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.zkHosts()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.zkMaxAge()",1,1,1
"org.apache.hive.hcatalog.templeton.AppConfig.zkSessionTimeout()",1,1,1
"org.apache.hive.hcatalog.templeton.BadParam.BadParam(String)",1,1,1
"org.apache.hive.hcatalog.templeton.BusyException.BusyException()",1,1,1
"org.apache.hive.hcatalog.templeton.CallbackFailedException.CallbackFailedException(String)",1,1,1
"org.apache.hive.hcatalog.templeton.CatchallExceptionMapper.toResponse(Exception)",2,2,2
"org.apache.hive.hcatalog.templeton.ColumnDesc.ColumnDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.ColumnDesc.ColumnDesc(String,String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.ColumnDesc.equals(Object)",3,4,6
"org.apache.hive.hcatalog.templeton.ColumnDesc.toString()",1,1,1
"org.apache.hive.hcatalog.templeton.CompleteBean.CompleteBean()",1,1,1
"org.apache.hive.hcatalog.templeton.CompleteBean.CompleteBean(String)",1,1,1
"org.apache.hive.hcatalog.templeton.CompleteDelegator.CompleteDelegator(AppConfig)",1,1,1
"org.apache.hive.hcatalog.templeton.CompleteDelegator.acceptWithError(String)",1,1,1
"org.apache.hive.hcatalog.templeton.CompleteDelegator.doCallback(String,String)",1,2,2
"org.apache.hive.hcatalog.templeton.CompleteDelegator.failed(String,Exception)",1,2,2
"org.apache.hive.hcatalog.templeton.CompleteDelegator.run(String,String)",3,8,9
"org.apache.hive.hcatalog.templeton.DatabaseDesc.DatabaseDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.DatabaseDesc.toString()",1,1,1
"org.apache.hive.hcatalog.templeton.DeleteDelegator.DeleteDelegator(AppConfig)",1,1,1
"org.apache.hive.hcatalog.templeton.DeleteDelegator.run(String,String)",2,7,8
"org.apache.hive.hcatalog.templeton.EnqueueBean.EnqueueBean()",1,1,1
"org.apache.hive.hcatalog.templeton.EnqueueBean.EnqueueBean(String)",1,1,1
"org.apache.hive.hcatalog.templeton.ExecBean.ExecBean()",1,1,1
"org.apache.hive.hcatalog.templeton.ExecBean.ExecBean(String,String,int)",1,1,1
"org.apache.hive.hcatalog.templeton.ExecBean.toString()",1,1,1
"org.apache.hive.hcatalog.templeton.ExecServiceImpl.ExecServiceImpl()",1,1,1
"org.apache.hive.hcatalog.templeton.ExecServiceImpl.auxRun(String,List<String>,Map<String, String>)",2,5,6
"org.apache.hive.hcatalog.templeton.ExecServiceImpl.execEnv(Map<String, String>)",1,5,5
"org.apache.hive.hcatalog.templeton.ExecServiceImpl.getInstance()",1,1,2
"org.apache.hive.hcatalog.templeton.ExecServiceImpl.makeCommandLine(String,List<String>)",1,3,3
"org.apache.hive.hcatalog.templeton.ExecServiceImpl.run(String,List<String>,Map<String, String>)",2,3,3
"org.apache.hive.hcatalog.templeton.ExecServiceImpl.runUnlimited(String,List<String>,Map<String, String>)",2,4,4
"org.apache.hive.hcatalog.templeton.ExecServiceImpl.validateProgram(String)",2,2,2
"org.apache.hive.hcatalog.templeton.GroupPermissionsDesc.GroupPermissionsDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.GroupPermissionsDesc.equals(Object)",3,2,4
"org.apache.hive.hcatalog.templeton.GroupPermissionsDesc.xequals(Object,Object)",3,1,3
"org.apache.hive.hcatalog.templeton.GroupPermissionsDesc.xequals(boolean,boolean)",1,1,1
"org.apache.hive.hcatalog.templeton.GroupPermissionsDesc.xequals(char,char)",1,1,1
"org.apache.hive.hcatalog.templeton.GroupPermissionsDesc.xequals(int,int)",1,1,1
"org.apache.hive.hcatalog.templeton.HcatDelegator.HcatDelegator(AppConfig,ExecService)",1,1,1
"org.apache.hive.hcatalog.templeton.HcatDelegator.addOneColumn(String,String,String,ColumnDesc)",1,2,3
"org.apache.hive.hcatalog.templeton.HcatDelegator.addOnePartition(String,String,String,PartitionDesc)",2,3,5
"org.apache.hive.hcatalog.templeton.HcatDelegator.addOneTableProperty(String,String,String,TablePropertyDesc)",1,1,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.createDatabase(String,DatabaseDesc)",1,4,5
"org.apache.hive.hcatalog.templeton.HcatDelegator.createTable(String,String,TableDesc)",1,1,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.createTableLike(String,String,TableLikeDesc)",1,2,4
"org.apache.hive.hcatalog.templeton.HcatDelegator.descDatabase(String,String,boolean)",1,1,3
"org.apache.hive.hcatalog.templeton.HcatDelegator.descExtendedTable(String,String,String)",2,4,6
"org.apache.hive.hcatalog.templeton.HcatDelegator.descOneColumn(String,String,String,String)",7,4,9
"org.apache.hive.hcatalog.templeton.HcatDelegator.descOnePartition(String,String,String,String)",2,4,4
"org.apache.hive.hcatalog.templeton.HcatDelegator.descTable(String,String,String,boolean)",1,1,3
"org.apache.hive.hcatalog.templeton.HcatDelegator.descTableProperty(String,String,String,String)",2,3,4
"org.apache.hive.hcatalog.templeton.HcatDelegator.dropDatabase(String,String,boolean,String,String,String)",1,1,3
"org.apache.hive.hcatalog.templeton.HcatDelegator.dropPartition(String,String,String,String,boolean,String,String)",1,1,3
"org.apache.hive.hcatalog.templeton.HcatDelegator.dropTable(String,String,String,boolean,String,String)",1,1,3
"org.apache.hive.hcatalog.templeton.HcatDelegator.isValid(ExecBean,boolean)",6,2,7
"org.apache.hive.hcatalog.templeton.HcatDelegator.jsonRun(String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.HcatDelegator.jsonRun(String,String,String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.HcatDelegator.jsonRun(String,String,String,String,boolean)",2,1,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.listColumns(String,String,String)",1,1,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.listDatabases(String,String)",1,1,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.listPartitions(String,String,String)",1,1,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.listTableProperties(String,String,String)",2,1,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.listTables(String,String,String)",1,1,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.makeArgs(String,boolean,String,String)",1,6,6
"org.apache.hive.hcatalog.templeton.HcatDelegator.makeClusterSortList(List<ClusterSortOrderDesc>)",1,2,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.makeClusteredBy(ClusteredByDesc)",1,2,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.makeCols(List<ColumnDesc>)",1,2,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.makeCreateTable(String,TableDesc)",1,8,10
"org.apache.hive.hcatalog.templeton.HcatDelegator.makeOneClusterSort(ClusterSortOrderDesc)",1,1,1
"org.apache.hive.hcatalog.templeton.HcatDelegator.makeOneCol(ColumnDesc)",1,2,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.makePropertiesStatement(Map<String, String>)",1,2,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.makeRowFormat(RowFormatDesc)",3,3,3
"org.apache.hive.hcatalog.templeton.HcatDelegator.makeSerdeFormat(SerdeDesc)",1,2,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.makeStorageFormat(StorageFormatDesc)",1,4,4
"org.apache.hive.hcatalog.templeton.HcatDelegator.makeStoredBy(StoredByDesc)",1,2,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.makeTermBy(String,String)",2,2,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.renameTable(String,String,String,String,String,String)",1,1,2
"org.apache.hive.hcatalog.templeton.HcatDelegator.run(String,String,boolean,String,String)",2,3,4
"org.apache.hive.hcatalog.templeton.HcatDelegator.singleTable(String,String)",3,2,3
"org.apache.hive.hcatalog.templeton.HcatDelegator.tableProperties(Object)",3,1,3
"org.apache.hive.hcatalog.templeton.HcatException.HcatException(String,ExecBean,String)",1,1,1
"org.apache.hive.hcatalog.templeton.HiveDelegator.HiveDelegator(AppConfig)",1,1,1
"org.apache.hive.hcatalog.templeton.HiveDelegator.makeArgs(String,String,List<String>,List<String>,String,String,String,boolean)",1,8,8
"org.apache.hive.hcatalog.templeton.HiveDelegator.makeBasicArgs(String,String,String,String,String,boolean)",1,5,5
"org.apache.hive.hcatalog.templeton.HiveDelegator.run(String,Map<String, Object>,String,String,List<String>,List<String>,String,String,String,String,boolean)",1,1,1
"org.apache.hive.hcatalog.templeton.JarDelegator.JarDelegator(AppConfig)",1,1,1
"org.apache.hive.hcatalog.templeton.JarDelegator.makeArgs(String,String,String,String,List<String>,List<String>,String,boolean,String,boolean,JobType)",1,9,9
"org.apache.hive.hcatalog.templeton.JarDelegator.run(String,Map<String, Object>,String,String,String,String,List<String>,List<String>,String,String,boolean,String,boolean,JobType)",1,1,1
"org.apache.hive.hcatalog.templeton.JobItemBean.JobItemBean()",1,1,1
"org.apache.hive.hcatalog.templeton.JobItemBean.JobItemBean(String,QueueStatusBean)",1,1,1
"org.apache.hive.hcatalog.templeton.JsonBuilder.JsonBuilder(String)",1,1,1
"org.apache.hive.hcatalog.templeton.JsonBuilder.build()",1,1,1
"org.apache.hive.hcatalog.templeton.JsonBuilder.buildJson()",1,1,1
"org.apache.hive.hcatalog.templeton.JsonBuilder.buildResponse()",1,3,4
"org.apache.hive.hcatalog.templeton.JsonBuilder.buildResponse(int)",1,1,1
"org.apache.hive.hcatalog.templeton.JsonBuilder.create()",1,1,1
"org.apache.hive.hcatalog.templeton.JsonBuilder.create(String)",1,1,1
"org.apache.hive.hcatalog.templeton.JsonBuilder.createError(String,int)",1,1,1
"org.apache.hive.hcatalog.templeton.JsonBuilder.getMap()",1,1,1
"org.apache.hive.hcatalog.templeton.JsonBuilder.isError(Map)",1,2,2
"org.apache.hive.hcatalog.templeton.JsonBuilder.isset()",1,1,1
"org.apache.hive.hcatalog.templeton.JsonBuilder.jsonToMap(String)",2,2,2
"org.apache.hive.hcatalog.templeton.JsonBuilder.mapToJson(Object)",1,1,1
"org.apache.hive.hcatalog.templeton.JsonBuilder.put(String,Object)",1,2,2
"org.apache.hive.hcatalog.templeton.JsonBuilder.remove(String)",1,1,1
"org.apache.hive.hcatalog.templeton.LauncherDelegator.LauncherDelegator(AppConfig)",1,1,1
"org.apache.hive.hcatalog.templeton.LauncherDelegator.addCacheFiles(List<String>,AppConfig)",1,2,2
"org.apache.hive.hcatalog.templeton.LauncherDelegator.addCompletionVars(List<String>,String)",1,1,1
"org.apache.hive.hcatalog.templeton.LauncherDelegator.addDef(List<String>,String,String)",1,2,2
"org.apache.hive.hcatalog.templeton.LauncherDelegator.addHiveMetaStoreTokenArg()",2,1,2
"org.apache.hive.hcatalog.templeton.LauncherDelegator.addStorageVars(List<String>)",1,1,1
"org.apache.hive.hcatalog.templeton.LauncherDelegator.enqueueController(String,Map<String, Object>,String,List<String>)",2,1,3
"org.apache.hive.hcatalog.templeton.LauncherDelegator.getShimLibjars()",1,1,2
"org.apache.hive.hcatalog.templeton.LauncherDelegator.makeLauncherArgs(AppConfig,String,String,List<String>,boolean,JobType)",1,1,1
"org.apache.hive.hcatalog.templeton.LauncherDelegator.makeOverrideClasspath(AppConfig)",2,2,3
"org.apache.hive.hcatalog.templeton.LauncherDelegator.queueAsUser(UserGroupInformation,List<String>)",1,2,2
"org.apache.hive.hcatalog.templeton.LauncherDelegator.registerJob(String,String,String,Map<String, Object>)",1,2,2
"org.apache.hive.hcatalog.templeton.ListDelegator.ListDelegator(AppConfig)",1,1,1
"org.apache.hive.hcatalog.templeton.ListDelegator.run(String,boolean)",1,7,7
"org.apache.hive.hcatalog.templeton.Main.Main(String[])",1,1,1
"org.apache.hive.hcatalog.templeton.Main.UserNameHandler.allowAnonymous(FilterHolder)",1,1,1
"org.apache.hive.hcatalog.templeton.Main.UserNameHandler.getUserName(HttpServletRequest)",2,4,4
"org.apache.hive.hcatalog.templeton.Main.addRedirects(Server)",1,2,2
"org.apache.hive.hcatalog.templeton.Main.checkCurrentDirPermissions()",1,2,2
"org.apache.hive.hcatalog.templeton.Main.checkEnv()",1,1,1
"org.apache.hive.hcatalog.templeton.Main.getAppConfigInstance()",1,2,2
"org.apache.hive.hcatalog.templeton.Main.init(String[])",1,1,1
"org.apache.hive.hcatalog.templeton.Main.initLogger()",1,2,2
"org.apache.hive.hcatalog.templeton.Main.loadConfig(String[])",1,3,3
"org.apache.hive.hcatalog.templeton.Main.main(String[])",1,1,1
"org.apache.hive.hcatalog.templeton.Main.makeAuthFilter()",1,2,2
"org.apache.hive.hcatalog.templeton.Main.makeJerseyConfig()",1,1,1
"org.apache.hive.hcatalog.templeton.Main.run()",1,2,2
"org.apache.hive.hcatalog.templeton.Main.runServer(int)",1,2,2
"org.apache.hive.hcatalog.templeton.Main.stop()",1,3,3
"org.apache.hive.hcatalog.templeton.Main.usage()",1,1,1
"org.apache.hive.hcatalog.templeton.MaxByteArrayOutputStream.MaxByteArrayOutputStream(int)",1,1,1
"org.apache.hive.hcatalog.templeton.MaxByteArrayOutputStream.write(byte[],int,int)",1,2,2
"org.apache.hive.hcatalog.templeton.MaxByteArrayOutputStream.write(int)",1,2,2
"org.apache.hive.hcatalog.templeton.NotAuthorizedException.NotAuthorizedException(String)",1,1,1
"org.apache.hive.hcatalog.templeton.PartitionDesc.PartitionDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.PartitionDesc.toString()",1,1,1
"org.apache.hive.hcatalog.templeton.PigDelegator.PigDelegator(AppConfig)",1,1,1
"org.apache.hive.hcatalog.templeton.PigDelegator.hasPigArgUseHcat(List<String>)",1,1,1
"org.apache.hive.hcatalog.templeton.PigDelegator.makeArgs(String,String,List<String>,String,String,boolean,String,boolean)",1,18,20
"org.apache.hive.hcatalog.templeton.PigDelegator.run(String,Map<String, Object>,String,String,List<String>,String,String,String,boolean,String,boolean)",1,1,1
"org.apache.hive.hcatalog.templeton.ProxyUserSupport.assertNotEmpty(String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.ProxyUserSupport.assertNotEmpty(String,String,String)",3,1,5
"org.apache.hive.hcatalog.templeton.ProxyUserSupport.normalizeHostname(String)",1,2,3
"org.apache.hive.hcatalog.templeton.ProxyUserSupport.processProxyuserConfig(AppConfig)",1,20,20
"org.apache.hive.hcatalog.templeton.ProxyUserSupport.validate(String,String,String)",2,2,2
"org.apache.hive.hcatalog.templeton.ProxyUserSupport.validateGroup(String,String)",5,6,7
"org.apache.hive.hcatalog.templeton.ProxyUserSupport.validateRequestorHost(String,String)",3,3,4
"org.apache.hive.hcatalog.templeton.QueueException.QueueException(String)",1,1,1
"org.apache.hive.hcatalog.templeton.QueueStatusBean.QueueStatusBean()",1,1,1
"org.apache.hive.hcatalog.templeton.QueueStatusBean.QueueStatusBean(JobState,JobStatus,JobProfile)",1,1,1
"org.apache.hive.hcatalog.templeton.SecureProxySupport.SecureProxySupport()",1,1,1
"org.apache.hive.hcatalog.templeton.SecureProxySupport.addArgs(List<String>)",1,2,2
"org.apache.hive.hcatalog.templeton.SecureProxySupport.addEnv(Map<String, String>)",1,2,2
"org.apache.hive.hcatalog.templeton.SecureProxySupport.buildHcatDelegationToken(String)",1,1,1
"org.apache.hive.hcatalog.templeton.SecureProxySupport.close()",1,2,2
"org.apache.hive.hcatalog.templeton.SecureProxySupport.getFSDelegationToken(String,Configuration)",1,1,1
"org.apache.hive.hcatalog.templeton.SecureProxySupport.getHcatServiceStr()",1,1,1
"org.apache.hive.hcatalog.templeton.SecureProxySupport.getTokenPath()",1,1,1
"org.apache.hive.hcatalog.templeton.SecureProxySupport.open(String,Configuration)",2,2,3
"org.apache.hive.hcatalog.templeton.SecureProxySupport.writeProxyDelegationTokens(Token<?>,Token<?>,Configuration,String,Path)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.addOneColumn(String,String,String,ColumnDesc)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.addOnePartition(String,String,String,PartitionDesc)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.addOneTableProperty(String,String,String,TablePropertyDesc)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.checkEnableLogPrerequisite(boolean,String)",3,3,5
"org.apache.hive.hcatalog.templeton.Server.completeJob(String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.createDatabase(String,DatabaseDesc)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.createFormats()",1,1,1
"org.apache.hive.hcatalog.templeton.Server.createStatusMsg()",1,1,1
"org.apache.hive.hcatalog.templeton.Server.createTable(String,String,TableDesc)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.createTableLike(String,String,String,TableLikeDesc)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.createVersions()",1,1,1
"org.apache.hive.hcatalog.templeton.Server.ddl(String,String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.deleteJobId(String)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.descColumn(String,String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.descDatabase(String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.descOneTableProperty(String,String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.descPartition(String,String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.descTable(String,String,String)",2,2,2
"org.apache.hive.hcatalog.templeton.Server.dropDatabase(String,boolean,String,String,String)",1,2,2
"org.apache.hive.hcatalog.templeton.Server.dropPartition(String,String,String,boolean,String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.dropTable(String,String,boolean,String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.getCompletedUrl()",3,1,3
"org.apache.hive.hcatalog.templeton.Server.getDoAsUser()",1,3,3
"org.apache.hive.hcatalog.templeton.Server.getRequestingHost(String,HttpServletRequest)",3,5,5
"org.apache.hive.hcatalog.templeton.Server.getRequestingUser()",3,2,4
"org.apache.hive.hcatalog.templeton.Server.hadoopVersion()",1,1,1
"org.apache.hive.hcatalog.templeton.Server.hive(String,String,List<String>,String,List<String>,String,String,boolean)",2,1,3
"org.apache.hive.hcatalog.templeton.Server.hiveVersion()",1,1,1
"org.apache.hive.hcatalog.templeton.Server.listColumns(String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.listDatabases(String)",1,1,2
"org.apache.hive.hcatalog.templeton.Server.listPartitions(String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.listTableProperties(String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.listTables(String,String)",1,1,2
"org.apache.hive.hcatalog.templeton.Server.mapReduceJar(String,String,String,String,List<String>,List<String>,String,String,boolean,boolean)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.mapReduceStreaming(List<String>,String,String,String,String,List<String>,String,List<String>,List<String>,List<String>,String,String,boolean)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.pig(String,String,List<String>,String,String,String,boolean,boolean)",2,1,3
"org.apache.hive.hcatalog.templeton.Server.pigVersion()",1,1,1
"org.apache.hive.hcatalog.templeton.Server.renameTable(String,String,String,String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.requestFormats()",1,1,1
"org.apache.hive.hcatalog.templeton.Server.showJobId(String)",1,1,1
"org.apache.hive.hcatalog.templeton.Server.showJobList(String,boolean,String,String)",8,10,15
"org.apache.hive.hcatalog.templeton.Server.sqoop(String,String,String,String,String,boolean)",3,1,5
"org.apache.hive.hcatalog.templeton.Server.sqoopVersion()",1,1,1
"org.apache.hive.hcatalog.templeton.Server.status()",1,1,1
"org.apache.hive.hcatalog.templeton.Server.verifyDdlParam(String,String)",2,1,2
"org.apache.hive.hcatalog.templeton.Server.verifyParam(List<String>,String)",2,2,3
"org.apache.hive.hcatalog.templeton.Server.verifyParam(String,String)",2,1,2
"org.apache.hive.hcatalog.templeton.Server.verifyUser()",2,4,5
"org.apache.hive.hcatalog.templeton.Server.version()",1,1,1
"org.apache.hive.hcatalog.templeton.SimpleExceptionMapper.toResponse(SimpleWebException)",1,1,1
"org.apache.hive.hcatalog.templeton.SimpleWebException.SimpleWebException(int,String)",1,1,1
"org.apache.hive.hcatalog.templeton.SimpleWebException.SimpleWebException(int,String,Map<String, Object>)",1,1,1
"org.apache.hive.hcatalog.templeton.SimpleWebException.buildMessage(int,Map<String, Object>,String)",1,2,3
"org.apache.hive.hcatalog.templeton.SimpleWebException.getResponse()",1,1,1
"org.apache.hive.hcatalog.templeton.SqoopDelegator.SqoopDelegator(AppConfig)",1,1,1
"org.apache.hive.hcatalog.templeton.SqoopDelegator.makeArgs(String,String,String,String,String,boolean)",1,8,8
"org.apache.hive.hcatalog.templeton.SqoopDelegator.makeBasicArgs(String,String,String,String,boolean)",1,5,5
"org.apache.hive.hcatalog.templeton.SqoopDelegator.run(String,Map<String, Object>,String,String,String,String,String,String,boolean)",1,1,1
"org.apache.hive.hcatalog.templeton.StatusDelegator.StatusDelegator(AppConfig)",1,1,1
"org.apache.hive.hcatalog.templeton.StatusDelegator.StringToJobID(String)",1,2,2
"org.apache.hive.hcatalog.templeton.StatusDelegator.makeStatus(WebHCatJTShim,JobID,JobState)",2,1,3
"org.apache.hive.hcatalog.templeton.StatusDelegator.run(String,String)",2,4,5
"org.apache.hive.hcatalog.templeton.StreamOutputWriter.StreamOutputWriter(InputStream,String,OutputStream)",1,1,1
"org.apache.hive.hcatalog.templeton.StreamOutputWriter.run()",1,3,3
"org.apache.hive.hcatalog.templeton.StreamingDelegator.StreamingDelegator(AppConfig)",1,1,1
"org.apache.hive.hcatalog.templeton.StreamingDelegator.makeArgs(List<String>,String,String,String,String,List<String>,List<String>,List<String>)",1,6,6
"org.apache.hive.hcatalog.templeton.StreamingDelegator.run(String,Map<String, Object>,List<String>,String,String,String,String,List<String>,String,List<String>,List<String>,List<String>,String,String,String,boolean,JobType)",1,1,1
"org.apache.hive.hcatalog.templeton.TableDesc.ClusterSortOrderDesc.ClusterSortOrderDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.TableDesc.ClusterSortOrderDesc.ClusterSortOrderDesc(String,SortDirectionDesc)",1,1,1
"org.apache.hive.hcatalog.templeton.TableDesc.ClusterSortOrderDesc.equals(Object)",3,2,4
"org.apache.hive.hcatalog.templeton.TableDesc.ClusterSortOrderDesc.toString()",1,1,1
"org.apache.hive.hcatalog.templeton.TableDesc.ClusteredByDesc.ClusteredByDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.TableDesc.ClusteredByDesc.equals(Object)",3,3,5
"org.apache.hive.hcatalog.templeton.TableDesc.ClusteredByDesc.toString()",1,1,1
"org.apache.hive.hcatalog.templeton.TableDesc.RowFormatDesc.RowFormatDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.TableDesc.RowFormatDesc.equals(Object)",3,5,7
"org.apache.hive.hcatalog.templeton.TableDesc.SerdeDesc.SerdeDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.TableDesc.SerdeDesc.equals(Object)",3,2,4
"org.apache.hive.hcatalog.templeton.TableDesc.StorageFormatDesc.StorageFormatDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.TableDesc.StorageFormatDesc.equals(Object)",3,3,5
"org.apache.hive.hcatalog.templeton.TableDesc.StoredByDesc.StoredByDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.TableDesc.StoredByDesc.equals(Object)",3,2,4
"org.apache.hive.hcatalog.templeton.TableDesc.TableDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.TableDesc.equals(Object)",3,11,13
"org.apache.hive.hcatalog.templeton.TableDesc.toString()",1,1,1
"org.apache.hive.hcatalog.templeton.TableLikeDesc.TableLikeDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.TableLikeDesc.toString()",1,1,1
"org.apache.hive.hcatalog.templeton.TablePropertyDesc.TablePropertyDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.TablePropertyDesc.toString()",1,1,1
"org.apache.hive.hcatalog.templeton.TempletonDelegator.TempletonDelegator(AppConfig)",1,1,1
"org.apache.hive.hcatalog.templeton.TestDesc.buildClusterBy()",1,1,1
"org.apache.hive.hcatalog.templeton.TestDesc.buildColumns()",1,1,1
"org.apache.hive.hcatalog.templeton.TestDesc.buildGenericProperties()",1,1,1
"org.apache.hive.hcatalog.templeton.TestDesc.buildPartitionedBy()",1,1,1
"org.apache.hive.hcatalog.templeton.TestDesc.buildRowFormat()",1,1,1
"org.apache.hive.hcatalog.templeton.TestDesc.buildSerde()",1,1,1
"org.apache.hive.hcatalog.templeton.TestDesc.buildSortedBy()",1,1,1
"org.apache.hive.hcatalog.templeton.TestDesc.buildStorageFormat()",1,1,1
"org.apache.hive.hcatalog.templeton.TestDesc.buildStoredBy()",1,1,1
"org.apache.hive.hcatalog.templeton.TestDesc.buildTableDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.TestDesc.fromJson(String,Class)",1,1,1
"org.apache.hive.hcatalog.templeton.TestDesc.testTableDesc()",1,1,1
"org.apache.hive.hcatalog.templeton.TestDesc.toJson(Object)",1,1,1
"org.apache.hive.hcatalog.templeton.TestServer.setUp()",1,1,1
"org.apache.hive.hcatalog.templeton.TestServer.testFormats()",1,1,1
"org.apache.hive.hcatalog.templeton.TestServer.testServer()",1,1,1
"org.apache.hive.hcatalog.templeton.TestServer.testStatus()",1,1,1
"org.apache.hive.hcatalog.templeton.TestServer.testVersions()",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.MethodCallRetVal.MethodCallRetVal(int,String,String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.MethodCallRetVal.getAssertMsg()",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.createDataBase()",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.createTable()",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.describeNoSuchTable()",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.doHttpCall(String,HTTP_METHOD_TYPE)",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.doHttpCall(String,HTTP_METHOD_TYPE,Map<String, Object>,NameValuePair[])",2,4,8
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.dropTableIfExists()",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.dropTableNoSuchDB()",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.dropTableNoSuchDbIfExists()",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getErrorCode(String)",1,2,2
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHadoopVersion()",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getHiveVersion()",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getPigVersion()",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.getStatus()",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.invalidPath()",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.listDataBases()",1,1,1
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.startHebHcatInMem()",1,2,2
"org.apache.hive.hcatalog.templeton.TestWebHCatE2e.stopWebHcatInMem()",1,2,2
"org.apache.hive.hcatalog.templeton.UgiFactory.getUgi(String)",2,2,2
"org.apache.hive.hcatalog.templeton.VersionDelegator.VersionDelegator(AppConfig)",1,1,1
"org.apache.hive.hcatalog.templeton.VersionDelegator.getHadoopVersion()",1,1,1
"org.apache.hive.hcatalog.templeton.VersionDelegator.getHiveVersion()",1,1,1
"org.apache.hive.hcatalog.templeton.VersionDelegator.getPigVersion()",1,1,1
"org.apache.hive.hcatalog.templeton.VersionDelegator.getSqoopVersion()",1,1,1
"org.apache.hive.hcatalog.templeton.VersionDelegator.getVersion(String)",5,5,5
"org.apache.hive.hcatalog.templeton.WadlConfig.configure()",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockExecService.run(String,List<String>,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockExecService.runUnlimited(String,List<String>,Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockServer.MockServer()",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockServer.getUser()",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockServer.resetUser()",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getAbsolutePath()",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getAbsolutePathBuilder()",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getBaseUri()",1,2,2
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getBaseUriBuilder()",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getMatchedResources()",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getMatchedURIs()",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getMatchedURIs(boolean)",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getPath()",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getPath(boolean)",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getPathParameters()",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getPathParameters(boolean)",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getPathSegments()",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getPathSegments(boolean)",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getQueryParameters()",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getQueryParameters(boolean)",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getRequestUri()",1,1,1
"org.apache.hive.hcatalog.templeton.mock.MockUriInfo.getRequestUriBuilder()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.DelegationTokenCache.getDelegationToken(JobId)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.DelegationTokenCache.getStringFormTokenCache()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.DelegationTokenCache.removeDelegationToken(JobId)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.DelegationTokenCache.storeDelegationToken(JobId,TokenObject)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.HDFSCleanup.HDFSCleanup(Configuration)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.HDFSCleanup.checkFiles(FileSystem)",1,4,5
"org.apache.hive.hcatalog.templeton.tool.HDFSCleanup.exit()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.HDFSCleanup.getInstance(Configuration)",2,1,2
"org.apache.hive.hcatalog.templeton.tool.HDFSCleanup.run()",1,5,5
"org.apache.hive.hcatalog.templeton.tool.HDFSCleanup.startInstance(Configuration)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.HDFSStorage.close(Closeable)",2,2,3
"org.apache.hive.hcatalog.templeton.tool.HDFSStorage.closeStorage()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.HDFSStorage.delete(Type,String)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.HDFSStorage.getAll()",1,2,2
"org.apache.hive.hcatalog.templeton.tool.HDFSStorage.getAllForKey(String,String)",1,3,3
"org.apache.hive.hcatalog.templeton.tool.HDFSStorage.getAllForType(Type)",1,3,3
"org.apache.hive.hcatalog.templeton.tool.HDFSStorage.getAllForTypeAndKey(Type,String,String)",1,4,4
"org.apache.hive.hcatalog.templeton.tool.HDFSStorage.getField(Type,String,String)",2,4,5
"org.apache.hive.hcatalog.templeton.tool.HDFSStorage.getFields(Type,String)",1,4,5
"org.apache.hive.hcatalog.templeton.tool.HDFSStorage.getPath(Type)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.HDFSStorage.getPath(Type,String)",2,2,3
"org.apache.hive.hcatalog.templeton.tool.HDFSStorage.openStorage(Configuration)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.HDFSStorage.saveField(Type,String,String,String)",2,2,3
"org.apache.hive.hcatalog.templeton.tool.HDFSStorage.startCleanup(Configuration)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.HiveJobIDParser.HiveJobIDParser(String,Configuration)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.HiveJobIDParser.parseJobID()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JarJobIDParser.JarJobIDParser(String,Configuration)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JarJobIDParser.parseJobID()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobIDParser.JobIDParser(String,Configuration)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobIDParser.findJobID(BufferedReader,String)",1,3,3
"org.apache.hive.hcatalog.templeton.tool.JobIDParser.openStatusFile(String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobIDParser.parseJobID(String,String)",1,2,3
"org.apache.hive.hcatalog.templeton.tool.JobState.JobState(String,Configuration)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.addChild(String)",1,1,4
"org.apache.hive.hcatalog.templeton.tool.JobState.close()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.delete()",1,2,2
"org.apache.hive.hcatalog.templeton.tool.JobState.getCallback()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.getChildren()",1,3,3
"org.apache.hive.hcatalog.templeton.tool.JobState.getCompleteStatus()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.getCreated()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.getExitValue()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.getField(String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.getId()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.getJobs(Configuration)",1,1,2
"org.apache.hive.hcatalog.templeton.tool.JobState.getLongField(String)",2,3,3
"org.apache.hive.hcatalog.templeton.tool.JobState.getNotifiedTime()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.getParent()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.getPercentComplete()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.getStorage(Configuration)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.getStorageInstance(Configuration)",1,3,3
"org.apache.hive.hcatalog.templeton.tool.JobState.getUser()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.getUserArgs()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.setCallback(String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.setCompleteStatus(String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.setCreated(long)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.setExitValue(long)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.setField(String,String)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.JobState.setLongField(String,long)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.JobState.setNotifiedTime(long)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.setParent(String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.setPercentComplete(String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.setUser(String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobState.setUserArgs(Map<String, Object>)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobStateTracker.JobStateTracker(String,ZooKeeper,boolean,String)",1,1,2
"org.apache.hive.hcatalog.templeton.tool.JobStateTracker.create()",2,3,5
"org.apache.hive.hcatalog.templeton.tool.JobStateTracker.delete()",1,2,2
"org.apache.hive.hcatalog.templeton.tool.JobStateTracker.getJobID()",1,1,3
"org.apache.hive.hcatalog.templeton.tool.JobStateTracker.getTrackingJobs(Configuration,ZooKeeper)",1,2,3
"org.apache.hive.hcatalog.templeton.tool.JobStateTracker.makeTrackingJobZnode(String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.JobStateTracker.makeTrackingZnode()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.KeepAlive.KeepAlive(Context)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.KeepAlive.makeDots(int)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.KeepAlive.run()",1,2,3
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.Watcher.Watcher(Configuration,JobID,InputStream,String,String)",1,2,3
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.Watcher.run()",1,11,14
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.copyLocal(String,Configuration)",1,3,3
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.executeWatcher(ExecutorService,Configuration,JobID,InputStream,String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.getTempletonLaunchTime(Configuration)",2,4,5
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.handleMapReduceJobTag(List<String>,String,String,String)",3,3,3
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.handlePigEnvVars(Configuration,Map<String, String>)",1,5,5
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.handleTokenFile(List<String>,String,String)",3,7,7
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.killLauncherChildJobs(Configuration,String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.run(Context)",2,7,7
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.startCounterKeepAlive(ExecutorService,Context)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.startJob(Context,String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.LaunchMapper.writeExitValue(Configuration,int,String)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.LogRetriever.AttemptInfo.toString()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.LogRetriever.LogRetriever(String,JobType,Configuration)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.LogRetriever.getCompletedAttempts(String,String,String)",1,4,4
"org.apache.hive.hcatalog.templeton.tool.LogRetriever.getFailedAttempts(String,String)",1,6,7
"org.apache.hive.hcatalog.templeton.tool.LogRetriever.getMatches(URL,Pattern[])",1,4,5
"org.apache.hive.hcatalog.templeton.tool.LogRetriever.logAttempt(String,AttemptInfo,String)",1,5,5
"org.apache.hive.hcatalog.templeton.tool.LogRetriever.logJob(String,String,PrintWriter)",1,7,7
"org.apache.hive.hcatalog.templeton.tool.LogRetriever.logJobConf(String,String,String)",1,4,4
"org.apache.hive.hcatalog.templeton.tool.LogRetriever.run()",2,7,10
"org.apache.hive.hcatalog.templeton.tool.NotFoundException.NotFoundException(String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.NotFoundException.NotFoundException(String,Throwable)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.NullRecordReader.close()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.NullRecordReader.getCurrentKey()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.NullRecordReader.getCurrentValue()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.NullRecordReader.getProgress()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.NullRecordReader.initialize(InputSplit,TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.NullRecordReader.nextKeyValue()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.NullSplit.getLength()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.NullSplit.getLocations()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.NullSplit.readFields(DataInput)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.NullSplit.write(DataOutput)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.PigJobIDParser.PigJobIDParser(String,Configuration)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.PigJobIDParser.parseJobID()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.SingleInputFormat.createRecordReader(InputSplit,TaskAttemptContext)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.SingleInputFormat.getSplits(JobContext)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.TempletonControllerJob(boolean,AppConfig)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.addHMSToken(Job,String)",2,1,2
"org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.buildHcatDelegationToken(String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.getSubmittedId()",2,2,2
"org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.run(String[])",1,5,5
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.addCmdForWindows(ArrayList<String>)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.addUserHomeDirectoryIfApplicable(String,String)",1,3,3
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.decodeArray(String)",2,2,3
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.encodeArray(List<String>)",2,1,2
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.encodeArray(String[])",2,2,4
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.extractChildJobId(String)",3,3,3
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.extractPercentComplete(String)",4,4,4
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.fetchUrl(URL)",1,1,2
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.findContainingJar(Class<?>,String)",4,5,6
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.hadoopFsFilename(String,Configuration,String)",2,2,2
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.hadoopFsIsMissing(FileSystem,Path)",1,1,2
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.hadoopFsListAsArray(String,Configuration,String)",2,2,4
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.hadoopFsListAsString(String,Configuration,String)",2,1,3
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.hadoopFsPath(String,Configuration,String)",3,2,5
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.hadoopUserEnv(String,String)",1,2,3
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.isset(Collection<T>)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.isset(Map<K, V>)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.isset(String)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.isset(T[])",1,1,2
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.isset(char)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.quoteForWindows(String)",6,6,9
"org.apache.hive.hcatalog.templeton.tool.TempletonUtils.unEscapeString(String)",1,3,3
"org.apache.hive.hcatalog.templeton.tool.TestJobIDParser.testParseHive()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.TestJobIDParser.testParseJar()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.TestJobIDParser.testParsePig()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.TestJobIDParser.testParseStreaming()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.setup()",1,2,2
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.tearDown()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testConstructingUserHomeDirectory()",1,2,3
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testDecodeArray()",1,3,3
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testEncodeArray()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testExtractPercentComplete()",1,2,2
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testFindContainingJar()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testHadoopFsFilename()",1,4,5
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testHadoopFsListAsArray()",1,4,5
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testHadoopFsListAsString()",1,4,5
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testHadoopFsPath()",1,6,7
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testIssetString()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testIssetTArray()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testPrintTaggedJobID()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testPropertiesParsing()",1,4,4
"org.apache.hive.hcatalog.templeton.tool.TestTrivialExecService.test()",1,3,5
"org.apache.hive.hcatalog.templeton.tool.TrivialExecService.getInstance()",1,1,2
"org.apache.hive.hcatalog.templeton.tool.TrivialExecService.logDebugInfo(String,Map<String, String>)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.TrivialExecService.run(List<String>,List<String>,Map<String, String>)",1,3,3
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.ZooKeeperCleanup(Configuration)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.checkAndDelete(String,ZooKeeper)",2,6,6
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.exit()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.getChildList(ZooKeeper)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.getInstance(Configuration)",2,1,2
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.run()",4,6,8
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.startInstance(Configuration)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.ZooKeeperStorage()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.close()",2,2,3
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.closeStorage()",1,1,1
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.create(Type,String)",3,7,11
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.delete(Type,String)",2,5,5
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.getAll()",1,2,2
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.getAllForKey(String,String)",1,3,3
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.getAllForType(Type)",1,1,2
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.getAllForTypeAndKey(Type,String,String)",1,6,6
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.getField(Type,String,String)",1,1,2
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.getFields(Type,String)",1,2,3
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.getPath(Type)",2,2,3
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.getPaths(String)",1,3,3
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.makeFieldZnode(Type,String,String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.makeZnode(Type,String)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.openStorage(Configuration)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.saveField(Type,String,String,String)",1,3,3
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.setFieldData(Type,String,String,String)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.startCleanup(Configuration)",1,2,2
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.zkOpen(Configuration)",1,1,1
"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage.zkOpen(String,int)",1,1,1
"org.apache.hive.hcatalog.utils.DataReaderMaster.main(String[])",1,2,2
"org.apache.hive.hcatalog.utils.DataReaderMaster.runsInMaster(Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.utils.DataReaderSlave.main(String[])",1,3,3
"org.apache.hive.hcatalog.utils.DataWriterMaster.commit(Map<String, String>,boolean,WriterContext)",1,2,2
"org.apache.hive.hcatalog.utils.DataWriterMaster.main(String[])",1,4,4
"org.apache.hive.hcatalog.utils.DataWriterMaster.runsInMaster(Map<String, String>)",1,1,1
"org.apache.hive.hcatalog.utils.DataWriterSlave.HCatRecordItr.HCatRecordItr(String)",1,1,1
"org.apache.hive.hcatalog.utils.DataWriterSlave.HCatRecordItr.hasNext()",1,2,3
"org.apache.hive.hcatalog.utils.DataWriterSlave.HCatRecordItr.next()",1,1,1
"org.apache.hive.hcatalog.utils.DataWriterSlave.HCatRecordItr.remove()",1,1,1
"org.apache.hive.hcatalog.utils.DataWriterSlave.main(String[])",1,1,1
"org.apache.hive.hcatalog.utils.GroupByAge.Map.map(WritableComparable,HCatRecord,Context)",1,1,1
"org.apache.hive.hcatalog.utils.GroupByAge.Reduce.reduce(IntWritable,Iterable<IntWritable>,Context)",1,2,2
"org.apache.hive.hcatalog.utils.GroupByAge.main(String[])",1,1,1
"org.apache.hive.hcatalog.utils.GroupByAge.run(String[])",1,2,3
"org.apache.hive.hcatalog.utils.HCatTestDriver.main(String[])",1,2,2
"org.apache.hive.hcatalog.utils.HCatTypeCheck.check(Byte,Object)",2,9,10
"org.apache.hive.hcatalog.utils.HCatTypeCheck.check(Class<?>,Object)",2,1,2
"org.apache.hive.hcatalog.utils.HCatTypeCheck.check(Map<String, String>)",1,4,4
"org.apache.hive.hcatalog.utils.HCatTypeCheck.die(String,Object)",1,1,1
"org.apache.hive.hcatalog.utils.HCatTypeCheck.exec(Tuple)",1,2,3
"org.apache.hive.hcatalog.utils.HCatTypeCheck.getSchemaFromString(String)",1,1,1
"org.apache.hive.hcatalog.utils.HCatTypeCheckHive.evaluate(DeferredObject[])",2,5,7
"org.apache.hive.hcatalog.utils.HCatTypeCheckHive.getDisplayString(String[])",1,1,1
"org.apache.hive.hcatalog.utils.HCatTypeCheckHive.getJavaObject(Object,ObjectInspector,List<Category>)",5,9,9
"org.apache.hive.hcatalog.utils.HCatTypeCheckHive.initialize(ObjectInspector[])",1,1,1
"org.apache.hive.hcatalog.utils.ReadJson.Map.map(WritableComparable,HCatRecord,Context)",1,4,4
"org.apache.hive.hcatalog.utils.ReadJson.main(String[])",1,1,1
"org.apache.hive.hcatalog.utils.ReadJson.run(String[])",1,2,3
"org.apache.hive.hcatalog.utils.ReadRC.Map.map(WritableComparable,HCatRecord,Context)",1,1,1
"org.apache.hive.hcatalog.utils.ReadRC.main(String[])",1,1,1
"org.apache.hive.hcatalog.utils.ReadRC.run(String[])",1,2,3
"org.apache.hive.hcatalog.utils.ReadText.Map.map(WritableComparable,HCatRecord,Context)",1,1,1
"org.apache.hive.hcatalog.utils.ReadText.main(String[])",1,1,1
"org.apache.hive.hcatalog.utils.ReadText.run(String[])",1,2,3
"org.apache.hive.hcatalog.utils.ReadWrite.Map.map(WritableComparable,HCatRecord,Context)",1,1,1
"org.apache.hive.hcatalog.utils.ReadWrite.main(String[])",1,1,1
"org.apache.hive.hcatalog.utils.ReadWrite.run(String[])",1,2,3
"org.apache.hive.hcatalog.utils.SimpleRead.Map.map(WritableComparable,HCatRecord,Context)",1,1,1
"org.apache.hive.hcatalog.utils.SimpleRead.main(String[])",1,1,1
"org.apache.hive.hcatalog.utils.SimpleRead.run(String[])",1,2,3
"org.apache.hive.hcatalog.utils.StoreComplex.ComplexMapper.map(WritableComparable,HCatRecord,Context)",1,1,1
"org.apache.hive.hcatalog.utils.StoreComplex.main(String[])",1,6,7
"org.apache.hive.hcatalog.utils.StoreComplex.usage()",1,1,1
"org.apache.hive.hcatalog.utils.StoreDemo.SumMapper.map(WritableComparable,HCatRecord,Context)",1,1,1
"org.apache.hive.hcatalog.utils.StoreDemo.main(String[])",1,5,6
"org.apache.hive.hcatalog.utils.StoreDemo.usage()",1,1,1
"org.apache.hive.hcatalog.utils.StoreNumbers.SumMapper.map(WritableComparable,HCatRecord,Context)",1,4,4
"org.apache.hive.hcatalog.utils.StoreNumbers.main(String[])",1,15,17
"org.apache.hive.hcatalog.utils.StoreNumbers.usage()",1,1,1
"org.apache.hive.hcatalog.utils.SumNumbers.ArrayWritable.ArrayWritable()",1,1,1
"org.apache.hive.hcatalog.utils.SumNumbers.ArrayWritable.ArrayWritable(IntWritable,IntWritable,IntWritable,IntWritable,LongWritable,FloatWritable,DoubleWritable)",1,1,1
"org.apache.hive.hcatalog.utils.SumNumbers.ArrayWritable.readFields(DataInput)",1,1,1
"org.apache.hive.hcatalog.utils.SumNumbers.ArrayWritable.write(DataOutput)",1,1,1
"org.apache.hive.hcatalog.utils.SumNumbers.SumMapper.map(WritableComparable,HCatRecord,Context)",1,1,1
"org.apache.hive.hcatalog.utils.SumNumbers.SumReducer.reduce(IntWritable,Iterable<ArrayWritable>,Context)",1,2,2
"org.apache.hive.hcatalog.utils.SumNumbers.main(String[])",1,5,6
"org.apache.hive.hcatalog.utils.TypeDataCheck.TypeDataCheckMapper.check(HCatRecord)",1,2,3
"org.apache.hive.hcatalog.utils.TypeDataCheck.TypeDataCheckMapper.map(WritableComparable,HCatRecord,Context)",1,1,1
"org.apache.hive.hcatalog.utils.TypeDataCheck.TypeDataCheckMapper.setup(Context)",1,3,4
"org.apache.hive.hcatalog.utils.TypeDataCheck.getConf()",1,1,1
"org.apache.hive.hcatalog.utils.TypeDataCheck.main(String[])",1,1,1
"org.apache.hive.hcatalog.utils.TypeDataCheck.run(String[])",1,7,9
"org.apache.hive.hcatalog.utils.TypeDataCheck.setConf(Configuration)",1,1,1
"org.apache.hive.hcatalog.utils.Util.check(Class<?>,Object)",2,1,2
"org.apache.hive.hcatalog.utils.Util.check(Map<String, String>)",2,4,5
"org.apache.hive.hcatalog.utils.Util.check(String,Object)",2,9,10
"org.apache.hive.hcatalog.utils.Util.die(String,Object)",1,1,1
"org.apache.hive.hcatalog.utils.WriteJson.Map.map(WritableComparable,HCatRecord,Context)",1,4,4
"org.apache.hive.hcatalog.utils.WriteJson.main(String[])",1,1,1
"org.apache.hive.hcatalog.utils.WriteJson.run(String[])",1,2,3
"org.apache.hive.hcatalog.utils.WriteRC.Map.map(WritableComparable,HCatRecord,Context)",1,5,5
"org.apache.hive.hcatalog.utils.WriteRC.main(String[])",1,1,1
"org.apache.hive.hcatalog.utils.WriteRC.run(String[])",1,2,3
"org.apache.hive.hcatalog.utils.WriteText.Map.map(WritableComparable,HCatRecord,Context)",1,1,1
"org.apache.hive.hcatalog.utils.WriteText.main(String[])",1,1,1
"org.apache.hive.hcatalog.utils.WriteText.run(String[])",1,2,3
"org.apache.hive.hcatalog.utils.WriteTextPartitioned.Map.map(WritableComparable,HCatRecord,Context)",1,2,3
"org.apache.hive.hcatalog.utils.WriteTextPartitioned.main(String[])",1,1,1
"org.apache.hive.hcatalog.utils.WriteTextPartitioned.run(String[])",1,3,5
"org.apache.hive.jdbc.HiveBaseResultSet.absolute(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.afterLast()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.beforeFirst()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.cancelRowUpdates()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.clearWarnings()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.close()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.deleteRow()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.evaluate(Type,Object)",9,5,9
"org.apache.hive.jdbc.HiveBaseResultSet.findColumn(String)",2,1,2
"org.apache.hive.jdbc.HiveBaseResultSet.first()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getArray(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getArray(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getAsciiStream(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getAsciiStream(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getBigDecimal(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getBigDecimal(String,int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getBigDecimal(int)",2,1,3
"org.apache.hive.jdbc.HiveBaseResultSet.getBigDecimal(int,int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getBinaryStream(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getBinaryStream(int)",5,5,6
"org.apache.hive.jdbc.HiveBaseResultSet.getBlob(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getBlob(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getBoolean(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getBoolean(int)",5,5,5
"org.apache.hive.jdbc.HiveBaseResultSet.getByte(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getByte(int)",3,2,3
"org.apache.hive.jdbc.HiveBaseResultSet.getBytes(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getBytes(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getCharacterStream(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getCharacterStream(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getClob(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getClob(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getColumnValue(int)",4,2,5
"org.apache.hive.jdbc.HiveBaseResultSet.getConcurrency()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getCursorName()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getDate(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getDate(String,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getDate(int)",4,3,5
"org.apache.hive.jdbc.HiveBaseResultSet.getDate(int,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getDouble(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getDouble(int)",4,5,5
"org.apache.hive.jdbc.HiveBaseResultSet.getFetchDirection()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getFetchSize()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getFloat(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getFloat(int)",4,5,5
"org.apache.hive.jdbc.HiveBaseResultSet.getHoldability()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getInt(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getInt(int)",4,5,5
"org.apache.hive.jdbc.HiveBaseResultSet.getLong(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getLong(int)",4,5,5
"org.apache.hive.jdbc.HiveBaseResultSet.getMetaData()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getNCharacterStream(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getNCharacterStream(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getNClob(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getNClob(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getNString(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getNString(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getObject(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getObject(String,Map<String, Class<?>>)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getObject(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getObject(int,Map<String, Class<?>>)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getRef(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getRef(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getRow()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getRowId(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getRowId(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getSQLXML(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getSQLXML(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getSchema()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getShort(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getShort(int)",4,5,5
"org.apache.hive.jdbc.HiveBaseResultSet.getStatement()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getString(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getString(int)",3,1,3
"org.apache.hive.jdbc.HiveBaseResultSet.getTime(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getTime(String,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getTime(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getTime(int,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getTimestamp(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getTimestamp(String,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getTimestamp(int)",4,2,4
"org.apache.hive.jdbc.HiveBaseResultSet.getTimestamp(int,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getType()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getURL(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getURL(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getUnicodeStream(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getUnicodeStream(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.getWarnings()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.insertRow()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.isAfterLast()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.isBeforeFirst()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.isClosed()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.isFirst()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.isLast()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.isWrapperFor(Class<?>)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.last()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.moveToCurrentRow()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.moveToInsertRow()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.previous()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.refreshRow()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.relative(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.rowDeleted()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.rowInserted()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.rowUpdated()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.setFetchDirection(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.setFetchSize(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.setSchema(TableSchema)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.unwrap(Class<T>)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateArray(String,Array)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateArray(int,Array)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateAsciiStream(String,InputStream)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateAsciiStream(String,InputStream,int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateAsciiStream(String,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateAsciiStream(int,InputStream)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateAsciiStream(int,InputStream,int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateAsciiStream(int,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBigDecimal(String,BigDecimal)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBigDecimal(int,BigDecimal)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBinaryStream(String,InputStream)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBinaryStream(String,InputStream,int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBinaryStream(String,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBinaryStream(int,InputStream)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBinaryStream(int,InputStream,int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBinaryStream(int,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBlob(String,Blob)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBlob(String,InputStream)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBlob(String,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBlob(int,Blob)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBlob(int,InputStream)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBlob(int,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBoolean(String,boolean)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBoolean(int,boolean)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateByte(String,byte)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateByte(int,byte)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBytes(String,byte[])",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateBytes(int,byte[])",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateCharacterStream(String,Reader)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateCharacterStream(String,Reader,int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateCharacterStream(String,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateCharacterStream(int,Reader)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateCharacterStream(int,Reader,int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateCharacterStream(int,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateClob(String,Clob)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateClob(String,Reader)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateClob(String,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateClob(int,Clob)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateClob(int,Reader)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateClob(int,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateDate(String,Date)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateDate(int,Date)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateDouble(String,double)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateDouble(int,double)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateFloat(String,float)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateFloat(int,float)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateInt(String,int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateInt(int,int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateLong(String,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateLong(int,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateNCharacterStream(String,Reader)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateNCharacterStream(String,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateNCharacterStream(int,Reader)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateNCharacterStream(int,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateNClob(String,NClob)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateNClob(String,Reader)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateNClob(String,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateNClob(int,NClob)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateNClob(int,Reader)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateNClob(int,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateNString(String,String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateNString(int,String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateNull(String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateNull(int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateObject(String,Object)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateObject(String,Object,int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateObject(int,Object)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateObject(int,Object,int)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateRef(String,Ref)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateRef(int,Ref)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateRow()",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateRowId(String,RowId)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateRowId(int,RowId)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateSQLXML(String,SQLXML)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateSQLXML(int,SQLXML)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateShort(String,short)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateShort(int,short)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateString(String,String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateString(int,String)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateTime(String,Time)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateTime(int,Time)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateTimestamp(String,Timestamp)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.updateTimestamp(int,Timestamp)",1,1,1
"org.apache.hive.jdbc.HiveBaseResultSet.wasNull()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.HiveCallableStatement(Connection)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.addBatch()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.addBatch(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.cancel()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.clearBatch()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.clearParameters()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.clearWarnings()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.close()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.closeOnCompletion()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.execute()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.execute(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.execute(String,String[])",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.execute(String,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.execute(String,int[])",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.executeBatch()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.executeQuery()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.executeQuery(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.executeUpdate()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.executeUpdate(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.executeUpdate(String,String[])",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.executeUpdate(String,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.executeUpdate(String,int[])",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getArray(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getArray(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getBigDecimal(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getBigDecimal(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getBigDecimal(int,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getBlob(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getBlob(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getBoolean(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getBoolean(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getByte(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getByte(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getBytes(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getBytes(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getCharacterStream(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getCharacterStream(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getClob(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getClob(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getConnection()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getDate(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getDate(String,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getDate(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getDate(int,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getDouble(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getDouble(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getFetchDirection()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getFetchSize()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getFloat(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getFloat(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getGeneratedKeys()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getInt(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getInt(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getLong(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getLong(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getMaxFieldSize()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getMaxRows()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getMetaData()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getMoreResults()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getMoreResults(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getNCharacterStream(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getNCharacterStream(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getNClob(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getNClob(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getNString(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getNString(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getObject(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getObject(String,Class<T>)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getObject(String,Map<String, Class<?>>)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getObject(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getObject(int,Class<T>)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getObject(int,Map<String, Class<?>>)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getParameterMetaData()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getQueryTimeout()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getRef(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getRef(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getResultSet()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getResultSetConcurrency()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getResultSetHoldability()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getResultSetType()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getRowId(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getRowId(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getSQLXML(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getSQLXML(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getShort(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getShort(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getString(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getString(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getTime(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getTime(String,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getTime(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getTime(int,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getTimestamp(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getTimestamp(String,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getTimestamp(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getTimestamp(int,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getURL(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getURL(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getUpdateCount()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.getWarnings()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.isCloseOnCompletion()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.isClosed()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.isPoolable()",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.isWrapperFor(Class<?>)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.registerOutParameter(String,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.registerOutParameter(String,int,String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.registerOutParameter(String,int,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.registerOutParameter(int,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.registerOutParameter(int,int,String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.registerOutParameter(int,int,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setArray(int,Array)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setAsciiStream(String,InputStream)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setAsciiStream(String,InputStream,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setAsciiStream(String,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setAsciiStream(int,InputStream)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setAsciiStream(int,InputStream,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setAsciiStream(int,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBigDecimal(String,BigDecimal)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBigDecimal(int,BigDecimal)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBinaryStream(String,InputStream)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBinaryStream(String,InputStream,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBinaryStream(String,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBinaryStream(int,InputStream)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBinaryStream(int,InputStream,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBinaryStream(int,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBlob(String,Blob)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBlob(String,InputStream)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBlob(String,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBlob(int,Blob)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBlob(int,InputStream)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBlob(int,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBoolean(String,boolean)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBoolean(int,boolean)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setByte(String,byte)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setByte(int,byte)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBytes(String,byte[])",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setBytes(int,byte[])",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setCharacterStream(String,Reader)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setCharacterStream(String,Reader,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setCharacterStream(String,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setCharacterStream(int,Reader)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setCharacterStream(int,Reader,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setCharacterStream(int,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setClob(String,Clob)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setClob(String,Reader)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setClob(String,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setClob(int,Clob)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setClob(int,Reader)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setClob(int,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setCursorName(String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setDate(String,Date)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setDate(String,Date,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setDate(int,Date)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setDate(int,Date,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setDouble(String,double)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setDouble(int,double)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setEscapeProcessing(boolean)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setFetchDirection(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setFetchSize(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setFloat(String,float)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setFloat(int,float)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setInt(String,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setInt(int,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setLong(String,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setLong(int,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setMaxFieldSize(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setMaxRows(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNCharacterStream(String,Reader)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNCharacterStream(String,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNCharacterStream(int,Reader)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNCharacterStream(int,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNClob(String,NClob)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNClob(String,Reader)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNClob(String,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNClob(int,NClob)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNClob(int,Reader)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNClob(int,Reader,long)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNString(String,String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNString(int,String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNull(String,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNull(String,int,String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNull(int,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setNull(int,int,String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setObject(String,Object)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setObject(String,Object,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setObject(String,Object,int,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setObject(int,Object)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setObject(int,Object,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setObject(int,Object,int,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setPoolable(boolean)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setQueryTimeout(int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setRef(int,Ref)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setRowId(String,RowId)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setRowId(int,RowId)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setSQLXML(String,SQLXML)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setSQLXML(int,SQLXML)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setShort(String,short)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setShort(int,short)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setString(String,String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setString(int,String)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setTime(String,Time)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setTime(String,Time,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setTime(int,Time)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setTime(int,Time,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setTimestamp(String,Timestamp)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setTimestamp(String,Timestamp,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setTimestamp(int,Timestamp)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setTimestamp(int,Timestamp,Calendar)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setURL(String,URL)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setURL(int,URL)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.setUnicodeStream(int,InputStream,int)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.unwrap(Class<T>)",1,1,1
"org.apache.hive.jdbc.HiveCallableStatement.wasNull()",1,1,1
"org.apache.hive.jdbc.HiveConnection.HiveConnection(String,Properties)",1,9,10
"org.apache.hive.jdbc.HiveConnection.abort(Executor)",1,1,1
"org.apache.hive.jdbc.HiveConnection.cancelDelegationToken(String)",1,2,2
"org.apache.hive.jdbc.HiveConnection.clearWarnings()",1,1,1
"org.apache.hive.jdbc.HiveConnection.close()",2,3,4
"org.apache.hive.jdbc.HiveConnection.commit()",1,1,1
"org.apache.hive.jdbc.HiveConnection.createArrayOf(String,Object[])",1,1,1
"org.apache.hive.jdbc.HiveConnection.createBinaryTransport()",4,11,11
"org.apache.hive.jdbc.HiveConnection.createBlob()",1,1,1
"org.apache.hive.jdbc.HiveConnection.createClob()",1,1,1
"org.apache.hive.jdbc.HiveConnection.createHttpTransport()",1,3,3
"org.apache.hive.jdbc.HiveConnection.createNClob()",1,1,1
"org.apache.hive.jdbc.HiveConnection.createSQLXML()",1,1,1
"org.apache.hive.jdbc.HiveConnection.createStatement()",2,1,2
"org.apache.hive.jdbc.HiveConnection.createStatement(int,int)",3,1,3
"org.apache.hive.jdbc.HiveConnection.createStatement(int,int,int)",1,1,1
"org.apache.hive.jdbc.HiveConnection.createStruct(String,Object[])",1,1,1
"org.apache.hive.jdbc.HiveConnection.getAutoCommit()",1,1,1
"org.apache.hive.jdbc.HiveConnection.getCatalog()",1,1,1
"org.apache.hive.jdbc.HiveConnection.getClientDelegationToken(Map<String, String>)",2,2,3
"org.apache.hive.jdbc.HiveConnection.getClientInfo()",1,1,1
"org.apache.hive.jdbc.HiveConnection.getClientInfo(String)",1,1,1
"org.apache.hive.jdbc.HiveConnection.getDelegationToken(String,String)",1,2,2
"org.apache.hive.jdbc.HiveConnection.getHoldability()",1,1,1
"org.apache.hive.jdbc.HiveConnection.getHttpClient(Boolean)",4,6,7
"org.apache.hive.jdbc.HiveConnection.getMetaData()",2,1,2
"org.apache.hive.jdbc.HiveConnection.getNetworkTimeout()",1,1,1
"org.apache.hive.jdbc.HiveConnection.getPassword()",1,1,1
"org.apache.hive.jdbc.HiveConnection.getProtocol()",1,1,1
"org.apache.hive.jdbc.HiveConnection.getSchema()",3,1,3
"org.apache.hive.jdbc.HiveConnection.getServerHttpUrl(boolean)",1,2,4
"org.apache.hive.jdbc.HiveConnection.getSessionValue(String,String)",1,2,3
"org.apache.hive.jdbc.HiveConnection.getTransactionIsolation()",1,1,1
"org.apache.hive.jdbc.HiveConnection.getTypeMap()",1,1,1
"org.apache.hive.jdbc.HiveConnection.getUserName()",1,1,1
"org.apache.hive.jdbc.HiveConnection.getWarnings()",1,1,1
"org.apache.hive.jdbc.HiveConnection.isClosed()",1,1,1
"org.apache.hive.jdbc.HiveConnection.isHttpTransportMode()",2,2,3
"org.apache.hive.jdbc.HiveConnection.isKerberosAuthMode()",1,2,2
"org.apache.hive.jdbc.HiveConnection.isReadOnly()",1,1,1
"org.apache.hive.jdbc.HiveConnection.isSslConnection()",1,1,1
"org.apache.hive.jdbc.HiveConnection.isValid(int)",1,1,1
"org.apache.hive.jdbc.HiveConnection.isWrapperFor(Class<?>)",1,1,1
"org.apache.hive.jdbc.HiveConnection.nativeSQL(String)",1,1,1
"org.apache.hive.jdbc.HiveConnection.openSession(JdbcConnectionParams)",2,5,6
"org.apache.hive.jdbc.HiveConnection.openTransport()",1,4,4
"org.apache.hive.jdbc.HiveConnection.prepareCall(String)",1,1,1
"org.apache.hive.jdbc.HiveConnection.prepareCall(String,int,int)",1,1,1
"org.apache.hive.jdbc.HiveConnection.prepareCall(String,int,int,int)",1,1,1
"org.apache.hive.jdbc.HiveConnection.prepareStatement(String)",1,1,1
"org.apache.hive.jdbc.HiveConnection.prepareStatement(String,String[])",1,1,1
"org.apache.hive.jdbc.HiveConnection.prepareStatement(String,int)",1,1,1
"org.apache.hive.jdbc.HiveConnection.prepareStatement(String,int,int)",1,1,1
"org.apache.hive.jdbc.HiveConnection.prepareStatement(String,int,int,int)",1,1,1
"org.apache.hive.jdbc.HiveConnection.prepareStatement(String,int[])",1,1,1
"org.apache.hive.jdbc.HiveConnection.releaseSavepoint(Savepoint)",1,1,1
"org.apache.hive.jdbc.HiveConnection.renewDelegationToken(String)",1,2,2
"org.apache.hive.jdbc.HiveConnection.rollback()",1,1,1
"org.apache.hive.jdbc.HiveConnection.rollback(Savepoint)",1,1,1
"org.apache.hive.jdbc.HiveConnection.setAutoCommit(boolean)",2,1,2
"org.apache.hive.jdbc.HiveConnection.setCatalog(String)",2,1,2
"org.apache.hive.jdbc.HiveConnection.setClientInfo(Properties)",1,1,1
"org.apache.hive.jdbc.HiveConnection.setClientInfo(String,String)",1,1,1
"org.apache.hive.jdbc.HiveConnection.setHoldability(int)",1,1,1
"org.apache.hive.jdbc.HiveConnection.setNetworkTimeout(Executor,int)",1,1,1
"org.apache.hive.jdbc.HiveConnection.setReadOnly(boolean)",1,1,1
"org.apache.hive.jdbc.HiveConnection.setSavepoint()",1,1,1
"org.apache.hive.jdbc.HiveConnection.setSavepoint(String)",1,1,1
"org.apache.hive.jdbc.HiveConnection.setSchema(String)",3,2,4
"org.apache.hive.jdbc.HiveConnection.setTransactionIsolation(int)",1,1,1
"org.apache.hive.jdbc.HiveConnection.setTypeMap(Map<String, Class<?>>)",1,1,1
"org.apache.hive.jdbc.HiveConnection.setupLoginTimeout()",1,1,2
"org.apache.hive.jdbc.HiveConnection.unwrap(Class<T>)",1,1,1
"org.apache.hive.jdbc.HiveDataSource.HiveDataSource()",1,1,1
"org.apache.hive.jdbc.HiveDataSource.getConnection()",1,1,1
"org.apache.hive.jdbc.HiveDataSource.getConnection(String,String)",1,1,2
"org.apache.hive.jdbc.HiveDataSource.getLogWriter()",1,1,1
"org.apache.hive.jdbc.HiveDataSource.getLoginTimeout()",1,1,1
"org.apache.hive.jdbc.HiveDataSource.getParentLogger()",1,1,1
"org.apache.hive.jdbc.HiveDataSource.isWrapperFor(Class<?>)",1,1,1
"org.apache.hive.jdbc.HiveDataSource.setLogWriter(PrintWriter)",1,1,1
"org.apache.hive.jdbc.HiveDataSource.setLoginTimeout(int)",1,1,1
"org.apache.hive.jdbc.HiveDataSource.unwrap(Class<T>)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.GetColumnsComparator.compare(JdbcColumn,JdbcColumn)",4,3,4
"org.apache.hive.jdbc.HiveDatabaseMetaData.GetTablesComparator.compare(JdbcTable,JdbcTable)",2,2,2
"org.apache.hive.jdbc.HiveDatabaseMetaData.HiveDatabaseMetaData(HiveConnection,Iface,TSessionHandle)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.allProceduresAreCallable()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.allTablesAreSelectable()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.autoCommitFailureClosesAllResultSets()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.convertPattern(String)",5,7,8
"org.apache.hive.jdbc.HiveDatabaseMetaData.dataDefinitionCausesTransactionCommit()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.dataDefinitionIgnoredInTransactions()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.deletesAreDetected(int)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.doesMaxRowSizeIncludeBlobs()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.generatedKeyAlwaysReturned()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getAttributes(String,String,String,String)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getBestRowIdentifier(String,String,String,int,boolean)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getCatalogSeparator()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getCatalogTerm()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getCatalogs()",1,2,2
"org.apache.hive.jdbc.HiveDatabaseMetaData.getClientInfoProperties()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getColumnPrivileges(String,String,String,String)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getColumns(String,String,String,String)",1,2,2
"org.apache.hive.jdbc.HiveDatabaseMetaData.getConnection()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getCrossReference(String,String,String,String,String,String)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getDatabaseMajorVersion()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getDatabaseMinorVersion()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getDatabaseProductName()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getDatabaseProductVersion()",2,1,2
"org.apache.hive.jdbc.HiveDatabaseMetaData.getDefaultTransactionIsolation()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getDriverMajorVersion()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getDriverMinorVersion()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getDriverName()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getDriverVersion()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getExportedKeys(String,String,String)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getExtraNameCharacters()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getFunctionColumns(String,String,String,String)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getFunctions(String,String,String)",1,2,2
"org.apache.hive.jdbc.HiveDatabaseMetaData.getIdentifierQuoteString()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getImportedKeys(String,String,String)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getIndexInfo(String,String,String,boolean,boolean)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getJDBCMajorVersion()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getJDBCMinorVersion()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxBinaryLiteralLength()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxCatalogNameLength()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxCharLiteralLength()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxColumnNameLength()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxColumnsInGroupBy()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxColumnsInIndex()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxColumnsInOrderBy()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxColumnsInSelect()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxColumnsInTable()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxConnections()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxCursorNameLength()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxIndexLength()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxProcedureNameLength()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxRowSize()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxSchemaNameLength()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxStatementLength()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxStatements()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxTableNameLength()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxTablesInSelect()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getMaxUserNameLength()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getNumericFunctions()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getPrimaryKeys(String,String,String)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getProcedureColumns(String,String,String,String)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getProcedureTerm()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getProcedures(String,String,String)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getPseudoColumns(String,String,String,String)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getResultSetHoldability()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getRowIdLifetime()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getSQLKeywords()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getSQLStateType()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getSchemaTerm()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getSchemas()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getSchemas(String,String)",1,3,4
"org.apache.hive.jdbc.HiveDatabaseMetaData.getSearchStringEscape()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getServerInfo(TGetInfoType)",1,2,2
"org.apache.hive.jdbc.HiveDatabaseMetaData.getStringFunctions()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getSuperTables(String,String,String)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getSuperTypes(String,String,String)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getSystemFunctions()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getTablePrivileges(String,String,String)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getTableTypes()",1,2,2
"org.apache.hive.jdbc.HiveDatabaseMetaData.getTables(String,String,String,String[])",1,4,5
"org.apache.hive.jdbc.HiveDatabaseMetaData.getTimeDateFunctions()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getTypeInfo()",1,2,2
"org.apache.hive.jdbc.HiveDatabaseMetaData.getUDTs(String,String,String,int[])",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getURL()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getUserName()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.getVersionColumns(String,String,String)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.insertsAreDetected(int)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.isCatalogAtStart()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.isReadOnly()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.isWrapperFor(Class<?>)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.locatorsUpdateCopy()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.main(String[])",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.nullPlusNonNullIsNull()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.nullsAreSortedAtEnd()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.nullsAreSortedAtStart()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.nullsAreSortedHigh()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.nullsAreSortedLow()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.othersDeletesAreVisible(int)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.othersInsertsAreVisible(int)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.othersUpdatesAreVisible(int)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.ownDeletesAreVisible(int)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.ownInsertsAreVisible(int)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.ownUpdatesAreVisible(int)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.storesLowerCaseIdentifiers()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.storesLowerCaseQuotedIdentifiers()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.storesMixedCaseIdentifiers()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.storesMixedCaseQuotedIdentifiers()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.storesUpperCaseIdentifiers()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.storesUpperCaseQuotedIdentifiers()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsANSI92EntryLevelSQL()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsANSI92FullSQL()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsANSI92IntermediateSQL()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsAlterTableWithAddColumn()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsAlterTableWithDropColumn()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsBatchUpdates()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsCatalogsInDataManipulation()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsCatalogsInIndexDefinitions()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsCatalogsInPrivilegeDefinitions()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsCatalogsInProcedureCalls()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsCatalogsInTableDefinitions()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsColumnAliasing()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsConvert()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsConvert(int,int)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsCoreSQLGrammar()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsCorrelatedSubqueries()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsDataDefinitionAndDataManipulationTransactions()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsDataManipulationTransactionsOnly()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsDifferentTableCorrelationNames()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsExpressionsInOrderBy()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsExtendedSQLGrammar()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsFullOuterJoins()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsGetGeneratedKeys()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsGroupBy()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsGroupByBeyondSelect()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsGroupByUnrelated()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsIntegrityEnhancementFacility()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsLikeEscapeClause()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsLimitedOuterJoins()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsMinimumSQLGrammar()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsMixedCaseIdentifiers()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsMixedCaseQuotedIdentifiers()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsMultipleOpenResults()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsMultipleResultSets()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsMultipleTransactions()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsNamedParameters()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsNonNullableColumns()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsOpenCursorsAcrossCommit()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsOpenCursorsAcrossRollback()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsOpenStatementsAcrossCommit()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsOpenStatementsAcrossRollback()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsOrderByUnrelated()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsOuterJoins()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsPositionedDelete()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsPositionedUpdate()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsResultSetConcurrency(int,int)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsResultSetHoldability(int)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsResultSetType(int)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsSavepoints()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsSchemasInDataManipulation()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsSchemasInIndexDefinitions()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsSchemasInPrivilegeDefinitions()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsSchemasInProcedureCalls()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsSchemasInTableDefinitions()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsSelectForUpdate()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsStatementPooling()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsStoredFunctionsUsingCallSyntax()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsStoredProcedures()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsSubqueriesInComparisons()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsSubqueriesInExists()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsSubqueriesInIns()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsSubqueriesInQuantifieds()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsTableCorrelationNames()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsTransactionIsolationLevel(int)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsTransactions()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsUnion()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.supportsUnionAll()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.toJdbcTableType(String)",5,4,5
"org.apache.hive.jdbc.HiveDatabaseMetaData.unwrap(Class<T>)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.updatesAreDetected(int)",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.usesLocalFilePerTable()",1,1,1
"org.apache.hive.jdbc.HiveDatabaseMetaData.usesLocalFiles()",1,1,1
"org.apache.hive.jdbc.HiveDriver.HiveDriver()",1,2,2
"org.apache.hive.jdbc.HiveDriver.acceptsURL(String)",1,1,1
"org.apache.hive.jdbc.HiveDriver.connect(String,Properties)",1,1,2
"org.apache.hive.jdbc.HiveDriver.fetchManifestAttribute(Name)",1,1,2
"org.apache.hive.jdbc.HiveDriver.getMajorDriverVersion()",1,2,5
"org.apache.hive.jdbc.HiveDriver.getMajorVersion()",1,1,1
"org.apache.hive.jdbc.HiveDriver.getMinorDriverVersion()",1,2,5
"org.apache.hive.jdbc.HiveDriver.getMinorVersion()",1,1,1
"org.apache.hive.jdbc.HiveDriver.getParentLogger()",1,1,1
"org.apache.hive.jdbc.HiveDriver.getPropertyInfo(String,Properties)",1,3,4
"org.apache.hive.jdbc.HiveDriver.jdbcCompliant()",1,1,1
"org.apache.hive.jdbc.HiveDriver.loadManifestAttributes()",2,1,2
"org.apache.hive.jdbc.HiveDriver.parseURLforPropertyInfo(String,Properties)",2,3,7
"org.apache.hive.jdbc.HiveMetaDataResultSet.HiveMetaDataResultSet(List<String>,List<String>,List<M>)",1,1,4
"org.apache.hive.jdbc.HiveMetaDataResultSet.close()",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.HivePreparedStatement(HiveConnection,Iface,TSessionHandle,String)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.addBatch()",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.clearParameters()",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.execute()",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.executeQuery()",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.executeUpdate()",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.getCharIndexFromSqlByParamLocation(String,char,int)",5,2,7
"org.apache.hive.jdbc.HivePreparedStatement.getMetaData()",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.getParameterMetaData()",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setArray(int,Array)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setAsciiStream(int,InputStream)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setAsciiStream(int,InputStream,int)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setAsciiStream(int,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setBigDecimal(int,BigDecimal)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setBinaryStream(int,InputStream)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setBinaryStream(int,InputStream,int)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setBinaryStream(int,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setBlob(int,Blob)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setBlob(int,InputStream)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setBlob(int,InputStream,long)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setBoolean(int,boolean)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setByte(int,byte)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setBytes(int,byte[])",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setCharacterStream(int,Reader)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setCharacterStream(int,Reader,int)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setCharacterStream(int,Reader,long)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setClob(int,Clob)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setClob(int,Reader)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setClob(int,Reader,long)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setDate(int,Date)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setDate(int,Date,Calendar)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setDouble(int,double)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setFloat(int,float)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setInt(int,int)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setLong(int,long)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setNCharacterStream(int,Reader)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setNCharacterStream(int,Reader,long)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setNClob(int,NClob)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setNClob(int,Reader)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setNClob(int,Reader,long)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setNString(int,String)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setNull(int,int)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setNull(int,int,String)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setObject(int,Object)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setObject(int,Object,int)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setObject(int,Object,int,int)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setRef(int,Ref)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setRowId(int,RowId)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setSQLXML(int,SQLXML)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setShort(int,short)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setString(int,String)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setTime(int,Time)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setTime(int,Time,Calendar)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setTimestamp(int,Timestamp)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setTimestamp(int,Timestamp,Calendar)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setURL(int,URL)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.setUnicodeStream(int,InputStream,int)",1,1,1
"org.apache.hive.jdbc.HivePreparedStatement.updateSql(String,HashMap<Integer, String>)",2,3,4
"org.apache.hive.jdbc.HiveQueryResultSet.Builder.Builder(Connection)",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.Builder.Builder(Statement)",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.Builder.build()",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.Builder.getProtocolVersion()",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.Builder.setClient(Iface)",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.Builder.setEmptyResultSet(boolean)",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.Builder.setFetchSize(int)",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.Builder.setMaxRows(int)",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.Builder.setSchema(List<String>,List<String>)",1,2,2
"org.apache.hive.jdbc.HiveQueryResultSet.Builder.setSchema(List<String>,List<String>,List<JdbcColumnAttributes>)",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.Builder.setScrollable(boolean)",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.Builder.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.Builder.setStmtHandle(TOperationHandle)",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.HiveQueryResultSet(Builder)",1,2,3
"org.apache.hive.jdbc.HiveQueryResultSet.beforeFirst()",3,1,3
"org.apache.hive.jdbc.HiveQueryResultSet.close()",1,2,3
"org.apache.hive.jdbc.HiveQueryResultSet.getColumnAttributes(TPrimitiveTypeEntry)",2,6,8
"org.apache.hive.jdbc.HiveQueryResultSet.getFetchSize()",2,1,2
"org.apache.hive.jdbc.HiveQueryResultSet.getMetaData()",2,1,2
"org.apache.hive.jdbc.HiveQueryResultSet.getObject(String,Class<T>)",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.getObject(int,Class<T>)",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.getRow()",1,1,1
"org.apache.hive.jdbc.HiveQueryResultSet.getType()",3,1,3
"org.apache.hive.jdbc.HiveQueryResultSet.isBeforeFirst()",2,1,2
"org.apache.hive.jdbc.HiveQueryResultSet.next()",4,6,12
"org.apache.hive.jdbc.HiveQueryResultSet.retrieveSchema()",2,5,7
"org.apache.hive.jdbc.HiveQueryResultSet.setFetchSize(int)",2,1,2
"org.apache.hive.jdbc.HiveQueryResultSet.setSchema(List<String>,List<String>,List<JdbcColumnAttributes>)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.HiveResultSetMetaData(List<String>,List<String>,List<JdbcColumnAttributes>)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.getCatalogName(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.getColumnClassName(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.getColumnCount()",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.getColumnDisplaySize(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.getColumnLabel(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.getColumnName(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.getColumnType(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.getColumnTypeName(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.getPrecision(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.getScale(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.getSchemaName(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.getTableName(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.isAutoIncrement(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.isCaseSensitive(int)",2,1,2
"org.apache.hive.jdbc.HiveResultSetMetaData.isCurrency(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.isDefinitelyWritable(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.isNullable(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.isReadOnly(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.isSearchable(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.isSigned(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.isWrapperFor(Class<?>)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.isWritable(int)",1,1,1
"org.apache.hive.jdbc.HiveResultSetMetaData.toZeroIndex(int)",3,2,4
"org.apache.hive.jdbc.HiveResultSetMetaData.unwrap(Class<T>)",1,1,1
"org.apache.hive.jdbc.HiveStatement.HiveStatement(HiveConnection,Iface,TSessionHandle)",1,1,1
"org.apache.hive.jdbc.HiveStatement.HiveStatement(HiveConnection,Iface,TSessionHandle,boolean)",1,1,1
"org.apache.hive.jdbc.HiveStatement.addBatch(String)",1,1,1
"org.apache.hive.jdbc.HiveStatement.cancel()",1,3,4
"org.apache.hive.jdbc.HiveStatement.checkConnection(String)",2,1,2
"org.apache.hive.jdbc.HiveStatement.clearBatch()",1,1,1
"org.apache.hive.jdbc.HiveStatement.clearWarnings()",1,1,1
"org.apache.hive.jdbc.HiveStatement.close()",2,1,2
"org.apache.hive.jdbc.HiveStatement.closeClientOperation()",1,3,4
"org.apache.hive.jdbc.HiveStatement.closeOnCompletion()",1,1,1
"org.apache.hive.jdbc.HiveStatement.execute(String)",8,7,13
"org.apache.hive.jdbc.HiveStatement.execute(String,String[])",1,1,1
"org.apache.hive.jdbc.HiveStatement.execute(String,int)",1,1,1
"org.apache.hive.jdbc.HiveStatement.execute(String,int[])",1,1,1
"org.apache.hive.jdbc.HiveStatement.executeBatch()",1,1,1
"org.apache.hive.jdbc.HiveStatement.executeQuery(String)",2,1,2
"org.apache.hive.jdbc.HiveStatement.executeUpdate(String)",1,1,1
"org.apache.hive.jdbc.HiveStatement.executeUpdate(String,String[])",1,1,1
"org.apache.hive.jdbc.HiveStatement.executeUpdate(String,int)",1,1,1
"org.apache.hive.jdbc.HiveStatement.executeUpdate(String,int[])",1,1,1
"org.apache.hive.jdbc.HiveStatement.getConnection()",1,1,1
"org.apache.hive.jdbc.HiveStatement.getFetchDirection()",1,1,1
"org.apache.hive.jdbc.HiveStatement.getFetchSize()",1,1,1
"org.apache.hive.jdbc.HiveStatement.getGeneratedKeys()",1,1,1
"org.apache.hive.jdbc.HiveStatement.getMaxFieldSize()",1,1,1
"org.apache.hive.jdbc.HiveStatement.getMaxRows()",1,1,1
"org.apache.hive.jdbc.HiveStatement.getMoreResults()",1,1,1
"org.apache.hive.jdbc.HiveStatement.getMoreResults(int)",1,1,1
"org.apache.hive.jdbc.HiveStatement.getQueryTimeout()",1,1,1
"org.apache.hive.jdbc.HiveStatement.getResultSet()",1,1,1
"org.apache.hive.jdbc.HiveStatement.getResultSetConcurrency()",1,1,1
"org.apache.hive.jdbc.HiveStatement.getResultSetHoldability()",1,1,1
"org.apache.hive.jdbc.HiveStatement.getResultSetType()",1,1,1
"org.apache.hive.jdbc.HiveStatement.getUpdateCount()",1,1,1
"org.apache.hive.jdbc.HiveStatement.getWarnings()",1,1,1
"org.apache.hive.jdbc.HiveStatement.isCloseOnCompletion()",1,1,1
"org.apache.hive.jdbc.HiveStatement.isClosed()",1,1,1
"org.apache.hive.jdbc.HiveStatement.isPoolable()",1,1,1
"org.apache.hive.jdbc.HiveStatement.isWrapperFor(Class<?>)",1,1,1
"org.apache.hive.jdbc.HiveStatement.setCursorName(String)",1,1,1
"org.apache.hive.jdbc.HiveStatement.setEscapeProcessing(boolean)",2,1,2
"org.apache.hive.jdbc.HiveStatement.setFetchDirection(int)",2,1,2
"org.apache.hive.jdbc.HiveStatement.setFetchSize(int)",1,1,1
"org.apache.hive.jdbc.HiveStatement.setMaxFieldSize(int)",1,1,1
"org.apache.hive.jdbc.HiveStatement.setMaxRows(int)",2,1,2
"org.apache.hive.jdbc.HiveStatement.setPoolable(boolean)",1,1,1
"org.apache.hive.jdbc.HiveStatement.setQueryTimeout(int)",1,1,1
"org.apache.hive.jdbc.HiveStatement.unwrap(Class<T>)",1,1,1
"org.apache.hive.jdbc.HttpBasicAuthInterceptor.HttpBasicAuthInterceptor(String,String)",1,1,2
"org.apache.hive.jdbc.HttpBasicAuthInterceptor.process(HttpRequest,HttpContext)",1,1,1
"org.apache.hive.jdbc.HttpKerberosRequestInterceptor.HttpKerberosRequestInterceptor(String,String,String)",1,1,1
"org.apache.hive.jdbc.HttpKerberosRequestInterceptor.process(HttpRequest,HttpContext)",1,2,2
"org.apache.hive.jdbc.JdbcColumn.JdbcColumn(String,String,String,String,String,int)",1,1,1
"org.apache.hive.jdbc.JdbcColumn.columnClassName(int,JdbcColumnAttributes)",15,15,15
"org.apache.hive.jdbc.JdbcColumn.columnDisplaySize(int,JdbcColumnAttributes)",12,7,12
"org.apache.hive.jdbc.JdbcColumn.columnPrecision(int,JdbcColumnAttributes)",16,2,16
"org.apache.hive.jdbc.JdbcColumn.columnScale(int,JdbcColumnAttributes)",8,2,8
"org.apache.hive.jdbc.JdbcColumn.getColumnName()",1,1,1
"org.apache.hive.jdbc.JdbcColumn.getColumnTypeName(String)",19,18,19
"org.apache.hive.jdbc.JdbcColumn.getComment()",1,1,1
"org.apache.hive.jdbc.JdbcColumn.getNumPrecRadix()",8,7,8
"org.apache.hive.jdbc.JdbcColumn.getOrdinalPos()",1,1,1
"org.apache.hive.jdbc.JdbcColumn.getTableCatalog()",1,1,1
"org.apache.hive.jdbc.JdbcColumn.getTableName()",1,1,1
"org.apache.hive.jdbc.JdbcColumn.getType()",1,1,1
"org.apache.hive.jdbc.JdbcColumn.hiveTypeToSqlType(String)",18,17,18
"org.apache.hive.jdbc.JdbcColumnAttributes.JdbcColumnAttributes()",1,1,1
"org.apache.hive.jdbc.JdbcColumnAttributes.JdbcColumnAttributes(int,int)",1,1,1
"org.apache.hive.jdbc.JdbcColumnAttributes.toString()",1,1,1
"org.apache.hive.jdbc.JdbcTable.JdbcTable(String,String,String,String)",1,1,1
"org.apache.hive.jdbc.JdbcTable.getComment()",1,1,1
"org.apache.hive.jdbc.JdbcTable.getSqlTableType()",1,1,1
"org.apache.hive.jdbc.JdbcTable.getTableCatalog()",1,1,1
"org.apache.hive.jdbc.JdbcTable.getTableName()",1,1,1
"org.apache.hive.jdbc.JdbcTable.getType()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.SleepUDF.evaluate(Integer)",1,1,2
"org.apache.hive.jdbc.TestJdbcDriver2.TestJdbcDriver2()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.assertDpi(DriverPropertyInfo,String,String)",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.checkBadUrl(String)",1,2,2
"org.apache.hive.jdbc.TestJdbcDriver2.checkResultSetExpected(Statement,List<String>,String,boolean)",1,4,4
"org.apache.hive.jdbc.TestJdbcDriver2.doTestErrorCase(String,String,String,int)",1,2,2
"org.apache.hive.jdbc.TestJdbcDriver2.doTestSelectAll(String,int,int)",2,8,11
"org.apache.hive.jdbc.TestJdbcDriver2.execFetchFirst(String,String,boolean)",5,3,5
"org.apache.hive.jdbc.TestJdbcDriver2.failWithExceptionMsg(Exception)",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.getConnection(String)",1,2,2
"org.apache.hive.jdbc.TestJdbcDriver2.getTablesTest(String,String)",1,4,5
"org.apache.hive.jdbc.TestJdbcDriver2.metaDataGetTableTypeTest(Set<String>)",1,2,2
"org.apache.hive.jdbc.TestJdbcDriver2.setUp()",1,5,5
"org.apache.hive.jdbc.TestJdbcDriver2.setUpBeforeClass()",1,3,3
"org.apache.hive.jdbc.TestJdbcDriver2.tearDown()",1,1,2
"org.apache.hive.jdbc.TestJdbcDriver2.testBadURL()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testBuiltInUDFCol()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testCloseResultSet()",1,3,3
"org.apache.hive.jdbc.TestJdbcDriver2.testDataTypes()",1,3,3
"org.apache.hive.jdbc.TestJdbcDriver2.testDataTypes2()",1,2,2
"org.apache.hive.jdbc.TestJdbcDriver2.testDatabaseMetaData()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testDescribeTable()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testDriverProperties()",1,2,2
"org.apache.hive.jdbc.TestJdbcDriver2.testDuplicateColumnNameOrder()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testErrorDiag()",1,4,4
"org.apache.hive.jdbc.TestJdbcDriver2.testErrorMessages()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testExecutePreparedStatement()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testExecuteQueryException()",1,2,2
"org.apache.hive.jdbc.TestJdbcDriver2.testExplainStmt()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testExprCol()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testFetchFirstDfsCmds()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testFetchFirstError()",1,2,2
"org.apache.hive.jdbc.TestJdbcDriver2.testFetchFirstNonMR()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testFetchFirstQuery()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testFetchFirstSetCmds()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testImportedKeys()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testInvalidURL()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testMetaDataGetCatalogs()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testMetaDataGetClassicTableTypes()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testMetaDataGetColumns()",2,4,6
"org.apache.hive.jdbc.TestJdbcDriver2.testMetaDataGetColumnsMetaData()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testMetaDataGetHiveTableTypes()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testMetaDataGetSchemas()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testMetaDataGetTableTypes()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testMetaDataGetTables()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testMetaDataGetTablesClassic()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testMetaDataGetTablesHive()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testNonAsciiReturnValues()",1,2,3
"org.apache.hive.jdbc.TestJdbcDriver2.testNullResultSet()",1,2,2
"org.apache.hive.jdbc.TestJdbcDriver2.testNullType()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testOutOfBoundCols()",1,1,3
"org.apache.hive.jdbc.TestJdbcDriver2.testParentReferences()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testParseUrlHttpMode()",1,2,2
"org.apache.hive.jdbc.TestJdbcDriver2.testPostClose()",1,1,4
"org.apache.hive.jdbc.TestJdbcDriver2.testPrepareStatement()",2,3,7
"org.apache.hive.jdbc.TestJdbcDriver2.testPrimaryKeys()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testProcCols()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testProccedures()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testQueryCancel()",1,2,3
"org.apache.hive.jdbc.TestJdbcDriver2.testResultSetMetaData()",1,2,2
"org.apache.hive.jdbc.TestJdbcDriver2.testSelectAll()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testSelectAllFetchSize()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testSelectAllMaxRows()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testSelectAllPartioned()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testSetCommand()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testSetOnConnection()",1,2,2
"org.apache.hive.jdbc.TestJdbcDriver2.testShowColumns()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testShowGrant()",1,1,1
"org.apache.hive.jdbc.TestJdbcDriver2.testShowRoleGrant()",1,2,2
"org.apache.hive.jdbc.TestJdbcDriver2.testShowTables()",1,2,3
"org.apache.hive.jdbc.TestJdbcDriver2.testUnsupportedFetchTypes()",1,3,3
"org.apache.hive.jdbc.TestJdbcDriver2.verifyConfValue(Connection,String,String)",1,3,3
"org.apache.hive.jdbc.TestJdbcWithMiniHS2.afterTest()",1,2,2
"org.apache.hive.jdbc.TestJdbcWithMiniHS2.beforeTest()",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniHS2.getConnection(String,String,String)",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniHS2.setUp()",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniHS2.tearDown()",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniHS2.testConnection()",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniHS2.testConnectionSchemaAPIs()",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniHS2.testNewConnectionConfiguration()",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniHS2.testURIDatabaseName()",1,4,7
"org.apache.hive.jdbc.TestJdbcWithMiniHS2.verifyConfProperty(Statement,String,String)",1,2,2
"org.apache.hive.jdbc.TestJdbcWithMiniHS2.verifyCurrentDB(String,Connection)",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniMr.MiniMrTestSessionHook.run(HiveSessionHookContext)",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniMr.afterTest()",1,3,3
"org.apache.hive.jdbc.TestJdbcWithMiniMr.beforeTest()",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniMr.checkForNotExist(ResultSet)",1,2,2
"org.apache.hive.jdbc.TestJdbcWithMiniMr.createDb()",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniMr.setUp()",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniMr.setupKv1Tabs(String)",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniMr.tearDown()",1,2,2
"org.apache.hive.jdbc.TestJdbcWithMiniMr.testConnection()",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniMr.testKvQuery(String,String,String)",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniMr.testMrQuery()",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniMr.testNonMrQuery()",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniMr.testPermFunc()",1,1,2
"org.apache.hive.jdbc.TestJdbcWithMiniMr.testTempTable()",1,2,2
"org.apache.hive.jdbc.TestJdbcWithMiniMr.verifyProperty(String,String)",1,1,1
"org.apache.hive.jdbc.TestJdbcWithMiniMr.verifyResult(String,String,int)",1,1,1
"org.apache.hive.jdbc.TestSSL.beforeTest()",1,1,1
"org.apache.hive.jdbc.TestSSL.clearSslConfOverlay(Map<String, String>)",1,1,1
"org.apache.hive.jdbc.TestSSL.setBinaryConfOverlay(Map<String, String>)",1,1,1
"org.apache.hive.jdbc.TestSSL.setHttpConfOverlay(Map<String, String>)",1,1,1
"org.apache.hive.jdbc.TestSSL.setSslConfOverlay(Map<String, String>)",1,1,1
"org.apache.hive.jdbc.TestSSL.setUp()",1,2,2
"org.apache.hive.jdbc.TestSSL.setupTestTableWithData(String,Path,Connection)",1,1,1
"org.apache.hive.jdbc.TestSSL.tearDown()",1,4,4
"org.apache.hive.jdbc.TestSSL.testConnectionMismatch()",1,4,4
"org.apache.hive.jdbc.TestSSL.testInvalidConfig()",1,4,4
"org.apache.hive.jdbc.TestSSL.testSSLConnectionWithProperty()",1,1,1
"org.apache.hive.jdbc.TestSSL.testSSLConnectionWithURL()",1,1,1
"org.apache.hive.jdbc.TestSSL.testSSLFetch()",1,2,2
"org.apache.hive.jdbc.TestSSL.testSSLFetchHttp()",1,2,2
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.JdbcConnectionParams()",1,1,1
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.getDbName()",1,1,1
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.getHiveConfs()",1,1,1
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.getHiveVars()",1,1,1
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.getHost()",1,1,1
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.getPort()",1,1,1
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.getSessionVars()",1,1,1
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.isEmbeddedMode()",1,1,1
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.setDbName(String)",1,1,1
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.setEmbeddedMode(boolean)",1,1,1
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.setHiveConfs(Map<String, String>)",1,1,1
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.setHiveVars(Map<String, String>)",1,1,1
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.setHost(String)",1,1,1
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.setPort(int)",1,1,1
"org.apache.hive.jdbc.Utils.JdbcConnectionParams.setSessionVars(Map<String, String>)",1,1,1
"org.apache.hive.jdbc.Utils.getVersionPart(String,int)",1,2,5
"org.apache.hive.jdbc.Utils.parseURL(String)",9,17,18
"org.apache.hive.jdbc.Utils.verifySuccess(TStatus)",1,1,1
"org.apache.hive.jdbc.Utils.verifySuccess(TStatus,boolean)",2,3,4
"org.apache.hive.jdbc.Utils.verifySuccessWithInfo(TStatus)",1,1,1
"org.apache.hive.jdbc.authorization.TestCLIAuthzSessionContext.MockedHiveAuthorizerFactory.createHiveAuthorizer(HiveMetastoreClientFactory,HiveConf,HiveAuthenticationProvider,HiveAuthzSessionContext)",1,1,1
"org.apache.hive.jdbc.authorization.TestCLIAuthzSessionContext.afterTest()",1,1,1
"org.apache.hive.jdbc.authorization.TestCLIAuthzSessionContext.beforeTest()",1,1,1
"org.apache.hive.jdbc.authorization.TestCLIAuthzSessionContext.testAuthzSessionContextContents()",1,1,1
"org.apache.hive.jdbc.authorization.TestHS2AuthzContext.MockedHiveAuthorizerFactory.createHiveAuthorizer(HiveMetastoreClientFactory,HiveConf,HiveAuthenticationProvider,HiveAuthzSessionContext)",1,1,1
"org.apache.hive.jdbc.authorization.TestHS2AuthzContext.afterTest()",1,2,2
"org.apache.hive.jdbc.authorization.TestHS2AuthzContext.beforeTest()",1,1,1
"org.apache.hive.jdbc.authorization.TestHS2AuthzContext.getConnection(String)",1,1,1
"org.apache.hive.jdbc.authorization.TestHS2AuthzContext.testAuthzContextContentsCmdProcessorCmd()",1,1,1
"org.apache.hive.jdbc.authorization.TestHS2AuthzContext.testAuthzContextContentsDriverCmd()",1,1,1
"org.apache.hive.jdbc.authorization.TestHS2AuthzContext.verifyContextContents(String,String)",1,1,1
"org.apache.hive.jdbc.authorization.TestHS2AuthzSessionContext.MockedHiveAuthorizerFactory.createHiveAuthorizer(HiveMetastoreClientFactory,HiveConf,HiveAuthenticationProvider,HiveAuthzSessionContext)",1,1,1
"org.apache.hive.jdbc.authorization.TestHS2AuthzSessionContext.afterTest()",1,2,2
"org.apache.hive.jdbc.authorization.TestHS2AuthzSessionContext.beforeTest()",1,1,1
"org.apache.hive.jdbc.authorization.TestHS2AuthzSessionContext.testAuthzSessionContextContents()",1,1,1
"org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.afterTest()",1,2,2
"org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.beforeTest()",1,1,1
"org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.getConnection(String)",1,1,1
"org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.testAllowedCommands()",1,2,2
"org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.testAuthorization1()",1,2,2
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.AbstractHiveService(HiveConf,String,int,int)",1,1,1
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.clearProperties()",1,2,2
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.getBinaryPort()",1,1,1
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.getConfProperty(String)",1,1,1
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.getHiveConf()",1,1,1
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.getHost()",1,1,1
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.getHttpPort()",1,1,1
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.getWareHouseDir()",1,1,1
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.isStarted()",1,1,1
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.setBinaryPort(int)",1,1,1
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.setConfProperty(String,String)",1,1,1
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.setHost(String)",1,1,1
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.setHttpPort(int)",1,1,1
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.setStarted(boolean)",1,1,1
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.setWareHouseDir(String)",1,1,1
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.verifyNotStarted()",2,1,2
"org.apache.hive.jdbc.miniHS2.AbstractHiveService.verifyStarted()",2,1,2
"org.apache.hive.jdbc.miniHS2.MiniHS2.Builder.Builder()",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.Builder.build()",2,2,4
"org.apache.hive.jdbc.miniHS2.MiniHS2.Builder.withConf(HiveConf)",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.Builder.withHTTPTransport()",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.Builder.withMiniKdc(String,String)",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.Builder.withMiniMR()",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.Builder.withRemoteMetastore()",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.MiniHS2(HiveConf)",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.MiniHS2(HiveConf,boolean)",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.MiniHS2(HiveConf,boolean,boolean,String,String,boolean)",1,3,3
"org.apache.hive.jdbc.miniHS2.MiniHS2.getBaseJdbcURL()",2,2,2
"org.apache.hive.jdbc.miniHS2.MiniHS2.getDFS()",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.getDfs()",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.getJdbcDriverName()",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.getJdbcURL()",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.getJdbcURL(String)",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.getJdbcURL(String,String)",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.getJdbcURL(String,String,String)",1,1,6
"org.apache.hive.jdbc.miniHS2.MiniHS2.getMR()",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.getMr()",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.getServiceClient()",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.getServiceClientInternal()",4,1,4
"org.apache.hive.jdbc.miniHS2.MiniHS2.isHttpTransportMode()",1,2,2
"org.apache.hive.jdbc.miniHS2.MiniHS2.isUseMiniKdc()",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.isUseMiniMR()",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.setDfs(MiniDFSShim)",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.setMr(MiniMrShim)",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.setUseMiniMR(boolean)",1,1,1
"org.apache.hive.jdbc.miniHS2.MiniHS2.start(Map<String, String>)",1,3,3
"org.apache.hive.jdbc.miniHS2.MiniHS2.stop()",1,3,4
"org.apache.hive.jdbc.miniHS2.MiniHS2.waitForStartup()",3,3,4
"org.apache.hive.jdbc.miniHS2.TestHiveServer2.beforeTest()",1,1,1
"org.apache.hive.jdbc.miniHS2.TestHiveServer2.setUp()",1,1,1
"org.apache.hive.jdbc.miniHS2.TestHiveServer2.tearDown()",1,1,1
"org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection()",1,1,1
"org.apache.hive.jdbc.miniHS2.TestHiveServer2.testGetVariableValue()",1,1,1
"org.apache.hive.jdbc.miniHS2.TestMiniHS2.beforeTest()",1,1,1
"org.apache.hive.jdbc.miniHS2.TestMiniHS2.checkConfVal(String,String,Statement)",1,1,1
"org.apache.hive.jdbc.miniHS2.TestMiniHS2.tearDown()",1,1,1
"org.apache.hive.jdbc.miniHS2.TestMiniHS2.testConfInSession()",1,1,1
"org.apache.hive.minikdc.JdbcWithMiniKdcSQLAuthTest.afterTest()",1,1,1
"org.apache.hive.minikdc.JdbcWithMiniKdcSQLAuthTest.beforeTestBase()",1,1,1
"org.apache.hive.minikdc.JdbcWithMiniKdcSQLAuthTest.getConnection(String)",1,1,1
"org.apache.hive.minikdc.JdbcWithMiniKdcSQLAuthTest.setUp()",1,1,1
"org.apache.hive.minikdc.JdbcWithMiniKdcSQLAuthTest.tearDown()",1,2,3
"org.apache.hive.minikdc.JdbcWithMiniKdcSQLAuthTest.testAuthorization1()",1,2,2
"org.apache.hive.minikdc.MiniHiveKdc.HiveTestSimpleGroupMapping.cacheGroupsAdd(List<String>)",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.HiveTestSimpleGroupMapping.cacheGroupsRefresh()",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.HiveTestSimpleGroupMapping.getGroups(String)",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.MiniHiveKdc(Configuration)",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.addUserPrincipal(String)",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.getDefaultUserPrincipal()",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.getFullHiveServicePrincipal()",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.getFullyQualifiedServicePrincipal(String)",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.getFullyQualifiedUserPrincipal(String)",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.getHiveServicePrincipal()",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.getKdcConf()",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.getKeyTabFile(String)",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.getMiniHS2WithKerb(MiniHiveKdc,HiveConf)",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.getMiniHiveKdc(Configuration)",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.getServicePrincipalForUser(String)",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.loginUser(String)",1,1,1
"org.apache.hive.minikdc.MiniHiveKdc.shutDown()",1,1,1
"org.apache.hive.minikdc.TestHs2HooksWithMiniKdc.PostExecHook.run(HookContext)",1,2,2
"org.apache.hive.minikdc.TestHs2HooksWithMiniKdc.PreExecHook.run(HookContext)",1,2,2
"org.apache.hive.minikdc.TestHs2HooksWithMiniKdc.afterTest()",1,1,1
"org.apache.hive.minikdc.TestHs2HooksWithMiniKdc.beforeTest()",1,1,1
"org.apache.hive.minikdc.TestHs2HooksWithMiniKdc.setUp()",1,1,1
"org.apache.hive.minikdc.TestHs2HooksWithMiniKdc.tearDown()",1,2,3
"org.apache.hive.minikdc.TestHs2HooksWithMiniKdc.testIpUserName()",1,1,1
"org.apache.hive.minikdc.TestJdbcWithMiniKdc.SessionHookTest.run(HiveSessionHookContext)",1,1,1
"org.apache.hive.minikdc.TestJdbcWithMiniKdc.afterTest()",1,1,1
"org.apache.hive.minikdc.TestJdbcWithMiniKdc.beforeTest()",1,1,1
"org.apache.hive.minikdc.TestJdbcWithMiniKdc.setUp()",1,1,1
"org.apache.hive.minikdc.TestJdbcWithMiniKdc.storeToken(String,UserGroupInformation)",1,1,1
"org.apache.hive.minikdc.TestJdbcWithMiniKdc.tearDown()",1,2,3
"org.apache.hive.minikdc.TestJdbcWithMiniKdc.testConnection()",1,1,1
"org.apache.hive.minikdc.TestJdbcWithMiniKdc.testConnectionNeg()",1,2,2
"org.apache.hive.minikdc.TestJdbcWithMiniKdc.testNegativeProxyAuth()",1,2,2
"org.apache.hive.minikdc.TestJdbcWithMiniKdc.testNegativeTokenAuth()",1,1,1
"org.apache.hive.minikdc.TestJdbcWithMiniKdc.testProxyAuth()",1,1,1
"org.apache.hive.minikdc.TestJdbcWithMiniKdc.testTokenAuth()",1,2,2
"org.apache.hive.minikdc.TestJdbcWithMiniKdc.verifyProperty(String,String)",1,1,1
"org.apache.hive.minikdc.TestJdbcWithMiniKdcSQLAuthBinary.beforeTest()",1,1,1
"org.apache.hive.minikdc.TestJdbcWithMiniKdcSQLAuthHttp.beforeTest()",1,1,1
"org.apache.hive.minikdc.TestMiniHiveKdc.afterTest()",1,1,1
"org.apache.hive.minikdc.TestMiniHiveKdc.beforeTest()",1,1,1
"org.apache.hive.minikdc.TestMiniHiveKdc.setUp()",1,1,1
"org.apache.hive.minikdc.TestMiniHiveKdc.tearDown()",1,1,1
"org.apache.hive.minikdc.TestMiniHiveKdc.testLogin()",1,1,1
"org.apache.hive.ptest.api.Status.Status()",1,1,1
"org.apache.hive.ptest.api.Status.Status(Name,String)",1,1,1
"org.apache.hive.ptest.api.Status.assertOK(Status)",2,3,3
"org.apache.hive.ptest.api.Status.assertOKOrFailed(Status)",2,4,4
"org.apache.hive.ptest.api.Status.failed(String)",1,1,1
"org.apache.hive.ptest.api.Status.getMessage()",1,1,1
"org.apache.hive.ptest.api.Status.getName()",1,1,1
"org.apache.hive.ptest.api.Status.illegalArgument()",1,1,1
"org.apache.hive.ptest.api.Status.illegalArgument(String)",1,1,1
"org.apache.hive.ptest.api.Status.inProgress()",1,1,1
"org.apache.hive.ptest.api.Status.internalError(String)",1,1,1
"org.apache.hive.ptest.api.Status.isFailed(Status)",1,2,2
"org.apache.hive.ptest.api.Status.isIllegalArgument(Status)",1,2,2
"org.apache.hive.ptest.api.Status.isInProgress(Status)",1,2,2
"org.apache.hive.ptest.api.Status.isOK(Status)",1,2,2
"org.apache.hive.ptest.api.Status.isPending(Status)",1,2,2
"org.apache.hive.ptest.api.Status.ok()",1,1,1
"org.apache.hive.ptest.api.Status.pending()",1,1,1
"org.apache.hive.ptest.api.Status.queueFull()",1,1,1
"org.apache.hive.ptest.api.Status.setMessage(String)",1,1,1
"org.apache.hive.ptest.api.Status.setName(Name)",1,1,1
"org.apache.hive.ptest.api.Status.toString()",1,1,1
"org.apache.hive.ptest.api.client.PTestClient.EndPointResponsePair.EndPointResponsePair(String,Class<? extends GenericResponse>)",1,1,1
"org.apache.hive.ptest.api.client.PTestClient.EndPointResponsePair.getEndpoint()",1,1,1
"org.apache.hive.ptest.api.client.PTestClient.EndPointResponsePair.getResponseClass()",1,1,1
"org.apache.hive.ptest.api.client.PTestClient.PTestClient(String,String,String)",1,1,3
"org.apache.hive.ptest.api.client.PTestClient.PTestHttpRequestRetryHandler.retryRequest(IOException,int,HttpContext)",2,2,3
"org.apache.hive.ptest.api.client.PTestClient.assertRequired(CommandLine,String[])",3,2,3
"org.apache.hive.ptest.api.client.PTestClient.downloadTestResults(String,String)",2,3,3
"org.apache.hive.ptest.api.client.PTestClient.main(String[])",4,6,6
"org.apache.hive.ptest.api.client.PTestClient.post(Object,boolean)",2,5,5
"org.apache.hive.ptest.api.client.PTestClient.printLogs(String,long)",1,1,1
"org.apache.hive.ptest.api.client.PTestClient.testList()",1,2,2
"org.apache.hive.ptest.api.client.PTestClient.testStart(String,String,String,String,String,boolean)",3,3,4
"org.apache.hive.ptest.api.client.PTestClient.testTailLog(String)",2,5,6
"org.apache.hive.ptest.api.request.TestListRequest.TestListRequest()",1,1,1
"org.apache.hive.ptest.api.request.TestListRequest.getDummy()",1,1,1
"org.apache.hive.ptest.api.request.TestListRequest.setDummy(String)",1,1,1
"org.apache.hive.ptest.api.request.TestLogRequest.TestLogRequest()",1,1,1
"org.apache.hive.ptest.api.request.TestLogRequest.TestLogRequest(String,long,long)",1,1,1
"org.apache.hive.ptest.api.request.TestLogRequest.getLength()",1,1,1
"org.apache.hive.ptest.api.request.TestLogRequest.getOffset()",1,1,1
"org.apache.hive.ptest.api.request.TestLogRequest.getTestHandle()",1,1,1
"org.apache.hive.ptest.api.request.TestLogRequest.setLength(long)",1,1,1
"org.apache.hive.ptest.api.request.TestLogRequest.setOffset(long)",1,1,1
"org.apache.hive.ptest.api.request.TestLogRequest.setTestHandle(String)",1,1,1
"org.apache.hive.ptest.api.request.TestStartRequest.TestStartRequest()",1,1,1
"org.apache.hive.ptest.api.request.TestStartRequest.TestStartRequest(String,String,String,String,boolean)",1,1,1
"org.apache.hive.ptest.api.request.TestStartRequest.getJiraName()",1,1,1
"org.apache.hive.ptest.api.request.TestStartRequest.getPatchURL()",1,1,1
"org.apache.hive.ptest.api.request.TestStartRequest.getProfile()",1,1,1
"org.apache.hive.ptest.api.request.TestStartRequest.getTestHandle()",1,1,1
"org.apache.hive.ptest.api.request.TestStartRequest.isClearLibraryCache()",1,1,1
"org.apache.hive.ptest.api.request.TestStartRequest.setClearLibraryCache(boolean)",1,1,1
"org.apache.hive.ptest.api.request.TestStartRequest.setJiraName(String)",1,1,1
"org.apache.hive.ptest.api.request.TestStartRequest.setPatchURL(String)",1,1,1
"org.apache.hive.ptest.api.request.TestStartRequest.setProfile(String)",1,1,1
"org.apache.hive.ptest.api.request.TestStartRequest.setTestHandle(String)",1,1,1
"org.apache.hive.ptest.api.request.TestStartRequest.toString()",1,1,1
"org.apache.hive.ptest.api.request.TestStatusRequest.TestStatusRequest()",1,1,1
"org.apache.hive.ptest.api.request.TestStatusRequest.TestStatusRequest(String)",1,1,1
"org.apache.hive.ptest.api.request.TestStatusRequest.getTestHandle()",1,1,1
"org.apache.hive.ptest.api.request.TestStatusRequest.setTestHandle(String)",1,1,1
"org.apache.hive.ptest.api.request.TestStopRequest.TestStopRequest()",1,1,1
"org.apache.hive.ptest.api.request.TestStopRequest.TestStopRequest(String)",1,1,1
"org.apache.hive.ptest.api.request.TestStopRequest.getTestHandle()",1,1,1
"org.apache.hive.ptest.api.request.TestStopRequest.setTestHandle(String)",1,1,1
"org.apache.hive.ptest.api.response.TestListResponse.TestListResponse()",1,1,1
"org.apache.hive.ptest.api.response.TestListResponse.TestListResponse(Status)",1,1,1
"org.apache.hive.ptest.api.response.TestListResponse.TestListResponse(Status,List<TestStatus>)",1,1,1
"org.apache.hive.ptest.api.response.TestListResponse.getEntries()",1,1,1
"org.apache.hive.ptest.api.response.TestListResponse.getStatus()",1,1,1
"org.apache.hive.ptest.api.response.TestListResponse.setEntries(List<TestStatus>)",1,1,1
"org.apache.hive.ptest.api.response.TestListResponse.setStatus(Status)",1,1,1
"org.apache.hive.ptest.api.response.TestLogResponse.TestLogResponse()",1,1,1
"org.apache.hive.ptest.api.response.TestLogResponse.TestLogResponse(Status)",1,1,1
"org.apache.hive.ptest.api.response.TestLogResponse.TestLogResponse(Status,long,String)",1,1,1
"org.apache.hive.ptest.api.response.TestLogResponse.getBody()",1,1,1
"org.apache.hive.ptest.api.response.TestLogResponse.getOffset()",1,1,1
"org.apache.hive.ptest.api.response.TestLogResponse.getStatus()",1,1,1
"org.apache.hive.ptest.api.response.TestLogResponse.setBody(String)",1,1,1
"org.apache.hive.ptest.api.response.TestLogResponse.setOffset(long)",1,1,1
"org.apache.hive.ptest.api.response.TestLogResponse.setStatus(Status)",1,1,1
"org.apache.hive.ptest.api.response.TestStartResponse.TestStartResponse()",1,1,1
"org.apache.hive.ptest.api.response.TestStartResponse.TestStartResponse(Status)",1,1,1
"org.apache.hive.ptest.api.response.TestStartResponse.getStatus()",1,1,1
"org.apache.hive.ptest.api.response.TestStartResponse.setStatus(Status)",1,1,1
"org.apache.hive.ptest.api.response.TestStartResponse.toString()",1,1,1
"org.apache.hive.ptest.api.response.TestStatus.TestStatus()",1,1,1
"org.apache.hive.ptest.api.response.TestStatus.TestStatus(String,Status,long,long,long)",1,1,1
"org.apache.hive.ptest.api.response.TestStatus.getElapsedExecutionTime()",1,1,1
"org.apache.hive.ptest.api.response.TestStatus.getElapsedQueueTime()",1,1,1
"org.apache.hive.ptest.api.response.TestStatus.getLogFileLength()",1,1,1
"org.apache.hive.ptest.api.response.TestStatus.getStatus()",1,1,1
"org.apache.hive.ptest.api.response.TestStatus.getTestHandle()",1,1,1
"org.apache.hive.ptest.api.response.TestStatus.setElapsedExecutionTime(long)",1,1,1
"org.apache.hive.ptest.api.response.TestStatus.setElapsedQueueTime(long)",1,1,1
"org.apache.hive.ptest.api.response.TestStatus.setLogFileLength(long)",1,1,1
"org.apache.hive.ptest.api.response.TestStatus.setStatus(Status)",1,1,1
"org.apache.hive.ptest.api.response.TestStatus.setTestHandle(String)",1,1,1
"org.apache.hive.ptest.api.response.TestStatus.toMinutes(long)",1,1,1
"org.apache.hive.ptest.api.response.TestStatus.toString()",1,1,1
"org.apache.hive.ptest.api.response.TestStatusResponse.TestStatusResponse()",1,1,1
"org.apache.hive.ptest.api.response.TestStatusResponse.TestStatusResponse(Status)",1,1,1
"org.apache.hive.ptest.api.response.TestStatusResponse.TestStatusResponse(Status,TestStatus)",1,1,1
"org.apache.hive.ptest.api.response.TestStatusResponse.getStatus()",1,1,1
"org.apache.hive.ptest.api.response.TestStatusResponse.getTestStatus()",1,1,1
"org.apache.hive.ptest.api.response.TestStatusResponse.setStatus(Status)",1,1,1
"org.apache.hive.ptest.api.response.TestStatusResponse.setTestStatus(TestStatus)",1,1,1
"org.apache.hive.ptest.api.response.TestStopResponse.TestStopResponse()",1,1,1
"org.apache.hive.ptest.api.response.TestStopResponse.TestStopResponse(Status)",1,1,1
"org.apache.hive.ptest.api.response.TestStopResponse.getStatus()",1,1,1
"org.apache.hive.ptest.api.response.TestStopResponse.setStatus(Status)",1,1,1
"org.apache.hive.ptest.api.server.ExecutionController.ExecutionController()",1,5,5
"org.apache.hive.ptest.api.server.ExecutionController.assertTestHandleIsAvailable(String)",1,1,1
"org.apache.hive.ptest.api.server.ExecutionController.doStartTest(TestStartRequest,BindingResult)",4,7,7
"org.apache.hive.ptest.api.server.ExecutionController.testList(TestListRequest)",1,2,2
"org.apache.hive.ptest.api.server.ExecutionController.testLog(TestLogRequest,BindingResult)",3,11,11
"org.apache.hive.ptest.api.server.ExecutionController.testStart(TestStartRequest,BindingResult)",1,1,1
"org.apache.hive.ptest.api.server.ExecutionController.testStatus(TestStopRequest,BindingResult)",2,4,4
"org.apache.hive.ptest.api.server.ExecutionController.testStop(TestStopRequest,BindingResult)",2,4,4
"org.apache.hive.ptest.api.server.Test.Test(TestStartRequest,Status,long)",1,1,1
"org.apache.hive.ptest.api.server.Test.getDequeueTime()",1,1,1
"org.apache.hive.ptest.api.server.Test.getEnqueueTime()",1,1,1
"org.apache.hive.ptest.api.server.Test.getExecutionFinishTime()",1,1,1
"org.apache.hive.ptest.api.server.Test.getExecutionStartTime()",1,1,1
"org.apache.hive.ptest.api.server.Test.getOutputFile()",1,1,1
"org.apache.hive.ptest.api.server.Test.getStartRequest()",1,1,1
"org.apache.hive.ptest.api.server.Test.getStatus()",1,1,1
"org.apache.hive.ptest.api.server.Test.isStopRequested()",1,1,1
"org.apache.hive.ptest.api.server.Test.setDequeueTime(long)",1,1,1
"org.apache.hive.ptest.api.server.Test.setEnqueueTime(long)",1,1,1
"org.apache.hive.ptest.api.server.Test.setExecutionFinishTime(long)",1,1,1
"org.apache.hive.ptest.api.server.Test.setExecutionStartTime(long)",1,1,1
"org.apache.hive.ptest.api.server.Test.setOutputFile(File)",1,1,1
"org.apache.hive.ptest.api.server.Test.setStartRequest(TestStartRequest)",1,1,1
"org.apache.hive.ptest.api.server.Test.setStatus(Status)",1,1,1
"org.apache.hive.ptest.api.server.Test.setStopRequested(boolean)",1,1,1
"org.apache.hive.ptest.api.server.Test.toTestStatus()",1,5,5
"org.apache.hive.ptest.api.server.TestExecutor.TestExecutor(ExecutionContextConfiguration,ExecutionContextProvider,BlockingQueue<Test>,Builder)",1,1,1
"org.apache.hive.ptest.api.server.TestExecutor.createExceutionContext()",1,1,1
"org.apache.hive.ptest.api.server.TestExecutor.run()",3,12,12
"org.apache.hive.ptest.api.server.TestExecutor.shutdown()",1,1,1
"org.apache.hive.ptest.api.server.TestExecutor.terminateExecutionContext()",1,2,2
"org.apache.hive.ptest.api.server.TestLogger.LEVEL.LEVEL(int)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.TestLogger(PrintStream,LEVEL)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.debug(String)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.debug(String,Object)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.debug(String,Object,Object)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.debug(String,Object[])",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.debug(String,Throwable)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.error(String)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.error(String,Object)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.error(String,Object,Object)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.error(String,Object[])",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.error(String,Throwable)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.getCaller()",2,2,2
"org.apache.hive.ptest.api.server.TestLogger.getCallerShortName(StackTraceElement)",1,2,2
"org.apache.hive.ptest.api.server.TestLogger.info(String)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.info(String,Object)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.info(String,Object,Object)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.info(String,Object[])",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.info(String,Throwable)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.isDebugEnabled()",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.isErrorEnabled()",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.isInfoEnabled()",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.isTraceEnabled()",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.isWarnEnabled()",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.log(LEVEL,String,Throwable)",1,3,3
"org.apache.hive.ptest.api.server.TestLogger.trace(String)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.trace(String,Object)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.trace(String,Object,Object)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.trace(String,Object[])",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.trace(String,Throwable)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.warn(String)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.warn(String,Object)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.warn(String,Object,Object)",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.warn(String,Object[])",1,1,1
"org.apache.hive.ptest.api.server.TestLogger.warn(String,Throwable)",1,1,1
"org.apache.hive.ptest.api.server.TestTestExecutor.setup()",1,1,1
"org.apache.hive.ptest.api.server.TestTestExecutor.teardown()",1,2,2
"org.apache.hive.ptest.api.server.TestTestExecutor.testNoProfileProperties()",1,1,1
"org.apache.hive.ptest.api.server.TestTestExecutor.testSuccess()",1,1,1
"org.apache.hive.ptest.api.server.TestTestExecutor.testTestFailure()",1,1,1
"org.apache.hive.ptest.api.server.TestTestLogger.basicTest()",1,1,1
"org.apache.hive.ptest.execution.AbortDroneException.AbortDroneException(String)",1,1,1
"org.apache.hive.ptest.execution.AbortDroneException.AbortDroneException(String,Throwable)",1,1,1
"org.apache.hive.ptest.execution.AbstractTestPhase.createBaseDir(String)",1,2,2
"org.apache.hive.ptest.execution.AbstractTestPhase.createHostExecutor()",1,1,1
"org.apache.hive.ptest.execution.AbstractTestPhase.getExecutedCommands()",1,1,1
"org.apache.hive.ptest.execution.AbstractTestPhase.initialize(String)",1,1,1
"org.apache.hive.ptest.execution.AbstractTestPhase.isOSX()",1,1,1
"org.apache.hive.ptest.execution.AbstractTestPhase.returnNotNull(V)",1,1,1
"org.apache.hive.ptest.execution.Dirs.create(File)",3,1,3
"org.apache.hive.ptest.execution.Dirs.createEmpty(File)",1,1,1
"org.apache.hive.ptest.execution.Drone.Drone(String,String,String,int,String)",1,1,1
"org.apache.hive.ptest.execution.Drone.getHost()",1,1,1
"org.apache.hive.ptest.execution.Drone.getInstance()",1,1,1
"org.apache.hive.ptest.execution.Drone.getInstanceName()",1,1,1
"org.apache.hive.ptest.execution.Drone.getLocalDirectory()",1,1,1
"org.apache.hive.ptest.execution.Drone.getLocalLogDirectory()",1,1,1
"org.apache.hive.ptest.execution.Drone.getPrivateKey()",1,1,1
"org.apache.hive.ptest.execution.Drone.getUser()",1,1,1
"org.apache.hive.ptest.execution.Drone.toString()",1,1,1
"org.apache.hive.ptest.execution.ExecutionPhase.ExecutionPhase(List<HostExecutor>,ExecutionContext,HostExecutorBuilder,LocalCommandFactory,ImmutableMap<String, String>,File,File,Supplier<List<TestBatch>>,Set<String>,Set<String>,Logger)",1,1,1
"org.apache.hive.ptest.execution.ExecutionPhase.execute()",1,8,8
"org.apache.hive.ptest.execution.ExecutionPhase.replaceBadHosts(int)",3,8,8
"org.apache.hive.ptest.execution.ExtendedAssert.ExtendedAssert()",1,1,1
"org.apache.hive.ptest.execution.ExtendedAssert.assertListEquals(String,List<?>,List<?>)",1,3,4
"org.apache.hive.ptest.execution.HostExecutor.HostExecutor(Host,String,ListeningExecutorService,SSHCommandExecutor,RSyncCommandExecutor,ImmutableMap<String, String>,File,File,File,long,Logger)",1,2,2
"org.apache.hive.ptest.execution.HostExecutor.copyFromDroneToLocal(Drone,String,String)",2,2,3
"org.apache.hive.ptest.execution.HostExecutor.copyToDroneFromLocal(Drone,String,String)",3,3,4
"org.apache.hive.ptest.execution.HostExecutor.exec(String)",3,3,3
"org.apache.hive.ptest.execution.HostExecutor.execInstances(List<Drone>,String)",2,3,3
"org.apache.hive.ptest.execution.HostExecutor.execInstances(String)",1,1,1
"org.apache.hive.ptest.execution.HostExecutor.executeTestBatch(Drone,TestBatch,Set<TestBatch>)",3,6,6
"org.apache.hive.ptest.execution.HostExecutor.executeTests(BlockingQueue<TestBatch>,BlockingQueue<TestBatch>,Set<TestBatch>)",5,18,18
"org.apache.hive.ptest.execution.HostExecutor.getHost()",1,1,1
"org.apache.hive.ptest.execution.HostExecutor.isBad()",1,1,1
"org.apache.hive.ptest.execution.HostExecutor.isShutdown()",1,1,1
"org.apache.hive.ptest.execution.HostExecutor.remainingDrones()",1,1,1
"org.apache.hive.ptest.execution.HostExecutor.rsyncFromLocalToRemoteInstances(String,String)",3,4,4
"org.apache.hive.ptest.execution.HostExecutor.shutdownNow()",1,1,1
"org.apache.hive.ptest.execution.HostExecutor.submitTests(BlockingQueue<TestBatch>,BlockingQueue<TestBatch>,Set<TestBatch>)",1,1,1
"org.apache.hive.ptest.execution.JIRAService.Body.Body()",1,1,1
"org.apache.hive.ptest.execution.JIRAService.Body.Body(String)",1,1,1
"org.apache.hive.ptest.execution.JIRAService.Body.getBody()",1,1,1
"org.apache.hive.ptest.execution.JIRAService.Body.setBody(String)",1,1,1
"org.apache.hive.ptest.execution.JIRAService.JIRAService(Logger,TestConfiguration,String)",1,1,1
"org.apache.hive.ptest.execution.JIRAService.PreemptiveAuth.process(HttpRequest,HttpContext)",4,3,4
"org.apache.hive.ptest.execution.JIRAService.formatBuildTag(String)",2,2,2
"org.apache.hive.ptest.execution.JIRAService.formatBuildTagForLogs(String)",2,1,2
"org.apache.hive.ptest.execution.JIRAService.formatError(String)",1,1,1
"org.apache.hive.ptest.execution.JIRAService.formatSuccess(String)",1,1,1
"org.apache.hive.ptest.execution.JIRAService.main(String[])",1,1,1
"org.apache.hive.ptest.execution.JIRAService.parseAttachementId(String)",4,1,4
"org.apache.hive.ptest.execution.JIRAService.postComment(boolean,int,SortedSet<String>,List<String>)",2,11,11
"org.apache.hive.ptest.execution.JIRAService.trimMessages(List<String>)",1,2,2
"org.apache.hive.ptest.execution.JUnitReportParser.JUnitReportParser(Logger,File)",1,1,1
"org.apache.hive.ptest.execution.JUnitReportParser.getExecutedTests()",1,2,2
"org.apache.hive.ptest.execution.JUnitReportParser.getFailedTests()",1,2,2
"org.apache.hive.ptest.execution.JUnitReportParser.getFiles(File)",1,6,6
"org.apache.hive.ptest.execution.JUnitReportParser.parse()",1,14,15
"org.apache.hive.ptest.execution.LocalCommand.CollectLogPolicy.CollectLogPolicy(Logger)",1,1,1
"org.apache.hive.ptest.execution.LocalCommand.CollectLogPolicy.handleOutput(String)",1,1,1
"org.apache.hive.ptest.execution.LocalCommand.CollectPolicy.getOutput()",2,1,2
"org.apache.hive.ptest.execution.LocalCommand.CollectPolicy.handleOutput(String)",1,1,1
"org.apache.hive.ptest.execution.LocalCommand.CollectPolicy.handleThrowable(Throwable)",2,2,3
"org.apache.hive.ptest.execution.LocalCommand.LocalCommand(Logger,OutputPolicy,String)",1,1,1
"org.apache.hive.ptest.execution.LocalCommand.StreamReader.StreamReader(OutputPolicy,InputStream)",1,1,1
"org.apache.hive.ptest.execution.LocalCommand.StreamReader.run()",1,3,4
"org.apache.hive.ptest.execution.LocalCommand.getExitCode()",1,2,2
"org.apache.hive.ptest.execution.LocalCommand.kill()",1,1,1
"org.apache.hive.ptest.execution.LocalCommandFactory.LocalCommandFactory(Logger)",1,1,1
"org.apache.hive.ptest.execution.LocalCommandFactory.create(CollectPolicy,String)",1,1,1
"org.apache.hive.ptest.execution.LogDirectoryCleaner.LogDirectoryCleaner(File,int)",1,1,1
"org.apache.hive.ptest.execution.LogDirectoryCleaner.ProfileLogs.ProfileLogs(String)",1,1,1
"org.apache.hive.ptest.execution.LogDirectoryCleaner.ProfileLogs.getOldest()",1,3,4
"org.apache.hive.ptest.execution.LogDirectoryCleaner.run()",1,8,9
"org.apache.hive.ptest.execution.MockLocalCommandFactory.MockLocalCommandFactory(Logger)",1,1,1
"org.apache.hive.ptest.execution.MockLocalCommandFactory.create(CollectPolicy,String)",1,1,1
"org.apache.hive.ptest.execution.MockLocalCommandFactory.getCommands()",1,1,1
"org.apache.hive.ptest.execution.MockLocalCommandFactory.setInstance(LocalCommand)",1,1,1
"org.apache.hive.ptest.execution.MockRSyncCommandExecutor.MockRSyncCommandExecutor(Logger)",1,1,1
"org.apache.hive.ptest.execution.MockRSyncCommandExecutor.execute(RSyncCommand)",1,3,3
"org.apache.hive.ptest.execution.MockRSyncCommandExecutor.getCommands()",1,1,1
"org.apache.hive.ptest.execution.MockRSyncCommandExecutor.putFailure(String,Integer...)",1,3,3
"org.apache.hive.ptest.execution.MockSSHCommandExecutor.MockSSHCommandExecutor(Logger)",1,1,1
"org.apache.hive.ptest.execution.MockSSHCommandExecutor.execute(SSHCommand)",1,3,3
"org.apache.hive.ptest.execution.MockSSHCommandExecutor.getCommands()",1,1,1
"org.apache.hive.ptest.execution.MockSSHCommandExecutor.putFailure(String,Integer...)",1,3,3
"org.apache.hive.ptest.execution.PTest.Builder.build(TestConfiguration,ExecutionContext,String,File,LocalCommandFactory,SSHCommandExecutor,RSyncCommandExecutor,Logger)",1,1,1
"org.apache.hive.ptest.execution.PTest.PTest(TestConfiguration,ExecutionContext,String,File,LocalCommandFactory,SSHCommandExecutor,RSyncCommandExecutor,Logger)",1,3,3
"org.apache.hive.ptest.execution.PTest.main(String[])",2,16,18
"org.apache.hive.ptest.execution.PTest.publishJiraComment(boolean,List<String>,SortedSet<String>)",5,5,5
"org.apache.hive.ptest.execution.PTest.run()",2,12,13
"org.apache.hive.ptest.execution.Phase.Phase(List<HostExecutor>,LocalCommandFactory,ImmutableMap<String, String>,Logger)",1,1,1
"org.apache.hive.ptest.execution.Phase.execHosts(String)",1,2,2
"org.apache.hive.ptest.execution.Phase.execInstances(String)",1,2,2
"org.apache.hive.ptest.execution.Phase.execLocally(String)",2,2,2
"org.apache.hive.ptest.execution.Phase.flatten(List<ListenableFuture<List<ListenableFuture<T>>>>)",1,3,3
"org.apache.hive.ptest.execution.Phase.getTemplateDefaults()",1,1,1
"org.apache.hive.ptest.execution.Phase.initalizeHost(HostExecutor)",1,1,1
"org.apache.hive.ptest.execution.Phase.initalizeHosts()",1,5,5
"org.apache.hive.ptest.execution.Phase.rsyncFromLocalToRemoteInstances(String,String)",1,2,2
"org.apache.hive.ptest.execution.Phase.toListOfResults(List<ListenableFuture<T>>)",4,4,5
"org.apache.hive.ptest.execution.PrepPhase.PrepPhase(List<HostExecutor>,LocalCommandFactory,ImmutableMap<String, String>,File,File,Logger)",1,1,1
"org.apache.hive.ptest.execution.PrepPhase.execute()",3,2,3
"org.apache.hive.ptest.execution.ReportingPhase.ReportingPhase(List<HostExecutor>,LocalCommandFactory,ImmutableMap<String, String>,Logger)",1,1,1
"org.apache.hive.ptest.execution.ReportingPhase.execute()",1,1,1
"org.apache.hive.ptest.execution.Templates.Templates()",1,1,1
"org.apache.hive.ptest.execution.Templates.getTemplateResult(String,Map<String, String>)",2,2,3
"org.apache.hive.ptest.execution.Templates.readResource(String)",1,1,1
"org.apache.hive.ptest.execution.Templates.writeTemplateResult(String,File,Map<String, String>)",2,1,2
"org.apache.hive.ptest.execution.TestExecutionPhase.copyTestOutput(String,File,String)",1,1,1
"org.apache.hive.ptest.execution.TestExecutionPhase.getPhase()",1,1,1
"org.apache.hive.ptest.execution.TestExecutionPhase.setup()",1,1,1
"org.apache.hive.ptest.execution.TestExecutionPhase.setupQFile(boolean)",1,1,1
"org.apache.hive.ptest.execution.TestExecutionPhase.setupUnitTest()",1,1,1
"org.apache.hive.ptest.execution.TestExecutionPhase.teardown()",1,1,1
"org.apache.hive.ptest.execution.TestExecutionPhase.testFailingQFile()",1,1,1
"org.apache.hive.ptest.execution.TestExecutionPhase.testFailingUnitTest()",1,1,1
"org.apache.hive.ptest.execution.TestExecutionPhase.testPassingQFileTest()",1,1,1
"org.apache.hive.ptest.execution.TestExecutionPhase.testPassingUnitTest()",1,1,1
"org.apache.hive.ptest.execution.TestHostExecutor.createHostExecutor()",1,1,1
"org.apache.hive.ptest.execution.TestHostExecutor.getExecutedCommands()",1,1,1
"org.apache.hive.ptest.execution.TestHostExecutor.setup()",1,1,1
"org.apache.hive.ptest.execution.TestHostExecutor.teardown()",1,2,2
"org.apache.hive.ptest.execution.TestHostExecutor.testBasic()",1,1,1
"org.apache.hive.ptest.execution.TestHostExecutor.testIsolatedFailsOnExec()",1,1,1
"org.apache.hive.ptest.execution.TestHostExecutor.testIsolatedFailsOnRsyncOne()",1,1,1
"org.apache.hive.ptest.execution.TestHostExecutor.testIsolatedFailsOnRsyncUnknown()",1,1,1
"org.apache.hive.ptest.execution.TestHostExecutor.testParallelFailsOnExec()",1,1,1
"org.apache.hive.ptest.execution.TestHostExecutor.testParallelFailsOnRsync()",1,1,1
"org.apache.hive.ptest.execution.TestHostExecutor.testShutdownBeforeExec()",1,1,1
"org.apache.hive.ptest.execution.TestJIRAService.testFormatBuildTagNoDashNone()",1,1,1
"org.apache.hive.ptest.execution.TestJIRAService.testFormatBuildTagNoDashSlash()",1,1,1
"org.apache.hive.ptest.execution.TestJIRAService.testFormatBuildTagNoDashSpace()",1,1,1
"org.apache.hive.ptest.execution.TestJIRAService.testFormatBuildTagPositive()",1,1,1
"org.apache.hive.ptest.execution.TestJIRAService.testTrimMesssagesBoundry()",1,2,2
"org.apache.hive.ptest.execution.TestJIRAService.testTrimMesssagesNotTrimmed()",1,1,1
"org.apache.hive.ptest.execution.TestJIRAService.testTrimMesssagesTrimmed()",1,2,2
"org.apache.hive.ptest.execution.TestLocalCommand.testFailure()",1,1,1
"org.apache.hive.ptest.execution.TestLocalCommand.testSuccess()",1,1,1
"org.apache.hive.ptest.execution.TestLogDirectoryCleaner.create(String...)",1,2,2
"org.apache.hive.ptest.execution.TestLogDirectoryCleaner.testClean()",1,1,1
"org.apache.hive.ptest.execution.TestPhase.setup()",1,1,1
"org.apache.hive.ptest.execution.TestPhase.testExecHostsWithFailure()",1,1,1
"org.apache.hive.ptest.execution.TestPhase.testExecInstancesWithFailure()",1,1,1
"org.apache.hive.ptest.execution.TestPhase.testExecLocallyFails()",1,1,1
"org.apache.hive.ptest.execution.TestPhase.testExecLocallySucceeds()",1,1,1
"org.apache.hive.ptest.execution.TestPhase.testRsyncFromLocalToRemoteInstancesWithFailureOne()",1,1,1
"org.apache.hive.ptest.execution.TestPhase.testRsyncFromLocalToRemoteInstancesWithFailureUnknown()",1,1,1
"org.apache.hive.ptest.execution.TestPrepPhase.setup()",1,1,1
"org.apache.hive.ptest.execution.TestPrepPhase.teardown()",1,1,1
"org.apache.hive.ptest.execution.TestPrepPhase.testExecute()",1,1,1
"org.apache.hive.ptest.execution.TestReportParser.setup()",1,1,1
"org.apache.hive.ptest.execution.TestReportParser.teardown()",1,2,2
"org.apache.hive.ptest.execution.TestReportParser.test()",1,4,4
"org.apache.hive.ptest.execution.TestReportingPhase.setup()",1,1,1
"org.apache.hive.ptest.execution.TestReportingPhase.testExecute()",1,1,1
"org.apache.hive.ptest.execution.TestScripts.getTemplateResult(String,Map<String, String>)",2,2,3
"org.apache.hive.ptest.execution.TestScripts.readResource(String)",1,1,1
"org.apache.hive.ptest.execution.TestScripts.setup()",1,1,1
"org.apache.hive.ptest.execution.TestScripts.teardown()",1,2,2
"org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM()",1,1,1
"org.apache.hive.ptest.execution.TestScripts.testBatch()",1,1,1
"org.apache.hive.ptest.execution.TestScripts.testPrepGit()",1,1,1
"org.apache.hive.ptest.execution.TestScripts.testPrepNone()",1,1,1
"org.apache.hive.ptest.execution.TestScripts.testPrepSvn()",1,1,1
"org.apache.hive.ptest.execution.TestsFailedException.TestsFailedException(String)",1,1,1
"org.apache.hive.ptest.execution.conf.Context.Context()",1,1,1
"org.apache.hive.ptest.execution.conf.Context.Context(Map<String, String>)",1,1,1
"org.apache.hive.ptest.execution.conf.Context.clear()",1,1,1
"org.apache.hive.ptest.execution.conf.Context.get(String)",1,1,1
"org.apache.hive.ptest.execution.conf.Context.get(String,String)",2,1,2
"org.apache.hive.ptest.execution.conf.Context.getBoolean(String)",1,1,1
"org.apache.hive.ptest.execution.conf.Context.getBoolean(String,Boolean)",2,2,2
"org.apache.hive.ptest.execution.conf.Context.getFloat(String)",1,1,1
"org.apache.hive.ptest.execution.conf.Context.getFloat(String,Float)",2,2,2
"org.apache.hive.ptest.execution.conf.Context.getInteger(String)",1,1,1
"org.apache.hive.ptest.execution.conf.Context.getInteger(String,Integer)",2,2,2
"org.apache.hive.ptest.execution.conf.Context.getLong(String)",1,1,1
"org.apache.hive.ptest.execution.conf.Context.getLong(String,Long)",2,2,2
"org.apache.hive.ptest.execution.conf.Context.getParameters()",1,1,1
"org.apache.hive.ptest.execution.conf.Context.getString(String)",1,1,1
"org.apache.hive.ptest.execution.conf.Context.getString(String,String)",1,1,1
"org.apache.hive.ptest.execution.conf.Context.getSubProperties(String)",1,3,3
"org.apache.hive.ptest.execution.conf.Context.put(String,String)",1,1,1
"org.apache.hive.ptest.execution.conf.Context.putAll(Map<String, String>)",1,1,1
"org.apache.hive.ptest.execution.conf.Context.toString()",1,1,1
"org.apache.hive.ptest.execution.conf.ExecutionContextConfiguration.ExecutionContextConfiguration(Context)",2,3,3
"org.apache.hive.ptest.execution.conf.ExecutionContextConfiguration.fromFile(String)",1,1,1
"org.apache.hive.ptest.execution.conf.ExecutionContextConfiguration.fromInputStream(InputStream)",1,1,1
"org.apache.hive.ptest.execution.conf.ExecutionContextConfiguration.getExecutionContextProvider()",1,1,1
"org.apache.hive.ptest.execution.conf.ExecutionContextConfiguration.getGlobalLogDirectory()",1,1,1
"org.apache.hive.ptest.execution.conf.ExecutionContextConfiguration.getMaxLogDirectoriesPerProfile()",1,1,1
"org.apache.hive.ptest.execution.conf.ExecutionContextConfiguration.getMaxRsyncThreads()",1,1,1
"org.apache.hive.ptest.execution.conf.ExecutionContextConfiguration.getProfileDirectory()",1,1,1
"org.apache.hive.ptest.execution.conf.ExecutionContextConfiguration.getWorkingDirectory()",1,1,1
"org.apache.hive.ptest.execution.conf.Host.Host(String,String,String[],Integer)",1,1,1
"org.apache.hive.ptest.execution.conf.Host.equals(Object)",12,3,12
"org.apache.hive.ptest.execution.conf.Host.getLocalDirectories()",1,1,1
"org.apache.hive.ptest.execution.conf.Host.getName()",1,1,1
"org.apache.hive.ptest.execution.conf.Host.getThreads()",1,1,1
"org.apache.hive.ptest.execution.conf.Host.getUser()",1,1,1
"org.apache.hive.ptest.execution.conf.Host.hashCode()",1,3,3
"org.apache.hive.ptest.execution.conf.Host.toString()",1,1,1
"org.apache.hive.ptest.execution.conf.QFileTestBatch.QFileTestBatch(String,String,String,Set<String>,boolean)",1,2,2
"org.apache.hive.ptest.execution.conf.QFileTestBatch.equals(Object)",17,5,17
"org.apache.hive.ptest.execution.conf.QFileTestBatch.getDriver()",1,1,1
"org.apache.hive.ptest.execution.conf.QFileTestBatch.getName()",1,1,1
"org.apache.hive.ptest.execution.conf.QFileTestBatch.getTestArguments()",1,1,1
"org.apache.hive.ptest.execution.conf.QFileTestBatch.getTestClass()",1,1,1
"org.apache.hive.ptest.execution.conf.QFileTestBatch.hashCode()",1,5,6
"org.apache.hive.ptest.execution.conf.QFileTestBatch.isParallel()",1,1,1
"org.apache.hive.ptest.execution.conf.QFileTestBatch.toString()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.TestConfiguration(Context,Logger)",4,4,5
"org.apache.hive.ptest.execution.conf.TestConfiguration.fromFile(File,Logger)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.fromFile(String,Logger)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.fromInputStream(InputStream,Logger)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getAntArgs()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getAntEnvOpts()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getAntTestArgs()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getAntTestTarget()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getBranch()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getBuildTool()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getContext()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getJavaHome()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getJavaHomeForTests()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getJenkinsURL()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getJiraName()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getJiraPassword()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getJiraUrl()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getJiraUser()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getLogsURL()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getMavenArgs()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getMavenBuildArgs()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getMavenEnvOpts()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getMavenTestArgs()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getPatch()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getRepository()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getRepositoryName()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getRepositoryType()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.getTestCasePropertyName()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.isClearLibraryCache()",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setAntArgs(String)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setAntEnvOpts(String)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setAntTestArgs(String)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setAntTestTarget(String)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setBranch(String)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setClearLibraryCache(boolean)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setJavaHome(String)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setJavaHomeForTests(String)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setJiraName(String)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setMavenArgs(String)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setMavenEnvOpts(String)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setMavenTestArgs(String)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setPatch(String)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setRepository(String)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.setRepositoryName(String)",1,1,1
"org.apache.hive.ptest.execution.conf.TestConfiguration.toString()",1,1,1
"org.apache.hive.ptest.execution.conf.TestParser.TestParser(Context,String,File,Logger)",1,1,1
"org.apache.hive.ptest.execution.conf.TestParser.createQFileTestBatches(String,String,File,int,boolean,Set<String>,Set<String>,Set<String>)",1,11,11
"org.apache.hive.ptest.execution.conf.TestParser.expandTestProperties(Set<String>,Map<String, Properties>)",5,7,7
"org.apache.hive.ptest.execution.conf.TestParser.main(String[])",2,2,3
"org.apache.hive.ptest.execution.conf.TestParser.parse()",1,1,1
"org.apache.hive.ptest.execution.conf.TestParser.parseQFileTests()",1,5,5
"org.apache.hive.ptest.execution.conf.TestParser.parseQTestProperties()",2,4,5
"org.apache.hive.ptest.execution.conf.TestParser.parseTests()",2,14,14
"org.apache.hive.ptest.execution.conf.TestQFileTestBatch.setup()",1,1,1
"org.apache.hive.ptest.execution.conf.TestQFileTestBatch.testMoreThanThreeTests()",1,1,1
"org.apache.hive.ptest.execution.conf.TestQFileTestBatch.testNotParallel()",1,1,1
"org.apache.hive.ptest.execution.conf.TestQFileTestBatch.testParallel()",1,1,1
"org.apache.hive.ptest.execution.conf.TestTestConfiguration.testContext()",1,1,1
"org.apache.hive.ptest.execution.conf.TestTestConfiguration.testGettersSetters()",1,1,1
"org.apache.hive.ptest.execution.conf.TestTestParser.serialize(String,Properties)",1,1,1
"org.apache.hive.ptest.execution.conf.TestTestParser.setup()",1,1,1
"org.apache.hive.ptest.execution.conf.TestTestParser.teardown()",1,1,1
"org.apache.hive.ptest.execution.conf.TestTestParser.testParsePropertyFile()",1,1,1
"org.apache.hive.ptest.execution.conf.TestTestParser.testParseWithExcludes()",1,1,1
"org.apache.hive.ptest.execution.conf.TestTestParser.testParseWithIncludes()",1,1,1
"org.apache.hive.ptest.execution.conf.UnitTestBatch.UnitTestBatch(String,String,boolean)",1,1,1
"org.apache.hive.ptest.execution.conf.UnitTestBatch.equals(Object)",8,2,8
"org.apache.hive.ptest.execution.conf.UnitTestBatch.getName()",1,1,1
"org.apache.hive.ptest.execution.conf.UnitTestBatch.getTestArguments()",1,1,1
"org.apache.hive.ptest.execution.conf.UnitTestBatch.getTestClass()",1,1,1
"org.apache.hive.ptest.execution.conf.UnitTestBatch.hashCode()",1,2,3
"org.apache.hive.ptest.execution.conf.UnitTestBatch.isParallel()",1,1,1
"org.apache.hive.ptest.execution.conf.UnitTestBatch.toString()",1,1,1
"org.apache.hive.ptest.execution.context.CloudComputeService.CloudComputeService(String,String,String,String,String,String,String,Float)",1,1,1
"org.apache.hive.ptest.execution.context.CloudComputeService.close()",1,1,1
"org.apache.hive.ptest.execution.context.CloudComputeService.createFilterPTestPredicate(String,String)",4,2,5
"org.apache.hive.ptest.execution.context.CloudComputeService.createNodes(int)",1,1,1
"org.apache.hive.ptest.execution.context.CloudComputeService.destroyNode(String)",1,1,1
"org.apache.hive.ptest.execution.context.CloudComputeService.listRunningNodes()",1,1,1
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.Builder.build(Context,String)",1,1,1
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.CloudExecutionContextProvider(String,int,CloudComputeService,SSHCommandExecutor,String,String,String,String[],int,long,int)",1,1,1
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.close()",1,1,2
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.create(Context,String)",1,1,2
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.createExecutionContext()",1,2,2
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.createNodes(int)",3,6,8
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.getRunningNodes()",1,2,2
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.initialize()",1,5,5
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.performBackgroundWork()",1,4,4
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.persistHostnamesToLog(Set<? extends NodeMetadata>)",1,4,4
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.publicIp(NodeMetadata)",2,2,2
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.publicIpOrHostname(NodeMetadata)",2,2,2
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.replaceBadHosts(ExecutionContext)",1,5,5
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.syncLog()",1,2,2
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.terminate(ExecutionContext)",1,2,2
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.terminate(Set<String>,boolean)",1,5,5
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.terminateInternal(NodeMetadata)",1,4,4
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.terminateInternal(Set<? extends NodeMetadata>)",1,2,2
"org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.verifyHosts(Set<? extends NodeMetadata>)",2,7,8
"org.apache.hive.ptest.execution.context.CreateHostsFailedException.CreateHostsFailedException(String)",1,1,1
"org.apache.hive.ptest.execution.context.CreateHostsFailedException.CreateHostsFailedException(String,Throwable)",1,1,1
"org.apache.hive.ptest.execution.context.ExecutionContext.ExecutionContext(ExecutionContextProvider,Set<Host>,String,String)",1,1,1
"org.apache.hive.ptest.execution.context.ExecutionContext.addBadHost(Host)",1,1,1
"org.apache.hive.ptest.execution.context.ExecutionContext.addHost(Host)",1,1,1
"org.apache.hive.ptest.execution.context.ExecutionContext.clearBadHosts()",1,1,1
"org.apache.hive.ptest.execution.context.ExecutionContext.getBadHosts()",1,1,1
"org.apache.hive.ptest.execution.context.ExecutionContext.getHosts()",1,1,1
"org.apache.hive.ptest.execution.context.ExecutionContext.getLocalWorkingDirectory()",1,1,1
"org.apache.hive.ptest.execution.context.ExecutionContext.getPrivateKey()",1,1,1
"org.apache.hive.ptest.execution.context.ExecutionContext.removeHost(Host)",1,1,1
"org.apache.hive.ptest.execution.context.ExecutionContext.replaceBadHosts()",1,1,1
"org.apache.hive.ptest.execution.context.ExecutionContext.terminate()",1,1,1
"org.apache.hive.ptest.execution.context.FixedExecutionContextProvider.Builder.build(Context,String)",1,2,2
"org.apache.hive.ptest.execution.context.FixedExecutionContextProvider.FixedExecutionContextProvider(Set<Host>,String,String)",1,1,1
"org.apache.hive.ptest.execution.context.FixedExecutionContextProvider.close()",1,1,1
"org.apache.hive.ptest.execution.context.FixedExecutionContextProvider.createExecutionContext()",1,1,1
"org.apache.hive.ptest.execution.context.FixedExecutionContextProvider.replaceBadHosts(ExecutionContext)",1,1,1
"org.apache.hive.ptest.execution.context.FixedExecutionContextProvider.terminate(ExecutionContext)",1,1,1
"org.apache.hive.ptest.execution.context.ServiceNotAvailableException.ServiceNotAvailableException(String)",1,1,1
"org.apache.hive.ptest.execution.context.ServiceNotAvailableException.ServiceNotAvailableException(String,Throwable)",1,1,1
"org.apache.hive.ptest.execution.context.TestCloudComputeService.setup()",1,1,1
"org.apache.hive.ptest.execution.context.TestCloudComputeService.testBadGroup()",1,1,1
"org.apache.hive.ptest.execution.context.TestCloudComputeService.testBadName()",1,1,1
"org.apache.hive.ptest.execution.context.TestCloudComputeService.testBadTag()",1,1,1
"org.apache.hive.ptest.execution.context.TestCloudComputeService.testNotStarted()",1,1,1
"org.apache.hive.ptest.execution.context.TestCloudExecutionContextProvider.setup()",1,1,1
"org.apache.hive.ptest.execution.context.TestCloudExecutionContextProvider.teardown()",1,1,1
"org.apache.hive.ptest.execution.context.TestCloudExecutionContextProvider.testRetry()",2,2,3
"org.apache.hive.ptest.execution.ssh.AbstractSSHCommand.AbstractSSHCommand(String,String,String,int)",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHCommand.getException()",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHCommand.getExitCode()",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHCommand.getHost()",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHCommand.getInstance()",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHCommand.getOutput()",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHCommand.getPrivateKey()",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHCommand.getUser()",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHCommand.setException(Exception)",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHCommand.setExitCode(int)",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHCommand.setOutput(String)",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHResult.AbstractSSHResult(String,String,int,int,Exception,String)",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHResult.getException()",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHResult.getExitCode()",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHResult.getHost()",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHResult.getInstance()",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHResult.getOutput()",1,1,1
"org.apache.hive.ptest.execution.ssh.AbstractSSHResult.getUser()",1,1,1
"org.apache.hive.ptest.execution.ssh.NonZeroExitCodeException.NonZeroExitCodeException(String)",1,1,1
"org.apache.hive.ptest.execution.ssh.NonZeroExitCodeException.NonZeroExitCodeException(String,Throwable)",1,1,1
"org.apache.hive.ptest.execution.ssh.RSyncCommand.RSyncCommand(RSyncCommandExecutor,String,String,String,int,String,String,Type)",1,1,1
"org.apache.hive.ptest.execution.ssh.RSyncCommand.call()",1,1,1
"org.apache.hive.ptest.execution.ssh.RSyncCommand.getLocalFile()",1,1,1
"org.apache.hive.ptest.execution.ssh.RSyncCommand.getRemoteFile()",1,1,1
"org.apache.hive.ptest.execution.ssh.RSyncCommand.getType()",1,1,1
"org.apache.hive.ptest.execution.ssh.RSyncCommand.toString()",1,1,1
"org.apache.hive.ptest.execution.ssh.RSyncCommandExecutor.RSyncCommandExecutor(Logger,int,LocalCommandFactory)",1,1,1
"org.apache.hive.ptest.execution.ssh.RSyncCommandExecutor.execute(RSyncCommand)",5,10,11
"org.apache.hive.ptest.execution.ssh.RSyncCommandExecutor.isShutdown()",1,1,1
"org.apache.hive.ptest.execution.ssh.RSyncCommandExecutor.shutdownNow()",1,1,1
"org.apache.hive.ptest.execution.ssh.RSyncResult.RSyncResult(String,String,int,String,String,int,Exception,String)",1,1,1
"org.apache.hive.ptest.execution.ssh.RSyncResult.getLocalFile()",1,1,1
"org.apache.hive.ptest.execution.ssh.RSyncResult.getRemoteFile()",1,1,1
"org.apache.hive.ptest.execution.ssh.RSyncResult.toString()",1,1,1
"org.apache.hive.ptest.execution.ssh.SSHCommand.SSHCommand(SSHCommandExecutor,String,String,String,int,String)",1,1,1
"org.apache.hive.ptest.execution.ssh.SSHCommand.call()",1,1,1
"org.apache.hive.ptest.execution.ssh.SSHCommand.getCommand()",1,1,1
"org.apache.hive.ptest.execution.ssh.SSHCommand.toString()",1,1,1
"org.apache.hive.ptest.execution.ssh.SSHCommandExecutor.SSHCommandExecutor(Logger)",1,1,1
"org.apache.hive.ptest.execution.ssh.SSHCommandExecutor.SSHCommandExecutor(Logger,LocalCommandFactory)",1,1,1
"org.apache.hive.ptest.execution.ssh.SSHCommandExecutor.execute(SSHCommand)",3,7,8
"org.apache.hive.ptest.execution.ssh.SSHCommandExecutor.isShutdown()",1,1,1
"org.apache.hive.ptest.execution.ssh.SSHCommandExecutor.shutdownNow()",1,1,1
"org.apache.hive.ptest.execution.ssh.SSHExecutionException.SSHExecutionException(RemoteCommandResult)",1,1,1
"org.apache.hive.ptest.execution.ssh.SSHExecutionException.getMessage()",1,1,1
"org.apache.hive.ptest.execution.ssh.SSHResult.SSHResult(String,String,int,String,int,Exception,String)",1,1,1
"org.apache.hive.ptest.execution.ssh.SSHResult.getCommand()",1,1,1
"org.apache.hive.ptest.execution.ssh.SSHResult.toString()",1,1,1
"org.apache.hive.ptest.execution.ssh.TestRSyncCommandExecutor.setup()",1,1,1
"org.apache.hive.ptest.execution.ssh.TestRSyncCommandExecutor.testShutdownBeforeWaitFor()",2,2,2
"org.apache.hive.ptest.execution.ssh.TestRSyncCommandExecutor.testShutdownDuringWaitFor()",2,2,2
"org.apache.hive.ptest.execution.ssh.TestSSHCommandExecutor.setup()",1,1,1
"org.apache.hive.ptest.execution.ssh.TestSSHCommandExecutor.testShutdownBeforeWaitFor()",2,2,2
"org.apache.hive.ptest.execution.ssh.TestSSHCommandExecutor.testShutdownDuringWaitFor()",2,2,2
"org.apache.hive.service.AbstractService.AbstractService(String)",1,1,1
"org.apache.hive.service.AbstractService.changeState(STATE)",1,2,2
"org.apache.hive.service.AbstractService.ensureCurrentState(STATE)",1,1,1
"org.apache.hive.service.AbstractService.getHiveConf()",1,1,1
"org.apache.hive.service.AbstractService.getName()",1,1,1
"org.apache.hive.service.AbstractService.getServiceState()",1,1,1
"org.apache.hive.service.AbstractService.getStartTime()",1,1,1
"org.apache.hive.service.AbstractService.init(HiveConf)",1,1,1
"org.apache.hive.service.AbstractService.register(ServiceStateChangeListener)",1,1,1
"org.apache.hive.service.AbstractService.start()",1,1,1
"org.apache.hive.service.AbstractService.stop()",2,1,4
"org.apache.hive.service.AbstractService.unregister(ServiceStateChangeListener)",1,1,1
"org.apache.hive.service.BreakableService.BreakableService()",1,1,1
"org.apache.hive.service.BreakableService.BreakableService(boolean,boolean,boolean)",1,1,1
"org.apache.hive.service.BreakableService.BrokenLifecycleEvent.BrokenLifecycleEvent(String)",1,1,1
"org.apache.hive.service.BreakableService.convert(STATE)",6,2,6
"org.apache.hive.service.BreakableService.getCount(STATE)",1,1,1
"org.apache.hive.service.BreakableService.inc(STATE)",1,1,1
"org.apache.hive.service.BreakableService.init(HiveConf)",1,1,1
"org.apache.hive.service.BreakableService.maybeFail(boolean,String)",2,1,2
"org.apache.hive.service.BreakableService.setFailOnInit(boolean)",1,1,1
"org.apache.hive.service.BreakableService.setFailOnStart(boolean)",1,1,1
"org.apache.hive.service.BreakableService.setFailOnStop(boolean)",1,1,1
"org.apache.hive.service.BreakableService.start()",1,1,1
"org.apache.hive.service.BreakableService.stop()",1,1,1
"org.apache.hive.service.CompositeService.CompositeService(String)",1,1,1
"org.apache.hive.service.CompositeService.CompositeServiceShutdownHook.CompositeServiceShutdownHook(CompositeService)",1,1,1
"org.apache.hive.service.CompositeService.CompositeServiceShutdownHook.run()",1,2,2
"org.apache.hive.service.CompositeService.addService(Service)",1,1,1
"org.apache.hive.service.CompositeService.getServices()",1,1,1
"org.apache.hive.service.CompositeService.init(HiveConf)",1,2,2
"org.apache.hive.service.CompositeService.removeService(Service)",1,1,1
"org.apache.hive.service.CompositeService.start()",1,3,3
"org.apache.hive.service.CompositeService.stop()",2,2,3
"org.apache.hive.service.CompositeService.stop(int)",1,3,3
"org.apache.hive.service.FilterService.FilterService(Service)",1,1,1
"org.apache.hive.service.FilterService.getHiveConf()",1,1,1
"org.apache.hive.service.FilterService.getName()",1,1,1
"org.apache.hive.service.FilterService.getServiceState()",1,1,1
"org.apache.hive.service.FilterService.getStartTime()",1,1,1
"org.apache.hive.service.FilterService.init(HiveConf)",1,1,1
"org.apache.hive.service.FilterService.register(ServiceStateChangeListener)",1,1,1
"org.apache.hive.service.FilterService.start()",1,1,1
"org.apache.hive.service.FilterService.stop()",1,1,1
"org.apache.hive.service.FilterService.unregister(ServiceStateChangeListener)",1,1,1
"org.apache.hive.service.ServiceException.ServiceException(String)",1,1,1
"org.apache.hive.service.ServiceException.ServiceException(String,Throwable)",1,1,1
"org.apache.hive.service.ServiceException.ServiceException(Throwable)",1,1,1
"org.apache.hive.service.ServiceOperations.ServiceOperations()",1,1,1
"org.apache.hive.service.ServiceOperations.deploy(Service,HiveConf)",1,1,1
"org.apache.hive.service.ServiceOperations.ensureCurrentState(STATE,STATE)",2,1,2
"org.apache.hive.service.ServiceOperations.init(Service,HiveConf)",1,1,1
"org.apache.hive.service.ServiceOperations.start(Service)",1,1,1
"org.apache.hive.service.ServiceOperations.stop(Service)",1,3,3
"org.apache.hive.service.ServiceOperations.stopQuietly(Service)",1,2,2
"org.apache.hive.service.TestHS2ImpersonationWithRemoteMS.startServices()",1,1,1
"org.apache.hive.service.TestHS2ImpersonationWithRemoteMS.stopServices()",1,3,3
"org.apache.hive.service.TestHS2ImpersonationWithRemoteMS.testImpersonation()",1,5,5
"org.apache.hive.service.auth.AnonymousAuthenticationProviderImpl.Authenticate(String,String)",1,1,1
"org.apache.hive.service.auth.AuthenticationProviderFactory.AuthMethods.AuthMethods(String)",1,1,1
"org.apache.hive.service.auth.AuthenticationProviderFactory.AuthMethods.getAuthMethod()",1,1,1
"org.apache.hive.service.auth.AuthenticationProviderFactory.AuthMethods.getValidAuthMethod(String)",3,2,3
"org.apache.hive.service.auth.AuthenticationProviderFactory.AuthenticationProviderFactory()",1,1,1
"org.apache.hive.service.auth.AuthenticationProviderFactory.getAuthenticationProvider(AuthMethods)",5,4,5
"org.apache.hive.service.auth.CustomAuthenticationProviderImpl.Authenticate(String,String)",1,1,1
"org.apache.hive.service.auth.CustomAuthenticationProviderImpl.CustomAuthenticationProviderImpl()",1,1,1
"org.apache.hive.service.auth.HiveAuthFactory.AuthTypes.AuthTypes(String)",1,1,1
"org.apache.hive.service.auth.HiveAuthFactory.AuthTypes.getAuthName()",1,1,1
"org.apache.hive.service.auth.HiveAuthFactory.HiveAuthFactory(HiveConf)",3,6,7
"org.apache.hive.service.auth.HiveAuthFactory.cancelDelegationToken(String)",2,1,3
"org.apache.hive.service.auth.HiveAuthFactory.getAuthProcFactory(ThriftCLIService)",3,3,3
"org.apache.hive.service.auth.HiveAuthFactory.getAuthTransFactory()",7,8,8
"org.apache.hive.service.auth.HiveAuthFactory.getDelegationToken(String,String)",3,2,6
"org.apache.hive.service.auth.HiveAuthFactory.getIpAddress()",2,3,3
"org.apache.hive.service.auth.HiveAuthFactory.getRemoteUser()",2,2,2
"org.apache.hive.service.auth.HiveAuthFactory.getSSLSocket(String,int,int)",1,1,1
"org.apache.hive.service.auth.HiveAuthFactory.getSSLSocket(String,int,int,String,String)",1,1,1
"org.apache.hive.service.auth.HiveAuthFactory.getSaslProperties()",1,1,1
"org.apache.hive.service.auth.HiveAuthFactory.getServerSSLSocket(String,int,String,String)",1,3,3
"org.apache.hive.service.auth.HiveAuthFactory.getServerSocket(String,int)",1,2,3
"org.apache.hive.service.auth.HiveAuthFactory.getSocketTransport(String,int,int)",1,1,1
"org.apache.hive.service.auth.HiveAuthFactory.getUserFromToken(String)",2,1,3
"org.apache.hive.service.auth.HiveAuthFactory.loginFromKeytab(HiveConf)",2,3,3
"org.apache.hive.service.auth.HiveAuthFactory.loginFromSpnegoKeytabAndReturnUGI(HiveConf)",2,3,3
"org.apache.hive.service.auth.HiveAuthFactory.renewDelegationToken(String)",2,1,3
"org.apache.hive.service.auth.HiveAuthFactory.verifyProxyAccess(String,String,String,HiveConf)",1,3,4
"org.apache.hive.service.auth.HttpAuthUtils.HttpCLIServiceProcessorFactory.HttpCLIServiceProcessorFactory(ThriftCLIService)",1,1,1
"org.apache.hive.service.auth.HttpAuthUtils.HttpCLIServiceProcessorFactory.getProcessor(TTransport)",1,1,2
"org.apache.hive.service.auth.HttpAuthUtils.HttpKerberosClientAction.HttpKerberosClientAction(String,String,String)",1,1,1
"org.apache.hive.service.auth.HttpAuthUtils.HttpKerberosClientAction.run()",1,1,1
"org.apache.hive.service.auth.HttpAuthUtils.getAuthProcFactory(ThriftCLIService)",1,1,1
"org.apache.hive.service.auth.HttpAuthUtils.getClientUGI(String)",1,1,1
"org.apache.hive.service.auth.HttpAuthUtils.getKerberosServiceTicket(String,String,String)",1,1,1
"org.apache.hive.service.auth.HttpAuthUtils.getServerPrincipal(String,String)",1,1,1
"org.apache.hive.service.auth.HttpAuthenticationException.HttpAuthenticationException(String)",1,1,1
"org.apache.hive.service.auth.HttpAuthenticationException.HttpAuthenticationException(String,Throwable)",1,1,1
"org.apache.hive.service.auth.HttpAuthenticationException.HttpAuthenticationException(Throwable)",1,1,1
"org.apache.hive.service.auth.HttpCLIServiceUGIProcessor.HttpCLIServiceUGIProcessor(TProcessor)",1,1,1
"org.apache.hive.service.auth.HttpCLIServiceUGIProcessor.process(TProtocol,TProtocol)",2,3,6
"org.apache.hive.service.auth.KerberosSaslHelper.CLIServiceProcessorFactory.CLIServiceProcessorFactory(Server,ThriftCLIService)",1,1,1
"org.apache.hive.service.auth.KerberosSaslHelper.CLIServiceProcessorFactory.getProcessor(TTransport)",1,1,1
"org.apache.hive.service.auth.KerberosSaslHelper.createSubjectAssumedTransport(String,TTransport,Map<String, String>)",1,1,2
"org.apache.hive.service.auth.KerberosSaslHelper.getKerberosProcessorFactory(Server,ThriftCLIService)",1,1,1
"org.apache.hive.service.auth.KerberosSaslHelper.getKerberosTransport(String,String,TTransport,Map<String, String>,boolean)",3,2,4
"org.apache.hive.service.auth.KerberosSaslHelper.getTokenTransport(String,String,TTransport,Map<String, String>)",1,1,2
"org.apache.hive.service.auth.LdapAuthenticationProviderImpl.Authenticate(String,String)",1,1,4
"org.apache.hive.service.auth.LdapAuthenticationProviderImpl.LdapAuthenticationProviderImpl()",1,1,1
"org.apache.hive.service.auth.PamAuthenticationProviderImpl.Authenticate(String,String)",4,3,5
"org.apache.hive.service.auth.PamAuthenticationProviderImpl.PamAuthenticationProviderImpl()",1,1,1
"org.apache.hive.service.auth.PlainSaslHelper.PlainClientbackHandler.PlainClientbackHandler(String,String)",1,1,1
"org.apache.hive.service.auth.PlainSaslHelper.PlainClientbackHandler.handle(Callback[])",4,4,4
"org.apache.hive.service.auth.PlainSaslHelper.PlainServerCallbackHandler.PlainServerCallbackHandler(String)",1,1,1
"org.apache.hive.service.auth.PlainSaslHelper.PlainServerCallbackHandler.handle(Callback[])",5,5,6
"org.apache.hive.service.auth.PlainSaslHelper.SQLPlainProcessorFactory.SQLPlainProcessorFactory(ThriftCLIService)",1,1,1
"org.apache.hive.service.auth.PlainSaslHelper.SQLPlainProcessorFactory.getProcessor(TTransport)",1,1,1
"org.apache.hive.service.auth.PlainSaslHelper.getPlainProcessorFactory(ThriftCLIService)",1,1,1
"org.apache.hive.service.auth.PlainSaslHelper.getPlainTransport(String,String,TTransport)",1,1,1
"org.apache.hive.service.auth.PlainSaslHelper.getPlainTransportFactory(String)",1,1,2
"org.apache.hive.service.auth.PlainSaslServer.PlainSaslServer(CallbackHandler,String)",1,1,1
"org.apache.hive.service.auth.PlainSaslServer.SaslPlainProvider.SaslPlainProvider()",1,1,1
"org.apache.hive.service.auth.PlainSaslServer.SaslPlainServerFactory.createSaslServer(String,String,String,Map<String, ?>,CallbackHandler)",2,1,3
"org.apache.hive.service.auth.PlainSaslServer.SaslPlainServerFactory.getMechanismNames(Map<String, ?>)",1,1,1
"org.apache.hive.service.auth.PlainSaslServer.dispose()",1,1,1
"org.apache.hive.service.auth.PlainSaslServer.evaluateResponse(byte[])",5,7,14
"org.apache.hive.service.auth.PlainSaslServer.getAuthorizationID()",1,1,1
"org.apache.hive.service.auth.PlainSaslServer.getMechanismName()",1,1,1
"org.apache.hive.service.auth.PlainSaslServer.getNegotiatedProperty(String)",1,1,1
"org.apache.hive.service.auth.PlainSaslServer.isComplete()",1,1,1
"org.apache.hive.service.auth.PlainSaslServer.unwrap(byte[],int,int)",1,1,1
"org.apache.hive.service.auth.PlainSaslServer.wrap(byte[],int,int)",1,1,1
"org.apache.hive.service.auth.SaslQOP.SaslQOP(String)",1,1,1
"org.apache.hive.service.auth.SaslQOP.fromString(String)",2,3,3
"org.apache.hive.service.auth.SaslQOP.toString()",1,1,1
"org.apache.hive.service.auth.TSetIpAddressProcessor.TSetIpAddressProcessor(Iface)",1,1,1
"org.apache.hive.service.auth.TSetIpAddressProcessor.getUnderlyingSocketFromTransport(TTransport)",3,4,5
"org.apache.hive.service.auth.TSetIpAddressProcessor.getUserIpAddress()",1,1,1
"org.apache.hive.service.auth.TSetIpAddressProcessor.getUserName()",1,1,1
"org.apache.hive.service.auth.TSetIpAddressProcessor.process(TProtocol,TProtocol)",1,1,1
"org.apache.hive.service.auth.TSetIpAddressProcessor.setIpAddress(TProtocol)",1,2,2
"org.apache.hive.service.auth.TSetIpAddressProcessor.setUserName(TProtocol)",1,2,2
"org.apache.hive.service.auth.TSubjectAssumingTransport.TSubjectAssumingTransport(TTransport)",1,1,1
"org.apache.hive.service.auth.TSubjectAssumingTransport.open()",2,3,5
"org.apache.hive.service.auth.TestCustomAuthentication.SimpleAuthenticationProviderImpl.Authenticate(String,String)",3,1,3
"org.apache.hive.service.auth.TestCustomAuthentication.SimpleAuthenticationProviderImpl.SimpleAuthenticationProviderImpl()",1,1,1
"org.apache.hive.service.auth.TestCustomAuthentication.SimpleAuthenticationProviderImpl.init()",1,1,1
"org.apache.hive.service.auth.TestCustomAuthentication.setUp()",1,1,1
"org.apache.hive.service.auth.TestCustomAuthentication.tearDown()",1,3,4
"org.apache.hive.service.auth.TestCustomAuthentication.testCustomAuthentication()",1,2,2
"org.apache.hive.service.auth.TestPlainSaslHelper.testDoAsSetting()",1,1,1
"org.apache.hive.service.cli.CLIService.CLIService()",1,1,1
"org.apache.hive.service.cli.CLIService.cancelDelegationToken(SessionHandle,HiveAuthFactory,String)",1,1,1
"org.apache.hive.service.cli.CLIService.cancelOperation(OperationHandle)",1,1,1
"org.apache.hive.service.cli.CLIService.closeOperation(OperationHandle)",1,1,1
"org.apache.hive.service.cli.CLIService.closeSession(SessionHandle)",1,1,1
"org.apache.hive.service.cli.CLIService.executeStatement(SessionHandle,String,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.CLIService.executeStatementAsync(SessionHandle,String,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.CLIService.fetchResults(OperationHandle)",1,1,1
"org.apache.hive.service.cli.CLIService.fetchResults(OperationHandle,FetchOrientation,long,FetchType)",1,1,1
"org.apache.hive.service.cli.CLIService.getCatalogs(SessionHandle)",1,1,1
"org.apache.hive.service.cli.CLIService.getColumns(SessionHandle,String,String,String,String)",1,1,1
"org.apache.hive.service.cli.CLIService.getDelegationToken(SessionHandle,HiveAuthFactory,String,String)",1,1,1
"org.apache.hive.service.cli.CLIService.getDelegationTokenFromMetaStore(String)",3,4,5
"org.apache.hive.service.cli.CLIService.getFunctions(SessionHandle,String,String,String)",1,1,1
"org.apache.hive.service.cli.CLIService.getHttpUGI()",1,1,1
"org.apache.hive.service.cli.CLIService.getInfo(SessionHandle,GetInfoType)",1,1,1
"org.apache.hive.service.cli.CLIService.getOperationStatus(OperationHandle)",1,5,6
"org.apache.hive.service.cli.CLIService.getResultSetMetadata(OperationHandle)",1,1,1
"org.apache.hive.service.cli.CLIService.getSchemas(SessionHandle,String,String)",1,1,1
"org.apache.hive.service.cli.CLIService.getServiceUGI()",1,1,1
"org.apache.hive.service.cli.CLIService.getStaticPath(Path)",1,2,3
"org.apache.hive.service.cli.CLIService.getTableTypes(SessionHandle)",1,1,1
"org.apache.hive.service.cli.CLIService.getTables(SessionHandle,String,String,String,List<String>)",1,1,1
"org.apache.hive.service.cli.CLIService.getTypeInfo(SessionHandle)",1,1,1
"org.apache.hive.service.cli.CLIService.init(HiveConf)",2,5,7
"org.apache.hive.service.cli.CLIService.openSession(String,String,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.CLIService.openSession(TProtocolVersion,String,String,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.CLIService.openSession(TProtocolVersion,String,String,String,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.CLIService.openSessionWithImpersonation(String,String,Map<String, String>,String)",1,1,1
"org.apache.hive.service.cli.CLIService.openSessionWithImpersonation(TProtocolVersion,String,String,Map<String, String>,String)",1,1,1
"org.apache.hive.service.cli.CLIService.openSessionWithImpersonation(TProtocolVersion,String,String,String,Map<String, String>,String)",1,1,1
"org.apache.hive.service.cli.CLIService.renewDelegationToken(SessionHandle,HiveAuthFactory,String)",1,1,1
"org.apache.hive.service.cli.CLIService.setupStagingDir(String,boolean)",2,3,4
"org.apache.hive.service.cli.CLIService.start()",1,2,4
"org.apache.hive.service.cli.CLIService.stop()",1,1,1
"org.apache.hive.service.cli.CLIServiceClient.fetchResults(OperationHandle)",1,1,1
"org.apache.hive.service.cli.CLIServiceClient.openSession(String,String)",1,1,1
"org.apache.hive.service.cli.CLIServiceTest.runQueryAsync(SessionHandle,String,Map<String, String>,OperationState,long)",4,4,7
"org.apache.hive.service.cli.CLIServiceTest.setUp()",1,1,1
"org.apache.hive.service.cli.CLIServiceTest.setupTestData(String,String,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.CLIServiceTest.tearDown()",1,1,1
"org.apache.hive.service.cli.CLIServiceTest.testConfOverlay()",1,1,2
"org.apache.hive.service.cli.CLIServiceTest.testExecuteStatement()",1,1,1
"org.apache.hive.service.cli.CLIServiceTest.testExecuteStatementAsync()",1,1,2
"org.apache.hive.service.cli.CLIServiceTest.testGetFunctions()",1,1,1
"org.apache.hive.service.cli.CLIServiceTest.testGetInfo()",1,1,1
"org.apache.hive.service.cli.CLIServiceTest.testOpenSession()",1,1,1
"org.apache.hive.service.cli.CLIServiceUtils.patternToRegex(String)",5,7,8
"org.apache.hive.service.cli.Column.Column(TColumn)",9,9,9
"org.apache.hive.service.cli.Column.Column(Type)",2,2,9
"org.apache.hive.service.cli.Column.Column(Type,BitSet,Object)",9,9,9
"org.apache.hive.service.cli.Column.addValue(Type,Object)",2,5,19
"org.apache.hive.service.cli.Column.boolVars()",2,2,2
"org.apache.hive.service.cli.Column.byteVars()",2,2,2
"org.apache.hive.service.cli.Column.doubleVars()",2,2,2
"org.apache.hive.service.cli.Column.extractSubset(int,int)",9,9,9
"org.apache.hive.service.cli.Column.get(int)",10,3,10
"org.apache.hive.service.cli.Column.getType()",1,1,1
"org.apache.hive.service.cli.Column.intVars()",2,2,2
"org.apache.hive.service.cli.Column.longVars()",2,2,2
"org.apache.hive.service.cli.Column.shortVars()",2,2,2
"org.apache.hive.service.cli.Column.size()",1,1,1
"org.apache.hive.service.cli.Column.toBinary(BitSet)",1,2,3
"org.apache.hive.service.cli.Column.toBitset(byte[])",1,2,2
"org.apache.hive.service.cli.Column.toTColumn()",2,2,9
"org.apache.hive.service.cli.ColumnBasedSet.ColumnBasedSet(TRowSet)",1,2,2
"org.apache.hive.service.cli.ColumnBasedSet.ColumnBasedSet(TableSchema)",1,2,2
"org.apache.hive.service.cli.ColumnBasedSet.ColumnBasedSet(Type[],List<Column>,long)",1,1,1
"org.apache.hive.service.cli.ColumnBasedSet.addRow(Object[])",1,2,2
"org.apache.hive.service.cli.ColumnBasedSet.extractSubset(int)",1,2,2
"org.apache.hive.service.cli.ColumnBasedSet.fill(int,Object[])",1,2,2
"org.apache.hive.service.cli.ColumnBasedSet.getColumns()",1,1,1
"org.apache.hive.service.cli.ColumnBasedSet.getStartOffset()",1,1,1
"org.apache.hive.service.cli.ColumnBasedSet.iterator()",1,2,2
"org.apache.hive.service.cli.ColumnBasedSet.numColumns()",1,1,1
"org.apache.hive.service.cli.ColumnBasedSet.numRows()",1,2,2
"org.apache.hive.service.cli.ColumnBasedSet.setStartOffset(long)",1,1,1
"org.apache.hive.service.cli.ColumnBasedSet.toTRowSet()",1,2,2
"org.apache.hive.service.cli.ColumnDescriptor.ColumnDescriptor(FieldSchema,int)",1,1,1
"org.apache.hive.service.cli.ColumnDescriptor.ColumnDescriptor(String,String,TypeDescriptor,int)",1,1,1
"org.apache.hive.service.cli.ColumnDescriptor.ColumnDescriptor(TColumnDesc)",1,1,1
"org.apache.hive.service.cli.ColumnDescriptor.getComment()",1,1,1
"org.apache.hive.service.cli.ColumnDescriptor.getName()",1,1,1
"org.apache.hive.service.cli.ColumnDescriptor.getOrdinalPosition()",1,1,1
"org.apache.hive.service.cli.ColumnDescriptor.getType()",1,1,1
"org.apache.hive.service.cli.ColumnDescriptor.getTypeDescriptor()",1,1,1
"org.apache.hive.service.cli.ColumnDescriptor.getTypeName()",1,1,1
"org.apache.hive.service.cli.ColumnDescriptor.isPrimitive()",1,1,1
"org.apache.hive.service.cli.ColumnDescriptor.newPrimitiveColumnDescriptor(String,String,Type,int)",1,1,1
"org.apache.hive.service.cli.ColumnDescriptor.toTColumnDesc()",1,1,1
"org.apache.hive.service.cli.ColumnValue.booleanValue(Boolean)",1,2,2
"org.apache.hive.service.cli.ColumnValue.byteValue(Byte)",1,2,2
"org.apache.hive.service.cli.ColumnValue.dateValue(Date)",1,2,2
"org.apache.hive.service.cli.ColumnValue.doubleValue(Double)",1,2,2
"org.apache.hive.service.cli.ColumnValue.floatValue(Float)",1,2,2
"org.apache.hive.service.cli.ColumnValue.getBigDecimalValue(TStringValue)",2,2,2
"org.apache.hive.service.cli.ColumnValue.getBinaryValue(TStringValue)",2,2,2
"org.apache.hive.service.cli.ColumnValue.getBooleanValue(TBoolValue)",2,2,2
"org.apache.hive.service.cli.ColumnValue.getByteValue(TByteValue)",2,2,2
"org.apache.hive.service.cli.ColumnValue.getDateValue(TStringValue)",2,2,2
"org.apache.hive.service.cli.ColumnValue.getDoubleValue(TDoubleValue)",2,2,2
"org.apache.hive.service.cli.ColumnValue.getIntegerValue(TI32Value)",2,2,2
"org.apache.hive.service.cli.ColumnValue.getLongValue(TI64Value)",2,2,2
"org.apache.hive.service.cli.ColumnValue.getShortValue(TI16Value)",2,2,2
"org.apache.hive.service.cli.ColumnValue.getStringValue(TStringValue)",2,2,2
"org.apache.hive.service.cli.ColumnValue.getTimestampValue(TStringValue)",2,2,2
"org.apache.hive.service.cli.ColumnValue.intValue(Integer)",1,2,2
"org.apache.hive.service.cli.ColumnValue.longValue(Long)",1,2,2
"org.apache.hive.service.cli.ColumnValue.shortValue(Short)",1,2,2
"org.apache.hive.service.cli.ColumnValue.stringValue(HiveChar)",1,2,2
"org.apache.hive.service.cli.ColumnValue.stringValue(HiveDecimal)",1,2,2
"org.apache.hive.service.cli.ColumnValue.stringValue(HiveVarchar)",1,2,2
"org.apache.hive.service.cli.ColumnValue.stringValue(String)",1,2,2
"org.apache.hive.service.cli.ColumnValue.timestampValue(Timestamp)",1,2,2
"org.apache.hive.service.cli.ColumnValue.toColumnValue(TColumnValue)",8,8,8
"org.apache.hive.service.cli.ColumnValue.toTColumnValue(Type,Object)",17,17,17
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.EmbeddedCLIServiceClient(ICLIService)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.cancelDelegationToken(SessionHandle,HiveAuthFactory,String)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.cancelOperation(OperationHandle)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.closeOperation(OperationHandle)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.closeSession(SessionHandle)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.executeStatement(SessionHandle,String,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.executeStatementAsync(SessionHandle,String,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.fetchResults(OperationHandle,FetchOrientation,long,FetchType)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.getCatalogs(SessionHandle)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.getColumns(SessionHandle,String,String,String,String)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.getDelegationToken(SessionHandle,HiveAuthFactory,String,String)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.getFunctions(SessionHandle,String,String,String)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.getInfo(SessionHandle,GetInfoType)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.getOperationStatus(OperationHandle)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.getResultSetMetadata(OperationHandle)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.getSchemas(SessionHandle,String,String)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.getTableTypes(SessionHandle)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.getTables(SessionHandle,String,String,String,List<String>)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.getTypeInfo(SessionHandle)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.openSession(String,String,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.openSessionWithImpersonation(String,String,Map<String, String>,String)",1,1,1
"org.apache.hive.service.cli.EmbeddedCLIServiceClient.renewDelegationToken(SessionHandle,HiveAuthFactory,String)",1,1,1
"org.apache.hive.service.cli.FetchOrientation.FetchOrientation(TFetchOrientation)",1,1,1
"org.apache.hive.service.cli.FetchOrientation.getFetchOrientation(TFetchOrientation)",3,2,3
"org.apache.hive.service.cli.FetchOrientation.toTFetchOrientation()",1,1,1
"org.apache.hive.service.cli.FetchType.FetchType(short)",1,1,1
"org.apache.hive.service.cli.FetchType.getFetchType(short)",3,2,3
"org.apache.hive.service.cli.FetchType.toTFetchType()",1,1,1
"org.apache.hive.service.cli.GetInfoType.GetInfoType(TGetInfoType)",1,1,1
"org.apache.hive.service.cli.GetInfoType.getGetInfoType(TGetInfoType)",3,2,3
"org.apache.hive.service.cli.GetInfoType.toTGetInfoType()",1,1,1
"org.apache.hive.service.cli.GetInfoValue.GetInfoValue(String)",1,1,1
"org.apache.hive.service.cli.GetInfoValue.GetInfoValue(TGetInfoValue)",2,2,3
"org.apache.hive.service.cli.GetInfoValue.GetInfoValue(int)",1,1,1
"org.apache.hive.service.cli.GetInfoValue.GetInfoValue(long)",1,1,1
"org.apache.hive.service.cli.GetInfoValue.GetInfoValue(short)",1,1,1
"org.apache.hive.service.cli.GetInfoValue.getIntValue()",1,1,1
"org.apache.hive.service.cli.GetInfoValue.getLongValue()",1,1,1
"org.apache.hive.service.cli.GetInfoValue.getShortValue()",1,1,1
"org.apache.hive.service.cli.GetInfoValue.getStringValue()",1,1,1
"org.apache.hive.service.cli.GetInfoValue.toTGetInfoValue()",1,2,2
"org.apache.hive.service.cli.Handle.Handle()",1,1,1
"org.apache.hive.service.cli.Handle.Handle(HandleIdentifier)",1,1,1
"org.apache.hive.service.cli.Handle.Handle(THandleIdentifier)",1,1,1
"org.apache.hive.service.cli.Handle.equals(Object)",7,2,7
"org.apache.hive.service.cli.Handle.getHandleIdentifier()",1,1,1
"org.apache.hive.service.cli.Handle.hashCode()",1,2,2
"org.apache.hive.service.cli.HandleIdentifier.HandleIdentifier()",1,1,1
"org.apache.hive.service.cli.HandleIdentifier.HandleIdentifier(THandleIdentifier)",1,1,1
"org.apache.hive.service.cli.HandleIdentifier.HandleIdentifier(UUID,UUID)",1,1,1
"org.apache.hive.service.cli.HandleIdentifier.equals(Object)",10,3,10
"org.apache.hive.service.cli.HandleIdentifier.getPublicId()",1,1,1
"org.apache.hive.service.cli.HandleIdentifier.getSecretId()",1,1,1
"org.apache.hive.service.cli.HandleIdentifier.hashCode()",1,3,3
"org.apache.hive.service.cli.HandleIdentifier.toString()",1,1,1
"org.apache.hive.service.cli.HandleIdentifier.toTHandleIdentifier()",1,1,1
"org.apache.hive.service.cli.HiveSQLException.HiveSQLException()",1,1,1
"org.apache.hive.service.cli.HiveSQLException.HiveSQLException(String)",1,1,1
"org.apache.hive.service.cli.HiveSQLException.HiveSQLException(String,String)",1,1,1
"org.apache.hive.service.cli.HiveSQLException.HiveSQLException(String,String,Throwable)",1,1,1
"org.apache.hive.service.cli.HiveSQLException.HiveSQLException(String,String,int)",1,1,1
"org.apache.hive.service.cli.HiveSQLException.HiveSQLException(String,String,int,Throwable)",1,1,1
"org.apache.hive.service.cli.HiveSQLException.HiveSQLException(String,Throwable)",1,1,1
"org.apache.hive.service.cli.HiveSQLException.HiveSQLException(TStatus)",1,2,2
"org.apache.hive.service.cli.HiveSQLException.HiveSQLException(Throwable)",1,1,1
"org.apache.hive.service.cli.HiveSQLException.enroll(Throwable,StackTraceElement[],int)",1,2,3
"org.apache.hive.service.cli.HiveSQLException.newInstance(String,String)",1,1,2
"org.apache.hive.service.cli.HiveSQLException.toCause(List<String>)",1,1,1
"org.apache.hive.service.cli.HiveSQLException.toStackTrace(List<String>,StackTraceElement[],int)",2,4,6
"org.apache.hive.service.cli.HiveSQLException.toString(Throwable)",1,1,1
"org.apache.hive.service.cli.HiveSQLException.toString(Throwable,StackTraceElement[])",1,5,6
"org.apache.hive.service.cli.HiveSQLException.toTStatus()",1,1,1
"org.apache.hive.service.cli.HiveSQLException.toTStatus(Exception)",2,2,2
"org.apache.hive.service.cli.OperationHandle.OperationHandle(OperationType,TProtocolVersion)",1,1,1
"org.apache.hive.service.cli.OperationHandle.OperationHandle(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.OperationHandle.OperationHandle(TOperationHandle,TProtocolVersion)",1,1,1
"org.apache.hive.service.cli.OperationHandle.equals(Object)",5,1,5
"org.apache.hive.service.cli.OperationHandle.getOperationType()",1,1,1
"org.apache.hive.service.cli.OperationHandle.getProtocolVersion()",1,1,1
"org.apache.hive.service.cli.OperationHandle.hasResultSet()",1,1,1
"org.apache.hive.service.cli.OperationHandle.hashCode()",1,2,2
"org.apache.hive.service.cli.OperationHandle.setHasResultSet(boolean)",1,1,1
"org.apache.hive.service.cli.OperationHandle.toString()",1,1,1
"org.apache.hive.service.cli.OperationHandle.toTOperationHandle()",1,1,1
"org.apache.hive.service.cli.OperationState.OperationState(TOperationState)",1,1,1
"org.apache.hive.service.cli.OperationState.getOperationState(TOperationState)",3,2,3
"org.apache.hive.service.cli.OperationState.toTOperationState()",1,1,1
"org.apache.hive.service.cli.OperationState.validateTransition(OperationState)",1,1,1
"org.apache.hive.service.cli.OperationState.validateTransition(OperationState,OperationState)",7,5,10
"org.apache.hive.service.cli.OperationStatus.OperationStatus(OperationState,HiveSQLException)",1,1,1
"org.apache.hive.service.cli.OperationStatus.getOperationException()",1,1,1
"org.apache.hive.service.cli.OperationStatus.getState()",1,1,1
"org.apache.hive.service.cli.OperationType.OperationType(TOperationType)",1,1,1
"org.apache.hive.service.cli.OperationType.getOperationType(TOperationType)",3,2,3
"org.apache.hive.service.cli.OperationType.toTOperationType()",1,1,1
"org.apache.hive.service.cli.PatternOrIdentifier.PatternOrIdentifier(String)",1,1,1
"org.apache.hive.service.cli.PatternOrIdentifier.isIdentifier()",1,1,1
"org.apache.hive.service.cli.PatternOrIdentifier.isPattern()",1,1,1
"org.apache.hive.service.cli.PatternOrIdentifier.toString()",1,1,1
"org.apache.hive.service.cli.RowBasedSet.RemovableList.RemovableList()",1,1,1
"org.apache.hive.service.cli.RowBasedSet.RemovableList.RemovableList(List<E>)",1,1,1
"org.apache.hive.service.cli.RowBasedSet.RemovableList.removeRange(int,int)",1,1,1
"org.apache.hive.service.cli.RowBasedSet.RowBasedSet(TRowSet)",1,1,1
"org.apache.hive.service.cli.RowBasedSet.RowBasedSet(TableSchema)",1,1,1
"org.apache.hive.service.cli.RowBasedSet.RowBasedSet(Type[],List<TRow>,long)",1,1,1
"org.apache.hive.service.cli.RowBasedSet.addRow(Object[])",1,2,2
"org.apache.hive.service.cli.RowBasedSet.extractSubset(int)",1,1,1
"org.apache.hive.service.cli.RowBasedSet.getSize()",1,1,1
"org.apache.hive.service.cli.RowBasedSet.getStartOffset()",1,1,1
"org.apache.hive.service.cli.RowBasedSet.iterator()",1,2,2
"org.apache.hive.service.cli.RowBasedSet.numColumns()",1,2,2
"org.apache.hive.service.cli.RowBasedSet.numRows()",1,1,1
"org.apache.hive.service.cli.RowBasedSet.setStartOffset(long)",1,1,1
"org.apache.hive.service.cli.RowBasedSet.toTRowSet()",1,1,1
"org.apache.hive.service.cli.RowSetFactory.create(TRowSet,TProtocolVersion)",2,1,2
"org.apache.hive.service.cli.RowSetFactory.create(TableSchema,TProtocolVersion)",2,1,2
"org.apache.hive.service.cli.SessionHandle.SessionHandle(TProtocolVersion)",1,1,1
"org.apache.hive.service.cli.SessionHandle.SessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.SessionHandle.SessionHandle(TSessionHandle,TProtocolVersion)",1,1,1
"org.apache.hive.service.cli.SessionHandle.getProtocolVersion()",1,1,1
"org.apache.hive.service.cli.SessionHandle.getSessionId()",1,1,1
"org.apache.hive.service.cli.SessionHandle.toString()",1,1,1
"org.apache.hive.service.cli.SessionHandle.toTSessionHandle()",1,1,1
"org.apache.hive.service.cli.TableSchema.TableSchema()",1,1,1
"org.apache.hive.service.cli.TableSchema.TableSchema(List<FieldSchema>)",1,2,2
"org.apache.hive.service.cli.TableSchema.TableSchema(Schema)",1,1,1
"org.apache.hive.service.cli.TableSchema.TableSchema(TTableSchema)",1,2,2
"org.apache.hive.service.cli.TableSchema.TableSchema(int)",1,1,1
"org.apache.hive.service.cli.TableSchema.addPrimitiveColumn(String,Type,String)",1,1,1
"org.apache.hive.service.cli.TableSchema.addStringColumn(String,String)",1,1,1
"org.apache.hive.service.cli.TableSchema.clear()",1,1,1
"org.apache.hive.service.cli.TableSchema.getColumnDescriptorAt(int)",1,1,1
"org.apache.hive.service.cli.TableSchema.getColumnDescriptors()",1,1,1
"org.apache.hive.service.cli.TableSchema.getSize()",1,1,1
"org.apache.hive.service.cli.TableSchema.toTTableSchema()",1,2,2
"org.apache.hive.service.cli.TableSchema.toTypes()",1,2,2
"org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.setUp()",1,1,1
"org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.setUpBeforeClass()",1,1,1
"org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.tearDown()",1,1,1
"org.apache.hive.service.cli.TestHiveSQLException.ex1()",1,1,1
"org.apache.hive.service.cli.TestHiveSQLException.ex2()",1,1,1
"org.apache.hive.service.cli.TestHiveSQLException.testExceptionMarshalling()",1,1,1
"org.apache.hive.service.cli.TestScratchDir.stageDirTest(String,String,boolean)",1,2,2
"org.apache.hive.service.cli.TestScratchDir.testLocalScratchDirs()",1,1,1
"org.apache.hive.service.cli.TestScratchDir.testResourceDirs()",1,1,1
"org.apache.hive.service.cli.TestScratchDir.testScratchDirs()",1,1,1
"org.apache.hive.service.cli.Type.Type(String,int,TTypeId)",1,1,1
"org.apache.hive.service.cli.Type.Type(String,int,TTypeId,boolean,boolean)",1,1,1
"org.apache.hive.service.cli.Type.Type(String,int,TTypeId,boolean,boolean,boolean)",1,1,1
"org.apache.hive.service.cli.Type.getCreateParams()",1,1,1
"org.apache.hive.service.cli.Type.getLiteralPrefix()",1,1,1
"org.apache.hive.service.cli.Type.getLiteralSuffix()",1,1,1
"org.apache.hive.service.cli.Type.getLocalizedName()",1,1,1
"org.apache.hive.service.cli.Type.getMaxPrecision()",9,2,9
"org.apache.hive.service.cli.Type.getMaximumScale()",1,1,1
"org.apache.hive.service.cli.Type.getMinimumScale()",1,1,1
"org.apache.hive.service.cli.Type.getName()",1,1,1
"org.apache.hive.service.cli.Type.getNullable()",1,1,1
"org.apache.hive.service.cli.Type.getNumPrecRadix()",2,1,2
"org.apache.hive.service.cli.Type.getSearchable()",2,1,2
"org.apache.hive.service.cli.Type.getType(String)",6,5,7
"org.apache.hive.service.cli.Type.getType(TTypeId)",3,2,3
"org.apache.hive.service.cli.Type.isAutoIncrement()",1,1,1
"org.apache.hive.service.cli.Type.isCaseSensitive()",3,2,3
"org.apache.hive.service.cli.Type.isCollectionType()",1,1,1
"org.apache.hive.service.cli.Type.isComplexType()",1,1,1
"org.apache.hive.service.cli.Type.isFixedPrecScale()",1,1,1
"org.apache.hive.service.cli.Type.isNumericType()",3,2,3
"org.apache.hive.service.cli.Type.isPrimitiveType()",1,1,1
"org.apache.hive.service.cli.Type.isQualifiedType()",1,1,1
"org.apache.hive.service.cli.Type.isUnsignedAttribute()",2,1,2
"org.apache.hive.service.cli.Type.toJavaSQLType()",1,1,1
"org.apache.hive.service.cli.Type.toTType()",1,1,1
"org.apache.hive.service.cli.TypeDescriptor.TypeDescriptor(String)",1,3,3
"org.apache.hive.service.cli.TypeDescriptor.TypeDescriptor(TTypeDesc)",1,2,2
"org.apache.hive.service.cli.TypeDescriptor.TypeDescriptor(Type)",1,1,1
"org.apache.hive.service.cli.TypeDescriptor.getColumnSize()",7,4,7
"org.apache.hive.service.cli.TypeDescriptor.getDecimalDigits()",7,3,7
"org.apache.hive.service.cli.TypeDescriptor.getPrecision()",2,2,2
"org.apache.hive.service.cli.TypeDescriptor.getType()",1,1,1
"org.apache.hive.service.cli.TypeDescriptor.getTypeName()",2,2,2
"org.apache.hive.service.cli.TypeDescriptor.getTypeQualifiers()",1,1,1
"org.apache.hive.service.cli.TypeDescriptor.setTypeQualifiers(TypeQualifiers)",1,1,1
"org.apache.hive.service.cli.TypeDescriptor.toTTypeDesc()",1,2,2
"org.apache.hive.service.cli.TypeQualifiers.TypeQualifiers()",1,1,1
"org.apache.hive.service.cli.TypeQualifiers.fromTTypeQualifiers(TTypeQualifiers)",1,5,5
"org.apache.hive.service.cli.TypeQualifiers.fromTypeInfo(PrimitiveTypeInfo)",1,4,4
"org.apache.hive.service.cli.TypeQualifiers.getCharacterMaximumLength()",1,1,1
"org.apache.hive.service.cli.TypeQualifiers.getPrecision()",1,1,1
"org.apache.hive.service.cli.TypeQualifiers.getScale()",1,1,1
"org.apache.hive.service.cli.TypeQualifiers.setCharacterMaximumLength(int)",1,1,1
"org.apache.hive.service.cli.TypeQualifiers.setPrecision(Integer)",1,1,1
"org.apache.hive.service.cli.TypeQualifiers.setScale(Integer)",1,1,1
"org.apache.hive.service.cli.TypeQualifiers.toTTypeQualifiers()",1,4,5
"org.apache.hive.service.cli.operation.ClassicTableTypeMapping.ClassicTableTypeMapping()",1,1,1
"org.apache.hive.service.cli.operation.ClassicTableTypeMapping.getTableTypeNames()",1,2,2
"org.apache.hive.service.cli.operation.ClassicTableTypeMapping.mapToClientType(String)",2,2,2
"org.apache.hive.service.cli.operation.ClassicTableTypeMapping.mapToHiveType(String)",2,2,2
"org.apache.hive.service.cli.operation.ExecuteStatementOperation.ExecuteStatementOperation(HiveSession,String,Map<String, String>,boolean)",1,1,1
"org.apache.hive.service.cli.operation.ExecuteStatementOperation.getConfOverlay()",1,1,1
"org.apache.hive.service.cli.operation.ExecuteStatementOperation.getStatement()",1,1,1
"org.apache.hive.service.cli.operation.ExecuteStatementOperation.newExecuteStatementOperation(HiveSession,String,Map<String, String>,boolean)",2,2,3
"org.apache.hive.service.cli.operation.ExecuteStatementOperation.setConfOverlay(Map<String, String>)",1,1,2
"org.apache.hive.service.cli.operation.GetCatalogsOperation.GetCatalogsOperation(HiveSession)",1,1,1
"org.apache.hive.service.cli.operation.GetCatalogsOperation.getNextRowSet(FetchOrientation,long)",1,2,2
"org.apache.hive.service.cli.operation.GetCatalogsOperation.getResultSetSchema()",1,1,1
"org.apache.hive.service.cli.operation.GetCatalogsOperation.runInternal()",1,1,1
"org.apache.hive.service.cli.operation.GetColumnsOperation.GetColumnsOperation(HiveSession,String,String,String,String)",1,1,1
"org.apache.hive.service.cli.operation.GetColumnsOperation.getNextRowSet(FetchOrientation,long)",1,2,2
"org.apache.hive.service.cli.operation.GetColumnsOperation.getResultSetSchema()",1,1,1
"org.apache.hive.service.cli.operation.GetColumnsOperation.runInternal()",5,7,8
"org.apache.hive.service.cli.operation.GetFunctionsOperation.GetFunctionsOperation(HiveSession,String,String,String)",1,1,1
"org.apache.hive.service.cli.operation.GetFunctionsOperation.getNextRowSet(FetchOrientation,long)",1,2,2
"org.apache.hive.service.cli.operation.GetFunctionsOperation.getResultSetSchema()",1,1,1
"org.apache.hive.service.cli.operation.GetFunctionsOperation.runInternal()",1,7,8
"org.apache.hive.service.cli.operation.GetSchemasOperation.GetSchemasOperation(HiveSession,String,String)",1,1,1
"org.apache.hive.service.cli.operation.GetSchemasOperation.getNextRowSet(FetchOrientation,long)",1,2,2
"org.apache.hive.service.cli.operation.GetSchemasOperation.getResultSetSchema()",1,1,1
"org.apache.hive.service.cli.operation.GetSchemasOperation.runInternal()",1,3,3
"org.apache.hive.service.cli.operation.GetTableTypesOperation.GetTableTypesOperation(HiveSession)",1,1,1
"org.apache.hive.service.cli.operation.GetTableTypesOperation.getNextRowSet(FetchOrientation,long)",1,2,2
"org.apache.hive.service.cli.operation.GetTableTypesOperation.getResultSetSchema()",1,1,1
"org.apache.hive.service.cli.operation.GetTableTypesOperation.runInternal()",1,3,3
"org.apache.hive.service.cli.operation.GetTablesOperation.GetTablesOperation(HiveSession,String,String,String,List<String>)",1,2,2
"org.apache.hive.service.cli.operation.GetTablesOperation.getNextRowSet(FetchOrientation,long)",1,2,2
"org.apache.hive.service.cli.operation.GetTablesOperation.getResultSetSchema()",1,1,1
"org.apache.hive.service.cli.operation.GetTablesOperation.runInternal()",1,6,6
"org.apache.hive.service.cli.operation.GetTypeInfoOperation.GetTypeInfoOperation(HiveSession)",1,1,1
"org.apache.hive.service.cli.operation.GetTypeInfoOperation.getNextRowSet(FetchOrientation,long)",1,2,2
"org.apache.hive.service.cli.operation.GetTypeInfoOperation.getResultSetSchema()",1,1,1
"org.apache.hive.service.cli.operation.GetTypeInfoOperation.runInternal()",1,3,3
"org.apache.hive.service.cli.operation.HiveCommandOperation.HiveCommandOperation(HiveSession,String,CommandProcessor,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.operation.HiveCommandOperation.cleanTmpFile()",1,1,1
"org.apache.hive.service.cli.operation.HiveCommandOperation.close()",1,1,1
"org.apache.hive.service.cli.operation.HiveCommandOperation.getNextRowSet(FetchOrientation,long)",1,3,3
"org.apache.hive.service.cli.operation.HiveCommandOperation.getResultSetSchema()",1,1,1
"org.apache.hive.service.cli.operation.HiveCommandOperation.readResults(int)",4,6,7
"org.apache.hive.service.cli.operation.HiveCommandOperation.resetResultReader()",1,2,2
"org.apache.hive.service.cli.operation.HiveCommandOperation.runInternal()",2,5,5
"org.apache.hive.service.cli.operation.HiveCommandOperation.setupSessionIO(SessionState)",1,3,3
"org.apache.hive.service.cli.operation.HiveCommandOperation.tearDownSessionIO()",1,1,1
"org.apache.hive.service.cli.operation.HiveTableTypeMapping.getTableTypeNames()",1,2,2
"org.apache.hive.service.cli.operation.HiveTableTypeMapping.mapToClientType(String)",1,1,1
"org.apache.hive.service.cli.operation.HiveTableTypeMapping.mapToHiveType(String)",1,1,1
"org.apache.hive.service.cli.operation.LogDivertAppender.LogDivertAppender(Layout,OperationManager)",1,1,1
"org.apache.hive.service.cli.operation.LogDivertAppender.NameExclusionFilter.NameExclusionFilter(String)",1,1,1
"org.apache.hive.service.cli.operation.LogDivertAppender.NameExclusionFilter.decide(LoggingEvent)",2,1,2
"org.apache.hive.service.cli.operation.LogDivertAppender.subAppend(LoggingEvent)",2,2,2
"org.apache.hive.service.cli.operation.MetadataOperation.MetadataOperation(HiveSession,OperationType)",1,1,1
"org.apache.hive.service.cli.operation.MetadataOperation.close()",1,1,1
"org.apache.hive.service.cli.operation.MetadataOperation.convertIdentifierPattern(String,boolean)",2,2,2
"org.apache.hive.service.cli.operation.MetadataOperation.convertPattern(String,boolean)",1,1,2
"org.apache.hive.service.cli.operation.MetadataOperation.convertSchemaPattern(String)",2,3,3
"org.apache.hive.service.cli.operation.Operation.Operation(HiveSession,OperationType,boolean)",1,1,1
"org.apache.hive.service.cli.operation.Operation.afterRun()",1,1,1
"org.apache.hive.service.cli.operation.Operation.assertState(OperationState)",2,1,2
"org.apache.hive.service.cli.operation.Operation.beforeRun()",1,1,1
"org.apache.hive.service.cli.operation.Operation.cancel()",1,1,1
"org.apache.hive.service.cli.operation.Operation.cleanupOperationLog()",1,3,3
"org.apache.hive.service.cli.operation.Operation.createOperationLog()",4,8,8
"org.apache.hive.service.cli.operation.Operation.getBackgroundHandle()",1,1,1
"org.apache.hive.service.cli.operation.Operation.getConfiguration()",1,1,1
"org.apache.hive.service.cli.operation.Operation.getHandle()",1,1,1
"org.apache.hive.service.cli.operation.Operation.getNextRowSet()",1,1,1
"org.apache.hive.service.cli.operation.Operation.getOperationLog()",1,1,1
"org.apache.hive.service.cli.operation.Operation.getParentSession()",1,1,1
"org.apache.hive.service.cli.operation.Operation.getProtocolVersion()",1,1,1
"org.apache.hive.service.cli.operation.Operation.getStatus()",1,1,1
"org.apache.hive.service.cli.operation.Operation.getType()",1,1,1
"org.apache.hive.service.cli.operation.Operation.hasResultSet()",1,1,1
"org.apache.hive.service.cli.operation.Operation.isCanceled()",1,1,1
"org.apache.hive.service.cli.operation.Operation.isFailed()",1,1,1
"org.apache.hive.service.cli.operation.Operation.isFinished()",1,1,1
"org.apache.hive.service.cli.operation.Operation.isRunning()",1,1,1
"org.apache.hive.service.cli.operation.Operation.run()",1,1,1
"org.apache.hive.service.cli.operation.Operation.setBackgroundHandle(Future<?>)",1,1,1
"org.apache.hive.service.cli.operation.Operation.setConfiguration(HiveConf)",1,1,1
"org.apache.hive.service.cli.operation.Operation.setHasResultSet(boolean)",1,1,1
"org.apache.hive.service.cli.operation.Operation.setOperationException(HiveSQLException)",1,1,1
"org.apache.hive.service.cli.operation.Operation.setState(OperationState)",1,1,1
"org.apache.hive.service.cli.operation.Operation.shouldRunAsync()",1,1,1
"org.apache.hive.service.cli.operation.Operation.toSQLException(String,CommandProcessorResponse)",1,2,2
"org.apache.hive.service.cli.operation.Operation.unregisterOperationLog()",1,2,2
"org.apache.hive.service.cli.operation.Operation.validateDefaultFetchOrientation(FetchOrientation)",1,1,1
"org.apache.hive.service.cli.operation.Operation.validateFetchOrientation(FetchOrientation,EnumSet<FetchOrientation>)",2,2,2
"org.apache.hive.service.cli.operation.OperationLog.LogFile.LogFile(File)",1,1,1
"org.apache.hive.service.cli.operation.OperationLog.LogFile.read(FetchOrientation,long)",1,2,2
"org.apache.hive.service.cli.operation.OperationLog.LogFile.readResults(long)",6,8,9
"org.apache.hive.service.cli.operation.OperationLog.LogFile.remove()",1,2,2
"org.apache.hive.service.cli.operation.OperationLog.LogFile.resetIn()",1,2,2
"org.apache.hive.service.cli.operation.OperationLog.LogFile.write(String)",1,1,1
"org.apache.hive.service.cli.operation.OperationLog.OperationLog(String,File)",1,1,1
"org.apache.hive.service.cli.operation.OperationLog.close()",1,1,1
"org.apache.hive.service.cli.operation.OperationLog.getCurrentOperationLog()",1,1,1
"org.apache.hive.service.cli.operation.OperationLog.readOperationLog(FetchOrientation,long)",1,1,1
"org.apache.hive.service.cli.operation.OperationLog.removeCurrentOperationLog()",1,1,1
"org.apache.hive.service.cli.operation.OperationLog.setCurrentOperationLog(OperationLog)",1,1,1
"org.apache.hive.service.cli.operation.OperationLog.writeOperationLog(String)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.OperationManager()",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.addOperation(Operation)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.cancelOperation(OperationHandle)",1,2,6
"org.apache.hive.service.cli.operation.OperationManager.closeOperation(OperationHandle)",2,1,2
"org.apache.hive.service.cli.operation.OperationManager.getLogSchema()",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.getOperation(OperationHandle)",2,1,2
"org.apache.hive.service.cli.operation.OperationManager.getOperationLogByThread()",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.getOperationLogRowSet(OperationHandle,FetchOrientation,long)",2,2,3
"org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationHandle)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationHandle,FetchOrientation,long)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.getOperationResultSetSchema(OperationHandle)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.getOperationStatus(OperationHandle)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.init(HiveConf)",1,2,2
"org.apache.hive.service.cli.operation.OperationManager.initOperationLogCapture()",3,4,4
"org.apache.hive.service.cli.operation.OperationManager.newExecuteStatementOperation(HiveSession,String,Map<String, String>,boolean)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.newGetCatalogsOperation(HiveSession)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.newGetColumnsOperation(HiveSession,String,String,String,String)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.newGetFunctionsOperation(HiveSession,String,String,String)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.newGetSchemasOperation(HiveSession,String,String)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.newGetTableTypesOperation(HiveSession)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.newGetTablesOperation(HiveSession,String,String,String,List<String>)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.newGetTypeInfoOperation(HiveSession)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.removeOperation(OperationHandle)",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.start()",1,1,1
"org.apache.hive.service.cli.operation.OperationManager.stop()",1,1,1
"org.apache.hive.service.cli.operation.SQLOperation.SQLOperation(HiveSession,String,Map<String, String>,boolean)",1,1,1
"org.apache.hive.service.cli.operation.SQLOperation.cancel()",1,1,1
"org.apache.hive.service.cli.operation.SQLOperation.cleanup(OperationState)",1,5,5
"org.apache.hive.service.cli.operation.SQLOperation.close()",1,1,1
"org.apache.hive.service.cli.operation.SQLOperation.decode(List<Object>,RowSet)",2,2,2
"org.apache.hive.service.cli.operation.SQLOperation.decodeFromString(List<Object>,RowSet)",2,3,4
"org.apache.hive.service.cli.operation.SQLOperation.getConfigForOperation()",3,4,5
"org.apache.hive.service.cli.operation.SQLOperation.getCurrentUGI(HiveConf)",1,1,2
"org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(FetchOrientation,long)",2,4,7
"org.apache.hive.service.cli.operation.SQLOperation.getResultSetSchema()",1,2,2
"org.apache.hive.service.cli.operation.SQLOperation.getSerDe()",2,8,9
"org.apache.hive.service.cli.operation.SQLOperation.getSessionHive()",1,1,2
"org.apache.hive.service.cli.operation.SQLOperation.prepare(HiveConf)",6,8,9
"org.apache.hive.service.cli.operation.SQLOperation.prepareFromRow(List<Object>,RowSet)",1,2,2
"org.apache.hive.service.cli.operation.SQLOperation.registerCurrentOperationLog()",3,3,3
"org.apache.hive.service.cli.operation.SQLOperation.runInternal()",2,6,6
"org.apache.hive.service.cli.operation.SQLOperation.runQuery(HiveConf)",3,5,5
"org.apache.hive.service.cli.operation.TableTypeMappingFactory.getTableTypeMapping(String)",2,1,2
"org.apache.hive.service.cli.operation.TestOperationLoggingAPI.calculateProperMaxRows(int)",3,1,3
"org.apache.hive.service.cli.operation.TestOperationLoggingAPI.setUp()",1,1,1
"org.apache.hive.service.cli.operation.TestOperationLoggingAPI.setupSession()",1,1,1
"org.apache.hive.service.cli.operation.TestOperationLoggingAPI.tearDown()",1,1,1
"org.apache.hive.service.cli.operation.TestOperationLoggingAPI.testFetchResultsOfLog()",1,1,1
"org.apache.hive.service.cli.operation.TestOperationLoggingAPI.testFetchResultsOfLogAsync()",3,3,8
"org.apache.hive.service.cli.operation.TestOperationLoggingAPI.testFetchResultsOfLogCleanup()",1,4,4
"org.apache.hive.service.cli.operation.TestOperationLoggingAPI.testFetchResultsOfLogWithOrientation()",1,2,2
"org.apache.hive.service.cli.operation.TestOperationLoggingAPI.verifyFetchedLog(RowSet)",1,2,2
"org.apache.hive.service.cli.operation.TestOperationLoggingAPI.verifyFetchedLog(String)",1,2,2
"org.apache.hive.service.cli.session.HiveSessionHookContextImpl.HiveSessionHookContextImpl(HiveSession)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionHookContextImpl.getSessionConf()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionHookContextImpl.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionHookContextImpl.getSessionUser()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.GlobalHivercFileProcessor.loadFile(String)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.GlobalHivercFileProcessor.processCmd(String)",1,2,2
"org.apache.hive.service.cli.session.HiveSessionImpl.HiveSessionImpl(TProtocolVersion,String,String,HiveConf,String)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.acquire()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.cancelDelegationToken(HiveAuthFactory,String)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.cancelOperation(OperationHandle)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.cleanupSessionLogDir()",1,3,3
"org.apache.hive.service.cli.session.HiveSessionImpl.close()",1,4,5
"org.apache.hive.service.cli.session.HiveSessionImpl.closeOperation(OperationHandle)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.configureSession(Map<String, String>)",1,4,4
"org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(String,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(String,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(String,Map<String, String>,boolean)",1,3,3
"org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(OperationHandle,FetchOrientation,long,FetchType)",2,2,2
"org.apache.hive.service.cli.session.HiveSessionImpl.getCatalogs()",1,2,2
"org.apache.hive.service.cli.session.HiveSessionImpl.getColumns(String,String,String,String)",1,2,2
"org.apache.hive.service.cli.session.HiveSessionImpl.getDelegationToken(HiveAuthFactory,String,String)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.getFunctions(String,String,String)",1,2,2
"org.apache.hive.service.cli.session.HiveSessionImpl.getHiveConf()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.getInfo(GetInfoType)",8,3,8
"org.apache.hive.service.cli.session.HiveSessionImpl.getIpAddress()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.getMetaStoreClient()",2,2,3
"org.apache.hive.service.cli.session.HiveSessionImpl.getOperationLogSessionDir()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.getOperationManager()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.getPassword()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.getProtocolVersion()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.getResultSetMetadata(OperationHandle)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.getSchemas(String,String)",1,2,2
"org.apache.hive.service.cli.session.HiveSessionImpl.getSession()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.getSessionManager()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.getSessionState()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.getTableTypes()",1,2,2
"org.apache.hive.service.cli.session.HiveSessionImpl.getTables(String,String,String,List<String>)",1,2,2
"org.apache.hive.service.cli.session.HiveSessionImpl.getTypeInfo()",1,2,2
"org.apache.hive.service.cli.session.HiveSessionImpl.getUserFromToken(HiveAuthFactory,String)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.getUserName()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.getUsername()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.initialize(Map<String, String>)",1,2,2
"org.apache.hive.service.cli.session.HiveSessionImpl.isOperationLogEnabled()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.open()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.processGlobalInitFile()",1,5,5
"org.apache.hive.service.cli.session.HiveSessionImpl.release()",1,2,2
"org.apache.hive.service.cli.session.HiveSessionImpl.renewDelegationToken(HiveAuthFactory,String)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.setIpAddress(String)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.setOperationLogSessionDir(File)",1,4,4
"org.apache.hive.service.cli.session.HiveSessionImpl.setOperationManager(OperationManager)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.setSessionManager(SessionManager)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImpl.setUserName(String)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImplwithUGI.HiveSessionImplwithUGI(TProtocolVersion,String,String,HiveConf,String,String)",1,1,2
"org.apache.hive.service.cli.session.HiveSessionImplwithUGI.acquire()",1,2,2
"org.apache.hive.service.cli.session.HiveSessionImplwithUGI.cancelDelegationToken()",2,2,3
"org.apache.hive.service.cli.session.HiveSessionImplwithUGI.cancelDelegationToken(HiveAuthFactory,String)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImplwithUGI.close()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImplwithUGI.getDelegationToken()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImplwithUGI.getDelegationToken(HiveAuthFactory,String,String)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImplwithUGI.getSession()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImplwithUGI.getSessionUgi()",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImplwithUGI.renewDelegationToken(HiveAuthFactory,String)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImplwithUGI.setDelegationToken(String)",2,2,3
"org.apache.hive.service.cli.session.HiveSessionImplwithUGI.setProxySession(HiveSession)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionImplwithUGI.setSessionUGI(String)",3,2,4
"org.apache.hive.service.cli.session.HiveSessionProxy.HiveSessionProxy(HiveSession,UserGroupInformation)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionProxy.getProxy(HiveSession,UserGroupInformation)",1,1,1
"org.apache.hive.service.cli.session.HiveSessionProxy.invoke(Method,Object[])",2,3,5
"org.apache.hive.service.cli.session.HiveSessionProxy.invoke(Object,Method,Object[])",3,4,4
"org.apache.hive.service.cli.session.SessionManager.SessionManager()",1,1,1
"org.apache.hive.service.cli.session.SessionManager.applyAuthorizationConfigPolicy(HiveConf)",1,1,1
"org.apache.hive.service.cli.session.SessionManager.cleanupLoggingRootDir()",1,3,3
"org.apache.hive.service.cli.session.SessionManager.clearIpAddress()",1,1,1
"org.apache.hive.service.cli.session.SessionManager.clearProxyUserName()",1,1,1
"org.apache.hive.service.cli.session.SessionManager.clearUserName()",1,1,1
"org.apache.hive.service.cli.session.SessionManager.closeSession(SessionHandle)",2,1,2
"org.apache.hive.service.cli.session.SessionManager.createBackgroundOperationPool()",1,1,1
"org.apache.hive.service.cli.session.SessionManager.executeSessionHooks(HiveSession)",1,2,2
"org.apache.hive.service.cli.session.SessionManager.getIpAddress()",1,1,1
"org.apache.hive.service.cli.session.SessionManager.getOperationManager()",1,1,1
"org.apache.hive.service.cli.session.SessionManager.getProxyUserName()",1,1,1
"org.apache.hive.service.cli.session.SessionManager.getSession(SessionHandle)",2,1,2
"org.apache.hive.service.cli.session.SessionManager.getUserName()",1,1,1
"org.apache.hive.service.cli.session.SessionManager.init(HiveConf)",1,2,3
"org.apache.hive.service.cli.session.SessionManager.initOperationLogRootDir()",1,7,7
"org.apache.hive.service.cli.session.SessionManager.openSession(TProtocolVersion,String,String,String,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.session.SessionManager.openSession(TProtocolVersion,String,String,String,Map<String, String>,boolean,String)",1,3,5
"org.apache.hive.service.cli.session.SessionManager.setIpAddress(String)",1,1,1
"org.apache.hive.service.cli.session.SessionManager.setProxyUserName(String)",1,1,1
"org.apache.hive.service.cli.session.SessionManager.setUserName(String)",1,1,1
"org.apache.hive.service.cli.session.SessionManager.start()",1,1,1
"org.apache.hive.service.cli.session.SessionManager.stop()",1,3,3
"org.apache.hive.service.cli.session.SessionManager.submitBackgroundOperation(Runnable)",1,1,1
"org.apache.hive.service.cli.session.TestSessionGlobalInitFile.FakeEmbeddedThriftBinaryCLIService.FakeEmbeddedThriftBinaryCLIService(HiveConf)",1,1,1
"org.apache.hive.service.cli.session.TestSessionGlobalInitFile.FakeEmbeddedThriftBinaryCLIService.getService()",1,1,1
"org.apache.hive.service.cli.session.TestSessionGlobalInitFile.setUp()",1,1,1
"org.apache.hive.service.cli.session.TestSessionGlobalInitFile.tearDown()",1,1,1
"org.apache.hive.service.cli.session.TestSessionGlobalInitFile.testSessionGlobalInitFile()",1,1,1
"org.apache.hive.service.cli.session.TestSessionGlobalInitFile.testSessionGlobalInitFileAndConfOverlay()",1,1,1
"org.apache.hive.service.cli.session.TestSessionGlobalInitFile.testSessionGlobalInitFileWithUser()",1,1,1
"org.apache.hive.service.cli.session.TestSessionGlobalInitFile.verifyInitProperty(String,String,SessionHandle)",1,1,1
"org.apache.hive.service.cli.session.TestSessionHooks.SessionHookTest.run(HiveSessionHookContext)",1,1,1
"org.apache.hive.service.cli.session.TestSessionHooks.setUp()",1,1,1
"org.apache.hive.service.cli.session.TestSessionHooks.testProxyUser()",1,1,1
"org.apache.hive.service.cli.session.TestSessionHooks.testSessionHook()",1,1,1
"org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService.EmbeddedThriftBinaryCLIService()",1,1,1
"org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService.getService()",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.TArrayTypeEntry()",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.TArrayTypeEntry(TArrayTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.TArrayTypeEntry(int)",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.TArrayTypeEntryStandardScheme.read(TProtocol,TArrayTypeEntry)",4,4,6
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.TArrayTypeEntryStandardScheme.write(TProtocol,TArrayTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.TArrayTypeEntryStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.TArrayTypeEntryTupleScheme.read(TProtocol,TArrayTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.TArrayTypeEntryTupleScheme.write(TProtocol,TArrayTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.TArrayTypeEntryTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TArrayTypeEntry._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TArrayTypeEntry._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.compareTo(TArrayTypeEntry)",5,3,5
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.equals(TArrayTypeEntry)",5,1,7
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.getObjectTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.hashCode()",1,2,2
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.isSetObjectTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.setObjectTypePtr(int)",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.setObjectTypePtrIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.toString()",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.unsetObjectTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.validate()",2,2,2
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TArrayTypeEntry.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TBinaryColumn.TBinaryColumn()",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.TBinaryColumn(List<ByteBuffer>,ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.TBinaryColumn(TBinaryColumn)",1,4,4
"org.apache.hive.service.cli.thrift.TBinaryColumn.TBinaryColumnStandardScheme.read(TProtocol,TBinaryColumn)",4,6,9
"org.apache.hive.service.cli.thrift.TBinaryColumn.TBinaryColumnStandardScheme.write(TProtocol,TBinaryColumn)",1,4,4
"org.apache.hive.service.cli.thrift.TBinaryColumn.TBinaryColumnStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.TBinaryColumnTupleScheme.read(TProtocol,TBinaryColumn)",1,2,2
"org.apache.hive.service.cli.thrift.TBinaryColumn.TBinaryColumnTupleScheme.write(TProtocol,TBinaryColumn)",1,2,2
"org.apache.hive.service.cli.thrift.TBinaryColumn.TBinaryColumnTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TBinaryColumn._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TBinaryColumn._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.addToValues(ByteBuffer)",1,1,2
"org.apache.hive.service.cli.thrift.TBinaryColumn.bufferForNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.compareTo(TBinaryColumn)",8,4,8
"org.apache.hive.service.cli.thrift.TBinaryColumn.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TBinaryColumn.equals(TBinaryColumn)",8,7,16
"org.apache.hive.service.cli.thrift.TBinaryColumn.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TBinaryColumn.getNulls()",1,2,2
"org.apache.hive.service.cli.thrift.TBinaryColumn.getValues()",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.getValuesIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TBinaryColumn.getValuesSize()",1,2,2
"org.apache.hive.service.cli.thrift.TBinaryColumn.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TBinaryColumn.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TBinaryColumn.isSetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.isSetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TBinaryColumn.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TBinaryColumn.setNulls(ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.setNulls(byte[])",1,2,2
"org.apache.hive.service.cli.thrift.TBinaryColumn.setNullsIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TBinaryColumn.setValues(List<ByteBuffer>)",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.setValuesIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TBinaryColumn.toString()",1,4,4
"org.apache.hive.service.cli.thrift.TBinaryColumn.unsetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.unsetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.validate()",3,3,3
"org.apache.hive.service.cli.thrift.TBinaryColumn.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TBinaryColumn.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TBoolColumn.TBoolColumn()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.TBoolColumn(List<Boolean>,ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.TBoolColumn(TBoolColumn)",1,4,4
"org.apache.hive.service.cli.thrift.TBoolColumn.TBoolColumnStandardScheme.read(TProtocol,TBoolColumn)",4,6,9
"org.apache.hive.service.cli.thrift.TBoolColumn.TBoolColumnStandardScheme.write(TProtocol,TBoolColumn)",1,4,4
"org.apache.hive.service.cli.thrift.TBoolColumn.TBoolColumnStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.TBoolColumnTupleScheme.read(TProtocol,TBoolColumn)",1,2,2
"org.apache.hive.service.cli.thrift.TBoolColumn.TBoolColumnTupleScheme.write(TProtocol,TBoolColumn)",1,2,2
"org.apache.hive.service.cli.thrift.TBoolColumn.TBoolColumnTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TBoolColumn._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TBoolColumn._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.addToValues(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TBoolColumn.bufferForNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.compareTo(TBoolColumn)",8,4,8
"org.apache.hive.service.cli.thrift.TBoolColumn.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TBoolColumn.equals(TBoolColumn)",8,7,16
"org.apache.hive.service.cli.thrift.TBoolColumn.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TBoolColumn.getNulls()",1,2,2
"org.apache.hive.service.cli.thrift.TBoolColumn.getValues()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.getValuesIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TBoolColumn.getValuesSize()",1,2,2
"org.apache.hive.service.cli.thrift.TBoolColumn.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TBoolColumn.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TBoolColumn.isSetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.isSetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TBoolColumn.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TBoolColumn.setNulls(ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.setNulls(byte[])",1,2,2
"org.apache.hive.service.cli.thrift.TBoolColumn.setNullsIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TBoolColumn.setValues(List<Boolean>)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.setValuesIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TBoolColumn.toString()",1,4,4
"org.apache.hive.service.cli.thrift.TBoolColumn.unsetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.unsetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.validate()",3,3,3
"org.apache.hive.service.cli.thrift.TBoolColumn.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolColumn.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TBoolValue.TBoolValue()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.TBoolValue(TBoolValue)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.TBoolValueStandardScheme.read(TProtocol,TBoolValue)",4,4,6
"org.apache.hive.service.cli.thrift.TBoolValue.TBoolValueStandardScheme.write(TProtocol,TBoolValue)",1,2,2
"org.apache.hive.service.cli.thrift.TBoolValue.TBoolValueStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.TBoolValueTupleScheme.read(TProtocol,TBoolValue)",1,2,2
"org.apache.hive.service.cli.thrift.TBoolValue.TBoolValueTupleScheme.write(TProtocol,TBoolValue)",1,3,3
"org.apache.hive.service.cli.thrift.TBoolValue.TBoolValueTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TBoolValue._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TBoolValue._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.compareTo(TBoolValue)",5,3,5
"org.apache.hive.service.cli.thrift.TBoolValue.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TBoolValue.equals(TBoolValue)",5,3,9
"org.apache.hive.service.cli.thrift.TBoolValue.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TBoolValue.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TBoolValue.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TBoolValue.isSetValue()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.isValue()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TBoolValue.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TBoolValue.setValue(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.setValueIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TBoolValue.unsetValue()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.validate()",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TBoolValue.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TByteColumn.TByteColumn()",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.TByteColumn(List<Byte>,ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.TByteColumn(TByteColumn)",1,4,4
"org.apache.hive.service.cli.thrift.TByteColumn.TByteColumnStandardScheme.read(TProtocol,TByteColumn)",4,6,9
"org.apache.hive.service.cli.thrift.TByteColumn.TByteColumnStandardScheme.write(TProtocol,TByteColumn)",1,4,4
"org.apache.hive.service.cli.thrift.TByteColumn.TByteColumnStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.TByteColumnTupleScheme.read(TProtocol,TByteColumn)",1,2,2
"org.apache.hive.service.cli.thrift.TByteColumn.TByteColumnTupleScheme.write(TProtocol,TByteColumn)",1,2,2
"org.apache.hive.service.cli.thrift.TByteColumn.TByteColumnTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TByteColumn._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TByteColumn._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.addToValues(byte)",1,1,2
"org.apache.hive.service.cli.thrift.TByteColumn.bufferForNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.compareTo(TByteColumn)",8,4,8
"org.apache.hive.service.cli.thrift.TByteColumn.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TByteColumn.equals(TByteColumn)",8,7,16
"org.apache.hive.service.cli.thrift.TByteColumn.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TByteColumn.getNulls()",1,2,2
"org.apache.hive.service.cli.thrift.TByteColumn.getValues()",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.getValuesIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TByteColumn.getValuesSize()",1,2,2
"org.apache.hive.service.cli.thrift.TByteColumn.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TByteColumn.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TByteColumn.isSetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.isSetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TByteColumn.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TByteColumn.setNulls(ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.setNulls(byte[])",1,2,2
"org.apache.hive.service.cli.thrift.TByteColumn.setNullsIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TByteColumn.setValues(List<Byte>)",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.setValuesIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TByteColumn.toString()",1,4,4
"org.apache.hive.service.cli.thrift.TByteColumn.unsetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.unsetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.validate()",3,3,3
"org.apache.hive.service.cli.thrift.TByteColumn.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TByteColumn.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TByteValue.TByteValue()",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.TByteValue(TByteValue)",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.TByteValueStandardScheme.read(TProtocol,TByteValue)",4,4,6
"org.apache.hive.service.cli.thrift.TByteValue.TByteValueStandardScheme.write(TProtocol,TByteValue)",1,2,2
"org.apache.hive.service.cli.thrift.TByteValue.TByteValueStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.TByteValueTupleScheme.read(TProtocol,TByteValue)",1,2,2
"org.apache.hive.service.cli.thrift.TByteValue.TByteValueTupleScheme.write(TProtocol,TByteValue)",1,3,3
"org.apache.hive.service.cli.thrift.TByteValue.TByteValueTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TByteValue._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TByteValue._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.compareTo(TByteValue)",5,3,5
"org.apache.hive.service.cli.thrift.TByteValue.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TByteValue.equals(TByteValue)",5,3,9
"org.apache.hive.service.cli.thrift.TByteValue.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TByteValue.getValue()",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TByteValue.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TByteValue.isSetValue()",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TByteValue.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TByteValue.setValue(byte)",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.setValueIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TByteValue.unsetValue()",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.validate()",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TByteValue.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.AsyncClient(TProtocolFactory,TAsyncClientManager,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CancelDelegationToken(TCancelDelegationTokenReq,AsyncMethodCallback<CancelDelegationToken_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CancelDelegationToken_call.CancelDelegationToken_call(TCancelDelegationTokenReq,AsyncMethodCallback<CancelDelegationToken_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CancelDelegationToken_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CancelDelegationToken_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CancelOperation(TCancelOperationReq,AsyncMethodCallback<CancelOperation_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CancelOperation_call.CancelOperation_call(TCancelOperationReq,AsyncMethodCallback<CancelOperation_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CancelOperation_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CancelOperation_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CloseOperation(TCloseOperationReq,AsyncMethodCallback<CloseOperation_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CloseOperation_call.CloseOperation_call(TCloseOperationReq,AsyncMethodCallback<CloseOperation_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CloseOperation_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CloseOperation_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CloseSession(TCloseSessionReq,AsyncMethodCallback<CloseSession_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CloseSession_call.CloseSession_call(TCloseSessionReq,AsyncMethodCallback<CloseSession_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CloseSession_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.CloseSession_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.ExecuteStatement(TExecuteStatementReq,AsyncMethodCallback<ExecuteStatement_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.ExecuteStatement_call.ExecuteStatement_call(TExecuteStatementReq,AsyncMethodCallback<ExecuteStatement_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.ExecuteStatement_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.ExecuteStatement_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.Factory.Factory(TAsyncClientManager,TProtocolFactory)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.Factory.getAsyncClient(TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.FetchResults(TFetchResultsReq,AsyncMethodCallback<FetchResults_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.FetchResults_call.FetchResults_call(TFetchResultsReq,AsyncMethodCallback<FetchResults_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.FetchResults_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.FetchResults_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetCatalogs(TGetCatalogsReq,AsyncMethodCallback<GetCatalogs_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetCatalogs_call.GetCatalogs_call(TGetCatalogsReq,AsyncMethodCallback<GetCatalogs_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetCatalogs_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetCatalogs_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetColumns(TGetColumnsReq,AsyncMethodCallback<GetColumns_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetColumns_call.GetColumns_call(TGetColumnsReq,AsyncMethodCallback<GetColumns_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetColumns_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetColumns_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetDelegationToken(TGetDelegationTokenReq,AsyncMethodCallback<GetDelegationToken_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetDelegationToken_call.GetDelegationToken_call(TGetDelegationTokenReq,AsyncMethodCallback<GetDelegationToken_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetDelegationToken_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetDelegationToken_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetFunctions(TGetFunctionsReq,AsyncMethodCallback<GetFunctions_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetFunctions_call.GetFunctions_call(TGetFunctionsReq,AsyncMethodCallback<GetFunctions_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetFunctions_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetFunctions_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetInfo(TGetInfoReq,AsyncMethodCallback<GetInfo_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetInfo_call.GetInfo_call(TGetInfoReq,AsyncMethodCallback<GetInfo_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetInfo_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetInfo_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetOperationStatus(TGetOperationStatusReq,AsyncMethodCallback<GetOperationStatus_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetOperationStatus_call.GetOperationStatus_call(TGetOperationStatusReq,AsyncMethodCallback<GetOperationStatus_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetOperationStatus_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetOperationStatus_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetResultSetMetadata(TGetResultSetMetadataReq,AsyncMethodCallback<GetResultSetMetadata_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetResultSetMetadata_call.GetResultSetMetadata_call(TGetResultSetMetadataReq,AsyncMethodCallback<GetResultSetMetadata_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetResultSetMetadata_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetResultSetMetadata_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetSchemas(TGetSchemasReq,AsyncMethodCallback<GetSchemas_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetSchemas_call.GetSchemas_call(TGetSchemasReq,AsyncMethodCallback<GetSchemas_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetSchemas_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetSchemas_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetTableTypes(TGetTableTypesReq,AsyncMethodCallback<GetTableTypes_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetTableTypes_call.GetTableTypes_call(TGetTableTypesReq,AsyncMethodCallback<GetTableTypes_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetTableTypes_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetTableTypes_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetTables(TGetTablesReq,AsyncMethodCallback<GetTables_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetTables_call.GetTables_call(TGetTablesReq,AsyncMethodCallback<GetTables_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetTables_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetTables_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetTypeInfo(TGetTypeInfoReq,AsyncMethodCallback<GetTypeInfo_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetTypeInfo_call.GetTypeInfo_call(TGetTypeInfoReq,AsyncMethodCallback<GetTypeInfo_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetTypeInfo_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.GetTypeInfo_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.OpenSession(TOpenSessionReq,AsyncMethodCallback<OpenSession_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.OpenSession_call.OpenSession_call(TOpenSessionReq,AsyncMethodCallback<OpenSession_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.OpenSession_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.OpenSession_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.RenewDelegationToken(TRenewDelegationTokenReq,AsyncMethodCallback<RenewDelegationToken_call>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.RenewDelegationToken_call.RenewDelegationToken_call(TRenewDelegationTokenReq,AsyncMethodCallback<RenewDelegationToken_call>,TAsyncClient,TProtocolFactory,TNonblockingTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.RenewDelegationToken_call.getResult()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.AsyncClient.RenewDelegationToken_call.write_args(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.CancelDelegationToken_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.CancelDelegationToken_args(CancelDelegationToken_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.CancelDelegationToken_args(TCancelDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.CancelDelegationToken_argsStandardScheme.read(TProtocol,CancelDelegationToken_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.CancelDelegationToken_argsStandardScheme.write(TProtocol,CancelDelegationToken_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.CancelDelegationToken_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.CancelDelegationToken_argsTupleScheme.read(TProtocol,CancelDelegationToken_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.CancelDelegationToken_argsTupleScheme.write(TProtocol,CancelDelegationToken_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.CancelDelegationToken_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.compareTo(CancelDelegationToken_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.equals(CancelDelegationToken_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.setReq(TCancelDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.CancelDelegationToken_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.CancelDelegationToken_result(CancelDelegationToken_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.CancelDelegationToken_result(TCancelDelegationTokenResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.CancelDelegationToken_resultStandardScheme.read(TProtocol,CancelDelegationToken_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.CancelDelegationToken_resultStandardScheme.write(TProtocol,CancelDelegationToken_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.CancelDelegationToken_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.CancelDelegationToken_resultTupleScheme.read(TProtocol,CancelDelegationToken_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.CancelDelegationToken_resultTupleScheme.write(TProtocol,CancelDelegationToken_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.CancelDelegationToken_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.compareTo(CancelDelegationToken_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.equals(CancelDelegationToken_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.setSuccess(TCancelDelegationTokenResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelDelegationToken_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.CancelOperation_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.CancelOperation_args(CancelOperation_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.CancelOperation_args(TCancelOperationReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.CancelOperation_argsStandardScheme.read(TProtocol,CancelOperation_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.CancelOperation_argsStandardScheme.write(TProtocol,CancelOperation_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.CancelOperation_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.CancelOperation_argsTupleScheme.read(TProtocol,CancelOperation_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.CancelOperation_argsTupleScheme.write(TProtocol,CancelOperation_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.CancelOperation_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.compareTo(CancelOperation_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.equals(CancelOperation_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.setReq(TCancelOperationReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.CancelOperation_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.CancelOperation_result(CancelOperation_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.CancelOperation_result(TCancelOperationResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.CancelOperation_resultStandardScheme.read(TProtocol,CancelOperation_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.CancelOperation_resultStandardScheme.write(TProtocol,CancelOperation_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.CancelOperation_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.CancelOperation_resultTupleScheme.read(TProtocol,CancelOperation_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.CancelOperation_resultTupleScheme.write(TProtocol,CancelOperation_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.CancelOperation_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.compareTo(CancelOperation_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.equals(CancelOperation_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.setSuccess(TCancelOperationResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CancelOperation_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.CancelDelegationToken(TCancelDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.CancelOperation(TCancelOperationReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.Client(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.Client(TProtocol,TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.CloseOperation(TCloseOperationReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.CloseSession(TCloseSessionReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.ExecuteStatement(TExecuteStatementReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.Factory.Factory()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.Factory.getClient(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.Factory.getClient(TProtocol,TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.FetchResults(TFetchResultsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.GetCatalogs(TGetCatalogsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.GetColumns(TGetColumnsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.GetDelegationToken(TGetDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.GetFunctions(TGetFunctionsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.GetInfo(TGetInfoReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.GetOperationStatus(TGetOperationStatusReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.GetResultSetMetadata(TGetResultSetMetadataReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.GetSchemas(TGetSchemasReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.GetTableTypes(TGetTableTypesReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.GetTables(TGetTablesReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.GetTypeInfo(TGetTypeInfoReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.OpenSession(TOpenSessionReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.RenewDelegationToken(TRenewDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_CancelDelegationToken()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_CancelOperation()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_CloseOperation()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_CloseSession()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_ExecuteStatement()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_FetchResults()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_GetCatalogs()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_GetColumns()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_GetDelegationToken()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_GetFunctions()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_GetInfo()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_GetOperationStatus()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_GetResultSetMetadata()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_GetSchemas()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_GetTableTypes()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_GetTables()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_GetTypeInfo()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_OpenSession()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.recv_RenewDelegationToken()",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_CancelDelegationToken(TCancelDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_CancelOperation(TCancelOperationReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_CloseOperation(TCloseOperationReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_CloseSession(TCloseSessionReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_ExecuteStatement(TExecuteStatementReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_FetchResults(TFetchResultsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_GetCatalogs(TGetCatalogsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_GetColumns(TGetColumnsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_GetDelegationToken(TGetDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_GetFunctions(TGetFunctionsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_GetInfo(TGetInfoReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_GetOperationStatus(TGetOperationStatusReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_GetResultSetMetadata(TGetResultSetMetadataReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_GetSchemas(TGetSchemasReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_GetTableTypes(TGetTableTypesReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_GetTables(TGetTablesReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_GetTypeInfo(TGetTypeInfoReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_OpenSession(TOpenSessionReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Client.send_RenewDelegationToken(TRenewDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.CloseOperation_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.CloseOperation_args(CloseOperation_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.CloseOperation_args(TCloseOperationReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.CloseOperation_argsStandardScheme.read(TProtocol,CloseOperation_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.CloseOperation_argsStandardScheme.write(TProtocol,CloseOperation_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.CloseOperation_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.CloseOperation_argsTupleScheme.read(TProtocol,CloseOperation_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.CloseOperation_argsTupleScheme.write(TProtocol,CloseOperation_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.CloseOperation_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.compareTo(CloseOperation_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.equals(CloseOperation_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.setReq(TCloseOperationReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.CloseOperation_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.CloseOperation_result(CloseOperation_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.CloseOperation_result(TCloseOperationResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.CloseOperation_resultStandardScheme.read(TProtocol,CloseOperation_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.CloseOperation_resultStandardScheme.write(TProtocol,CloseOperation_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.CloseOperation_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.CloseOperation_resultTupleScheme.read(TProtocol,CloseOperation_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.CloseOperation_resultTupleScheme.write(TProtocol,CloseOperation_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.CloseOperation_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.compareTo(CloseOperation_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.equals(CloseOperation_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.setSuccess(TCloseOperationResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseOperation_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.CloseSession_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.CloseSession_args(CloseSession_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.CloseSession_args(TCloseSessionReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.CloseSession_argsStandardScheme.read(TProtocol,CloseSession_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.CloseSession_argsStandardScheme.write(TProtocol,CloseSession_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.CloseSession_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.CloseSession_argsTupleScheme.read(TProtocol,CloseSession_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.CloseSession_argsTupleScheme.write(TProtocol,CloseSession_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.CloseSession_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.compareTo(CloseSession_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.equals(CloseSession_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.setReq(TCloseSessionReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.CloseSession_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.CloseSession_result(CloseSession_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.CloseSession_result(TCloseSessionResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.CloseSession_resultStandardScheme.read(TProtocol,CloseSession_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.CloseSession_resultStandardScheme.write(TProtocol,CloseSession_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.CloseSession_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.CloseSession_resultTupleScheme.read(TProtocol,CloseSession_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.CloseSession_resultTupleScheme.write(TProtocol,CloseSession_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.CloseSession_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.compareTo(CloseSession_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.equals(CloseSession_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.setSuccess(TCloseSessionResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.CloseSession_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.ExecuteStatement_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.ExecuteStatement_args(ExecuteStatement_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.ExecuteStatement_args(TExecuteStatementReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.ExecuteStatement_argsStandardScheme.read(TProtocol,ExecuteStatement_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.ExecuteStatement_argsStandardScheme.write(TProtocol,ExecuteStatement_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.ExecuteStatement_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.ExecuteStatement_argsTupleScheme.read(TProtocol,ExecuteStatement_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.ExecuteStatement_argsTupleScheme.write(TProtocol,ExecuteStatement_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.ExecuteStatement_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.compareTo(ExecuteStatement_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.equals(ExecuteStatement_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.setReq(TExecuteStatementReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.ExecuteStatement_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.ExecuteStatement_result(ExecuteStatement_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.ExecuteStatement_result(TExecuteStatementResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.ExecuteStatement_resultStandardScheme.read(TProtocol,ExecuteStatement_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.ExecuteStatement_resultStandardScheme.write(TProtocol,ExecuteStatement_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.ExecuteStatement_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.ExecuteStatement_resultTupleScheme.read(TProtocol,ExecuteStatement_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.ExecuteStatement_resultTupleScheme.write(TProtocol,ExecuteStatement_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.ExecuteStatement_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.compareTo(ExecuteStatement_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.equals(ExecuteStatement_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.setSuccess(TExecuteStatementResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.ExecuteStatement_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.FetchResults_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.FetchResults_args(FetchResults_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.FetchResults_args(TFetchResultsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.FetchResults_argsStandardScheme.read(TProtocol,FetchResults_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.FetchResults_argsStandardScheme.write(TProtocol,FetchResults_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.FetchResults_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.FetchResults_argsTupleScheme.read(TProtocol,FetchResults_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.FetchResults_argsTupleScheme.write(TProtocol,FetchResults_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.FetchResults_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.compareTo(FetchResults_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.equals(FetchResults_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.setReq(TFetchResultsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.FetchResults_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.FetchResults_result(FetchResults_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.FetchResults_result(TFetchResultsResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.FetchResults_resultStandardScheme.read(TProtocol,FetchResults_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.FetchResults_resultStandardScheme.write(TProtocol,FetchResults_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.FetchResults_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.FetchResults_resultTupleScheme.read(TProtocol,FetchResults_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.FetchResults_resultTupleScheme.write(TProtocol,FetchResults_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.FetchResults_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.compareTo(FetchResults_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.equals(FetchResults_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.setSuccess(TFetchResultsResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.FetchResults_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.GetCatalogs_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.GetCatalogs_args(GetCatalogs_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.GetCatalogs_args(TGetCatalogsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.GetCatalogs_argsStandardScheme.read(TProtocol,GetCatalogs_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.GetCatalogs_argsStandardScheme.write(TProtocol,GetCatalogs_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.GetCatalogs_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.GetCatalogs_argsTupleScheme.read(TProtocol,GetCatalogs_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.GetCatalogs_argsTupleScheme.write(TProtocol,GetCatalogs_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.GetCatalogs_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.compareTo(GetCatalogs_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.equals(GetCatalogs_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.setReq(TGetCatalogsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.GetCatalogs_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.GetCatalogs_result(GetCatalogs_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.GetCatalogs_result(TGetCatalogsResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.GetCatalogs_resultStandardScheme.read(TProtocol,GetCatalogs_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.GetCatalogs_resultStandardScheme.write(TProtocol,GetCatalogs_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.GetCatalogs_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.GetCatalogs_resultTupleScheme.read(TProtocol,GetCatalogs_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.GetCatalogs_resultTupleScheme.write(TProtocol,GetCatalogs_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.GetCatalogs_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.compareTo(GetCatalogs_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.equals(GetCatalogs_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.setSuccess(TGetCatalogsResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetCatalogs_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.GetColumns_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.GetColumns_args(GetColumns_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.GetColumns_args(TGetColumnsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.GetColumns_argsStandardScheme.read(TProtocol,GetColumns_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.GetColumns_argsStandardScheme.write(TProtocol,GetColumns_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.GetColumns_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.GetColumns_argsTupleScheme.read(TProtocol,GetColumns_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.GetColumns_argsTupleScheme.write(TProtocol,GetColumns_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.GetColumns_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.compareTo(GetColumns_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.equals(GetColumns_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.setReq(TGetColumnsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.GetColumns_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.GetColumns_result(GetColumns_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.GetColumns_result(TGetColumnsResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.GetColumns_resultStandardScheme.read(TProtocol,GetColumns_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.GetColumns_resultStandardScheme.write(TProtocol,GetColumns_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.GetColumns_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.GetColumns_resultTupleScheme.read(TProtocol,GetColumns_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.GetColumns_resultTupleScheme.write(TProtocol,GetColumns_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.GetColumns_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.compareTo(GetColumns_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.equals(GetColumns_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.setSuccess(TGetColumnsResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetColumns_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.GetDelegationToken_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.GetDelegationToken_args(GetDelegationToken_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.GetDelegationToken_args(TGetDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.GetDelegationToken_argsStandardScheme.read(TProtocol,GetDelegationToken_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.GetDelegationToken_argsStandardScheme.write(TProtocol,GetDelegationToken_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.GetDelegationToken_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.GetDelegationToken_argsTupleScheme.read(TProtocol,GetDelegationToken_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.GetDelegationToken_argsTupleScheme.write(TProtocol,GetDelegationToken_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.GetDelegationToken_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.compareTo(GetDelegationToken_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.equals(GetDelegationToken_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.setReq(TGetDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.GetDelegationToken_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.GetDelegationToken_result(GetDelegationToken_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.GetDelegationToken_result(TGetDelegationTokenResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.GetDelegationToken_resultStandardScheme.read(TProtocol,GetDelegationToken_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.GetDelegationToken_resultStandardScheme.write(TProtocol,GetDelegationToken_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.GetDelegationToken_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.GetDelegationToken_resultTupleScheme.read(TProtocol,GetDelegationToken_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.GetDelegationToken_resultTupleScheme.write(TProtocol,GetDelegationToken_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.GetDelegationToken_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.compareTo(GetDelegationToken_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.equals(GetDelegationToken_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.setSuccess(TGetDelegationTokenResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetDelegationToken_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.GetFunctions_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.GetFunctions_args(GetFunctions_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.GetFunctions_args(TGetFunctionsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.GetFunctions_argsStandardScheme.read(TProtocol,GetFunctions_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.GetFunctions_argsStandardScheme.write(TProtocol,GetFunctions_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.GetFunctions_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.GetFunctions_argsTupleScheme.read(TProtocol,GetFunctions_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.GetFunctions_argsTupleScheme.write(TProtocol,GetFunctions_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.GetFunctions_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.compareTo(GetFunctions_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.equals(GetFunctions_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.setReq(TGetFunctionsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.GetFunctions_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.GetFunctions_result(GetFunctions_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.GetFunctions_result(TGetFunctionsResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.GetFunctions_resultStandardScheme.read(TProtocol,GetFunctions_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.GetFunctions_resultStandardScheme.write(TProtocol,GetFunctions_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.GetFunctions_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.GetFunctions_resultTupleScheme.read(TProtocol,GetFunctions_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.GetFunctions_resultTupleScheme.write(TProtocol,GetFunctions_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.GetFunctions_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.compareTo(GetFunctions_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.equals(GetFunctions_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.setSuccess(TGetFunctionsResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetFunctions_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.GetInfo_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.GetInfo_args(GetInfo_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.GetInfo_args(TGetInfoReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.GetInfo_argsStandardScheme.read(TProtocol,GetInfo_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.GetInfo_argsStandardScheme.write(TProtocol,GetInfo_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.GetInfo_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.GetInfo_argsTupleScheme.read(TProtocol,GetInfo_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.GetInfo_argsTupleScheme.write(TProtocol,GetInfo_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.GetInfo_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.compareTo(GetInfo_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.equals(GetInfo_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.setReq(TGetInfoReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.GetInfo_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.GetInfo_result(GetInfo_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.GetInfo_result(TGetInfoResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.GetInfo_resultStandardScheme.read(TProtocol,GetInfo_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.GetInfo_resultStandardScheme.write(TProtocol,GetInfo_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.GetInfo_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.GetInfo_resultTupleScheme.read(TProtocol,GetInfo_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.GetInfo_resultTupleScheme.write(TProtocol,GetInfo_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.GetInfo_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.compareTo(GetInfo_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.equals(GetInfo_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.setSuccess(TGetInfoResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetInfo_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.GetOperationStatus_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.GetOperationStatus_args(GetOperationStatus_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.GetOperationStatus_args(TGetOperationStatusReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.GetOperationStatus_argsStandardScheme.read(TProtocol,GetOperationStatus_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.GetOperationStatus_argsStandardScheme.write(TProtocol,GetOperationStatus_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.GetOperationStatus_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.GetOperationStatus_argsTupleScheme.read(TProtocol,GetOperationStatus_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.GetOperationStatus_argsTupleScheme.write(TProtocol,GetOperationStatus_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.GetOperationStatus_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.compareTo(GetOperationStatus_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.equals(GetOperationStatus_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.setReq(TGetOperationStatusReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.GetOperationStatus_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.GetOperationStatus_result(GetOperationStatus_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.GetOperationStatus_result(TGetOperationStatusResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.GetOperationStatus_resultStandardScheme.read(TProtocol,GetOperationStatus_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.GetOperationStatus_resultStandardScheme.write(TProtocol,GetOperationStatus_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.GetOperationStatus_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.GetOperationStatus_resultTupleScheme.read(TProtocol,GetOperationStatus_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.GetOperationStatus_resultTupleScheme.write(TProtocol,GetOperationStatus_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.GetOperationStatus_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.compareTo(GetOperationStatus_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.equals(GetOperationStatus_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.setSuccess(TGetOperationStatusResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetOperationStatus_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.GetResultSetMetadata_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.GetResultSetMetadata_args(GetResultSetMetadata_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.GetResultSetMetadata_args(TGetResultSetMetadataReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.GetResultSetMetadata_argsStandardScheme.read(TProtocol,GetResultSetMetadata_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.GetResultSetMetadata_argsStandardScheme.write(TProtocol,GetResultSetMetadata_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.GetResultSetMetadata_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.GetResultSetMetadata_argsTupleScheme.read(TProtocol,GetResultSetMetadata_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.GetResultSetMetadata_argsTupleScheme.write(TProtocol,GetResultSetMetadata_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.GetResultSetMetadata_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.compareTo(GetResultSetMetadata_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.equals(GetResultSetMetadata_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.setReq(TGetResultSetMetadataReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.GetResultSetMetadata_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.GetResultSetMetadata_result(GetResultSetMetadata_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.GetResultSetMetadata_result(TGetResultSetMetadataResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.GetResultSetMetadata_resultStandardScheme.read(TProtocol,GetResultSetMetadata_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.GetResultSetMetadata_resultStandardScheme.write(TProtocol,GetResultSetMetadata_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.GetResultSetMetadata_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.GetResultSetMetadata_resultTupleScheme.read(TProtocol,GetResultSetMetadata_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.GetResultSetMetadata_resultTupleScheme.write(TProtocol,GetResultSetMetadata_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.GetResultSetMetadata_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.compareTo(GetResultSetMetadata_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.equals(GetResultSetMetadata_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.setSuccess(TGetResultSetMetadataResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetResultSetMetadata_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.GetSchemas_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.GetSchemas_args(GetSchemas_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.GetSchemas_args(TGetSchemasReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.GetSchemas_argsStandardScheme.read(TProtocol,GetSchemas_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.GetSchemas_argsStandardScheme.write(TProtocol,GetSchemas_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.GetSchemas_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.GetSchemas_argsTupleScheme.read(TProtocol,GetSchemas_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.GetSchemas_argsTupleScheme.write(TProtocol,GetSchemas_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.GetSchemas_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.compareTo(GetSchemas_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.equals(GetSchemas_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.setReq(TGetSchemasReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.GetSchemas_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.GetSchemas_result(GetSchemas_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.GetSchemas_result(TGetSchemasResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.GetSchemas_resultStandardScheme.read(TProtocol,GetSchemas_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.GetSchemas_resultStandardScheme.write(TProtocol,GetSchemas_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.GetSchemas_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.GetSchemas_resultTupleScheme.read(TProtocol,GetSchemas_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.GetSchemas_resultTupleScheme.write(TProtocol,GetSchemas_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.GetSchemas_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.compareTo(GetSchemas_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.equals(GetSchemas_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.setSuccess(TGetSchemasResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetSchemas_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.GetTableTypes_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.GetTableTypes_args(GetTableTypes_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.GetTableTypes_args(TGetTableTypesReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.GetTableTypes_argsStandardScheme.read(TProtocol,GetTableTypes_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.GetTableTypes_argsStandardScheme.write(TProtocol,GetTableTypes_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.GetTableTypes_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.GetTableTypes_argsTupleScheme.read(TProtocol,GetTableTypes_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.GetTableTypes_argsTupleScheme.write(TProtocol,GetTableTypes_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.GetTableTypes_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.compareTo(GetTableTypes_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.equals(GetTableTypes_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.setReq(TGetTableTypesReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.GetTableTypes_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.GetTableTypes_result(GetTableTypes_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.GetTableTypes_result(TGetTableTypesResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.GetTableTypes_resultStandardScheme.read(TProtocol,GetTableTypes_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.GetTableTypes_resultStandardScheme.write(TProtocol,GetTableTypes_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.GetTableTypes_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.GetTableTypes_resultTupleScheme.read(TProtocol,GetTableTypes_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.GetTableTypes_resultTupleScheme.write(TProtocol,GetTableTypes_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.GetTableTypes_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.compareTo(GetTableTypes_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.equals(GetTableTypes_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.setSuccess(TGetTableTypesResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTableTypes_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.GetTables_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.GetTables_args(GetTables_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.GetTables_args(TGetTablesReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.GetTables_argsStandardScheme.read(TProtocol,GetTables_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.GetTables_argsStandardScheme.write(TProtocol,GetTables_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.GetTables_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.GetTables_argsTupleScheme.read(TProtocol,GetTables_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.GetTables_argsTupleScheme.write(TProtocol,GetTables_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.GetTables_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.compareTo(GetTables_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.equals(GetTables_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.setReq(TGetTablesReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.GetTables_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.GetTables_result(GetTables_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.GetTables_result(TGetTablesResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.GetTables_resultStandardScheme.read(TProtocol,GetTables_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.GetTables_resultStandardScheme.write(TProtocol,GetTables_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.GetTables_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.GetTables_resultTupleScheme.read(TProtocol,GetTables_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.GetTables_resultTupleScheme.write(TProtocol,GetTables_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.GetTables_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.compareTo(GetTables_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.equals(GetTables_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.setSuccess(TGetTablesResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTables_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.GetTypeInfo_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.GetTypeInfo_args(GetTypeInfo_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.GetTypeInfo_args(TGetTypeInfoReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.GetTypeInfo_argsStandardScheme.read(TProtocol,GetTypeInfo_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.GetTypeInfo_argsStandardScheme.write(TProtocol,GetTypeInfo_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.GetTypeInfo_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.GetTypeInfo_argsTupleScheme.read(TProtocol,GetTypeInfo_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.GetTypeInfo_argsTupleScheme.write(TProtocol,GetTypeInfo_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.GetTypeInfo_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.compareTo(GetTypeInfo_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.equals(GetTypeInfo_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.setReq(TGetTypeInfoReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.GetTypeInfo_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.GetTypeInfo_result(GetTypeInfo_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.GetTypeInfo_result(TGetTypeInfoResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.GetTypeInfo_resultStandardScheme.read(TProtocol,GetTypeInfo_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.GetTypeInfo_resultStandardScheme.write(TProtocol,GetTypeInfo_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.GetTypeInfo_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.GetTypeInfo_resultTupleScheme.read(TProtocol,GetTypeInfo_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.GetTypeInfo_resultTupleScheme.write(TProtocol,GetTypeInfo_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.GetTypeInfo_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.compareTo(GetTypeInfo_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.equals(GetTypeInfo_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.setSuccess(TGetTypeInfoResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.GetTypeInfo_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.OpenSession_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.OpenSession_args(OpenSession_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.OpenSession_args(TOpenSessionReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.OpenSession_argsStandardScheme.read(TProtocol,OpenSession_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.OpenSession_argsStandardScheme.write(TProtocol,OpenSession_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.OpenSession_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.OpenSession_argsTupleScheme.read(TProtocol,OpenSession_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.OpenSession_argsTupleScheme.write(TProtocol,OpenSession_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.OpenSession_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.compareTo(OpenSession_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.equals(OpenSession_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.setReq(TOpenSessionReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.OpenSession_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.OpenSession_result(OpenSession_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.OpenSession_result(TOpenSessionResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.OpenSession_resultStandardScheme.read(TProtocol,OpenSession_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.OpenSession_resultStandardScheme.write(TProtocol,OpenSession_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.OpenSession_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.OpenSession_resultTupleScheme.read(TProtocol,OpenSession_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.OpenSession_resultTupleScheme.write(TProtocol,OpenSession_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.OpenSession_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.compareTo(OpenSession_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.equals(OpenSession_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.setSuccess(TOpenSessionResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.OpenSession_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CancelDelegationToken.CancelDelegationToken()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CancelDelegationToken.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CancelDelegationToken.getResult(I,CancelDelegationToken_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CancelDelegationToken.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CancelOperation.CancelOperation()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CancelOperation.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CancelOperation.getResult(I,CancelOperation_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CancelOperation.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CloseOperation.CloseOperation()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CloseOperation.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CloseOperation.getResult(I,CloseOperation_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CloseOperation.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CloseSession.CloseSession()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CloseSession.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CloseSession.getResult(I,CloseSession_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.CloseSession.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.ExecuteStatement.ExecuteStatement()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.ExecuteStatement.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.ExecuteStatement.getResult(I,ExecuteStatement_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.ExecuteStatement.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.FetchResults.FetchResults()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.FetchResults.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.FetchResults.getResult(I,FetchResults_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.FetchResults.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetCatalogs.GetCatalogs()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetCatalogs.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetCatalogs.getResult(I,GetCatalogs_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetCatalogs.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetColumns.GetColumns()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetColumns.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetColumns.getResult(I,GetColumns_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetColumns.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetDelegationToken.GetDelegationToken()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetDelegationToken.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetDelegationToken.getResult(I,GetDelegationToken_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetDelegationToken.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetFunctions.GetFunctions()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetFunctions.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetFunctions.getResult(I,GetFunctions_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetFunctions.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetInfo.GetInfo()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetInfo.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetInfo.getResult(I,GetInfo_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetInfo.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetOperationStatus.GetOperationStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetOperationStatus.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetOperationStatus.getResult(I,GetOperationStatus_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetOperationStatus.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetResultSetMetadata.GetResultSetMetadata()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetResultSetMetadata.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetResultSetMetadata.getResult(I,GetResultSetMetadata_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetResultSetMetadata.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetSchemas.GetSchemas()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetSchemas.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetSchemas.getResult(I,GetSchemas_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetSchemas.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetTableTypes.GetTableTypes()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetTableTypes.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetTableTypes.getResult(I,GetTableTypes_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetTableTypes.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetTables.GetTables()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetTables.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetTables.getResult(I,GetTables_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetTables.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetTypeInfo.GetTypeInfo()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetTypeInfo.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetTypeInfo.getResult(I,GetTypeInfo_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.GetTypeInfo.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.OpenSession.OpenSession()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.OpenSession.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.OpenSession.getResult(I,OpenSession_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.OpenSession.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.Processor(I)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.Processor(I,Map<String, ProcessFunction<I, ? extends TBase>>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.RenewDelegationToken.RenewDelegationToken()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.RenewDelegationToken.getEmptyArgsInstance()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.RenewDelegationToken.getResult(I,RenewDelegationToken_args)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.RenewDelegationToken.isOneway()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.Processor.getProcessMap(Map<String, ProcessFunction<I, ? extends TBase>>)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.RenewDelegationToken_args()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.RenewDelegationToken_args(RenewDelegationToken_args)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.RenewDelegationToken_args(TRenewDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.RenewDelegationToken_argsStandardScheme.read(TProtocol,RenewDelegationToken_args)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.RenewDelegationToken_argsStandardScheme.write(TProtocol,RenewDelegationToken_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.RenewDelegationToken_argsStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.RenewDelegationToken_argsTupleScheme.read(TProtocol,RenewDelegationToken_args)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.RenewDelegationToken_argsTupleScheme.write(TProtocol,RenewDelegationToken_args)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.RenewDelegationToken_argsTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.compareTo(RenewDelegationToken_args)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.equals(RenewDelegationToken_args)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.getReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.isSetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.setReq(TRenewDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.setReqIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.unsetReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_args.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.RenewDelegationToken_result()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.RenewDelegationToken_result(RenewDelegationToken_result)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.RenewDelegationToken_result(TRenewDelegationTokenResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.RenewDelegationToken_resultStandardScheme.read(TProtocol,RenewDelegationToken_result)",4,4,6
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.RenewDelegationToken_resultStandardScheme.write(TProtocol,RenewDelegationToken_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.RenewDelegationToken_resultStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.RenewDelegationToken_resultTupleScheme.read(TProtocol,RenewDelegationToken_result)",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.RenewDelegationToken_resultTupleScheme.write(TProtocol,RenewDelegationToken_result)",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.RenewDelegationToken_resultTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.compareTo(RenewDelegationToken_result)",5,3,5
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.equals(RenewDelegationToken_result)",5,4,9
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.getSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.isSetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.setSuccess(TRenewDelegationTokenResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.setSuccessIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.unsetSuccess()",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.validate()",1,2,2
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCLIService.RenewDelegationToken_result.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.TCancelDelegationTokenReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.TCancelDelegationTokenReq(TCancelDelegationTokenReq)",1,1,3
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.TCancelDelegationTokenReq(TSessionHandle,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.TCancelDelegationTokenReqStandardScheme.read(TProtocol,TCancelDelegationTokenReq)",4,5,8
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.TCancelDelegationTokenReqStandardScheme.write(TProtocol,TCancelDelegationTokenReq)",1,3,3
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.TCancelDelegationTokenReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.TCancelDelegationTokenReqTupleScheme.read(TProtocol,TCancelDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.TCancelDelegationTokenReqTupleScheme.write(TProtocol,TCancelDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.TCancelDelegationTokenReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.compareTo(TCancelDelegationTokenReq)",8,4,8
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.equals(TCancelDelegationTokenReq)",8,7,16
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.getDelegationToken()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.isSetDelegationToken()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.isSetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.setDelegationToken(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.setDelegationTokenIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.setSessionHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.toString()",1,4,4
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.unsetDelegationToken()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.unsetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.validate()",3,4,4
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.TCancelDelegationTokenResp()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.TCancelDelegationTokenResp(TCancelDelegationTokenResp)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.TCancelDelegationTokenResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.TCancelDelegationTokenRespStandardScheme.read(TProtocol,TCancelDelegationTokenResp)",4,4,6
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.TCancelDelegationTokenRespStandardScheme.write(TProtocol,TCancelDelegationTokenResp)",1,2,2
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.TCancelDelegationTokenRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.TCancelDelegationTokenRespTupleScheme.read(TProtocol,TCancelDelegationTokenResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.TCancelDelegationTokenRespTupleScheme.write(TProtocol,TCancelDelegationTokenResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.TCancelDelegationTokenRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.compareTo(TCancelDelegationTokenResp)",5,3,5
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.equals(TCancelDelegationTokenResp)",5,4,9
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelDelegationTokenResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelOperationReq.TCancelOperationReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.TCancelOperationReq(TCancelOperationReq)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelOperationReq.TCancelOperationReq(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.TCancelOperationReqStandardScheme.read(TProtocol,TCancelOperationReq)",4,4,6
"org.apache.hive.service.cli.thrift.TCancelOperationReq.TCancelOperationReqStandardScheme.write(TProtocol,TCancelOperationReq)",1,2,2
"org.apache.hive.service.cli.thrift.TCancelOperationReq.TCancelOperationReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.TCancelOperationReqTupleScheme.read(TProtocol,TCancelOperationReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.TCancelOperationReqTupleScheme.write(TProtocol,TCancelOperationReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.TCancelOperationReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCancelOperationReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCancelOperationReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.compareTo(TCancelOperationReq)",5,3,5
"org.apache.hive.service.cli.thrift.TCancelOperationReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCancelOperationReq.equals(TCancelOperationReq)",5,4,9
"org.apache.hive.service.cli.thrift.TCancelOperationReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCancelOperationReq.getOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCancelOperationReq.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCancelOperationReq.isSetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelOperationReq.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCancelOperationReq.setOperationHandle(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.setOperationHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelOperationReq.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCancelOperationReq.unsetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TCancelOperationReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelOperationResp.TCancelOperationResp()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.TCancelOperationResp(TCancelOperationResp)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelOperationResp.TCancelOperationResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.TCancelOperationRespStandardScheme.read(TProtocol,TCancelOperationResp)",4,4,6
"org.apache.hive.service.cli.thrift.TCancelOperationResp.TCancelOperationRespStandardScheme.write(TProtocol,TCancelOperationResp)",1,2,2
"org.apache.hive.service.cli.thrift.TCancelOperationResp.TCancelOperationRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.TCancelOperationRespTupleScheme.read(TProtocol,TCancelOperationResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.TCancelOperationRespTupleScheme.write(TProtocol,TCancelOperationResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.TCancelOperationRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCancelOperationResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCancelOperationResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.compareTo(TCancelOperationResp)",5,3,5
"org.apache.hive.service.cli.thrift.TCancelOperationResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCancelOperationResp.equals(TCancelOperationResp)",5,4,9
"org.apache.hive.service.cli.thrift.TCancelOperationResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCancelOperationResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCancelOperationResp.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCancelOperationResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelOperationResp.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCancelOperationResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCancelOperationResp.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCancelOperationResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TCancelOperationResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCancelOperationResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseOperationReq.TCloseOperationReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.TCloseOperationReq(TCloseOperationReq)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseOperationReq.TCloseOperationReq(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.TCloseOperationReqStandardScheme.read(TProtocol,TCloseOperationReq)",4,4,6
"org.apache.hive.service.cli.thrift.TCloseOperationReq.TCloseOperationReqStandardScheme.write(TProtocol,TCloseOperationReq)",1,2,2
"org.apache.hive.service.cli.thrift.TCloseOperationReq.TCloseOperationReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.TCloseOperationReqTupleScheme.read(TProtocol,TCloseOperationReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.TCloseOperationReqTupleScheme.write(TProtocol,TCloseOperationReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.TCloseOperationReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCloseOperationReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCloseOperationReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.compareTo(TCloseOperationReq)",5,3,5
"org.apache.hive.service.cli.thrift.TCloseOperationReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCloseOperationReq.equals(TCloseOperationReq)",5,4,9
"org.apache.hive.service.cli.thrift.TCloseOperationReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCloseOperationReq.getOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCloseOperationReq.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCloseOperationReq.isSetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseOperationReq.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCloseOperationReq.setOperationHandle(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.setOperationHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseOperationReq.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCloseOperationReq.unsetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TCloseOperationReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseOperationResp.TCloseOperationResp()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.TCloseOperationResp(TCloseOperationResp)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseOperationResp.TCloseOperationResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.TCloseOperationRespStandardScheme.read(TProtocol,TCloseOperationResp)",4,4,6
"org.apache.hive.service.cli.thrift.TCloseOperationResp.TCloseOperationRespStandardScheme.write(TProtocol,TCloseOperationResp)",1,2,2
"org.apache.hive.service.cli.thrift.TCloseOperationResp.TCloseOperationRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.TCloseOperationRespTupleScheme.read(TProtocol,TCloseOperationResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.TCloseOperationRespTupleScheme.write(TProtocol,TCloseOperationResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.TCloseOperationRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCloseOperationResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCloseOperationResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.compareTo(TCloseOperationResp)",5,3,5
"org.apache.hive.service.cli.thrift.TCloseOperationResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCloseOperationResp.equals(TCloseOperationResp)",5,4,9
"org.apache.hive.service.cli.thrift.TCloseOperationResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCloseOperationResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCloseOperationResp.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCloseOperationResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseOperationResp.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCloseOperationResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseOperationResp.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCloseOperationResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TCloseOperationResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseOperationResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseSessionReq.TCloseSessionReq()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.TCloseSessionReq(TCloseSessionReq)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseSessionReq.TCloseSessionReq(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.TCloseSessionReqStandardScheme.read(TProtocol,TCloseSessionReq)",4,4,6
"org.apache.hive.service.cli.thrift.TCloseSessionReq.TCloseSessionReqStandardScheme.write(TProtocol,TCloseSessionReq)",1,2,2
"org.apache.hive.service.cli.thrift.TCloseSessionReq.TCloseSessionReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.TCloseSessionReqTupleScheme.read(TProtocol,TCloseSessionReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.TCloseSessionReqTupleScheme.write(TProtocol,TCloseSessionReq)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.TCloseSessionReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCloseSessionReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCloseSessionReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.compareTo(TCloseSessionReq)",5,3,5
"org.apache.hive.service.cli.thrift.TCloseSessionReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCloseSessionReq.equals(TCloseSessionReq)",5,4,9
"org.apache.hive.service.cli.thrift.TCloseSessionReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCloseSessionReq.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCloseSessionReq.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCloseSessionReq.isSetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseSessionReq.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCloseSessionReq.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.setSessionHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseSessionReq.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCloseSessionReq.unsetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TCloseSessionReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseSessionResp.TCloseSessionResp()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.TCloseSessionResp(TCloseSessionResp)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseSessionResp.TCloseSessionResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.TCloseSessionRespStandardScheme.read(TProtocol,TCloseSessionResp)",4,4,6
"org.apache.hive.service.cli.thrift.TCloseSessionResp.TCloseSessionRespStandardScheme.write(TProtocol,TCloseSessionResp)",1,2,2
"org.apache.hive.service.cli.thrift.TCloseSessionResp.TCloseSessionRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.TCloseSessionRespTupleScheme.read(TProtocol,TCloseSessionResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.TCloseSessionRespTupleScheme.write(TProtocol,TCloseSessionResp)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.TCloseSessionRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TCloseSessionResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TCloseSessionResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.compareTo(TCloseSessionResp)",5,3,5
"org.apache.hive.service.cli.thrift.TCloseSessionResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TCloseSessionResp.equals(TCloseSessionResp)",5,4,9
"org.apache.hive.service.cli.thrift.TCloseSessionResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TCloseSessionResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TCloseSessionResp.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TCloseSessionResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseSessionResp.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TCloseSessionResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TCloseSessionResp.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TCloseSessionResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TCloseSessionResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TCloseSessionResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TColumn.TColumn()",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.TColumn(TColumn)",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.TColumn(_Fields,Object)",1,1,1
"org.apache.hive.service.cli.thrift.TColumn._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TColumn._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TColumn._Fields.findByThriftId(int)",10,2,10
"org.apache.hive.service.cli.thrift.TColumn._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TColumn._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TColumn._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.binaryVal(TBinaryColumn)",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.boolVal(TBoolColumn)",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.byteVal(TByteColumn)",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.checkType(_Fields,Object)",2,2,18
"org.apache.hive.service.cli.thrift.TColumn.compareTo(TColumn)",2,2,2
"org.apache.hive.service.cli.thrift.TColumn.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.doubleVal(TDoubleColumn)",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.enumForId(short)",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.equals(Object)",2,2,2
"org.apache.hive.service.cli.thrift.TColumn.equals(TColumn)",1,3,3
"org.apache.hive.service.cli.thrift.TColumn.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.getBinaryVal()",2,2,2
"org.apache.hive.service.cli.thrift.TColumn.getBoolVal()",2,2,2
"org.apache.hive.service.cli.thrift.TColumn.getByteVal()",2,2,2
"org.apache.hive.service.cli.thrift.TColumn.getDoubleVal()",2,2,2
"org.apache.hive.service.cli.thrift.TColumn.getFieldDesc(_Fields)",10,2,10
"org.apache.hive.service.cli.thrift.TColumn.getI16Val()",2,2,2
"org.apache.hive.service.cli.thrift.TColumn.getI32Val()",2,2,2
"org.apache.hive.service.cli.thrift.TColumn.getI64Val()",2,2,2
"org.apache.hive.service.cli.thrift.TColumn.getStringVal()",2,2,2
"org.apache.hive.service.cli.thrift.TColumn.getStructDesc()",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TColumn.i16Val(TI16Column)",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.i32Val(TI32Column)",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.i64Val(TI64Column)",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.isSetBinaryVal()",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.isSetBoolVal()",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.isSetByteVal()",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.isSetDoubleVal()",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.isSetI16Val()",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.isSetI32Val()",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.isSetI64Val()",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.isSetStringVal()",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TColumn.setBinaryVal(TBinaryColumn)",2,1,2
"org.apache.hive.service.cli.thrift.TColumn.setBoolVal(TBoolColumn)",2,1,2
"org.apache.hive.service.cli.thrift.TColumn.setByteVal(TByteColumn)",2,1,2
"org.apache.hive.service.cli.thrift.TColumn.setDoubleVal(TDoubleColumn)",2,1,2
"org.apache.hive.service.cli.thrift.TColumn.setI16Val(TI16Column)",2,1,2
"org.apache.hive.service.cli.thrift.TColumn.setI32Val(TI32Column)",2,1,2
"org.apache.hive.service.cli.thrift.TColumn.setI64Val(TI64Column)",2,1,2
"org.apache.hive.service.cli.thrift.TColumn.setStringVal(TStringColumn)",2,1,2
"org.apache.hive.service.cli.thrift.TColumn.standardSchemeReadValue(TProtocol,TField)",19,19,19
"org.apache.hive.service.cli.thrift.TColumn.standardSchemeWriteValue(TProtocol)",2,2,10
"org.apache.hive.service.cli.thrift.TColumn.stringVal(TStringColumn)",1,1,1
"org.apache.hive.service.cli.thrift.TColumn.tupleSchemeReadValue(TProtocol,short)",3,3,11
"org.apache.hive.service.cli.thrift.TColumn.tupleSchemeWriteValue(TProtocol)",2,2,10
"org.apache.hive.service.cli.thrift.TColumn.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TColumnDesc.TColumnDesc()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.TColumnDesc(String,TTypeDesc,int)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.TColumnDesc(TColumnDesc)",1,1,4
"org.apache.hive.service.cli.thrift.TColumnDesc.TColumnDescStandardScheme.read(TProtocol,TColumnDesc)",4,7,12
"org.apache.hive.service.cli.thrift.TColumnDesc.TColumnDescStandardScheme.write(TProtocol,TColumnDesc)",1,5,5
"org.apache.hive.service.cli.thrift.TColumnDesc.TColumnDescStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.TColumnDescTupleScheme.read(TProtocol,TColumnDesc)",1,2,2
"org.apache.hive.service.cli.thrift.TColumnDesc.TColumnDescTupleScheme.write(TProtocol,TColumnDesc)",1,3,3
"org.apache.hive.service.cli.thrift.TColumnDesc.TColumnDescTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc._Fields.findByThriftId(int)",6,2,6
"org.apache.hive.service.cli.thrift.TColumnDesc._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TColumnDesc._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.compareTo(TColumnDesc)",14,6,14
"org.apache.hive.service.cli.thrift.TColumnDesc.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TColumnDesc.equals(TColumnDesc)",14,10,28
"org.apache.hive.service.cli.thrift.TColumnDesc.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.getColumnName()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.getComment()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.getFieldValue(_Fields)",5,5,5
"org.apache.hive.service.cli.thrift.TColumnDesc.getPosition()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.getTypeDesc()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.hashCode()",1,8,8
"org.apache.hive.service.cli.thrift.TColumnDesc.isSet(_Fields)",6,5,6
"org.apache.hive.service.cli.thrift.TColumnDesc.isSetColumnName()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.isSetComment()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.isSetPosition()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.isSetTypeDesc()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TColumnDesc.setColumnName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.setColumnNameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TColumnDesc.setComment(String)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.setCommentIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TColumnDesc.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hive.service.cli.thrift.TColumnDesc.setPosition(int)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.setPositionIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.setTypeDesc(TTypeDesc)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.setTypeDescIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TColumnDesc.toString()",1,8,8
"org.apache.hive.service.cli.thrift.TColumnDesc.unsetColumnName()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.unsetComment()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.unsetPosition()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.unsetTypeDesc()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.validate()",4,5,5
"org.apache.hive.service.cli.thrift.TColumnDesc.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnDesc.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TColumnValue.TColumnValue()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.TColumnValue(TColumnValue)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.TColumnValue(_Fields,Object)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue._Fields.findByThriftId(int)",9,2,9
"org.apache.hive.service.cli.thrift.TColumnValue._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TColumnValue._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.boolVal(TBoolValue)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.byteVal(TByteValue)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.checkType(_Fields,Object)",2,2,16
"org.apache.hive.service.cli.thrift.TColumnValue.compareTo(TColumnValue)",2,2,2
"org.apache.hive.service.cli.thrift.TColumnValue.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.doubleVal(TDoubleValue)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.enumForId(short)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.equals(Object)",2,2,2
"org.apache.hive.service.cli.thrift.TColumnValue.equals(TColumnValue)",1,3,3
"org.apache.hive.service.cli.thrift.TColumnValue.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.getBoolVal()",2,2,2
"org.apache.hive.service.cli.thrift.TColumnValue.getByteVal()",2,2,2
"org.apache.hive.service.cli.thrift.TColumnValue.getDoubleVal()",2,2,2
"org.apache.hive.service.cli.thrift.TColumnValue.getFieldDesc(_Fields)",9,2,9
"org.apache.hive.service.cli.thrift.TColumnValue.getI16Val()",2,2,2
"org.apache.hive.service.cli.thrift.TColumnValue.getI32Val()",2,2,2
"org.apache.hive.service.cli.thrift.TColumnValue.getI64Val()",2,2,2
"org.apache.hive.service.cli.thrift.TColumnValue.getStringVal()",2,2,2
"org.apache.hive.service.cli.thrift.TColumnValue.getStructDesc()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TColumnValue.i16Val(TI16Value)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.i32Val(TI32Value)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.i64Val(TI64Value)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.isSetBoolVal()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.isSetByteVal()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.isSetDoubleVal()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.isSetI16Val()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.isSetI32Val()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.isSetI64Val()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.isSetStringVal()",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TColumnValue.setBoolVal(TBoolValue)",2,1,2
"org.apache.hive.service.cli.thrift.TColumnValue.setByteVal(TByteValue)",2,1,2
"org.apache.hive.service.cli.thrift.TColumnValue.setDoubleVal(TDoubleValue)",2,1,2
"org.apache.hive.service.cli.thrift.TColumnValue.setI16Val(TI16Value)",2,1,2
"org.apache.hive.service.cli.thrift.TColumnValue.setI32Val(TI32Value)",2,1,2
"org.apache.hive.service.cli.thrift.TColumnValue.setI64Val(TI64Value)",2,1,2
"org.apache.hive.service.cli.thrift.TColumnValue.setStringVal(TStringValue)",2,1,2
"org.apache.hive.service.cli.thrift.TColumnValue.standardSchemeReadValue(TProtocol,TField)",17,17,17
"org.apache.hive.service.cli.thrift.TColumnValue.standardSchemeWriteValue(TProtocol)",2,2,9
"org.apache.hive.service.cli.thrift.TColumnValue.stringVal(TStringValue)",1,1,1
"org.apache.hive.service.cli.thrift.TColumnValue.tupleSchemeReadValue(TProtocol,short)",3,3,10
"org.apache.hive.service.cli.thrift.TColumnValue.tupleSchemeWriteValue(TProtocol)",2,2,9
"org.apache.hive.service.cli.thrift.TColumnValue.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TDoubleColumn.TDoubleColumn()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.TDoubleColumn(List<Double>,ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.TDoubleColumn(TDoubleColumn)",1,4,4
"org.apache.hive.service.cli.thrift.TDoubleColumn.TDoubleColumnStandardScheme.read(TProtocol,TDoubleColumn)",4,6,9
"org.apache.hive.service.cli.thrift.TDoubleColumn.TDoubleColumnStandardScheme.write(TProtocol,TDoubleColumn)",1,4,4
"org.apache.hive.service.cli.thrift.TDoubleColumn.TDoubleColumnStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.TDoubleColumnTupleScheme.read(TProtocol,TDoubleColumn)",1,2,2
"org.apache.hive.service.cli.thrift.TDoubleColumn.TDoubleColumnTupleScheme.write(TProtocol,TDoubleColumn)",1,2,2
"org.apache.hive.service.cli.thrift.TDoubleColumn.TDoubleColumnTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TDoubleColumn._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TDoubleColumn._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.addToValues(double)",1,1,2
"org.apache.hive.service.cli.thrift.TDoubleColumn.bufferForNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.compareTo(TDoubleColumn)",8,4,8
"org.apache.hive.service.cli.thrift.TDoubleColumn.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TDoubleColumn.equals(TDoubleColumn)",8,7,16
"org.apache.hive.service.cli.thrift.TDoubleColumn.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TDoubleColumn.getNulls()",1,2,2
"org.apache.hive.service.cli.thrift.TDoubleColumn.getValues()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.getValuesIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TDoubleColumn.getValuesSize()",1,2,2
"org.apache.hive.service.cli.thrift.TDoubleColumn.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TDoubleColumn.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TDoubleColumn.isSetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.isSetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TDoubleColumn.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TDoubleColumn.setNulls(ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.setNulls(byte[])",1,2,2
"org.apache.hive.service.cli.thrift.TDoubleColumn.setNullsIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TDoubleColumn.setValues(List<Double>)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.setValuesIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TDoubleColumn.toString()",1,4,4
"org.apache.hive.service.cli.thrift.TDoubleColumn.unsetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.unsetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.validate()",3,3,3
"org.apache.hive.service.cli.thrift.TDoubleColumn.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleColumn.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TDoubleValue.TDoubleValue()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.TDoubleValue(TDoubleValue)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.TDoubleValueStandardScheme.read(TProtocol,TDoubleValue)",4,4,6
"org.apache.hive.service.cli.thrift.TDoubleValue.TDoubleValueStandardScheme.write(TProtocol,TDoubleValue)",1,2,2
"org.apache.hive.service.cli.thrift.TDoubleValue.TDoubleValueStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.TDoubleValueTupleScheme.read(TProtocol,TDoubleValue)",1,2,2
"org.apache.hive.service.cli.thrift.TDoubleValue.TDoubleValueTupleScheme.write(TProtocol,TDoubleValue)",1,3,3
"org.apache.hive.service.cli.thrift.TDoubleValue.TDoubleValueTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TDoubleValue._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TDoubleValue._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.compareTo(TDoubleValue)",5,3,5
"org.apache.hive.service.cli.thrift.TDoubleValue.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TDoubleValue.equals(TDoubleValue)",5,3,9
"org.apache.hive.service.cli.thrift.TDoubleValue.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TDoubleValue.getValue()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TDoubleValue.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TDoubleValue.isSetValue()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TDoubleValue.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TDoubleValue.setValue(double)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.setValueIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TDoubleValue.unsetValue()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.validate()",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TDoubleValue.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.TExecuteStatementReq()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.TExecuteStatementReq(TExecuteStatementReq)",1,3,5
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.TExecuteStatementReq(TSessionHandle,String)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.TExecuteStatementReqStandardScheme.read(TProtocol,TExecuteStatementReq)",4,8,13
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.TExecuteStatementReqStandardScheme.write(TProtocol,TExecuteStatementReq)",1,7,7
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.TExecuteStatementReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.TExecuteStatementReqTupleScheme.read(TProtocol,TExecuteStatementReq)",1,4,4
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.TExecuteStatementReqTupleScheme.write(TProtocol,TExecuteStatementReq)",1,6,6
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.TExecuteStatementReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq._Fields.findByThriftId(int)",6,2,6
"org.apache.hive.service.cli.thrift.TExecuteStatementReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TExecuteStatementReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.compareTo(TExecuteStatementReq)",14,6,14
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.equals(TExecuteStatementReq)",14,12,30
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.getConfOverlay()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.getConfOverlaySize()",1,2,2
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.getFieldValue(_Fields)",5,5,5
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.getStatement()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.hashCode()",1,9,9
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.isRunAsync()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.isSet(_Fields)",6,5,6
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.isSetConfOverlay()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.isSetRunAsync()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.isSetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.isSetStatement()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.putToConfOverlay(String,String)",1,1,2
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.setConfOverlay(Map<String, String>)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.setConfOverlayIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.setRunAsync(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.setRunAsyncIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.setSessionHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.setStatement(String)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.setStatementIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.toString()",1,9,9
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.unsetConfOverlay()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.unsetRunAsync()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.unsetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.unsetStatement()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.validate()",3,4,4
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.TExecuteStatementResp()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.TExecuteStatementResp(TExecuteStatementResp)",1,1,3
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.TExecuteStatementResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.TExecuteStatementRespStandardScheme.read(TProtocol,TExecuteStatementResp)",4,5,8
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.TExecuteStatementRespStandardScheme.write(TProtocol,TExecuteStatementResp)",1,4,4
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.TExecuteStatementRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.TExecuteStatementRespTupleScheme.read(TProtocol,TExecuteStatementResp)",1,2,2
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.TExecuteStatementRespTupleScheme.write(TProtocol,TExecuteStatementResp)",1,3,3
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.TExecuteStatementRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TExecuteStatementResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TExecuteStatementResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.compareTo(TExecuteStatementResp)",8,4,8
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.equals(TExecuteStatementResp)",8,7,16
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.getOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.isSetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.setOperationHandle(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.setOperationHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.toString()",1,5,5
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.unsetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.validate()",2,4,4
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TExecuteStatementResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TFetchOrientation.TFetchOrientation(int)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchOrientation.findByValue(int)",8,2,8
"org.apache.hive.service.cli.thrift.TFetchOrientation.getValue()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.TFetchResultsReq()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.TFetchResultsReq(TFetchResultsReq)",1,1,3
"org.apache.hive.service.cli.thrift.TFetchResultsReq.TFetchResultsReq(TOperationHandle,TFetchOrientation,long)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.TFetchResultsReqStandardScheme.read(TProtocol,TFetchResultsReq)",4,7,12
"org.apache.hive.service.cli.thrift.TFetchResultsReq.TFetchResultsReqStandardScheme.write(TProtocol,TFetchResultsReq)",1,4,4
"org.apache.hive.service.cli.thrift.TFetchResultsReq.TFetchResultsReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.TFetchResultsReqTupleScheme.read(TProtocol,TFetchResultsReq)",1,2,2
"org.apache.hive.service.cli.thrift.TFetchResultsReq.TFetchResultsReqTupleScheme.write(TProtocol,TFetchResultsReq)",1,3,3
"org.apache.hive.service.cli.thrift.TFetchResultsReq.TFetchResultsReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq._Fields.findByThriftId(int)",6,2,6
"org.apache.hive.service.cli.thrift.TFetchResultsReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TFetchResultsReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.compareTo(TFetchResultsReq)",14,6,14
"org.apache.hive.service.cli.thrift.TFetchResultsReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TFetchResultsReq.equals(TFetchResultsReq)",14,9,28
"org.apache.hive.service.cli.thrift.TFetchResultsReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.getFetchType()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.getFieldValue(_Fields)",5,5,5
"org.apache.hive.service.cli.thrift.TFetchResultsReq.getMaxRows()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.getOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.getOrientation()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.hashCode()",1,8,8
"org.apache.hive.service.cli.thrift.TFetchResultsReq.isSet(_Fields)",6,5,6
"org.apache.hive.service.cli.thrift.TFetchResultsReq.isSetFetchType()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.isSetMaxRows()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.isSetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.isSetOrientation()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TFetchResultsReq.setFetchType(short)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.setFetchTypeIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hive.service.cli.thrift.TFetchResultsReq.setMaxRows(long)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.setMaxRowsIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.setOperationHandle(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.setOperationHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TFetchResultsReq.setOrientation(TFetchOrientation)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.setOrientationIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TFetchResultsReq.toString()",1,7,7
"org.apache.hive.service.cli.thrift.TFetchResultsReq.unsetFetchType()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.unsetMaxRows()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.unsetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.unsetOrientation()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.validate()",4,5,5
"org.apache.hive.service.cli.thrift.TFetchResultsReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TFetchResultsResp.TFetchResultsResp()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.TFetchResultsResp(TFetchResultsResp)",1,1,3
"org.apache.hive.service.cli.thrift.TFetchResultsResp.TFetchResultsResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.TFetchResultsRespStandardScheme.read(TProtocol,TFetchResultsResp)",4,6,10
"org.apache.hive.service.cli.thrift.TFetchResultsResp.TFetchResultsRespStandardScheme.write(TProtocol,TFetchResultsResp)",1,5,5
"org.apache.hive.service.cli.thrift.TFetchResultsResp.TFetchResultsRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.TFetchResultsRespTupleScheme.read(TProtocol,TFetchResultsResp)",1,3,3
"org.apache.hive.service.cli.thrift.TFetchResultsResp.TFetchResultsRespTupleScheme.write(TProtocol,TFetchResultsResp)",1,5,5
"org.apache.hive.service.cli.thrift.TFetchResultsResp.TFetchResultsRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp._Fields.findByThriftId(int)",5,2,5
"org.apache.hive.service.cli.thrift.TFetchResultsResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TFetchResultsResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.compareTo(TFetchResultsResp)",11,5,11
"org.apache.hive.service.cli.thrift.TFetchResultsResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TFetchResultsResp.equals(TFetchResultsResp)",11,9,23
"org.apache.hive.service.cli.thrift.TFetchResultsResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.getFieldValue(_Fields)",4,4,4
"org.apache.hive.service.cli.thrift.TFetchResultsResp.getResults()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.hashCode()",1,7,7
"org.apache.hive.service.cli.thrift.TFetchResultsResp.isHasMoreRows()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.isSet(_Fields)",5,4,5
"org.apache.hive.service.cli.thrift.TFetchResultsResp.isSetHasMoreRows()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.isSetResults()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TFetchResultsResp.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hive.service.cli.thrift.TFetchResultsResp.setHasMoreRows(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.setHasMoreRowsIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.setResults(TRowSet)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.setResultsIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TFetchResultsResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TFetchResultsResp.toString()",1,7,7
"org.apache.hive.service.cli.thrift.TFetchResultsResp.unsetHasMoreRows()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.unsetResults()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.validate()",2,4,4
"org.apache.hive.service.cli.thrift.TFetchResultsResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TFetchResultsResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.TGetCatalogsReq()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.TGetCatalogsReq(TGetCatalogsReq)",1,1,2
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.TGetCatalogsReq(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.TGetCatalogsReqStandardScheme.read(TProtocol,TGetCatalogsReq)",4,4,6
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.TGetCatalogsReqStandardScheme.write(TProtocol,TGetCatalogsReq)",1,2,2
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.TGetCatalogsReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.TGetCatalogsReqTupleScheme.read(TProtocol,TGetCatalogsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.TGetCatalogsReqTupleScheme.write(TProtocol,TGetCatalogsReq)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.TGetCatalogsReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TGetCatalogsReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetCatalogsReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.compareTo(TGetCatalogsReq)",5,3,5
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.equals(TGetCatalogsReq)",5,4,9
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.isSetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.setSessionHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.unsetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.TGetCatalogsResp()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.TGetCatalogsResp(TGetCatalogsResp)",1,1,3
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.TGetCatalogsResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.TGetCatalogsRespStandardScheme.read(TProtocol,TGetCatalogsResp)",4,5,8
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.TGetCatalogsRespStandardScheme.write(TProtocol,TGetCatalogsResp)",1,4,4
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.TGetCatalogsRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.TGetCatalogsRespTupleScheme.read(TProtocol,TGetCatalogsResp)",1,2,2
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.TGetCatalogsRespTupleScheme.write(TProtocol,TGetCatalogsResp)",1,3,3
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.TGetCatalogsRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TGetCatalogsResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetCatalogsResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.compareTo(TGetCatalogsResp)",8,4,8
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.equals(TGetCatalogsResp)",8,7,16
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.getOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.isSetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.setOperationHandle(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.setOperationHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.toString()",1,5,5
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.unsetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.validate()",2,4,4
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetCatalogsResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetColumnsReq.TGetColumnsReq()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.TGetColumnsReq(TGetColumnsReq)",1,1,6
"org.apache.hive.service.cli.thrift.TGetColumnsReq.TGetColumnsReq(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.TGetColumnsReqStandardScheme.read(TProtocol,TGetColumnsReq)",4,8,14
"org.apache.hive.service.cli.thrift.TGetColumnsReq.TGetColumnsReqStandardScheme.write(TProtocol,TGetColumnsReq)",1,10,10
"org.apache.hive.service.cli.thrift.TGetColumnsReq.TGetColumnsReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.TGetColumnsReqTupleScheme.read(TProtocol,TGetColumnsReq)",1,5,5
"org.apache.hive.service.cli.thrift.TGetColumnsReq.TGetColumnsReqTupleScheme.write(TProtocol,TGetColumnsReq)",1,9,9
"org.apache.hive.service.cli.thrift.TGetColumnsReq.TGetColumnsReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq._Fields.findByThriftId(int)",7,2,7
"org.apache.hive.service.cli.thrift.TGetColumnsReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetColumnsReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.compareTo(TGetColumnsReq)",17,7,17
"org.apache.hive.service.cli.thrift.TGetColumnsReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetColumnsReq.equals(TGetColumnsReq)",17,16,37
"org.apache.hive.service.cli.thrift.TGetColumnsReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.getCatalogName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.getColumnName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.getFieldValue(_Fields)",6,6,6
"org.apache.hive.service.cli.thrift.TGetColumnsReq.getSchemaName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.getTableName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.hashCode()",1,11,11
"org.apache.hive.service.cli.thrift.TGetColumnsReq.isSet(_Fields)",7,6,7
"org.apache.hive.service.cli.thrift.TGetColumnsReq.isSetCatalogName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.isSetColumnName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.isSetSchemaName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.isSetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.isSetTableName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetColumnsReq.setCatalogName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.setCatalogNameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetColumnsReq.setColumnName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.setColumnNameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetColumnsReq.setFieldValue(_Fields,Object)",2,7,11
"org.apache.hive.service.cli.thrift.TGetColumnsReq.setSchemaName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.setSchemaNameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetColumnsReq.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.setSessionHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetColumnsReq.setTableName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.setTableNameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetColumnsReq.toString()",1,14,14
"org.apache.hive.service.cli.thrift.TGetColumnsReq.unsetCatalogName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.unsetColumnName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.unsetSchemaName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.unsetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.unsetTableName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TGetColumnsReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetColumnsResp.TGetColumnsResp()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.TGetColumnsResp(TGetColumnsResp)",1,1,3
"org.apache.hive.service.cli.thrift.TGetColumnsResp.TGetColumnsResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.TGetColumnsRespStandardScheme.read(TProtocol,TGetColumnsResp)",4,5,8
"org.apache.hive.service.cli.thrift.TGetColumnsResp.TGetColumnsRespStandardScheme.write(TProtocol,TGetColumnsResp)",1,4,4
"org.apache.hive.service.cli.thrift.TGetColumnsResp.TGetColumnsRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.TGetColumnsRespTupleScheme.read(TProtocol,TGetColumnsResp)",1,2,2
"org.apache.hive.service.cli.thrift.TGetColumnsResp.TGetColumnsRespTupleScheme.write(TProtocol,TGetColumnsResp)",1,3,3
"org.apache.hive.service.cli.thrift.TGetColumnsResp.TGetColumnsRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TGetColumnsResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetColumnsResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.compareTo(TGetColumnsResp)",8,4,8
"org.apache.hive.service.cli.thrift.TGetColumnsResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetColumnsResp.equals(TGetColumnsResp)",8,7,16
"org.apache.hive.service.cli.thrift.TGetColumnsResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TGetColumnsResp.getOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TGetColumnsResp.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TGetColumnsResp.isSetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetColumnsResp.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TGetColumnsResp.setOperationHandle(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.setOperationHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetColumnsResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetColumnsResp.toString()",1,5,5
"org.apache.hive.service.cli.thrift.TGetColumnsResp.unsetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.validate()",2,4,4
"org.apache.hive.service.cli.thrift.TGetColumnsResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetColumnsResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.TGetDelegationTokenReq()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.TGetDelegationTokenReq(TGetDelegationTokenReq)",1,1,4
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.TGetDelegationTokenReq(TSessionHandle,String,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.TGetDelegationTokenReqStandardScheme.read(TProtocol,TGetDelegationTokenReq)",4,6,10
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.TGetDelegationTokenReqStandardScheme.write(TProtocol,TGetDelegationTokenReq)",1,4,4
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.TGetDelegationTokenReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.TGetDelegationTokenReqTupleScheme.read(TProtocol,TGetDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.TGetDelegationTokenReqTupleScheme.write(TProtocol,TGetDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.TGetDelegationTokenReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq._Fields.findByThriftId(int)",5,2,5
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.compareTo(TGetDelegationTokenReq)",11,5,11
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.equals(TGetDelegationTokenReq)",11,10,23
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.getFieldValue(_Fields)",4,4,4
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.getOwner()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.getRenewer()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.hashCode()",1,7,7
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.isSet(_Fields)",5,4,5
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.isSetOwner()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.isSetRenewer()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.isSetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.setOwner(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.setOwnerIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.setRenewer(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.setRenewerIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.setSessionHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.toString()",1,6,6
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.unsetOwner()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.unsetRenewer()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.unsetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.validate()",4,5,5
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.TGetDelegationTokenResp()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.TGetDelegationTokenResp(TGetDelegationTokenResp)",1,1,3
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.TGetDelegationTokenResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.TGetDelegationTokenRespStandardScheme.read(TProtocol,TGetDelegationTokenResp)",4,5,8
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.TGetDelegationTokenRespStandardScheme.write(TProtocol,TGetDelegationTokenResp)",1,4,4
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.TGetDelegationTokenRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.TGetDelegationTokenRespTupleScheme.read(TProtocol,TGetDelegationTokenResp)",1,2,2
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.TGetDelegationTokenRespTupleScheme.write(TProtocol,TGetDelegationTokenResp)",1,3,3
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.TGetDelegationTokenRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.compareTo(TGetDelegationTokenResp)",8,4,8
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.equals(TGetDelegationTokenResp)",8,7,16
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.getDelegationToken()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.isSetDelegationToken()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.setDelegationToken(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.setDelegationTokenIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.toString()",1,5,5
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.unsetDelegationToken()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetDelegationTokenResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.TGetFunctionsReq()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.TGetFunctionsReq(TGetFunctionsReq)",1,1,5
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.TGetFunctionsReq(TSessionHandle,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.TGetFunctionsReqStandardScheme.read(TProtocol,TGetFunctionsReq)",4,7,12
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.TGetFunctionsReqStandardScheme.write(TProtocol,TGetFunctionsReq)",1,7,7
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.TGetFunctionsReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.TGetFunctionsReqTupleScheme.read(TProtocol,TGetFunctionsReq)",1,3,3
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.TGetFunctionsReqTupleScheme.write(TProtocol,TGetFunctionsReq)",1,5,5
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.TGetFunctionsReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq._Fields.findByThriftId(int)",6,2,6
"org.apache.hive.service.cli.thrift.TGetFunctionsReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetFunctionsReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.compareTo(TGetFunctionsReq)",14,6,14
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.equals(TGetFunctionsReq)",14,13,30
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.getCatalogName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.getFieldValue(_Fields)",5,5,5
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.getFunctionName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.getSchemaName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.hashCode()",1,9,9
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.isSet(_Fields)",6,5,6
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.isSetCatalogName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.isSetFunctionName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.isSetSchemaName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.isSetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.setCatalogName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.setCatalogNameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.setFunctionName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.setFunctionNameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.setSchemaName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.setSchemaNameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.setSessionHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.toString()",1,10,10
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.unsetCatalogName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.unsetFunctionName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.unsetSchemaName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.unsetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.validate()",3,4,4
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.TGetFunctionsResp()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.TGetFunctionsResp(TGetFunctionsResp)",1,1,3
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.TGetFunctionsResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.TGetFunctionsRespStandardScheme.read(TProtocol,TGetFunctionsResp)",4,5,8
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.TGetFunctionsRespStandardScheme.write(TProtocol,TGetFunctionsResp)",1,4,4
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.TGetFunctionsRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.TGetFunctionsRespTupleScheme.read(TProtocol,TGetFunctionsResp)",1,2,2
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.TGetFunctionsRespTupleScheme.write(TProtocol,TGetFunctionsResp)",1,3,3
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.TGetFunctionsRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TGetFunctionsResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetFunctionsResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.compareTo(TGetFunctionsResp)",8,4,8
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.equals(TGetFunctionsResp)",8,7,16
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.getOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.isSetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.setOperationHandle(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.setOperationHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.toString()",1,5,5
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.unsetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.validate()",2,4,4
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetFunctionsResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetInfoReq.TGetInfoReq()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.TGetInfoReq(TGetInfoReq)",1,1,3
"org.apache.hive.service.cli.thrift.TGetInfoReq.TGetInfoReq(TSessionHandle,TGetInfoType)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.TGetInfoReqStandardScheme.read(TProtocol,TGetInfoReq)",4,5,8
"org.apache.hive.service.cli.thrift.TGetInfoReq.TGetInfoReqStandardScheme.write(TProtocol,TGetInfoReq)",1,3,3
"org.apache.hive.service.cli.thrift.TGetInfoReq.TGetInfoReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.TGetInfoReqTupleScheme.read(TProtocol,TGetInfoReq)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.TGetInfoReqTupleScheme.write(TProtocol,TGetInfoReq)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.TGetInfoReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TGetInfoReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetInfoReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.compareTo(TGetInfoReq)",8,4,8
"org.apache.hive.service.cli.thrift.TGetInfoReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetInfoReq.equals(TGetInfoReq)",8,7,16
"org.apache.hive.service.cli.thrift.TGetInfoReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TGetInfoReq.getInfoType()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TGetInfoReq.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TGetInfoReq.isSetInfoType()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.isSetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetInfoReq.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TGetInfoReq.setInfoType(TGetInfoType)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.setInfoTypeIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetInfoReq.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.setSessionHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetInfoReq.toString()",1,4,4
"org.apache.hive.service.cli.thrift.TGetInfoReq.unsetInfoType()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.unsetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.validate()",3,4,4
"org.apache.hive.service.cli.thrift.TGetInfoReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetInfoResp.TGetInfoResp()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.TGetInfoResp(TGetInfoResp)",1,1,3
"org.apache.hive.service.cli.thrift.TGetInfoResp.TGetInfoResp(TStatus,TGetInfoValue)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.TGetInfoRespStandardScheme.read(TProtocol,TGetInfoResp)",4,5,8
"org.apache.hive.service.cli.thrift.TGetInfoResp.TGetInfoRespStandardScheme.write(TProtocol,TGetInfoResp)",1,3,3
"org.apache.hive.service.cli.thrift.TGetInfoResp.TGetInfoRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.TGetInfoRespTupleScheme.read(TProtocol,TGetInfoResp)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.TGetInfoRespTupleScheme.write(TProtocol,TGetInfoResp)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.TGetInfoRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TGetInfoResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetInfoResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.compareTo(TGetInfoResp)",8,4,8
"org.apache.hive.service.cli.thrift.TGetInfoResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetInfoResp.equals(TGetInfoResp)",8,7,16
"org.apache.hive.service.cli.thrift.TGetInfoResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TGetInfoResp.getInfoValue()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TGetInfoResp.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TGetInfoResp.isSetInfoValue()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetInfoResp.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TGetInfoResp.setInfoValue(TGetInfoValue)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.setInfoValueIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetInfoResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetInfoResp.toString()",1,4,4
"org.apache.hive.service.cli.thrift.TGetInfoResp.unsetInfoValue()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.validate()",3,4,4
"org.apache.hive.service.cli.thrift.TGetInfoResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetInfoType.TGetInfoType(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoType.findByValue(int)",49,2,49
"org.apache.hive.service.cli.thrift.TGetInfoType.getValue()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.TGetInfoValue()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.TGetInfoValue(TGetInfoValue)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.TGetInfoValue(_Fields,Object)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue._Fields.findByThriftId(int)",8,2,8
"org.apache.hive.service.cli.thrift.TGetInfoValue._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetInfoValue._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.binaryValue(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.checkType(_Fields,Object)",2,2,14
"org.apache.hive.service.cli.thrift.TGetInfoValue.compareTo(TGetInfoValue)",2,2,2
"org.apache.hive.service.cli.thrift.TGetInfoValue.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.enumForId(short)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.equals(Object)",2,2,2
"org.apache.hive.service.cli.thrift.TGetInfoValue.equals(TGetInfoValue)",1,3,3
"org.apache.hive.service.cli.thrift.TGetInfoValue.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.getBinaryValue()",2,2,2
"org.apache.hive.service.cli.thrift.TGetInfoValue.getFieldDesc(_Fields)",8,2,8
"org.apache.hive.service.cli.thrift.TGetInfoValue.getIntegerBitmask()",2,2,2
"org.apache.hive.service.cli.thrift.TGetInfoValue.getIntegerFlag()",2,2,2
"org.apache.hive.service.cli.thrift.TGetInfoValue.getLenValue()",2,2,2
"org.apache.hive.service.cli.thrift.TGetInfoValue.getSmallIntValue()",2,2,2
"org.apache.hive.service.cli.thrift.TGetInfoValue.getStringValue()",2,2,2
"org.apache.hive.service.cli.thrift.TGetInfoValue.getStructDesc()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TGetInfoValue.integerBitmask(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.integerFlag(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.isSetBinaryValue()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.isSetIntegerBitmask()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.isSetIntegerFlag()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.isSetLenValue()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.isSetSmallIntValue()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.isSetStringValue()",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.lenValue(long)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetInfoValue.setBinaryValue(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.setIntegerBitmask(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.setIntegerFlag(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.setLenValue(long)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.setSmallIntValue(short)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.setStringValue(String)",2,1,2
"org.apache.hive.service.cli.thrift.TGetInfoValue.smallIntValue(short)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.standardSchemeReadValue(TProtocol,TField)",15,15,15
"org.apache.hive.service.cli.thrift.TGetInfoValue.standardSchemeWriteValue(TProtocol)",2,2,8
"org.apache.hive.service.cli.thrift.TGetInfoValue.stringValue(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetInfoValue.tupleSchemeReadValue(TProtocol,short)",3,3,9
"org.apache.hive.service.cli.thrift.TGetInfoValue.tupleSchemeWriteValue(TProtocol)",2,2,8
"org.apache.hive.service.cli.thrift.TGetInfoValue.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.TGetOperationStatusReq()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.TGetOperationStatusReq(TGetOperationStatusReq)",1,1,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.TGetOperationStatusReq(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.TGetOperationStatusReqStandardScheme.read(TProtocol,TGetOperationStatusReq)",4,4,6
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.TGetOperationStatusReqStandardScheme.write(TProtocol,TGetOperationStatusReq)",1,2,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.TGetOperationStatusReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.TGetOperationStatusReqTupleScheme.read(TProtocol,TGetOperationStatusReq)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.TGetOperationStatusReqTupleScheme.write(TProtocol,TGetOperationStatusReq)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.TGetOperationStatusReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.compareTo(TGetOperationStatusReq)",5,3,5
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.equals(TGetOperationStatusReq)",5,4,9
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.getOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.isSetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.setOperationHandle(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.setOperationHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.unsetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.TGetOperationStatusResp()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.TGetOperationStatusResp(TGetOperationStatusResp)",1,1,5
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.TGetOperationStatusResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.TGetOperationStatusRespStandardScheme.read(TProtocol,TGetOperationStatusResp)",4,8,14
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.TGetOperationStatusRespStandardScheme.write(TProtocol,TGetOperationStatusResp)",1,9,9
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.TGetOperationStatusRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.TGetOperationStatusRespTupleScheme.read(TProtocol,TGetOperationStatusResp)",1,5,5
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.TGetOperationStatusRespTupleScheme.write(TProtocol,TGetOperationStatusResp)",1,9,9
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.TGetOperationStatusRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp._Fields.findByThriftId(int)",7,2,7
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.compareTo(TGetOperationStatusResp)",17,7,17
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.equals(TGetOperationStatusResp)",17,15,37
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.getErrorCode()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.getErrorMessage()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.getFieldValue(_Fields)",6,6,6
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.getOperationState()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.getSqlState()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.hashCode()",1,11,11
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.isSet(_Fields)",7,6,7
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.isSetErrorCode()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.isSetErrorMessage()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.isSetOperationState()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.isSetSqlState()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.setErrorCode(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.setErrorCodeIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.setErrorMessage(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.setErrorMessageIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.setFieldValue(_Fields,Object)",2,7,11
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.setOperationState(TOperationState)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.setOperationStateIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.setSqlState(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.setSqlStateIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.toString()",1,13,13
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.unsetErrorCode()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.unsetErrorMessage()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.unsetOperationState()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.unsetSqlState()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetOperationStatusResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.TGetResultSetMetadataReq()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.TGetResultSetMetadataReq(TGetResultSetMetadataReq)",1,1,2
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.TGetResultSetMetadataReq(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.TGetResultSetMetadataReqStandardScheme.read(TProtocol,TGetResultSetMetadataReq)",4,4,6
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.TGetResultSetMetadataReqStandardScheme.write(TProtocol,TGetResultSetMetadataReq)",1,2,2
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.TGetResultSetMetadataReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.TGetResultSetMetadataReqTupleScheme.read(TProtocol,TGetResultSetMetadataReq)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.TGetResultSetMetadataReqTupleScheme.write(TProtocol,TGetResultSetMetadataReq)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.TGetResultSetMetadataReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.compareTo(TGetResultSetMetadataReq)",5,3,5
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.equals(TGetResultSetMetadataReq)",5,4,9
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.getOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.isSetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.setOperationHandle(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.setOperationHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.unsetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.TGetResultSetMetadataResp()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.TGetResultSetMetadataResp(TGetResultSetMetadataResp)",1,1,3
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.TGetResultSetMetadataResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.TGetResultSetMetadataRespStandardScheme.read(TProtocol,TGetResultSetMetadataResp)",4,5,8
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.TGetResultSetMetadataRespStandardScheme.write(TProtocol,TGetResultSetMetadataResp)",1,4,4
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.TGetResultSetMetadataRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.TGetResultSetMetadataRespTupleScheme.read(TProtocol,TGetResultSetMetadataResp)",1,2,2
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.TGetResultSetMetadataRespTupleScheme.write(TProtocol,TGetResultSetMetadataResp)",1,3,3
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.TGetResultSetMetadataRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.compareTo(TGetResultSetMetadataResp)",8,4,8
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.equals(TGetResultSetMetadataResp)",8,7,16
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.getSchema()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.isSetSchema()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.setSchema(TTableSchema)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.setSchemaIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.toString()",1,5,5
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.unsetSchema()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.validate()",2,4,4
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetResultSetMetadataResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetSchemasReq.TGetSchemasReq()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.TGetSchemasReq(TGetSchemasReq)",1,1,4
"org.apache.hive.service.cli.thrift.TGetSchemasReq.TGetSchemasReq(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.TGetSchemasReqStandardScheme.read(TProtocol,TGetSchemasReq)",4,6,10
"org.apache.hive.service.cli.thrift.TGetSchemasReq.TGetSchemasReqStandardScheme.write(TProtocol,TGetSchemasReq)",1,6,6
"org.apache.hive.service.cli.thrift.TGetSchemasReq.TGetSchemasReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.TGetSchemasReqTupleScheme.read(TProtocol,TGetSchemasReq)",1,3,3
"org.apache.hive.service.cli.thrift.TGetSchemasReq.TGetSchemasReqTupleScheme.write(TProtocol,TGetSchemasReq)",1,5,5
"org.apache.hive.service.cli.thrift.TGetSchemasReq.TGetSchemasReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq._Fields.findByThriftId(int)",5,2,5
"org.apache.hive.service.cli.thrift.TGetSchemasReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetSchemasReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.compareTo(TGetSchemasReq)",11,5,11
"org.apache.hive.service.cli.thrift.TGetSchemasReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetSchemasReq.equals(TGetSchemasReq)",11,10,23
"org.apache.hive.service.cli.thrift.TGetSchemasReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.getCatalogName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.getFieldValue(_Fields)",4,4,4
"org.apache.hive.service.cli.thrift.TGetSchemasReq.getSchemaName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.hashCode()",1,7,7
"org.apache.hive.service.cli.thrift.TGetSchemasReq.isSet(_Fields)",5,4,5
"org.apache.hive.service.cli.thrift.TGetSchemasReq.isSetCatalogName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.isSetSchemaName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.isSetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetSchemasReq.setCatalogName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.setCatalogNameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetSchemasReq.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hive.service.cli.thrift.TGetSchemasReq.setSchemaName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.setSchemaNameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetSchemasReq.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.setSessionHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetSchemasReq.toString()",1,8,8
"org.apache.hive.service.cli.thrift.TGetSchemasReq.unsetCatalogName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.unsetSchemaName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.unsetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TGetSchemasReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetSchemasResp.TGetSchemasResp()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.TGetSchemasResp(TGetSchemasResp)",1,1,3
"org.apache.hive.service.cli.thrift.TGetSchemasResp.TGetSchemasResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.TGetSchemasRespStandardScheme.read(TProtocol,TGetSchemasResp)",4,5,8
"org.apache.hive.service.cli.thrift.TGetSchemasResp.TGetSchemasRespStandardScheme.write(TProtocol,TGetSchemasResp)",1,4,4
"org.apache.hive.service.cli.thrift.TGetSchemasResp.TGetSchemasRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.TGetSchemasRespTupleScheme.read(TProtocol,TGetSchemasResp)",1,2,2
"org.apache.hive.service.cli.thrift.TGetSchemasResp.TGetSchemasRespTupleScheme.write(TProtocol,TGetSchemasResp)",1,3,3
"org.apache.hive.service.cli.thrift.TGetSchemasResp.TGetSchemasRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TGetSchemasResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetSchemasResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.compareTo(TGetSchemasResp)",8,4,8
"org.apache.hive.service.cli.thrift.TGetSchemasResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetSchemasResp.equals(TGetSchemasResp)",8,7,16
"org.apache.hive.service.cli.thrift.TGetSchemasResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TGetSchemasResp.getOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TGetSchemasResp.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TGetSchemasResp.isSetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetSchemasResp.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TGetSchemasResp.setOperationHandle(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.setOperationHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetSchemasResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetSchemasResp.toString()",1,5,5
"org.apache.hive.service.cli.thrift.TGetSchemasResp.unsetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.validate()",2,4,4
"org.apache.hive.service.cli.thrift.TGetSchemasResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetSchemasResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.TGetTableTypesReq()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.TGetTableTypesReq(TGetTableTypesReq)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.TGetTableTypesReq(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.TGetTableTypesReqStandardScheme.read(TProtocol,TGetTableTypesReq)",4,4,6
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.TGetTableTypesReqStandardScheme.write(TProtocol,TGetTableTypesReq)",1,2,2
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.TGetTableTypesReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.TGetTableTypesReqTupleScheme.read(TProtocol,TGetTableTypesReq)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.TGetTableTypesReqTupleScheme.write(TProtocol,TGetTableTypesReq)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.TGetTableTypesReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TGetTableTypesReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetTableTypesReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.compareTo(TGetTableTypesReq)",5,3,5
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.equals(TGetTableTypesReq)",5,4,9
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.isSetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.setSessionHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.unsetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.TGetTableTypesResp()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.TGetTableTypesResp(TGetTableTypesResp)",1,1,3
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.TGetTableTypesResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.TGetTableTypesRespStandardScheme.read(TProtocol,TGetTableTypesResp)",4,5,8
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.TGetTableTypesRespStandardScheme.write(TProtocol,TGetTableTypesResp)",1,4,4
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.TGetTableTypesRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.TGetTableTypesRespTupleScheme.read(TProtocol,TGetTableTypesResp)",1,2,2
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.TGetTableTypesRespTupleScheme.write(TProtocol,TGetTableTypesResp)",1,3,3
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.TGetTableTypesRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TGetTableTypesResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetTableTypesResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.compareTo(TGetTableTypesResp)",8,4,8
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.equals(TGetTableTypesResp)",8,7,16
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.getOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.isSetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.setOperationHandle(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.setOperationHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.toString()",1,5,5
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.unsetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.validate()",2,4,4
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTableTypesResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTablesReq.TGetTablesReq()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.TGetTablesReq(TGetTablesReq)",1,3,7
"org.apache.hive.service.cli.thrift.TGetTablesReq.TGetTablesReq(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.TGetTablesReqStandardScheme.read(TProtocol,TGetTablesReq)",4,9,15
"org.apache.hive.service.cli.thrift.TGetTablesReq.TGetTablesReqStandardScheme.write(TProtocol,TGetTablesReq)",1,11,11
"org.apache.hive.service.cli.thrift.TGetTablesReq.TGetTablesReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.TGetTablesReqTupleScheme.read(TProtocol,TGetTablesReq)",1,6,6
"org.apache.hive.service.cli.thrift.TGetTablesReq.TGetTablesReqTupleScheme.write(TProtocol,TGetTablesReq)",1,10,10
"org.apache.hive.service.cli.thrift.TGetTablesReq.TGetTablesReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq._Fields.findByThriftId(int)",7,2,7
"org.apache.hive.service.cli.thrift.TGetTablesReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetTablesReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.addToTableTypes(String)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTablesReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.compareTo(TGetTablesReq)",17,7,17
"org.apache.hive.service.cli.thrift.TGetTablesReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetTablesReq.equals(TGetTablesReq)",17,16,37
"org.apache.hive.service.cli.thrift.TGetTablesReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.getCatalogName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.getFieldValue(_Fields)",6,6,6
"org.apache.hive.service.cli.thrift.TGetTablesReq.getSchemaName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.getTableName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.getTableTypes()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.getTableTypesIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TGetTablesReq.getTableTypesSize()",1,2,2
"org.apache.hive.service.cli.thrift.TGetTablesReq.hashCode()",1,11,11
"org.apache.hive.service.cli.thrift.TGetTablesReq.isSet(_Fields)",7,6,7
"org.apache.hive.service.cli.thrift.TGetTablesReq.isSetCatalogName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.isSetSchemaName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.isSetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.isSetTableName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.isSetTableTypes()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTablesReq.setCatalogName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.setCatalogNameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTablesReq.setFieldValue(_Fields,Object)",2,7,11
"org.apache.hive.service.cli.thrift.TGetTablesReq.setSchemaName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.setSchemaNameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTablesReq.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.setSessionHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTablesReq.setTableName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.setTableNameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTablesReq.setTableTypes(List<String>)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.setTableTypesIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTablesReq.toString()",1,14,14
"org.apache.hive.service.cli.thrift.TGetTablesReq.unsetCatalogName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.unsetSchemaName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.unsetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.unsetTableName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.unsetTableTypes()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TGetTablesReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTablesResp.TGetTablesResp()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.TGetTablesResp(TGetTablesResp)",1,1,3
"org.apache.hive.service.cli.thrift.TGetTablesResp.TGetTablesResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.TGetTablesRespStandardScheme.read(TProtocol,TGetTablesResp)",4,5,8
"org.apache.hive.service.cli.thrift.TGetTablesResp.TGetTablesRespStandardScheme.write(TProtocol,TGetTablesResp)",1,4,4
"org.apache.hive.service.cli.thrift.TGetTablesResp.TGetTablesRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.TGetTablesRespTupleScheme.read(TProtocol,TGetTablesResp)",1,2,2
"org.apache.hive.service.cli.thrift.TGetTablesResp.TGetTablesRespTupleScheme.write(TProtocol,TGetTablesResp)",1,3,3
"org.apache.hive.service.cli.thrift.TGetTablesResp.TGetTablesRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TGetTablesResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetTablesResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.compareTo(TGetTablesResp)",8,4,8
"org.apache.hive.service.cli.thrift.TGetTablesResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetTablesResp.equals(TGetTablesResp)",8,7,16
"org.apache.hive.service.cli.thrift.TGetTablesResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TGetTablesResp.getOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TGetTablesResp.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TGetTablesResp.isSetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTablesResp.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TGetTablesResp.setOperationHandle(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.setOperationHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTablesResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTablesResp.toString()",1,5,5
"org.apache.hive.service.cli.thrift.TGetTablesResp.unsetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.validate()",2,4,4
"org.apache.hive.service.cli.thrift.TGetTablesResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTablesResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.TGetTypeInfoReq()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.TGetTypeInfoReq(TGetTypeInfoReq)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.TGetTypeInfoReq(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.TGetTypeInfoReqStandardScheme.read(TProtocol,TGetTypeInfoReq)",4,4,6
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.TGetTypeInfoReqStandardScheme.write(TProtocol,TGetTypeInfoReq)",1,2,2
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.TGetTypeInfoReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.TGetTypeInfoReqTupleScheme.read(TProtocol,TGetTypeInfoReq)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.TGetTypeInfoReqTupleScheme.write(TProtocol,TGetTypeInfoReq)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.TGetTypeInfoReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.compareTo(TGetTypeInfoReq)",5,3,5
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.equals(TGetTypeInfoReq)",5,4,9
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.isSetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.setSessionHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.unsetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.TGetTypeInfoResp()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.TGetTypeInfoResp(TGetTypeInfoResp)",1,1,3
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.TGetTypeInfoResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.TGetTypeInfoRespStandardScheme.read(TProtocol,TGetTypeInfoResp)",4,5,8
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.TGetTypeInfoRespStandardScheme.write(TProtocol,TGetTypeInfoResp)",1,4,4
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.TGetTypeInfoRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.TGetTypeInfoRespTupleScheme.read(TProtocol,TGetTypeInfoResp)",1,2,2
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.TGetTypeInfoRespTupleScheme.write(TProtocol,TGetTypeInfoResp)",1,3,3
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.TGetTypeInfoRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.compareTo(TGetTypeInfoResp)",8,4,8
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.equals(TGetTypeInfoResp)",8,7,16
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.getOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.isSetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.setOperationHandle(TOperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.setOperationHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.toString()",1,5,5
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.unsetOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.validate()",2,4,4
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TGetTypeInfoResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.THandleIdentifier.THandleIdentifier()",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.THandleIdentifier(ByteBuffer,ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.THandleIdentifier(THandleIdentifier)",1,3,3
"org.apache.hive.service.cli.thrift.THandleIdentifier.THandleIdentifierStandardScheme.read(TProtocol,THandleIdentifier)",4,5,8
"org.apache.hive.service.cli.thrift.THandleIdentifier.THandleIdentifierStandardScheme.write(TProtocol,THandleIdentifier)",1,3,3
"org.apache.hive.service.cli.thrift.THandleIdentifier.THandleIdentifierStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.THandleIdentifierTupleScheme.read(TProtocol,THandleIdentifier)",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.THandleIdentifierTupleScheme.write(TProtocol,THandleIdentifier)",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.THandleIdentifierTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.THandleIdentifier._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.THandleIdentifier._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.bufferForGuid()",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.bufferForSecret()",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.clear()",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.compareTo(THandleIdentifier)",8,4,8
"org.apache.hive.service.cli.thrift.THandleIdentifier.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.THandleIdentifier.equals(THandleIdentifier)",8,7,16
"org.apache.hive.service.cli.thrift.THandleIdentifier.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.THandleIdentifier.getGuid()",1,2,2
"org.apache.hive.service.cli.thrift.THandleIdentifier.getSecret()",1,2,2
"org.apache.hive.service.cli.thrift.THandleIdentifier.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.THandleIdentifier.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.THandleIdentifier.isSetGuid()",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.isSetSecret()",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.THandleIdentifier.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.THandleIdentifier.setGuid(ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.setGuid(byte[])",1,2,2
"org.apache.hive.service.cli.thrift.THandleIdentifier.setGuidIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.THandleIdentifier.setSecret(ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.setSecret(byte[])",1,2,2
"org.apache.hive.service.cli.thrift.THandleIdentifier.setSecretIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.THandleIdentifier.toString()",1,4,4
"org.apache.hive.service.cli.thrift.THandleIdentifier.unsetGuid()",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.unsetSecret()",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.validate()",3,3,3
"org.apache.hive.service.cli.thrift.THandleIdentifier.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.THandleIdentifier.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TI16Column.TI16Column()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.TI16Column(List<Short>,ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.TI16Column(TI16Column)",1,4,4
"org.apache.hive.service.cli.thrift.TI16Column.TI16ColumnStandardScheme.read(TProtocol,TI16Column)",4,6,9
"org.apache.hive.service.cli.thrift.TI16Column.TI16ColumnStandardScheme.write(TProtocol,TI16Column)",1,4,4
"org.apache.hive.service.cli.thrift.TI16Column.TI16ColumnStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.TI16ColumnTupleScheme.read(TProtocol,TI16Column)",1,2,2
"org.apache.hive.service.cli.thrift.TI16Column.TI16ColumnTupleScheme.write(TProtocol,TI16Column)",1,2,2
"org.apache.hive.service.cli.thrift.TI16Column.TI16ColumnTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TI16Column._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TI16Column._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.addToValues(short)",1,1,2
"org.apache.hive.service.cli.thrift.TI16Column.bufferForNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.compareTo(TI16Column)",8,4,8
"org.apache.hive.service.cli.thrift.TI16Column.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TI16Column.equals(TI16Column)",8,7,16
"org.apache.hive.service.cli.thrift.TI16Column.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TI16Column.getNulls()",1,2,2
"org.apache.hive.service.cli.thrift.TI16Column.getValues()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.getValuesIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TI16Column.getValuesSize()",1,2,2
"org.apache.hive.service.cli.thrift.TI16Column.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TI16Column.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TI16Column.isSetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.isSetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TI16Column.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TI16Column.setNulls(ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.setNulls(byte[])",1,2,2
"org.apache.hive.service.cli.thrift.TI16Column.setNullsIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TI16Column.setValues(List<Short>)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.setValuesIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TI16Column.toString()",1,4,4
"org.apache.hive.service.cli.thrift.TI16Column.unsetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.unsetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.validate()",3,3,3
"org.apache.hive.service.cli.thrift.TI16Column.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Column.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TI16Value.TI16Value()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.TI16Value(TI16Value)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.TI16ValueStandardScheme.read(TProtocol,TI16Value)",4,4,6
"org.apache.hive.service.cli.thrift.TI16Value.TI16ValueStandardScheme.write(TProtocol,TI16Value)",1,2,2
"org.apache.hive.service.cli.thrift.TI16Value.TI16ValueStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.TI16ValueTupleScheme.read(TProtocol,TI16Value)",1,2,2
"org.apache.hive.service.cli.thrift.TI16Value.TI16ValueTupleScheme.write(TProtocol,TI16Value)",1,3,3
"org.apache.hive.service.cli.thrift.TI16Value.TI16ValueTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TI16Value._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TI16Value._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.compareTo(TI16Value)",5,3,5
"org.apache.hive.service.cli.thrift.TI16Value.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TI16Value.equals(TI16Value)",5,3,9
"org.apache.hive.service.cli.thrift.TI16Value.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TI16Value.getValue()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TI16Value.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TI16Value.isSetValue()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TI16Value.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TI16Value.setValue(short)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.setValueIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TI16Value.unsetValue()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.validate()",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TI16Value.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TI32Column.TI32Column()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.TI32Column(List<Integer>,ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.TI32Column(TI32Column)",1,4,4
"org.apache.hive.service.cli.thrift.TI32Column.TI32ColumnStandardScheme.read(TProtocol,TI32Column)",4,6,9
"org.apache.hive.service.cli.thrift.TI32Column.TI32ColumnStandardScheme.write(TProtocol,TI32Column)",1,4,4
"org.apache.hive.service.cli.thrift.TI32Column.TI32ColumnStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.TI32ColumnTupleScheme.read(TProtocol,TI32Column)",1,2,2
"org.apache.hive.service.cli.thrift.TI32Column.TI32ColumnTupleScheme.write(TProtocol,TI32Column)",1,2,2
"org.apache.hive.service.cli.thrift.TI32Column.TI32ColumnTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TI32Column._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TI32Column._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.addToValues(int)",1,1,2
"org.apache.hive.service.cli.thrift.TI32Column.bufferForNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.compareTo(TI32Column)",8,4,8
"org.apache.hive.service.cli.thrift.TI32Column.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TI32Column.equals(TI32Column)",8,7,16
"org.apache.hive.service.cli.thrift.TI32Column.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TI32Column.getNulls()",1,2,2
"org.apache.hive.service.cli.thrift.TI32Column.getValues()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.getValuesIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TI32Column.getValuesSize()",1,2,2
"org.apache.hive.service.cli.thrift.TI32Column.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TI32Column.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TI32Column.isSetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.isSetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TI32Column.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TI32Column.setNulls(ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.setNulls(byte[])",1,2,2
"org.apache.hive.service.cli.thrift.TI32Column.setNullsIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TI32Column.setValues(List<Integer>)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.setValuesIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TI32Column.toString()",1,4,4
"org.apache.hive.service.cli.thrift.TI32Column.unsetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.unsetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.validate()",3,3,3
"org.apache.hive.service.cli.thrift.TI32Column.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Column.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TI32Value.TI32Value()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.TI32Value(TI32Value)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.TI32ValueStandardScheme.read(TProtocol,TI32Value)",4,4,6
"org.apache.hive.service.cli.thrift.TI32Value.TI32ValueStandardScheme.write(TProtocol,TI32Value)",1,2,2
"org.apache.hive.service.cli.thrift.TI32Value.TI32ValueStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.TI32ValueTupleScheme.read(TProtocol,TI32Value)",1,2,2
"org.apache.hive.service.cli.thrift.TI32Value.TI32ValueTupleScheme.write(TProtocol,TI32Value)",1,3,3
"org.apache.hive.service.cli.thrift.TI32Value.TI32ValueTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TI32Value._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TI32Value._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.compareTo(TI32Value)",5,3,5
"org.apache.hive.service.cli.thrift.TI32Value.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TI32Value.equals(TI32Value)",5,3,9
"org.apache.hive.service.cli.thrift.TI32Value.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TI32Value.getValue()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TI32Value.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TI32Value.isSetValue()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TI32Value.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TI32Value.setValue(int)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.setValueIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TI32Value.unsetValue()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.validate()",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TI32Value.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TI64Column.TI64Column()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.TI64Column(List<Long>,ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.TI64Column(TI64Column)",1,4,4
"org.apache.hive.service.cli.thrift.TI64Column.TI64ColumnStandardScheme.read(TProtocol,TI64Column)",4,6,9
"org.apache.hive.service.cli.thrift.TI64Column.TI64ColumnStandardScheme.write(TProtocol,TI64Column)",1,4,4
"org.apache.hive.service.cli.thrift.TI64Column.TI64ColumnStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.TI64ColumnTupleScheme.read(TProtocol,TI64Column)",1,2,2
"org.apache.hive.service.cli.thrift.TI64Column.TI64ColumnTupleScheme.write(TProtocol,TI64Column)",1,2,2
"org.apache.hive.service.cli.thrift.TI64Column.TI64ColumnTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TI64Column._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TI64Column._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.addToValues(long)",1,1,2
"org.apache.hive.service.cli.thrift.TI64Column.bufferForNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.compareTo(TI64Column)",8,4,8
"org.apache.hive.service.cli.thrift.TI64Column.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TI64Column.equals(TI64Column)",8,7,16
"org.apache.hive.service.cli.thrift.TI64Column.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TI64Column.getNulls()",1,2,2
"org.apache.hive.service.cli.thrift.TI64Column.getValues()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.getValuesIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TI64Column.getValuesSize()",1,2,2
"org.apache.hive.service.cli.thrift.TI64Column.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TI64Column.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TI64Column.isSetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.isSetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TI64Column.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TI64Column.setNulls(ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.setNulls(byte[])",1,2,2
"org.apache.hive.service.cli.thrift.TI64Column.setNullsIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TI64Column.setValues(List<Long>)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.setValuesIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TI64Column.toString()",1,4,4
"org.apache.hive.service.cli.thrift.TI64Column.unsetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.unsetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.validate()",3,3,3
"org.apache.hive.service.cli.thrift.TI64Column.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Column.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TI64Value.TI64Value()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.TI64Value(TI64Value)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.TI64ValueStandardScheme.read(TProtocol,TI64Value)",4,4,6
"org.apache.hive.service.cli.thrift.TI64Value.TI64ValueStandardScheme.write(TProtocol,TI64Value)",1,2,2
"org.apache.hive.service.cli.thrift.TI64Value.TI64ValueStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.TI64ValueTupleScheme.read(TProtocol,TI64Value)",1,2,2
"org.apache.hive.service.cli.thrift.TI64Value.TI64ValueTupleScheme.write(TProtocol,TI64Value)",1,3,3
"org.apache.hive.service.cli.thrift.TI64Value.TI64ValueTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TI64Value._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TI64Value._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.compareTo(TI64Value)",5,3,5
"org.apache.hive.service.cli.thrift.TI64Value.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TI64Value.equals(TI64Value)",5,3,9
"org.apache.hive.service.cli.thrift.TI64Value.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TI64Value.getValue()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TI64Value.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TI64Value.isSetValue()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TI64Value.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TI64Value.setValue(long)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.setValueIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TI64Value.unsetValue()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.validate()",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TI64Value.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TMapTypeEntry.TMapTypeEntry()",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.TMapTypeEntry(TMapTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.TMapTypeEntry(int,int)",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.TMapTypeEntryStandardScheme.read(TProtocol,TMapTypeEntry)",4,5,8
"org.apache.hive.service.cli.thrift.TMapTypeEntry.TMapTypeEntryStandardScheme.write(TProtocol,TMapTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.TMapTypeEntryStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.TMapTypeEntryTupleScheme.read(TProtocol,TMapTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.TMapTypeEntryTupleScheme.write(TProtocol,TMapTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.TMapTypeEntryTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TMapTypeEntry._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TMapTypeEntry._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.compareTo(TMapTypeEntry)",8,4,8
"org.apache.hive.service.cli.thrift.TMapTypeEntry.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TMapTypeEntry.equals(TMapTypeEntry)",8,1,12
"org.apache.hive.service.cli.thrift.TMapTypeEntry.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TMapTypeEntry.getKeyTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.getValueTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TMapTypeEntry.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TMapTypeEntry.isSetKeyTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.isSetValueTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TMapTypeEntry.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TMapTypeEntry.setKeyTypePtr(int)",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.setKeyTypePtrIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.setValueTypePtr(int)",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.setValueTypePtrIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TMapTypeEntry.unsetKeyTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.unsetValueTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.validate()",3,3,3
"org.apache.hive.service.cli.thrift.TMapTypeEntry.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TMapTypeEntry.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionReq.TOpenSessionReq()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.TOpenSessionReq(TOpenSessionReq)",1,3,6
"org.apache.hive.service.cli.thrift.TOpenSessionReq.TOpenSessionReq(TProtocolVersion)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.TOpenSessionReqStandardScheme.read(TProtocol,TOpenSessionReq)",4,8,13
"org.apache.hive.service.cli.thrift.TOpenSessionReq.TOpenSessionReqStandardScheme.write(TProtocol,TOpenSessionReq)",1,9,9
"org.apache.hive.service.cli.thrift.TOpenSessionReq.TOpenSessionReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.TOpenSessionReqTupleScheme.read(TProtocol,TOpenSessionReq)",1,5,5
"org.apache.hive.service.cli.thrift.TOpenSessionReq.TOpenSessionReqTupleScheme.write(TProtocol,TOpenSessionReq)",1,8,8
"org.apache.hive.service.cli.thrift.TOpenSessionReq.TOpenSessionReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq._Fields.findByThriftId(int)",6,2,6
"org.apache.hive.service.cli.thrift.TOpenSessionReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.compareTo(TOpenSessionReq)",14,6,14
"org.apache.hive.service.cli.thrift.TOpenSessionReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TOpenSessionReq.equals(TOpenSessionReq)",14,13,30
"org.apache.hive.service.cli.thrift.TOpenSessionReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.getClient_protocol()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.getConfiguration()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.getConfigurationSize()",1,2,2
"org.apache.hive.service.cli.thrift.TOpenSessionReq.getFieldValue(_Fields)",5,5,5
"org.apache.hive.service.cli.thrift.TOpenSessionReq.getPassword()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.getUsername()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.hashCode()",1,9,9
"org.apache.hive.service.cli.thrift.TOpenSessionReq.isSet(_Fields)",6,5,6
"org.apache.hive.service.cli.thrift.TOpenSessionReq.isSetClient_protocol()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.isSetConfiguration()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.isSetPassword()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.isSetUsername()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.putToConfiguration(String,String)",1,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionReq.setClient_protocol(TProtocolVersion)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.setClient_protocolIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionReq.setConfiguration(Map<String, String>)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.setConfigurationIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionReq.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hive.service.cli.thrift.TOpenSessionReq.setPassword(String)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.setPasswordIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionReq.setUsername(String)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.setUsernameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionReq.toString()",1,11,11
"org.apache.hive.service.cli.thrift.TOpenSessionReq.unsetClient_protocol()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.unsetConfiguration()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.unsetPassword()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.unsetUsername()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.validate()",2,2,2
"org.apache.hive.service.cli.thrift.TOpenSessionReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionResp.TOpenSessionResp()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.TOpenSessionResp(TOpenSessionResp)",1,3,6
"org.apache.hive.service.cli.thrift.TOpenSessionResp.TOpenSessionResp(TStatus,TProtocolVersion)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.TOpenSessionRespStandardScheme.read(TProtocol,TOpenSessionResp)",4,8,13
"org.apache.hive.service.cli.thrift.TOpenSessionResp.TOpenSessionRespStandardScheme.write(TProtocol,TOpenSessionResp)",1,8,8
"org.apache.hive.service.cli.thrift.TOpenSessionResp.TOpenSessionRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.TOpenSessionRespTupleScheme.read(TProtocol,TOpenSessionResp)",1,4,4
"org.apache.hive.service.cli.thrift.TOpenSessionResp.TOpenSessionRespTupleScheme.write(TProtocol,TOpenSessionResp)",1,6,6
"org.apache.hive.service.cli.thrift.TOpenSessionResp.TOpenSessionRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp._Fields.findByThriftId(int)",6,2,6
"org.apache.hive.service.cli.thrift.TOpenSessionResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.compareTo(TOpenSessionResp)",14,6,14
"org.apache.hive.service.cli.thrift.TOpenSessionResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TOpenSessionResp.equals(TOpenSessionResp)",14,13,30
"org.apache.hive.service.cli.thrift.TOpenSessionResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.getConfiguration()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.getConfigurationSize()",1,2,2
"org.apache.hive.service.cli.thrift.TOpenSessionResp.getFieldValue(_Fields)",5,5,5
"org.apache.hive.service.cli.thrift.TOpenSessionResp.getServerProtocolVersion()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.hashCode()",1,9,9
"org.apache.hive.service.cli.thrift.TOpenSessionResp.isSet(_Fields)",6,5,6
"org.apache.hive.service.cli.thrift.TOpenSessionResp.isSetConfiguration()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.isSetServerProtocolVersion()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.isSetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.putToConfiguration(String,String)",1,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionResp.setConfiguration(Map<String, String>)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.setConfigurationIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionResp.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hive.service.cli.thrift.TOpenSessionResp.setServerProtocolVersion(TProtocolVersion)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.setServerProtocolVersionIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionResp.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.setSessionHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TOpenSessionResp.toString()",1,10,10
"org.apache.hive.service.cli.thrift.TOpenSessionResp.unsetConfiguration()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.unsetServerProtocolVersion()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.unsetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.validate()",3,5,5
"org.apache.hive.service.cli.thrift.TOpenSessionResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TOpenSessionResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TOperationHandle.TOperationHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.TOperationHandle(THandleIdentifier,TOperationType,boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.TOperationHandle(TOperationHandle)",1,1,3
"org.apache.hive.service.cli.thrift.TOperationHandle.TOperationHandleStandardScheme.read(TProtocol,TOperationHandle)",4,7,12
"org.apache.hive.service.cli.thrift.TOperationHandle.TOperationHandleStandardScheme.write(TProtocol,TOperationHandle)",1,4,4
"org.apache.hive.service.cli.thrift.TOperationHandle.TOperationHandleStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.TOperationHandleTupleScheme.read(TProtocol,TOperationHandle)",1,2,2
"org.apache.hive.service.cli.thrift.TOperationHandle.TOperationHandleTupleScheme.write(TProtocol,TOperationHandle)",1,3,3
"org.apache.hive.service.cli.thrift.TOperationHandle.TOperationHandleTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle._Fields.findByThriftId(int)",6,2,6
"org.apache.hive.service.cli.thrift.TOperationHandle._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TOperationHandle._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.compareTo(TOperationHandle)",14,6,14
"org.apache.hive.service.cli.thrift.TOperationHandle.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TOperationHandle.equals(TOperationHandle)",14,9,28
"org.apache.hive.service.cli.thrift.TOperationHandle.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.getFieldValue(_Fields)",5,5,5
"org.apache.hive.service.cli.thrift.TOperationHandle.getModifiedRowCount()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.getOperationId()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.getOperationType()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.hashCode()",1,8,8
"org.apache.hive.service.cli.thrift.TOperationHandle.isHasResultSet()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.isSet(_Fields)",6,5,6
"org.apache.hive.service.cli.thrift.TOperationHandle.isSetHasResultSet()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.isSetModifiedRowCount()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.isSetOperationId()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.isSetOperationType()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TOperationHandle.setFieldValue(_Fields,Object)",2,6,9
"org.apache.hive.service.cli.thrift.TOperationHandle.setHasResultSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.setHasResultSetIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.setModifiedRowCount(double)",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.setModifiedRowCountIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.setOperationId(THandleIdentifier)",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.setOperationIdIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TOperationHandle.setOperationType(TOperationType)",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.setOperationTypeIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TOperationHandle.toString()",1,7,7
"org.apache.hive.service.cli.thrift.TOperationHandle.unsetHasResultSet()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.unsetModifiedRowCount()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.unsetOperationId()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.unsetOperationType()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.validate()",4,5,5
"org.apache.hive.service.cli.thrift.TOperationHandle.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TOperationHandle.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TOperationState.TOperationState(int)",1,1,1
"org.apache.hive.service.cli.thrift.TOperationState.findByValue(int)",10,2,10
"org.apache.hive.service.cli.thrift.TOperationState.getValue()",1,1,1
"org.apache.hive.service.cli.thrift.TOperationType.TOperationType(int)",1,1,1
"org.apache.hive.service.cli.thrift.TOperationType.findByValue(int)",11,2,11
"org.apache.hive.service.cli.thrift.TOperationType.getValue()",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.TPrimitiveTypeEntry()",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.TPrimitiveTypeEntry(TPrimitiveTypeEntry)",1,1,3
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.TPrimitiveTypeEntry(TTypeId)",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.TPrimitiveTypeEntryStandardScheme.read(TProtocol,TPrimitiveTypeEntry)",4,5,8
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.TPrimitiveTypeEntryStandardScheme.write(TProtocol,TPrimitiveTypeEntry)",1,4,4
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.TPrimitiveTypeEntryStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.TPrimitiveTypeEntryTupleScheme.read(TProtocol,TPrimitiveTypeEntry)",1,2,2
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.TPrimitiveTypeEntryTupleScheme.write(TProtocol,TPrimitiveTypeEntry)",1,3,3
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.TPrimitiveTypeEntryTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.compareTo(TPrimitiveTypeEntry)",8,4,8
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.equals(TPrimitiveTypeEntry)",8,7,16
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.getType()",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.getTypeQualifiers()",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.isSetType()",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.isSetTypeQualifiers()",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.setType(TTypeId)",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.setTypeIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.setTypeQualifiers(TTypeQualifiers)",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.setTypeQualifiersIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.toString()",1,5,5
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.unsetType()",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.unsetTypeQualifiers()",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TPrimitiveTypeEntry.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TProtocolVersion.TProtocolVersion(int)",1,1,1
"org.apache.hive.service.cli.thrift.TProtocolVersion.findByValue(int)",9,2,9
"org.apache.hive.service.cli.thrift.TProtocolVersion.getValue()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.TRenewDelegationTokenReq()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.TRenewDelegationTokenReq(TRenewDelegationTokenReq)",1,1,3
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.TRenewDelegationTokenReq(TSessionHandle,String)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.TRenewDelegationTokenReqStandardScheme.read(TProtocol,TRenewDelegationTokenReq)",4,5,8
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.TRenewDelegationTokenReqStandardScheme.write(TProtocol,TRenewDelegationTokenReq)",1,3,3
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.TRenewDelegationTokenReqStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.TRenewDelegationTokenReqTupleScheme.read(TProtocol,TRenewDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.TRenewDelegationTokenReqTupleScheme.write(TProtocol,TRenewDelegationTokenReq)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.TRenewDelegationTokenReqTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.compareTo(TRenewDelegationTokenReq)",8,4,8
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.equals(TRenewDelegationTokenReq)",8,7,16
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.getDelegationToken()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.getSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.isSetDelegationToken()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.isSetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.setDelegationToken(String)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.setDelegationTokenIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.setSessionHandle(TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.setSessionHandleIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.toString()",1,4,4
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.unsetDelegationToken()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.unsetSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.validate()",3,4,4
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenReq.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.TRenewDelegationTokenResp()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.TRenewDelegationTokenResp(TRenewDelegationTokenResp)",1,1,2
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.TRenewDelegationTokenResp(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.TRenewDelegationTokenRespStandardScheme.read(TProtocol,TRenewDelegationTokenResp)",4,4,6
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.TRenewDelegationTokenRespStandardScheme.write(TProtocol,TRenewDelegationTokenResp)",1,2,2
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.TRenewDelegationTokenRespStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.TRenewDelegationTokenRespTupleScheme.read(TProtocol,TRenewDelegationTokenResp)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.TRenewDelegationTokenRespTupleScheme.write(TProtocol,TRenewDelegationTokenResp)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.TRenewDelegationTokenRespTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.compareTo(TRenewDelegationTokenResp)",5,3,5
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.equals(TRenewDelegationTokenResp)",5,4,9
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.getStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.isSetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.setStatus(TStatus)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.setStatusIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.unsetStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TRenewDelegationTokenResp.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TRow.TRow()",1,1,1
"org.apache.hive.service.cli.thrift.TRow.TRow(List<TColumnValue>)",1,1,1
"org.apache.hive.service.cli.thrift.TRow.TRow(TRow)",1,3,3
"org.apache.hive.service.cli.thrift.TRow.TRowStandardScheme.read(TProtocol,TRow)",4,5,7
"org.apache.hive.service.cli.thrift.TRow.TRowStandardScheme.write(TProtocol,TRow)",1,3,3
"org.apache.hive.service.cli.thrift.TRow.TRowStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TRow.TRowTupleScheme.read(TProtocol,TRow)",1,2,2
"org.apache.hive.service.cli.thrift.TRow.TRowTupleScheme.write(TProtocol,TRow)",1,2,2
"org.apache.hive.service.cli.thrift.TRow.TRowTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TRow._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TRow._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TRow._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TRow._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TRow._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TRow._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TRow.addToColVals(TColumnValue)",1,1,2
"org.apache.hive.service.cli.thrift.TRow.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TRow.compareTo(TRow)",5,3,5
"org.apache.hive.service.cli.thrift.TRow.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TRow.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TRow.equals(TRow)",5,4,9
"org.apache.hive.service.cli.thrift.TRow.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TRow.getColVals()",1,1,1
"org.apache.hive.service.cli.thrift.TRow.getColValsIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TRow.getColValsSize()",1,2,2
"org.apache.hive.service.cli.thrift.TRow.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TRow.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TRow.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TRow.isSetColVals()",1,1,1
"org.apache.hive.service.cli.thrift.TRow.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TRow.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TRow.setColVals(List<TColumnValue>)",1,1,1
"org.apache.hive.service.cli.thrift.TRow.setColValsIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TRow.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TRow.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TRow.unsetColVals()",1,1,1
"org.apache.hive.service.cli.thrift.TRow.validate()",2,2,2
"org.apache.hive.service.cli.thrift.TRow.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TRow.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TRowSet.TRowSet()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.TRowSet(TRowSet)",1,5,5
"org.apache.hive.service.cli.thrift.TRowSet.TRowSet(long,List<TRow>)",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.TRowSetStandardScheme.read(TProtocol,TRowSet)",4,8,12
"org.apache.hive.service.cli.thrift.TRowSet.TRowSetStandardScheme.write(TProtocol,TRowSet)",1,6,6
"org.apache.hive.service.cli.thrift.TRowSet.TRowSetStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.TRowSetTupleScheme.read(TProtocol,TRowSet)",1,4,4
"org.apache.hive.service.cli.thrift.TRowSet.TRowSetTupleScheme.write(TProtocol,TRowSet)",1,5,5
"org.apache.hive.service.cli.thrift.TRowSet.TRowSetTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet._Fields.findByThriftId(int)",5,2,5
"org.apache.hive.service.cli.thrift.TRowSet._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TRowSet._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.addToColumns(TColumn)",1,1,2
"org.apache.hive.service.cli.thrift.TRowSet.addToRows(TRow)",1,1,2
"org.apache.hive.service.cli.thrift.TRowSet.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.compareTo(TRowSet)",11,5,11
"org.apache.hive.service.cli.thrift.TRowSet.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TRowSet.equals(TRowSet)",11,7,21
"org.apache.hive.service.cli.thrift.TRowSet.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.getColumns()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.getColumnsIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TRowSet.getColumnsSize()",1,2,2
"org.apache.hive.service.cli.thrift.TRowSet.getFieldValue(_Fields)",4,4,4
"org.apache.hive.service.cli.thrift.TRowSet.getRows()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.getRowsIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TRowSet.getRowsSize()",1,2,2
"org.apache.hive.service.cli.thrift.TRowSet.getStartRowOffset()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.hashCode()",1,6,6
"org.apache.hive.service.cli.thrift.TRowSet.isSet(_Fields)",5,4,5
"org.apache.hive.service.cli.thrift.TRowSet.isSetColumns()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.isSetRows()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.isSetStartRowOffset()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TRowSet.setColumns(List<TColumn>)",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.setColumnsIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TRowSet.setFieldValue(_Fields,Object)",2,5,7
"org.apache.hive.service.cli.thrift.TRowSet.setRows(List<TRow>)",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.setRowsIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TRowSet.setStartRowOffset(long)",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.setStartRowOffsetIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.toString()",1,6,6
"org.apache.hive.service.cli.thrift.TRowSet.unsetColumns()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.unsetRows()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.unsetStartRowOffset()",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.validate()",3,3,3
"org.apache.hive.service.cli.thrift.TRowSet.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TRowSet.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TSessionHandle.TSessionHandle()",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.TSessionHandle(THandleIdentifier)",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.TSessionHandle(TSessionHandle)",1,1,2
"org.apache.hive.service.cli.thrift.TSessionHandle.TSessionHandleStandardScheme.read(TProtocol,TSessionHandle)",4,4,6
"org.apache.hive.service.cli.thrift.TSessionHandle.TSessionHandleStandardScheme.write(TProtocol,TSessionHandle)",1,2,2
"org.apache.hive.service.cli.thrift.TSessionHandle.TSessionHandleStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.TSessionHandleTupleScheme.read(TProtocol,TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.TSessionHandleTupleScheme.write(TProtocol,TSessionHandle)",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.TSessionHandleTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TSessionHandle._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TSessionHandle._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.compareTo(TSessionHandle)",5,3,5
"org.apache.hive.service.cli.thrift.TSessionHandle.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TSessionHandle.equals(TSessionHandle)",5,4,9
"org.apache.hive.service.cli.thrift.TSessionHandle.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TSessionHandle.getSessionId()",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TSessionHandle.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TSessionHandle.isSetSessionId()",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TSessionHandle.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TSessionHandle.setSessionId(THandleIdentifier)",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.setSessionIdIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TSessionHandle.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TSessionHandle.unsetSessionId()",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.validate()",2,3,3
"org.apache.hive.service.cli.thrift.TSessionHandle.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TSessionHandle.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TStatus.TStatus()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.TStatus(TStatus)",1,3,6
"org.apache.hive.service.cli.thrift.TStatus.TStatus(TStatusCode)",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.TStatusStandardScheme.read(TProtocol,TStatus)",4,9,15
"org.apache.hive.service.cli.thrift.TStatus.TStatusStandardScheme.write(TProtocol,TStatus)",1,10,10
"org.apache.hive.service.cli.thrift.TStatus.TStatusStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.TStatusTupleScheme.read(TProtocol,TStatus)",1,6,6
"org.apache.hive.service.cli.thrift.TStatus.TStatusTupleScheme.write(TProtocol,TStatus)",1,10,10
"org.apache.hive.service.cli.thrift.TStatus.TStatusTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TStatus._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TStatus._Fields.findByThriftId(int)",7,2,7
"org.apache.hive.service.cli.thrift.TStatus._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TStatus._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.addToInfoMessages(String)",1,1,2
"org.apache.hive.service.cli.thrift.TStatus.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.compareTo(TStatus)",17,7,17
"org.apache.hive.service.cli.thrift.TStatus.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TStatus.equals(TStatus)",17,15,37
"org.apache.hive.service.cli.thrift.TStatus.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.getErrorCode()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.getErrorMessage()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.getFieldValue(_Fields)",6,6,6
"org.apache.hive.service.cli.thrift.TStatus.getInfoMessages()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.getInfoMessagesIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TStatus.getInfoMessagesSize()",1,2,2
"org.apache.hive.service.cli.thrift.TStatus.getSqlState()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.getStatusCode()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.hashCode()",1,11,11
"org.apache.hive.service.cli.thrift.TStatus.isSet(_Fields)",7,6,7
"org.apache.hive.service.cli.thrift.TStatus.isSetErrorCode()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.isSetErrorMessage()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.isSetInfoMessages()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.isSetSqlState()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.isSetStatusCode()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TStatus.setErrorCode(int)",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.setErrorCodeIsSet(boolean)",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.setErrorMessage(String)",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.setErrorMessageIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TStatus.setFieldValue(_Fields,Object)",2,7,11
"org.apache.hive.service.cli.thrift.TStatus.setInfoMessages(List<String>)",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.setInfoMessagesIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TStatus.setSqlState(String)",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.setSqlStateIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TStatus.setStatusCode(TStatusCode)",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.setStatusCodeIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TStatus.toString()",1,13,13
"org.apache.hive.service.cli.thrift.TStatus.unsetErrorCode()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.unsetErrorMessage()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.unsetInfoMessages()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.unsetSqlState()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.unsetStatusCode()",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.validate()",2,2,2
"org.apache.hive.service.cli.thrift.TStatus.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TStatus.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TStatusCode.TStatusCode(int)",1,1,1
"org.apache.hive.service.cli.thrift.TStatusCode.findByValue(int)",7,2,7
"org.apache.hive.service.cli.thrift.TStatusCode.getValue()",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.TStringColumn()",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.TStringColumn(List<String>,ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.TStringColumn(TStringColumn)",1,4,4
"org.apache.hive.service.cli.thrift.TStringColumn.TStringColumnStandardScheme.read(TProtocol,TStringColumn)",4,6,9
"org.apache.hive.service.cli.thrift.TStringColumn.TStringColumnStandardScheme.write(TProtocol,TStringColumn)",1,4,4
"org.apache.hive.service.cli.thrift.TStringColumn.TStringColumnStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.TStringColumnTupleScheme.read(TProtocol,TStringColumn)",1,2,2
"org.apache.hive.service.cli.thrift.TStringColumn.TStringColumnTupleScheme.write(TProtocol,TStringColumn)",1,2,2
"org.apache.hive.service.cli.thrift.TStringColumn.TStringColumnTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TStringColumn._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TStringColumn._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.addToValues(String)",1,1,2
"org.apache.hive.service.cli.thrift.TStringColumn.bufferForNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.compareTo(TStringColumn)",8,4,8
"org.apache.hive.service.cli.thrift.TStringColumn.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TStringColumn.equals(TStringColumn)",8,7,16
"org.apache.hive.service.cli.thrift.TStringColumn.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.getFieldValue(_Fields)",3,3,3
"org.apache.hive.service.cli.thrift.TStringColumn.getNulls()",1,2,2
"org.apache.hive.service.cli.thrift.TStringColumn.getValues()",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.getValuesIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TStringColumn.getValuesSize()",1,2,2
"org.apache.hive.service.cli.thrift.TStringColumn.hashCode()",1,5,5
"org.apache.hive.service.cli.thrift.TStringColumn.isSet(_Fields)",4,3,4
"org.apache.hive.service.cli.thrift.TStringColumn.isSetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.isSetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TStringColumn.setFieldValue(_Fields,Object)",2,4,5
"org.apache.hive.service.cli.thrift.TStringColumn.setNulls(ByteBuffer)",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.setNulls(byte[])",1,2,2
"org.apache.hive.service.cli.thrift.TStringColumn.setNullsIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TStringColumn.setValues(List<String>)",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.setValuesIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TStringColumn.toString()",1,4,4
"org.apache.hive.service.cli.thrift.TStringColumn.unsetNulls()",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.unsetValues()",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.validate()",3,3,3
"org.apache.hive.service.cli.thrift.TStringColumn.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TStringColumn.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TStringValue.TStringValue()",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue.TStringValue(TStringValue)",1,1,2
"org.apache.hive.service.cli.thrift.TStringValue.TStringValueStandardScheme.read(TProtocol,TStringValue)",4,4,6
"org.apache.hive.service.cli.thrift.TStringValue.TStringValueStandardScheme.write(TProtocol,TStringValue)",1,3,3
"org.apache.hive.service.cli.thrift.TStringValue.TStringValueStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue.TStringValueTupleScheme.read(TProtocol,TStringValue)",1,2,2
"org.apache.hive.service.cli.thrift.TStringValue.TStringValueTupleScheme.write(TProtocol,TStringValue)",1,3,3
"org.apache.hive.service.cli.thrift.TStringValue.TStringValueTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TStringValue._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TStringValue._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue.compareTo(TStringValue)",5,3,5
"org.apache.hive.service.cli.thrift.TStringValue.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TStringValue.equals(TStringValue)",5,4,9
"org.apache.hive.service.cli.thrift.TStringValue.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TStringValue.getValue()",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TStringValue.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TStringValue.isSetValue()",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TStringValue.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TStringValue.setValue(String)",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue.setValueIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TStringValue.toString()",1,3,3
"org.apache.hive.service.cli.thrift.TStringValue.unsetValue()",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue.validate()",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TStringValue.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TStructTypeEntry.TStructTypeEntry()",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry.TStructTypeEntry(Map<String, Integer>)",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry.TStructTypeEntry(TStructTypeEntry)",1,3,3
"org.apache.hive.service.cli.thrift.TStructTypeEntry.TStructTypeEntryStandardScheme.read(TProtocol,TStructTypeEntry)",4,5,7
"org.apache.hive.service.cli.thrift.TStructTypeEntry.TStructTypeEntryStandardScheme.write(TProtocol,TStructTypeEntry)",1,3,3
"org.apache.hive.service.cli.thrift.TStructTypeEntry.TStructTypeEntryStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry.TStructTypeEntryTupleScheme.read(TProtocol,TStructTypeEntry)",1,2,2
"org.apache.hive.service.cli.thrift.TStructTypeEntry.TStructTypeEntryTupleScheme.write(TProtocol,TStructTypeEntry)",1,2,2
"org.apache.hive.service.cli.thrift.TStructTypeEntry.TStructTypeEntryTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TStructTypeEntry._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TStructTypeEntry._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry.compareTo(TStructTypeEntry)",5,3,5
"org.apache.hive.service.cli.thrift.TStructTypeEntry.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TStructTypeEntry.equals(TStructTypeEntry)",5,4,9
"org.apache.hive.service.cli.thrift.TStructTypeEntry.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TStructTypeEntry.getNameToTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry.getNameToTypePtrSize()",1,2,2
"org.apache.hive.service.cli.thrift.TStructTypeEntry.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TStructTypeEntry.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TStructTypeEntry.isSetNameToTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry.putToNameToTypePtr(String,int)",1,1,2
"org.apache.hive.service.cli.thrift.TStructTypeEntry.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TStructTypeEntry.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TStructTypeEntry.setNameToTypePtr(Map<String, Integer>)",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry.setNameToTypePtrIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TStructTypeEntry.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TStructTypeEntry.unsetNameToTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry.validate()",2,2,2
"org.apache.hive.service.cli.thrift.TStructTypeEntry.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TStructTypeEntry.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TTableSchema.TTableSchema()",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema.TTableSchema(List<TColumnDesc>)",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema.TTableSchema(TTableSchema)",1,3,3
"org.apache.hive.service.cli.thrift.TTableSchema.TTableSchemaStandardScheme.read(TProtocol,TTableSchema)",4,5,7
"org.apache.hive.service.cli.thrift.TTableSchema.TTableSchemaStandardScheme.write(TProtocol,TTableSchema)",1,3,3
"org.apache.hive.service.cli.thrift.TTableSchema.TTableSchemaStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema.TTableSchemaTupleScheme.read(TProtocol,TTableSchema)",1,2,2
"org.apache.hive.service.cli.thrift.TTableSchema.TTableSchemaTupleScheme.write(TProtocol,TTableSchema)",1,2,2
"org.apache.hive.service.cli.thrift.TTableSchema.TTableSchemaTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TTableSchema._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TTableSchema._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema.addToColumns(TColumnDesc)",1,1,2
"org.apache.hive.service.cli.thrift.TTableSchema.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema.compareTo(TTableSchema)",5,3,5
"org.apache.hive.service.cli.thrift.TTableSchema.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TTableSchema.equals(TTableSchema)",5,4,9
"org.apache.hive.service.cli.thrift.TTableSchema.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema.getColumns()",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema.getColumnsIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TTableSchema.getColumnsSize()",1,2,2
"org.apache.hive.service.cli.thrift.TTableSchema.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TTableSchema.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TTableSchema.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TTableSchema.isSetColumns()",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TTableSchema.setColumns(List<TColumnDesc>)",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema.setColumnsIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TTableSchema.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TTableSchema.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TTableSchema.unsetColumns()",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema.validate()",2,2,2
"org.apache.hive.service.cli.thrift.TTableSchema.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TTableSchema.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TTypeDesc.TTypeDesc()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc.TTypeDesc(List<TTypeEntry>)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc.TTypeDesc(TTypeDesc)",1,3,3
"org.apache.hive.service.cli.thrift.TTypeDesc.TTypeDescStandardScheme.read(TProtocol,TTypeDesc)",4,5,7
"org.apache.hive.service.cli.thrift.TTypeDesc.TTypeDescStandardScheme.write(TProtocol,TTypeDesc)",1,3,3
"org.apache.hive.service.cli.thrift.TTypeDesc.TTypeDescStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc.TTypeDescTupleScheme.read(TProtocol,TTypeDesc)",1,2,2
"org.apache.hive.service.cli.thrift.TTypeDesc.TTypeDescTupleScheme.write(TProtocol,TTypeDesc)",1,2,2
"org.apache.hive.service.cli.thrift.TTypeDesc.TTypeDescTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TTypeDesc._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TTypeDesc._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc.addToTypes(TTypeEntry)",1,1,2
"org.apache.hive.service.cli.thrift.TTypeDesc.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc.compareTo(TTypeDesc)",5,3,5
"org.apache.hive.service.cli.thrift.TTypeDesc.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TTypeDesc.equals(TTypeDesc)",5,4,9
"org.apache.hive.service.cli.thrift.TTypeDesc.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TTypeDesc.getTypes()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc.getTypesIterator()",1,2,2
"org.apache.hive.service.cli.thrift.TTypeDesc.getTypesSize()",1,2,2
"org.apache.hive.service.cli.thrift.TTypeDesc.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TTypeDesc.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TTypeDesc.isSetTypes()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TTypeDesc.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TTypeDesc.setTypes(List<TTypeEntry>)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc.setTypesIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TTypeDesc.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TTypeDesc.unsetTypes()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc.validate()",2,2,2
"org.apache.hive.service.cli.thrift.TTypeDesc.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeDesc.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TTypeEntry.TTypeEntry()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.TTypeEntry(TTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.TTypeEntry(_Fields,Object)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry._Fields.findByThriftId(int)",8,2,8
"org.apache.hive.service.cli.thrift.TTypeEntry._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TTypeEntry._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.arrayEntry(TArrayTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.checkType(_Fields,Object)",2,2,14
"org.apache.hive.service.cli.thrift.TTypeEntry.compareTo(TTypeEntry)",2,2,2
"org.apache.hive.service.cli.thrift.TTypeEntry.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.enumForId(short)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.equals(Object)",2,2,2
"org.apache.hive.service.cli.thrift.TTypeEntry.equals(TTypeEntry)",1,3,3
"org.apache.hive.service.cli.thrift.TTypeEntry.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.getArrayEntry()",2,2,2
"org.apache.hive.service.cli.thrift.TTypeEntry.getFieldDesc(_Fields)",8,2,8
"org.apache.hive.service.cli.thrift.TTypeEntry.getMapEntry()",2,2,2
"org.apache.hive.service.cli.thrift.TTypeEntry.getPrimitiveEntry()",2,2,2
"org.apache.hive.service.cli.thrift.TTypeEntry.getStructDesc()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.getStructEntry()",2,2,2
"org.apache.hive.service.cli.thrift.TTypeEntry.getUnionEntry()",2,2,2
"org.apache.hive.service.cli.thrift.TTypeEntry.getUserDefinedTypeEntry()",2,2,2
"org.apache.hive.service.cli.thrift.TTypeEntry.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TTypeEntry.isSetArrayEntry()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.isSetMapEntry()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.isSetPrimitiveEntry()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.isSetStructEntry()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.isSetUnionEntry()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.isSetUserDefinedTypeEntry()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.mapEntry(TMapTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.primitiveEntry(TPrimitiveTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TTypeEntry.setArrayEntry(TArrayTypeEntry)",2,1,2
"org.apache.hive.service.cli.thrift.TTypeEntry.setMapEntry(TMapTypeEntry)",2,1,2
"org.apache.hive.service.cli.thrift.TTypeEntry.setPrimitiveEntry(TPrimitiveTypeEntry)",2,1,2
"org.apache.hive.service.cli.thrift.TTypeEntry.setStructEntry(TStructTypeEntry)",2,1,2
"org.apache.hive.service.cli.thrift.TTypeEntry.setUnionEntry(TUnionTypeEntry)",2,1,2
"org.apache.hive.service.cli.thrift.TTypeEntry.setUserDefinedTypeEntry(TUserDefinedTypeEntry)",2,1,2
"org.apache.hive.service.cli.thrift.TTypeEntry.standardSchemeReadValue(TProtocol,TField)",15,15,15
"org.apache.hive.service.cli.thrift.TTypeEntry.standardSchemeWriteValue(TProtocol)",2,2,8
"org.apache.hive.service.cli.thrift.TTypeEntry.structEntry(TStructTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.tupleSchemeReadValue(TProtocol,short)",3,3,9
"org.apache.hive.service.cli.thrift.TTypeEntry.tupleSchemeWriteValue(TProtocol)",2,2,8
"org.apache.hive.service.cli.thrift.TTypeEntry.unionEntry(TUnionTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.userDefinedTypeEntry(TUserDefinedTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeEntry.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TTypeId.TTypeId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeId.findByValue(int)",22,2,22
"org.apache.hive.service.cli.thrift.TTypeId.getValue()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.TTypeQualifierValue()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.TTypeQualifierValue(TTypeQualifierValue)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.TTypeQualifierValue(_Fields,Object)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue._Fields.findByThriftId(int)",4,2,4
"org.apache.hive.service.cli.thrift.TTypeQualifierValue._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TTypeQualifierValue._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.checkType(_Fields,Object)",2,2,6
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.compareTo(TTypeQualifierValue)",2,2,2
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.enumForId(short)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.equals(Object)",2,2,2
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.equals(TTypeQualifierValue)",1,3,3
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.getFieldDesc(_Fields)",4,2,4
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.getI32Value()",2,2,2
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.getStringValue()",2,2,2
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.getStructDesc()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.i32Value(int)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.isSetI32Value()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.isSetStringValue()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.setI32Value(int)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.setStringValue(String)",2,1,2
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.standardSchemeReadValue(TProtocol,TField)",7,7,7
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.standardSchemeWriteValue(TProtocol)",2,2,4
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.stringValue(String)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.tupleSchemeReadValue(TProtocol,short)",3,3,5
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.tupleSchemeWriteValue(TProtocol)",2,2,4
"org.apache.hive.service.cli.thrift.TTypeQualifierValue.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TTypeQualifiers.TTypeQualifiers()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers.TTypeQualifiers(Map<String, TTypeQualifierValue>)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers.TTypeQualifiers(TTypeQualifiers)",1,3,3
"org.apache.hive.service.cli.thrift.TTypeQualifiers.TTypeQualifiersStandardScheme.read(TProtocol,TTypeQualifiers)",4,5,7
"org.apache.hive.service.cli.thrift.TTypeQualifiers.TTypeQualifiersStandardScheme.write(TProtocol,TTypeQualifiers)",1,3,3
"org.apache.hive.service.cli.thrift.TTypeQualifiers.TTypeQualifiersStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers.TTypeQualifiersTupleScheme.read(TProtocol,TTypeQualifiers)",1,2,2
"org.apache.hive.service.cli.thrift.TTypeQualifiers.TTypeQualifiersTupleScheme.write(TProtocol,TTypeQualifiers)",1,2,2
"org.apache.hive.service.cli.thrift.TTypeQualifiers.TTypeQualifiersTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TTypeQualifiers._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TTypeQualifiers._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers.compareTo(TTypeQualifiers)",5,3,5
"org.apache.hive.service.cli.thrift.TTypeQualifiers.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TTypeQualifiers.equals(TTypeQualifiers)",5,4,9
"org.apache.hive.service.cli.thrift.TTypeQualifiers.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TTypeQualifiers.getQualifiers()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers.getQualifiersSize()",1,2,2
"org.apache.hive.service.cli.thrift.TTypeQualifiers.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TTypeQualifiers.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TTypeQualifiers.isSetQualifiers()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers.putToQualifiers(String,TTypeQualifierValue)",1,1,2
"org.apache.hive.service.cli.thrift.TTypeQualifiers.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TTypeQualifiers.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TTypeQualifiers.setQualifiers(Map<String, TTypeQualifierValue>)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers.setQualifiersIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TTypeQualifiers.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TTypeQualifiers.unsetQualifiers()",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers.validate()",2,2,2
"org.apache.hive.service.cli.thrift.TTypeQualifiers.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TTypeQualifiers.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.TUnionTypeEntry()",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.TUnionTypeEntry(Map<String, Integer>)",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.TUnionTypeEntry(TUnionTypeEntry)",1,3,3
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.TUnionTypeEntryStandardScheme.read(TProtocol,TUnionTypeEntry)",4,5,7
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.TUnionTypeEntryStandardScheme.write(TProtocol,TUnionTypeEntry)",1,3,3
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.TUnionTypeEntryStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.TUnionTypeEntryTupleScheme.read(TProtocol,TUnionTypeEntry)",1,2,2
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.TUnionTypeEntryTupleScheme.write(TProtocol,TUnionTypeEntry)",1,2,2
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.TUnionTypeEntryTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TUnionTypeEntry._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TUnionTypeEntry._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.compareTo(TUnionTypeEntry)",5,3,5
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.equals(TUnionTypeEntry)",5,4,9
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.getNameToTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.getNameToTypePtrSize()",1,2,2
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.isSetNameToTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.putToNameToTypePtr(String,int)",1,1,2
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.setNameToTypePtr(Map<String, Integer>)",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.setNameToTypePtrIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.unsetNameToTypePtr()",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.validate()",2,2,2
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TUnionTypeEntry.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.TUserDefinedTypeEntry()",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.TUserDefinedTypeEntry(String)",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.TUserDefinedTypeEntry(TUserDefinedTypeEntry)",1,1,2
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.TUserDefinedTypeEntryStandardScheme.read(TProtocol,TUserDefinedTypeEntry)",4,4,6
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.TUserDefinedTypeEntryStandardScheme.write(TProtocol,TUserDefinedTypeEntry)",1,2,2
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.TUserDefinedTypeEntryStandardSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.TUserDefinedTypeEntryTupleScheme.read(TProtocol,TUserDefinedTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.TUserDefinedTypeEntryTupleScheme.write(TProtocol,TUserDefinedTypeEntry)",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.TUserDefinedTypeEntryTupleSchemeFactory.getScheme()",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry._Fields._Fields(short,String)",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry._Fields.findByName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry._Fields.findByThriftId(int)",3,2,3
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry._Fields.findByThriftIdOrThrow(int)",2,1,2
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry._Fields.getFieldName()",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry._Fields.getThriftFieldId()",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.clear()",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.compareTo(TUserDefinedTypeEntry)",5,3,5
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.deepCopy()",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.equals(Object)",3,2,3
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.equals(TUserDefinedTypeEntry)",5,4,9
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.fieldForId(int)",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.getFieldValue(_Fields)",2,2,2
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.getTypeClassName()",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.hashCode()",1,3,3
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.isSet(_Fields)",3,2,3
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.isSetTypeClassName()",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.read(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.readObject(ObjectInputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.setFieldValue(_Fields,Object)",2,3,3
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.setTypeClassName(String)",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.setTypeClassNameIsSet(boolean)",1,1,2
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.toString()",1,2,2
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.unsetTypeClassName()",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.validate()",2,2,2
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.write(TProtocol)",1,1,1
"org.apache.hive.service.cli.thrift.TUserDefinedTypeEntry.writeObject(ObjectOutputStream)",1,1,2
"org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.setUp()",1,1,1
"org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.setUpBeforeClass()",1,1,1
"org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.tearDown()",1,1,1
"org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.tearDownAfterClass()",1,1,1
"org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.getClient(TTransport)",1,1,1
"org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.getHttpTransport()",1,1,1
"org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.getRawBinaryTransport()",1,1,1
"org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.setUp()",1,1,1
"org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.setUpBeforeClass()",1,1,1
"org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.tearDown()",1,1,1
"org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.tearDownAfterClass()",1,1,1
"org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testBinaryClientHttpServer()",1,1,1
"org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testIncorrectHttpPath()",1,1,1
"org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testOpenSessionExpectedException(Client)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.ThriftBinaryCLIService(CLIService)",1,1,1
"org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run()",3,6,8
"org.apache.hive.service.cli.thrift.ThriftCLIService.CancelDelegationToken(TCancelDelegationTokenReq)",1,3,3
"org.apache.hive.service.cli.thrift.ThriftCLIService.CancelOperation(TCancelOperationReq)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIService.CloseOperation(TCloseOperationReq)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIService.CloseSession(TCloseSessionReq)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(TExecuteStatementReq)",1,3,3
"org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(TFetchResultsReq)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIService.GetCatalogs(TGetCatalogsReq)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIService.GetColumns(TGetColumnsReq)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIService.GetDelegationToken(TGetDelegationTokenReq)",1,3,3
"org.apache.hive.service.cli.thrift.ThriftCLIService.GetFunctions(TGetFunctionsReq)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIService.GetInfo(TGetInfoReq)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIService.GetOperationStatus(TGetOperationStatusReq)",1,3,3
"org.apache.hive.service.cli.thrift.ThriftCLIService.GetResultSetMetadata(TGetResultSetMetadataReq)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIService.GetSchemas(TGetSchemasReq)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIService.GetTableTypes(TGetTableTypesReq)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIService.GetTables(TGetTablesReq)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIService.GetTypeInfo(TGetTypeInfoReq)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIService.OpenSession(TOpenSessionReq)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIService.RenewDelegationToken(TRenewDelegationTokenReq)",1,3,3
"org.apache.hive.service.cli.thrift.ThriftCLIService.ThriftCLIService(CLIService,String)",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIService.getDelegationToken(String)",2,2,4
"org.apache.hive.service.cli.thrift.ThriftCLIService.getIpAddress()",1,3,3
"org.apache.hive.service.cli.thrift.ThriftCLIService.getMinVersion(TProtocolVersion...)",3,4,5
"org.apache.hive.service.cli.thrift.ThriftCLIService.getProxyUser(String,Map<String, String>,String)",4,5,8
"org.apache.hive.service.cli.thrift.ThriftCLIService.getSessionHandle(TOpenSessionReq,TOpenSessionResp)",1,3,3
"org.apache.hive.service.cli.thrift.ThriftCLIService.getUserName(TOpenSessionReq)",1,5,5
"org.apache.hive.service.cli.thrift.ThriftCLIService.init(HiveConf)",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIService.isKerberosAuthMode()",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIService.start()",1,2,3
"org.apache.hive.service.cli.thrift.ThriftCLIService.stop()",1,6,7
"org.apache.hive.service.cli.thrift.ThriftCLIService.unsecureTokenErrorStatus()",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.ThriftCLIServiceClient(Iface)",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.cancelDelegationToken(SessionHandle,HiveAuthFactory,String)",1,1,2
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.cancelOperation(OperationHandle)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.checkStatus(TStatus)",2,1,2
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.closeOperation(OperationHandle)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.closeSession(SessionHandle)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.executeStatement(SessionHandle,String,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.executeStatementAsync(SessionHandle,String,Map<String, String>)",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.executeStatementInternal(SessionHandle,String,Map<String, String>,boolean)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.fetchResults(OperationHandle)",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.fetchResults(OperationHandle,FetchOrientation,long,FetchType)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.getCatalogs(SessionHandle)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.getColumns(SessionHandle,String,String,String,String)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.getDelegationToken(SessionHandle,HiveAuthFactory,String,String)",1,1,2
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.getFunctions(SessionHandle,String,String,String)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.getInfo(SessionHandle,GetInfoType)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.getOperationStatus(OperationHandle)",1,2,4
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.getResultSetMetadata(OperationHandle)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.getSchemas(SessionHandle,String,String)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.getTableTypes(SessionHandle)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.getTables(SessionHandle,String,String,String,List<String>)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.getTypeInfo(SessionHandle)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.openSession(String,String,Map<String, String>)",1,1,3
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.openSessionWithImpersonation(String,String,Map<String, String>,String)",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient.renewDelegationToken(SessionHandle,HiveAuthFactory,String)",1,1,2
"org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.getServiceClientInternal()",4,1,4
"org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.setUp()",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.setUpBeforeClass()",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.startHiveServer2WithConf(HiveConf)",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.stopHiveServer2()",1,2,2
"org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.tearDown()",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.tearDownAfterClass()",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.testExecuteStatement()",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.testExecuteStatementAsync()",5,5,13
"org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.testGetFunctions()",1,1,1
"org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.testOpenSession()",1,1,1
"org.apache.hive.service.cli.thrift.ThriftHttpCLIService.ThriftHttpCLIService(CLIService)",1,1,1
"org.apache.hive.service.cli.thrift.ThriftHttpCLIService.getHttpPath(String)",1,3,6
"org.apache.hive.service.cli.thrift.ThriftHttpCLIService.run()",3,4,6
"org.apache.hive.service.cli.thrift.ThriftHttpCLIService.verifyHttpConfiguration(HiveConf)",2,4,4
"org.apache.hive.service.cli.thrift.ThriftHttpServlet.HttpKerberosServerAction.HttpKerberosServerAction(HttpServletRequest,UserGroupInformation)",1,1,1
"org.apache.hive.service.cli.thrift.ThriftHttpServlet.HttpKerberosServerAction.getPrincipalWithoutRealm(String)",1,1,1
"org.apache.hive.service.cli.thrift.ThriftHttpServlet.HttpKerberosServerAction.run()",2,3,5
"org.apache.hive.service.cli.thrift.ThriftHttpServlet.ThriftHttpServlet(TProcessor,TProtocolFactory,String,UserGroupInformation,UserGroupInformation)",1,1,1
"org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth(HttpServletRequest)",2,4,4
"org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPasswdAuth(HttpServletRequest,String)",2,2,3
"org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(HttpServletRequest,HttpServletResponse)",1,5,5
"org.apache.hive.service.cli.thrift.ThriftHttpServlet.getAuthHeader(HttpServletRequest,String)",3,4,6
"org.apache.hive.service.cli.thrift.ThriftHttpServlet.getAuthHeaderTokens(HttpServletRequest,String)",1,1,1
"org.apache.hive.service.cli.thrift.ThriftHttpServlet.getDoAsQueryParam(String)",4,3,4
"org.apache.hive.service.cli.thrift.ThriftHttpServlet.getPassword(HttpServletRequest,String)",2,2,3
"org.apache.hive.service.cli.thrift.ThriftHttpServlet.getUsername(HttpServletRequest,String)",2,2,3
"org.apache.hive.service.cli.thrift.ThriftHttpServlet.isKerberosAuthMode(String)",1,1,1
"org.apache.hive.service.server.HiveServer2.HiveServer2()",1,1,1
"org.apache.hive.service.server.HiveServer2.init(HiveConf)",1,3,4
"org.apache.hive.service.server.HiveServer2.main(String[])",1,4,4
"org.apache.hive.service.server.HiveServer2.start()",1,1,1
"org.apache.hive.service.server.HiveServer2.startHiveServer2()",3,8,8
"org.apache.hive.service.server.HiveServer2.stop()",1,3,3
"org.apache.hive.service.server.ServerOptionsProcessor.ServerOptionsProcessor(String)",1,1,1
"org.apache.hive.service.server.ServerOptionsProcessor.getDebugMessage()",1,1,1
"org.apache.hive.service.server.ServerOptionsProcessor.printUsage()",1,1,1
"org.apache.hive.service.server.ServerOptionsProcessor.process(String[])",2,4,4
"org.apache.hive.service.server.TestHiveServer2Concurrency.QueryRunner.run()",1,1,1
"org.apache.hive.service.server.TestHiveServer2Concurrency.setUp()",1,1,1
"org.apache.hive.service.server.TestHiveServer2Concurrency.setUpBeforeClass()",1,1,1
"org.apache.hive.service.server.TestHiveServer2Concurrency.tearDown()",1,1,1
"org.apache.hive.service.server.TestHiveServer2Concurrency.tearDownAfterClass()",1,4,4
"org.apache.hive.service.server.TestHiveServer2Concurrency.test()",1,1,1
"org.apache.hive.service.server.TestServerOptionsProcessor.test()",1,1,1
"org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup.ThreadFactoryWithGarbageCleanup(String)",1,1,1
"org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup.getThreadRawStoreMap()",1,1,1
"org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup.newThread(Runnable)",1,1,1
"org.apache.hive.service.server.ThreadWithGarbageCleanup.ThreadWithGarbageCleanup(Runnable)",1,1,1
"org.apache.hive.service.server.ThreadWithGarbageCleanup.cacheThreadLocalRawStore()",1,3,3
"org.apache.hive.service.server.ThreadWithGarbageCleanup.cleanRawStore()",1,2,2
"org.apache.hive.service.server.ThreadWithGarbageCleanup.finalize()",1,1,1
"org.apache.hive.testutils.junit.runners.ConcurrentTestRunner.ConcurrentTestRunner(Class<?>)",1,2,2
"org.apache.hive.testutils.junit.runners.ConcurrentTestRunner.ConcurrentTestRunnerThreadFactory.newThread(Runnable)",1,1,1
"org.apache.hive.testutils.junit.runners.model.ConcurrentScheduler.ConcurrentScheduler(ExecutorService)",1,1,1
"org.apache.hive.testutils.junit.runners.model.ConcurrentScheduler.ConcurrentScheduler(ExecutorService,OutputStream)",1,1,1
"org.apache.hive.testutils.junit.runners.model.ConcurrentScheduler.finished()",1,2,2
"org.apache.hive.testutils.junit.runners.model.ConcurrentScheduler.schedule(Runnable)",1,1,1
"org.apache.hive.testutils.junit.runners.model.ConcurrentScheduler.successful(Boolean)",1,1,1
"org.apache.hive.testutils.junit.runners.model.ConcurrentScheduler.writeln(OutputStream,String)",1,1,2

